<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Saliency Detection in 360 • Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Zhang</surname></persName>
							<email>zhangzh@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Saliency Detection in 360 • Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* [0000−0002−4496−1861] , Yanyu Xu * [0000−0001−8926−7833] , Jingyi Yu, Shenghua Gao †[0000−0003−1626−2040]</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>Spherical convolution · Video saliency detection · 360 • VR videos</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. This paper presents a novel spherical convolutional neural network based scheme for saliency detection for 360</p><p>• videos. Specifically, in our spherical convolution neural network definition, kernel is defined on a spherical crown, and the convolution involves the rotation of the kernel along the sphere. Considering that the 360</p><p>• videos are usually stored with equirectangular panorama, we propose to implement the spherical convolution on panorama by stretching and rotating the kernel based on the location of patch to be convolved. Compared with existing spherical convolution, our definition has the parameter sharing property, which would greatly reduce the parameters to be learned. We further take the temporal coherence of the viewing process into consideration, and propose a sequential saliency detection by leveraging a spherical U-Net. To validate our approach, we construct a large-scale 360</p><p>• videos saliency detection benchmark that consists of 104 360</p><p>• videos viewed by 20+ human subjects. Comprehensive experiments validate the effectiveness of our spherical U-net for 360</p><p>• video saliency detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual attention prediction, commonly known as saliency detection, is the task of inferring the objects or regions that attract human's attention in a scene. It is an important way to mimic human's perception and has numerous applications in computer vision <ref type="bibr" target="#b0">[1]</ref> [2] <ref type="bibr" target="#b2">[3]</ref>. By far, almost all existing works focus on image or video saliency detection, where the participants are asked to look at images or videos with a limited field-of-view (FoV). And an eye tracker is adopted to record their eye fixations. The process, however, differs from the natural way human eyes perceive the 3D world: in real world, a participant actually actively explores the environment by rotating the head, seeking an omnidirectional understanding of the scene. In this paper, we propose to mimic this process by exploring the saliency detection problem on 360</p><p>• videos. Despite significant progresses in Convolutional Neural Networks (CNN) <ref type="bibr" target="#b3">[4]</ref> for saliency detection in images/videos <ref type="bibr" target="#b4">[5]</ref>  <ref type="bibr" target="#b5">[6]</ref>, there are very little, if few, studies on • image on sphere; Right: 360</p><p>• image on equirectangular panorama.</p><p>panoramic saliency detection. The brute-force approach of warping the panoramic contents onto perspective views is neither efficient nor robust: partitioning the panoramas into smaller tiles and project the results using local perspective projection can lead to high computational overhead as saliency detection would need to be applied on each tile. Directly applying perspective based saliency detection onto the panorama images is also problematic: panoramic images exhibit geometric distortion where many useful saliency cues are not valid. Some latest approaches attempt to employ tailored convolutional networks for the spherical panoramic data. Yet, they either focus on coping with spherical data with radius components while ignoring distortions caused by equiangular cube-sphere representation <ref type="bibr" target="#b6">[7]</ref>, or dynamically stretching the kernels to fit the local content and therefore cannot achieve kernel parameter sharing <ref type="bibr" target="#b7">[8]</ref>. Actually, when human explores the 360 • contents, our brain uses the same mechanism to detect saliency when changing the view angle or FOV. In other words, if we leverage the CNN for 360</p><p>• video saliency detection, the convolution operation corresponding to different view angle/FOV should maintain the same kernels.</p><p>To better cope the 360 • video saliency detection task, we propose a new type of spherical convolutional neural networks. Specifically, we define the kernel on a spherical crown, and the convolution corresponds to the rotation of kernel on sphere. This definition has the parameter sharing property. Further, considering that the 360</p><p>• videos are usually stored with equirectangular panorama, we propose to extend the spherical convolution to the panorama case by re-sampling the kernel based on the position of the patch to be convolved. We further propose a spherical mean square loss to compensate the distortion effect caused by equirectangular projection. In 360</p><p>• videos, the participants search the environment continuously. This implies that the gaze in the previous frame affects the gaze in the subsequent frames. Then we propose to leverage such temporal coherence for efficient saliency detection by instantiating the spherical convolutional neural networks with a novel spherical U-Net <ref type="bibr" target="#b8">[9]</ref>. Experiments validate the effectiveness of our scheme.</p><p>By far, nearly all saliency detection datasets are based on narrow FoV perspective images while only a few datasets on 360</p><p>• images. To validate our approach, we construct a large-scale 360</p><p>• videos saliency detection benchmark that consists of 104 360</p><p>• videos viewed by 20+ human subjects. The duration of each video ranges from 20 seconds to 60 seconds. We use the aGlass eye tracker to track gaze. <ref type="figure" target="#fig_1">Fig. 2</ref> shows some example frames on several 360</p><p>• videos in our dataset. We compare our spherical con- volutional neural network with several state-of-the-art techniques on this new data and show our technique significantly outperforms prior art in accuracy and robustness. The contributions of this paper are summarized as follows: i) A new type of spherical convolutional neural networks is defined, and the kernels are shared across all patches on the sphere. Thus our definition is more natural and useful for spherical saliency detection. We further extend it to panorama case; ii) We propose a sequential saliency detection scheme and instantiate the spherical convolutional neural networks with a spherical U-net architecture for frame-wise saliency detection; iii) We build a largescale 360</p><p>• video saliency detection dataset which would facilitate the evaluation of saliency detection in 360</p><p>• videos. The dataset and code have been released to facilitate further research on 360</p><p>• video saliency detection 1 .</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Convolution Neural Networks for Spherical Data</head><p>Though CNN has demonstrated their effectiveness for many computer vision tasks <ref type="bibr" target="#b9">[10]</ref> [11], the data fed into traditional CNN are perspective images. To tackle the spherical data, methods in <ref type="bibr" target="#b11">[12]</ref> [13] firstly project a spherical image with equirectangular projection, then standard CNN is applied. However, such equirectangular projects introduce distortion in <ref type="figure" target="#fig_0">Fig. 1</ref>. The patches of the same size on sphere may correspond to regions of different shapes based on their coordinates (θ, φ). So the standard convolution with the shared kernels is no longer perceptually meaningful. To solve this problem, <ref type="bibr" target="#b7">[8]</ref> propose to stretch kernels in standard CNN to fit the shape of the patch on the equirectangular plane in convolution. This can avoid the distortion problem to some extent. But the filters shape in their solution depends on the longitude of the patch on the sphere, consequently, the kernels in their method are not shared, which introduces the expensive computational and storage costs. Further, Boomsma et al. <ref type="bibr" target="#b6">[7]</ref> propose to adopt the equiangular cubed-sphere representation for sphere data representation, then concentric cubed-sphere convolution is applied. But their solution is proposed for sphere data with radii components (like a solid ball), which differs from the data type of ours. Besides, equiangular cubed-sphere representation still introduce distortion in each facet, which damages to the accuracy of convolution. Different from these work, in <ref type="bibr" target="#b13">[14]</ref>[15], the spherical image is repeatedly projected to tangent planes at all locations, and the convolution is conducted on these planes. Although such solution improves accuracy, it also brings expensive computational costs. Further, the disjoint projection planes make the intermediate representation cannot be shared for higher layer convolution. Recently, Cohen et al. <ref type="bibr" target="#b15">[16]</ref> propose a new type of Spherical CNN on SO(3) manifold, and their solution is expressive and rotation-equivariant. With Fast Fourier Transform, their solution can be greatly accelerated. However, the concept of SO(3) CNN is not so adhere to our intuition to process 2D spherical images and quite distincts from the concept of planner CNN.</p><p>Though many CNN models have been proposed for spherical data, none are customized for 360</p><p>• videos. Actually, when we change the FOV in 360</p><p>• videos, our brain actually uses the same mechanism for environment exploration. In other words, the kernels used for saliency detection should be shared across all views. This motivates us to design a new type of spherical CNN: we define kernels with the spherical crown shape, we rotate and convolve the kernel with patches on the sphere-polar coordinate system. <ref type="bibr" target="#b1">2</ref> In this way, the kernel can be shared. So our solution is more natural and more interpretable for saliency detection in 360</p><p>• videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Video Saliency Detection</head><p>Many efforts have been done to study the video saliency detection, either hand-crafted features based methods <ref type="bibr" target="#b16">[17]</ref>  <ref type="bibr" target="#b11">[12]</ref> are two pioneer work along this direction, but the 360</p><p>• data used in these work are static images. Actually, the videos with dynamic contents are more common in real applications. To understand the behavior of human in dynamic 360</p><p>• videos, especially 360</p><p>• sports videos, Hu et al. propose to predict the salient objects by feeding projected panorama images into CNN directly. However, the distortion of projection is not considered, which would reduce the accuracy. In addition, the salient objects are manually labeled, which does not necessarily reflect the real behavior of human visual attention. In order to better understand the users' behavior in VR scenes, we propose to study the eye fixation in 360</p><p>• videos. To the best of our knowledge, this is the first work that works on eye fixation prediction in 360</p><p>• videos. We also build a dataset to facilitate the evaluation of our work. 3 360</p><p>• Dynamic Video Saliency Detection Dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection</head><p>We collect the 360</p><p>• videos from Sports-360 dataset <ref type="bibr" target="#b12">[13]</ref>, and remove the video clips whose length is less than 20 seconds 3 , and use the remaining 104 video clips as the data used for saliency detection in 360</p><p>• videos. The video contents involve five sports (i.e. basketball, parkour, BMX, skateboarding, and dance), and the duration of each video is between 20 and 60 seconds. <ref type="figure" target="#fig_2">Fig. 3 (a)</ref> shows the distribution of five sports videos. Then we display the videos with an HTC VIVE HMD and a '7invensun a-Glass' eye tracker is embedded into the HMD to record the eye fixation points of the participants when they watching videos.</p><p>We recruit 27 volunteers (between 20 and 24 years) to participant in the experiments. All 104 videos are divided into 3 sessions and each session contains 35 360</p><p>• videos. Volunteers are asked to watch 360</p><p>• videos at a fixed starting location (θ = 90, φ = 180) in random orders. We set a shorter break (20sec) between 2 videos and a longer break (3 min) after watching 15 videos. We also calibrate the system after the long break. In the end, each video is watched by at least 20 volunteers. The total time used for data collection is about 2000 minutes. <ref type="figure" target="#fig_2">Fig. 3 (b)</ref> shows the distribution of all eye fixation angle in θ, φ of all participants over all the videos on equirectangular panorama. The peak in the center of panorama (θ = 90, φ = 180) is because all participants explore the environment with the fixed starting location. Further, we can see that the eye fixation points mostly centered around the equator, which means the volunteers tend to explore the environment along the horizontal direction, and they seldom look down/up. There are almost no eye fixation points around north/south pole. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution of Gaze Points</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Spherical Convolutional Neural Networks</head><p>In this section, we introduce our spherical convolution on sphere and its extension on panorama. A new spherical mean square error (sphere MSE) loss is also introduced for spherical convolution on panorama. Note that the convolution operation in deep learning usually refers to correlation in math.</p><p>The spherical convolution Spherical convolution is an operation on feature map f and kernel k on sphere manifold S 2 . S 2 is defined as the set of points x ∈ R 3 with norm 1, which can be parameterized by spherical coordinates θ ∈ [0, π] and φ ∈ [0, 2π] as shown in <ref type="figure" target="#fig_3">Fig. 4 (a)</ref>. To simplify the the notation, here we model spherical images and filters as continuous functions f : S 2 → R K , where K is the number of channels. Then the spherical convolution is formulated as <ref type="bibr" target="#b25">[26]</ref> :</p><formula xml:id="formula_0">[f * k](x) = S 2 f (Rn)k(R −1 x)dR (1)</formula><p>where n is the north pole and R is rotations on sphere represented by 3 × 3 matrices. In this paper, filter k only have non-zero values in spherical crown centered at north pole, whose size is parameterized by r k , which corresponds to orthodromic distance between north pole and borderline of the crown as shown in <ref type="figure" target="#fig_3">Fig. 4 (b)</ref>. So the radius r k controls the number of parameters in kernel k and the size of local receptive field. Larger r k means there are more parameters in k and the local receptive field is larger.</p><p>Convolution on equirectangular panorama The spherical images or videos are usually stored as 2-D panorama through equirectangular projection represented by (θ,φ) (θ ∈ [0, π] and φ ∈ [0, 2π]). So we extend equation <ref type="formula">(1)</ref> to the convolution between feature map f and kernel k on projected panorama as</p><formula xml:id="formula_1">[f * k](θ, φ) = f (θ ′ , φ ′ )k(θ ′ − θ, φ ′ − φ) sin θ ′ dθ ′ dφ ′<label>(2)</label></formula><p>There are several differences between spherical convolution on equirectangular and convolution on perspective image. Firstly, we denote the points set for the kernel centered at (θ 0 , φ 0 ) as D k (θ 0 , φ 0 ) = {(θ, φ)|g(θ, φ, θ 0 , φ 0 ) ≤ 0}, where g(θ, φ, θ 0 , φ 0 ) ≤ 0 corresponds to the equation of sphere crown (kernel) centered at (θ 0 , φ 0 ). The behavior of D k is different from that in standard convolution for perspective image. Specifically, when we move the kernel and when its center is (θ 0 + ∆ θ , φ 0 + ∆ φ ), the points set for the moved kernel cannot be directly obtained by simply moving the D k (θ 0 , φ 0 ) by (∆ θ , ∆ φ ), which can be mathematically written as follows:</p><formula xml:id="formula_2">D k (θ 0 + ∆ θ , φ 0 + ∆ φ ) = {(θ, φ)|g(θ − ∆ θ , φ − ∆ φ , θ 0 , φ 0 ) ≤ 0} = {(θ + ∆ θ , φ + ∆ φ )|g(θ, φ, θ 0 , φ 0 ) ≤ 0}<label>(3)</label></formula><p>Secondly, there is a sin θ ′ term in the integrand in spherical convolution in equation <ref type="formula" target="#formula_1">(2)</ref>, which causes the different behavior of spherical convolution compared with the convolution for projective images. Thirdly, there does not exist padding in spherical convolution in equation <ref type="formula">(1)</ref> owing to the omnibearing view of 360</p><p>• images. But, it indeed needs padding in convolution on equirectangular panorama in equation <ref type="formula" target="#formula_1">(2)</ref>, owing to the storage format. The padding needed here is circle shift. For example, when kernel locates at the far left region, it needs to borrow some pixels from far right region for computing convolution. To simplify the notation, we also term the convolution on equirectangular panorama as spherical convolution. In this way, we can implement convolution on sphere by using convolution on equirectangular panorama.</p><p>We define sample rate on equirectangular panorama as the number of pixels per rad. So sample rate of panorama is sr θ = H/π, sr φ = W/2π along θ and φ direction, respectively. Here H, W is the height and width of panorama. As a special case, for the kernel with radius equalling r k , when the kernel is centered at north pole, its projection on equirectangular panorama would be a rectangular, whose size is denoted as W k ×H k , and the sample rate along θ and φ direction are given by sr</p><formula xml:id="formula_3">θ k = H k /r k , sr φ k = W k /2π.</formula><p>Implement Details For discrete spherical convolution on panorama, we set the kernel parameters to be learnt as the equirectangular projection of kernel centered at the north pole (θ ≤ r k ). So the kernel projected on equirectangular panorama corresponds to a rectangular of size W k × H k . It is worth noting we can also set the kernels at other positions rather than north pole, yet sample grid of the kernel will change accordingly. The discrete spherical convolution on equirectangular panorama includes the following steps: determine non-zero region of kernel centered at (θ i , φ i ) and obtain the set of points fallen into the kernel area D k (θ i , φ i ), rotate those points back to D k (0, 0), resample the original kernel to find values for each sampling points in D k (θ i , φ i ).</p><p>Determine the points fallen into D k (θ i , φ i ). For the spherical crown kernel with radius r k centered at (θ i , φ i ), the points fallen into this kernel area with coordinates (θ, φ) satisfy the following equation:</p><formula xml:id="formula_4">sin θ i cos φ i sin θ cos φ + sin θ i sin φ i sin θ sin φ + cos θ i cos θ = cos θ k<label>(4)</label></formula><p>which can be simplified as sin(φ + ψ) = C</p><p>where Algorithm 1 Obtain the set of grid points on the kernel area Input: kernel radius r k , kernel location θ k , φ k . Output: The set of grid points on the kernel area S 1: S ← ∅ 2: Calculate range of Θ:</p><formula xml:id="formula_6">sin ψ = a √ a 2 +b 2 , cos ψ = b √ a 2 +b 2 , C = d−c √</formula><formula xml:id="formula_7">3: Θ ∈ [max(0, θ k − r k ), min(θ k + r k , π)] 4:</formula><note type="other">Calculate range of Φ for each θ ∈ Θ: 5: for Each θ ∈ Θ do 6: Find the solution of equation Eq. 5 7: if There exists infinitely many solutions then 8: φ ∈ [0, π] 9: else if There exists two solutions φ1 &lt; φ2 then 10: φ ∈ [φ1, φ2] 11: else if There exists no solutions then 12:</note><p>No grid points (θ, φ) on the kernel area 13:</p><p>end if 14:</p><p>Add (θ, φ) to S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15: end for</head><p>Once the kernel area on sphere is determined, we can sample the corresponding points on panorama to obtain the points needed for convolution. We list the main steps for such stage in Algorithm 1.</p><p>Rotate the set of sampled points back to north pole. Now that we have the sampled points of current kernel area, we also need to determine the correspondences between them to those kernel values centered at the north pole. To do this, we rotate these points back to original kernel region by leveraging matrix multiplication between their cartesian coordinate representations and rotation matrix along Y as well as Z axis. Note that after rotation, the sampled points might lie between original kernel points centered at the north pole.</p><p>Re-sample the original kernel In order to obtain kernel values for sampled points that lie between original kernel points centered at the north pole, we use grid sampling technique as used in Spatial Transform Network <ref type="bibr" target="#b26">[27]</ref>, which is basically a general interpolation method for such re-sampling problem on 2D image. The third row in <ref type="figure" target="#fig_5">Fig. 5</ref> shows the sampling grid corresponding to kernel located at θ = 0, π/4, π/2, 3π/4, π and φ = π.</p><p>Finally, the result of spherical convolution is given by sum of element-wise multiplication between re-sampled kernel points and corresponding panorama points, divided by the total number of re-sampled kernel points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Properties of spherical convolution</head><p>The sphere convolution has the following three properties: sparse interactions, parameter sharing, and equivariant representations.</p><p>-Sparse interactions. Standard CNNs usually have sparse interactions, which is accomplished by making the kernel smaller than the input. Our proposed spherical CNN also have this important property. Such sparse connection greatly reduces the number of parameters to be learned. Further, the convolution in higher layers corresponds to gradually larger local receptive field, which allows the network to efficiently model the complicated interactions between input and output. -Parameter sharing. Similar to the standard convolution for perspective image, the parameters of spherical crown kernel is the same everywhere on the sphere, which means the kernel is shared. This would greatly reduce the storage requirements of the model as well as the number of parameters to be learned. Kernel at different location is re-sampled as shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. -Equivariant representations. In both standard convolutions for perspective image and spherical convolution, the parameter sharing makes the layers with equivariance to translation property, which means if the input changes, the output changes in the same way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Spherical Mean Squared Error (MSE) Loss</head><p>Mean Squared Error (MSE) loss function is widely used in perspective image based CNN. However, the standard MSE is designed for perspective image. For perspective image, discretization is performed homogeneously in space, which differs from the case of panorama. To adopt MSE loss to panorama, we weight the square error for each pixel on panorama with it's solid angle on sphere before average them. We define the solid angle in steradian equals the area of a segment of a unit sphere in the same way a planar angle in radians equals the length of an arc of a unit circle, which is the following ratio: Ω = A/r 2 , where A is the spherical surface area and r is the  radius of the considered sphere. Thus, for a unit sphere, the solid angle of the spherical crown with radius r and centered at the north pole is given as: Ω = 2π(1 − cos r). It is desirable that the image patch corresponding to the same solid angle would have the same weight for sphere MSE, thus we arrive at the following objective function:</p><formula xml:id="formula_8">L = 1 n n k=1 Θ,Φ θ=0,φ=0 w θ,φ (S (k) θ,φ −Ŝ (k) θ,φ ) 2<label>(6)</label></formula><p>whereŜ (k) and S (k) are the ground truth saliency map and predicted saliency map for the k th image, and w θ,φ is weights for each points which is proportional to its solid angle and w i,j ∝ Ω(θ, φ). Ω(θ, φ) is solid angle corresponding to the sampled area on saliency map located at (θ, φ). In our implementation, we just set w θ,φ = Ω(θ, φ)/4π (4π is the solid angle of the unit ball).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Spherical U-Net Based 360</head><p>• Video Saliency Detection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Problem Formulation</head><p>Given a sequence of frames V = {v 1 , v 2 , . . . , v T }, our goal is to predict the saliency map corresponding to this video S = {s 2 , s 2 , s 3 , . . . , s T }. So deep learning based 360</p><p>• video saliency detection aims at learning a mapping G that maps input V to S. However, different from perspective videos whose saliency merely depends on the video contents, where a participant look at in 360 • videos also depends on the starting position of the participant. We define s 0 as the eye fixation map at the starting position, which is the saliency map corresponding to the starting point, then the video saliency detection can be formulated as</p><formula xml:id="formula_9">G * = arg min F S − G(V, s 0 ) 2 F<label>(7)</label></formula><p>In practice, we can initialize s 0 with a Gaussian kernel centered at the staring position. Further, the participants actually watch the 360</p><p>• video in a frame-by-frame manner, and Saliency Detection in 360</p><p>• Videos 11 the eye fixation in previous frame helps the prediction of that in next frame. So we can encode such prior into the objective and arrive at the following objective :</p><formula xml:id="formula_10">F * = arg min F T t=1 s t − F (v t , s t−1 ) 2<label>(8)</label></formula><p>Here F is the prediction function which takes the saliency map of previous frame and video frame at current moment as input for saliency prediction of current moment. Inspired by the success of U-Net <ref type="bibr" target="#b8">[9]</ref> we propose to adapted it with a Spherical U-Net as F for the frame-wise saliency detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Spherical U-Net</head><p>The network architecture of Spherical U-Net is listed in <ref type="table" target="#tab_2">Table 1</ref>. The input to the network is projected spherical images v t at time t and project spherical saliency map s t−1 at time t − 1. Similar to U-net <ref type="bibr" target="#b8">[9]</ref>, our spherical U-net also consists of a contracting path (left side) and an expansive path (right side). In the contracting path, there are three spherical convolutional layers followed by rectified linear unit (ReLU) and a 2x2 spherical max pooling is adopted to down-sample the data. The expansive path consists of three spherical convolutional layers followed by ReLU and up-sampling. For the last three spherical convolutional layer, their inputs are concatenation of the outputs of their previous layer and the corresponding layer with the same output size from the contracting path. A spherical convolution ins final layer is used to map each feature vector to a saliency map. In total the network has 7 spherical convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>We implement our spherical U-Net with the PyTorch framework. We train our network with the following hyper-parameters setting: mini-batch size (32), learning rate (3e-1), momentum (0.9), weight decay (0.00005), and number of iterations (4000).</p><p>Datasets. We evaluate our proposed spherical CNN model on both Salient360! dtaset and our video saliency dataset. Salient360! <ref type="bibr" target="#b27">[28]</ref> is comprised of 98 different 360</p><p>• images viewed by 63 observers. Our video saliency dataset consists of 104 360</p><p>• videos viewed by 20 observers. For Salient360, we follow the standard training/testing split provided in <ref type="bibr" target="#b27">[28]</ref>. For the experiments on our dataset, 80 videos are randomly selected as training data, and the remaining 24 videos are used for testing. All baseline methods are compared based on the same training/testing split. For image saliency, we regress the saliency maps directly from the RGB 360</p><p>• images.</p><p>Metrics. We create the ground truth saliency maps through a way similar to spherical convolution using a crown Gaussian kernel with sigma equaling to 3.34</p><p>• . Owing to the distortion during projection, it does not make sense to directly compare two panorama saliency maps like typical 2D saliency maps. Therefore, we utilize the metrics including CC, AUC-judd, and NSS introduced in <ref type="bibr" target="#b27">[28]</ref> to measure the errors between the predicted saliency maps and the ground truth.</p><p>LDS <ref type="bibr" target="#b28">[29]</ref>   <ref type="table">Table 2</ref>. The performance comparison of state-of-the-art methods with our spherical U-Net on our video saliency dataset.</p><p>Baselines. We compare our proposed spherical U-Net with the following state-of-theart: image saliency detection methods, including LDS <ref type="bibr" target="#b28">[29]</ref>, Sal-Net <ref type="bibr" target="#b4">[5]</ref> and SALICON <ref type="bibr" target="#b29">[30]</ref>, video saliency detection methods, including GBVS <ref type="bibr" target="#b30">[31]</ref> and a more recent dynamic saliency <ref type="bibr" target="#b31">[32]</ref> and the 360</p><p>• image saliency models <ref type="bibr" target="#b32">[33]</ref>. Of all these methods, Sal-Net and SALICON are deep learning based methods, and we retrain the models with panorama images on the datasets used in our paper for performance comparison. We also design the following baselines to validate the effectiveness of our spherical model.</p><p>-Standard U-Net. Compared with our spherical U-Net method, the CNN and MSE loss in such baseline is conventional CNN and standard MSE loss. -Spherical U-Net w.o. sal. Compared with our spherical U-Net method, the only difference is that the saliency of previous frame is not considered for the saliency prediction of current frame.</p><p>In addition, we measure human performance by following the strategies in <ref type="bibr" target="#b33">[34]</ref>:</p><p>-Baseline-one human: It measures the difference between the saliency map viewed by one observer and the average saliency map viewed by the rest observers. -Baseline-infinite humans: It measures the difference between the average saliency map viewed by a subset of viewers and the average saliency map viewed by the remaining observers.</p><p>Recent work has employed several top-down cues for saliency detection. Previous work <ref type="bibr" target="#b34">[35]</ref> shows that human face boosts saliency detection. Therefore, we also design a baseline Top-down cue (face) to use human face as cue, and post-process saliency map following <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance Evaluation</head><p>We compare our method with all baselines in <ref type="table">Table 2</ref>. It shows that our method outperforms all baseline methods on our video saliency dataset, which validates the effectiveness of our scheme for 360</p><p>• video saliency detection. In order to show the rotation equivariant in θ and φ direction, we rotate 60</p><p>• , 120</p><p>• along φ direction and 30</p><p>• , 60</p><p>• along θ direction on test data, and do data augmentation by rotating random degree on both direction on training set. The results are shown in <ref type="figure" target="#fig_7">Fig. 7</ref>. We can see that compared to rotation in φ direction, that in θ direction slightly changes because of the change of sample grid when rotating along θ direction.</p><p>To evaluate the performance of our method on the Salient360! dataset, we have modified our model to directly predict the saliency map for a static 360</p><p>• image. In 6, we can see that our method outperforms all baselines, which validates the effectiveness of our method for static 360</p><p>• image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Evaluation of Different Components in Spherical U-Net</head><p>We conduct ablation study by replacing the spherical CNN with standard CNN (Ours w. standard conv) and replacing spherical MSE with standard MSE (Ours w. standard MSE), respectively. The performance of these baselines is listed in table in <ref type="figure" target="#fig_6">Fig. 6</ref>. We can see that both spherical CNN and Spherical MSE contributes the performance. We also evaluate spherical U-Net with different spherical kernel sizes and the comparison with spherical U-Net with smaller kernel sizes than our spherical U-Net (Ours w. smaller kernel) is shown in table in <ref type="figure" target="#fig_6">Fig. 6</ref>. We can see that larger kernel leads to better performance. One possible reason is that a larger spherical kernel could involve more parameters, which could increase the capability of our network. Another reason is that larger kernel increases kernel sample rate, which might improve the accuracy when re-sample the kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Spherical pooling.</head><p>We do comparison between planner pooling and spherical pooling in the table in <ref type="figure" target="#fig_6">Fig. 6</ref>. In this paper, spherical pooling could be regarded as a special spherical convolution, similar to the relationship between planner ones. The spherical pooling outperforms planner pooling, responsible for consistency between receptive field of kernel with spherical feature maps. To note that, <ref type="bibr" target="#b15">[16]</ref> also uses planner (3D) pooling to downsample feature maps. Since planner pooling achieves similar performance as spherical pooling and has lower computational cost, following <ref type="bibr" target="#b15">[16]</ref>, currently we use planner pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Saliency Prediction for a Longer Time</head><p>Middle and right figures in <ref type="figure" target="#fig_6">Fig. 6</ref> show the results when our model predicts saliency maps for a longer time based on CC, NSS, and AUC-judd metric. We can see that the performance of saliency prediction degenerates for as time elapse. One possible reason is that as time goes longer, the prediction of previous frame becomes less accurate, which consequently would affect the saliency detection of current frame. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Time and Memory Costs</head><p>Our model is trained on four NVIDIA Tesla P40 GPUs. We calculate the average running time for each image batch. The average running time of our model is 5.1 s/iter. The spherical U-Net listed in <ref type="table" target="#tab_2">Table 1</ref> has about 6.07 M parameters, and it consumes 21 × 4 GB of memory when batch size is 32 when training. It takes about 36 hours to train the model on the our video saliency dataset (the total number of iterations is 4000.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Discussions</head><p>Our work attempts to exploit the saliency detection in dynamic 360</p><p>• videos. To this end, we introduce a new type of spherical CNN where the kernels are shared across all image patches on the sphere. Considering that the 360</p><p>• videos are stored with panorama, we extent spherical CNN to the panorama case, and we propose to re-sample kernel based on its location for spherical convolution on panorama. Then we propose a spherical UNet for 360</p><p>• video saliency detection. We also build a large-scale 360</p><p>• video saliency dataset for performance evaluation. Extensive experiments validate the effectiveness of our proposed method. It is worth noting our spherical CNN is a general framework, it can also be applied to other tasks involving 360</p><p>• video/image.</p><p>There still exists some space to improve our method for video saliency prediction. Currently, to simplify the problem, we only consider the saliency map of the previous frame for the prediction of current frame. Considering the saliency map over a longer time range may boost the performance, for example, we can also combine our spherical U-Net with LSTM. The combination of spherical CNN with other types of deep neural network is beyond the study scope of this paper, and we will leave them for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgement</head><p>This project is supported by the NSFC (No. 61502304).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Distortion introduced by equirectangular projection. Left: 360 • image on sphere; Right: 360 • image on equirectangular panorama.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The examples of five domains in our 360 • video dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Dataset Analysis: (a) the distribution of the five sports domains based on the number of videos; (b) the distribution of eye fixations on equirectangular panorama. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) Spherical coordinate system: φ is the angle between X axis and the orthogonal projection of the line on the XOY plane, and θ is the angle between the Z axis and the line; (b) Spherical crown kernel: the red line represents the radius r. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>a 2 +b 2 and a = sin θ i cos φ i sin θ, b = sin θ i sin φ i sin θ, c = cos θ i cos θ and d = cos θ k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Parameter sharing. This figure indicates how spherical crown kernel changes on sphere and projected panorama from north pole to south pole with angle interval equalling π/4. The first raw is the region of the spherical crown kernel in sphere. The second raw shows the region of spherical crown kernel in the projected panorama. The third row shows sampling grid corresponding to each kernel location. Red curve represents θ sampling grid and blue curve represents φ sampling grid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The First:The performance comparison of state-of-the-art methods with our spherical CN-N on Salient360[36] dataset. The Second: The performance comparison of different components on our method on our video saliency dataset. The Third and The Forth: the performance of saliency map prediction for a longer time based on CC, AUC-judd, and NSS metrics respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The rotation equivariant in θ and φ direction: The first and third columns are rotated frames and the second and forth columns are our predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 .</head><label>1</label><figDesc>The architecture of the CNN.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">GitHub: https://github.com/xuyanyu-shh/Saliency-detection-in-360-video</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Since the sphere image is usually stored with planar format, we actually project spherical image on the Euclidean plane with equirectangular projection, then we re-sample kernel based on the shape of the projected patch to be convolved, after that we convolved the target patch with the transformed kernel.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We only use videos longer than 20 seconds rather than entire Sports-360 dataset because in [12] Sitzmann et al. evaluate the exploration time for a given still scene, and show that 'on average, users fully explored each scene after about 19 seconds'.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic foveation for video compression using a neurobiological model of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1304" to="1318" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic image retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th international conference on Mobile and ubiquitous multimedia</title>
		<meeting>the 4th international conference on Mobile and ubiquitous multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic information positioning scheme in arassisted maintenance based on visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y C</forename><surname>Nee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Augmented Reality</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="453" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Shallow and deep convolutional networks for saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giroinieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Oconnor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="598" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Recurrent mixture density network for spatiotemporal visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08199</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spherical convolutions and their application in molecular modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Boomsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frellsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3436" to="3446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning spherical convolution for fast features from 360 imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computerassisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Saliency in vr: How do people explore virtual environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pavel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep 360 pilot: Learning a deep agent for piloting through 360deg sports video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Making 360 video watchable in 2d: Learning videography for click free viewing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pano2vid: Automatic cinematography for watching 360 • videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="154" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>arX- iv:1801.10130</idno>
		<title level="m">Spherical cnns</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video saliency detection via dynamic consistent spatio-temporal attention modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: AAAI</title>
		<imprint>
			<biblScope unit="page" from="1063" to="1069" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Time-mapping using space-time saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bing Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3358" to="3365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Realistic avatar eye and head animation using a neurobiological model of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dhavale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pighin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications and Science of Neural Networks, Fuzzy Systems, and Evolutionary Computation VI</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5200</biblScope>
			<biblScope unit="page" from="64" to="79" />
		</imprint>
	</monogr>
	<note>International Society for Optics and Photonics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Regularized feature reconstruction for spatiotemporal saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3120" to="3132" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for dynamic saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04730</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video salient object detection via fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="49" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep learning for saliency prediction in natural video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaabouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benois-Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predicting salient face in multipleface videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4420" to="4428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A review of eye gaze in virtual agents, social robotics and hci: Behaviour generation, user interaction and perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ruhland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andrist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Badler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">I</forename><surname>Badler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="299" to="326" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Computing fourier transforms and convolutions on the 2-sphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Driscoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Healy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in applied mathematics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="202" to="250" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Spatial transformer networks. In: Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A dataset of head and eye movements for 360 degree images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gutiérrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Le Callet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM on Multimedia Systems Conference</title>
		<meeting>the 8th ACM on Multimedia Systems Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="205" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning discriminative subspaces on random contrasts for image saliency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1095" to="1108" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Revisiting video saliency: A large-scale benchmark and a new model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Saltinet: Scan-path prediction on 360 degree images using saliency volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Assens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giroinieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A benchmark of computational models of saliency to predict human fixations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT Technical Report</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Context-aware saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goferman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnikmanor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1915</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A dataset of head and eye movements for 360 degree images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Callet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM on Multimedia Systems Conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="205" to="210" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
