<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepMVS: Learning Multi-view Stereopsis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Han</forename><surname>Huang</surname></persName>
							<email>phuang17@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Matzen</surname></persName>
							<email>matzen@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
							<email>jkopf@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
							<email>n-ahuja@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
							<email>jbhuang@vt.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Virginia Tech</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DeepMVS: Learning Multi-view Stereopsis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="formula">b)</ref> <p>we produce a set of plane-sweep volumes for a reference view and feed these into a convolutional neural network that predicts a disparity map; (c) our network produces high-quality disparity maps even for challenging cases containing poorly textured regions and thin structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-view Stereo (MVS) methods aim at reconstructing disparity maps from a collection of images with known camera poses and calibration, possibly estimated using Structure from Motion (SFM) algorithms. <ref type="bibr" target="#b0">1</ref> MVS is one of the fundamental computer vision problems that have seen decades of research and it is a core component in numerous important applications, including 3D reconstruction, novel view synthesis, augmented reality, and medical imaging <ref type="bibr" target="#b8">[9]</ref>.</p><p>Conventional MVS algorithms often estimate the disparity map by computing plane-sweep volumes and optimizing photometric consistency with handcrafted error functions to measure similarity between patches <ref type="bibr" target="#b8">[9]</ref>. Aside from photometric consistency, other 3D cues such as lighting <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b23">24]</ref>, shadows <ref type="bibr" target="#b0">[1]</ref>, color <ref type="bibr" target="#b10">[11]</ref>, geometric structures <ref type="bibr" target="#b6">[7]</ref>, and semantic cues <ref type="bibr" target="#b13">[14]</ref> have been incorporated into the MVS pipeline for improving the reconstruction accuracy. However, designing algorithms that make explicit use of all these cues is a non-trivial task. Despite extensive research, the results of state-of-the-art MVS algorithms often still contain numerous artifacts, in particular around poorly textured regions, thin structures, and reflective or transparent surfaces <ref type="bibr" target="#b8">[9]</ref>.</p><p>Deep Convolutional Neural Networks (ConvNets) have shown great success in many visual recognition tasks including image classification <ref type="bibr" target="#b22">[23]</ref> and object detection <ref type="bibr" target="#b11">[12]</ref>, as well as in dense pixel-level prediction tasks such as semantic segmentation <ref type="bibr" target="#b24">[25]</ref> and optical flow <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>For the use of ConvNets in visual reconstruction problems, early work focuses on learning patch similarity for stereo matching <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b25">26]</ref>. More recent work performs stereo reconstruction using end-to-end learning. However, these methods either impose constraints on relative camera poses <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref> or the number of input images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">37]</ref>, or produce a coarse volumetric reconstruction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>In this paper, we present DeepMVS, a deep ConvNet for multi-view stereo that addresses these limitations. Given a reference image and an arbitrary number of neighbor views of the scene, we first perform a standard SFM reconstruction to recover the camera calibration and pose for each image. We then produce a disparity map for the reference image in three stages, as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. First, we generate a plane-sweep volume for each neighbor image that contains the warped neighbor colors at every disparity, and let our network extract features from each patch pair (reference patch vs. patch in plane-sweep volume). Second, we use an encoder-decoder architecture with skip connections to aggregate the features across large spatial regions. We incorporate the feature activations from a VGG-Net <ref type="bibr" target="#b34">[35]</ref> pretrained on ImageNet to guide our decoder for disparity predictions. Third, we fuse the information extracted by each neighbor image with a max-pooling layer and produce the final disparity prediction. In contrast to Recurrent Networkbased approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>, the use of max-pooling allows us to process an arbitrary number of unordered input images Training deep ConvNets for disparity reconstruction requires a large number of ground truth disparity maps. A solution is to train the network on the combination of a largescale synthetic dataset and a smaller real-world dataset <ref type="bibr" target="#b26">[27]</ref>. Synthetic datasets provide dense pixel-wise ground truth labels for training, but they do not reflect the complexity of realistic photometric effects, illumination, and natural image noise. On the other hand, real-world datasets are limited in scale and often do not have labels for regions in which it is difficult to obtain ground-truth data, such as sky and reflective surfaces. To address this issue, we introduce the MVS-SYNTH dataset -a set of 120 photorealistic sequences of synthetic urban scenes for learning-based MVS algorithms. We show that the use of a photorealistic synthetic dataset greatly improves the quality of disparity prediction.</p><p>We validate the effectiveness of DeepMVS on the recently introduced ETH3D benchmark dataset <ref type="bibr" target="#b33">[34]</ref>. Our results show that DeepMVS outperforms DeMoN <ref type="bibr" target="#b36">[37]</ref> in the setting of multi-view stereo, and achieves competitive performance with COLMAP <ref type="bibr" target="#b32">[33]</ref>, the state-of-the-art among conventional MVS algorithms. In particular, we observe that our network is often able to produce correct disparities in poorly textured regions, such as sky, walls, floors, and desk surfaces, where conventional algorithms fail.</p><p>In summary, we make the following contributions:</p><p>• We propose DeepMVS, a novel learning-based method for multi-view stereo.</p><p>• Unlike existing work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b18">19]</ref>, DeepMVS can process an arbitrary number of input images. The disparity estimation result is invariant to the order in which the inputs are processed.</p><p>• Through extensive evaluation, we show that the incorporation of semantic features, training on photorealistic synthetic MVS-SYNTH dataset, and encoderdecoder architecture for aggregating features over large areas all contribute to the improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Multi-view stereo reconstruction. Conventional MVS algorithms focus on designing neighbor selection algorithms and photometric error measures. Recent advances include robust neighbor view selection <ref type="bibr" target="#b23">[24]</ref>, incorporation of visibility consistency <ref type="bibr" target="#b9">[10]</ref>, and clustering-based techniques for efficient reconstruction <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b7">8]</ref>. Recently, Schönberger et al. present a MVS system -COLMAP <ref type="bibr" target="#b32">[33]</ref> -that jointly estimates depth and surface normal, leverages photometric and geometric priors for pixelwise view selection, and uses geometric consistency for simultaneous refinement. Through a tight integration of multiple techniques, COLMAP performs among the best algorithms in several public multi-view stereo benchmarks. We refer readers to <ref type="bibr" target="#b8">[9]</ref> for a comprehensive overview of multi-view stereo reconstruction algorithms. While impressive results have been shown, conventional MVS algorithms rely heavily on photometric consistency and often have difficulty in handling poorly textured and reflective surfaces where photometric consistency is unreliable. In addition, these algorithms do not consider other visual cues for depth perceptions such as lighting, shadows, and semantics (e.g., a building has a planar structure). Incorporating such information through hand-crafted objective functions is non-trivial. In this work, we aim at implicitly leveraging these cues through learning from data.</p><p>Learning-based MVS. A line of work focuses on learning a good similarity measure for patch matching across two views <ref type="bibr" target="#b41">[42]</ref> and multiple views <ref type="bibr" target="#b14">[15]</ref> using ConvNets. With the learned stereo matching cost, these methods produce disparity maps by a series of post-processing steps. In contrast, DeepMVS produces disparity maps directly from a set of posed images.</p><p>Another line of recent work uses ConvNets that take a plane-sweep volume as input and produce disparity maps (or synthesizes novel view) for the reference images. However, these approaches assume a fixed number of input images <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b36">37]</ref>. Our proposed DeepMVS can take an arbitrary number of images to produce high-quality disparity maps. Several recent works approach multi-view recon-struction with volumetric methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>. These methods take a sequence of images captured from different views and generate a 3D shape of the object using a voxel occupancy grid. Nevertheless, the dimension of the voxel grid is quite constrained by the available GPU memory (e.g., coarse grids of 32×32×32 voxels). It is unclear how the volumetric algorithms can be generalized and applied to high-resolution stereo reconstruction in the real-world.</p><p>Learning from simulation. Synthetic datasets alleviate the difficulty and the cost of collecting large-scale training datasets from the real world. Examples of synthetic datasets for training and evaluating computer vision algorithms include indoor scene understanding <ref type="bibr" target="#b42">[43]</ref>, semantic segmentation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b28">29]</ref>, and depth and flow estimation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref>. We also found that training with a synthetic dataset improves performance in our context. Our newly collected MVS-SYNTH dataset complements the missing ground truth depth measurements in the real-world such as sky and reflective surfaces like windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning Multi-view Stereopsis</head><p>The entire pipeline of our algorithm can be broken into four steps. We first preprocess the input image sequence (Section 3.1), and then generate plane-sweep volumes (Section 3.2). Next, our network estimates disparity maps from the plane-sweep volumes (Section 3.3), followed by final refinement to improve the results (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Input</head><p>The input to our algorithm is a sequence of images and their camera poses and calibration (if necessary, we use the SFM algorithm in COLMAP <ref type="bibr" target="#b31">[32]</ref> to estimate them). One of the input images is designated as the reference image, for which we seek to obtain a disparity map.</p><p>We start by selecting a subset of neighbor images for the reference to be used in the stereopsis using a similar approach to COLMAP <ref type="bibr" target="#b32">[33]</ref>. The images which share the most common features with the reference are chosen to be neighbor images. However, unlike COLMAP, we do not discard the neighbor images which have small triangulation angles with the reference, and we do not estimate per-image weights, since we intend to train the network so it automatically determines whether a plane-sweep volume is reliable or not by comparing it with the reference image.</p><p>We also estimate the disparity range of the reference image. Following the approach as COLMAP, we estimate the maximum disparity by projecting all the features in the sparse reconstruction model to the reference view and computing the disparities of the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Plane-sweep Volume Generation</head><p>For each neighbor image we compute a plane-sweep volume with respect to the reference image as follows. We assume that the scene geometry is an infinite plane, frontoparallel to the reference view, and at specific disparities: {0, δ , 2δ , . . . , (D − 1)δ }. The disparity step, δ , is chosen such that (D − 1)δ equals to the estimated maximum disparity of the reference image. We warp the neighbor image accordingly and store the result as a layer in the volume. If any of the assumed disparity is correct and that portion of the scene is not occluded in the neighbor image, we expect that the warped neighbor image matches the reference image well.</p><p>By doing this with all the neighbor images, we obtain a stack of plane-sweep volumes with N × D images, which we denote as</p><formula xml:id="formula_0">V = V n,d : 0 ≤ n &lt; N, 0 ≤ d &lt; D .</formula><p>We normalize the RGB values to the range [−0.5, 0.5] and fill the parts in the plane-sweep volumes that are not visible to the corresponding neighbor image with zeros.</p><p>The number of disparity levels, D, is predetermined. Increasing D allows us to use a smaller disparity step δ to reduce the quantization errors in the results, but also increases the number of parameters in the network and thus the GPU memory. As a compromise, we choose disparity level D = 100. <ref type="figure" target="#fig_0">Figure 2</ref> and <ref type="figure" target="#fig_1">Figure 3</ref> illustrate the architecture of Deep-MVS with the hyper-parameters. Our network can be broken into three parts: 1) the patch matching network, 2) the intra-volume feature aggregation network, and 3) the intervolume feature aggregation network. Except for the very last layer of the network, all the convolutional layers in the network are followed by a Scaled Exponential Linear Unit (SELU) layer <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Architecture</head><p>Patch matching. The goal of our patch matching network is to extract a set of per-pixel features that can better aid in the comparison of patches than hand-crafted photometric descriptors could do alone. The patch matching network takes a patch from the reference image I R and a single patch V n,d from the plane-sweep volume that corresponds to the n-th neighbor image at d-th disparity level as input. The first convolutional layer extracts 64-channel features from the two patches. The features are then concatenated and passed through three more convolutional layers before turning into 4-channel patch matching features. We repeat this process for all N × D plane-swept images.</p><p>Intra-volume feature aggregation. For each neighbor image, we concatenate the 4-channel patch matching features of all D disparity levels to form a 4 × D-channel vol-  ume. Following that is a U-Net structure composed of an encoder, a decoder, and skip connections. Each level of the encoder is formed by a stride-2 convolutional layer followed by an ordinary convolutional layer; each level of the decoder is formed by two convolutional layers followed by a bilinear upsampling layer. We show in <ref type="figure" target="#fig_1">Figure 3</ref> the detailed structures and hyper-parameters of the proposed intra-volume feature aggregation network.</p><p>In addition, we add semantic features at each level of the decoder. We pass the reference image into the VGG-19 <ref type="bibr" target="#b34">[35]</ref> network pre-trained on ImageNet, and take the layers conv1 2, conv2 2, conv3 2, conv4 2, and conv5 2 as semantic features. These semantic features are first multiplied by 0.01 and passed through a convolutional layer so as to reduce dimensionality and to improve numerical stability. Finally, these feature maps are concatenated to each level of the decoder as shown in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>This part of the network is intended to pass the features to larger spatial regions and enable the network to make predictions with non-local information. It also aids the disparity predictions using the VGG feature inputs. The output of the intra-volume feature aggregation network is an 800-channel volume F n containing the disparity prediction information gathered from the n-th neighbor image.</p><p>Inter-volume feature aggregation. In this step, we take the N volumes, {F 0 , . . . , F N−1 }, generated from each of the neighbor images and aggregate them using element-wise max-pooling. The use of max-pooling enables the network to gather information from an arbitrary number of neighbor images, and also ensures that the results are invariant with respect to the order of the neighbor images. This technique was previously used in PointNet <ref type="bibr" target="#b27">[28]</ref> and in the work by Hartmann et al. in <ref type="bibr" target="#b14">[15]</ref> to allow inputs with varying sizes. Finally, we use two convolutional layers converting the aggregated volume into the pixel-wise disparity predictions.</p><p>During training, we randomly select the number of neighbor images N from {1, 2, 3, 4}. By varying N, the network learns to make use of the max-pooling to collect only the useful information from each neighbor image. Even though N is restricted to be no larger than 4 during training (due to the limited size of the GPU memory), we show that our trained network can be applied to an arbitrary number of neighbor images in Section 4.4.</p><p>Training loss. We pose disparity prediction as a multiclass classification problem, and use the cross-entropy loss to train the network. The predicted disparity map can be made by taking the disparity level at which the predicted probability is the highest for each pixel. Namely, for the output distribution y = (y 0 , . . . , y D−1 ) of each pixel, the predicted disparity can be chosen bŷ</p><formula xml:id="formula_1">d raw = argmax d y d .</formula><p>We refer to thisd raw as the raw predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Refinement</head><p>To further improve the quality of the results, we apply the Fully-Connected Conditional Random Field (Dense-CRF) <ref type="bibr" target="#b21">[22]</ref> to our raw disparity predictions. The use of DenseCRF encourages the pixels which are spatially close and with similar colors to have closer disparity predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>DeMoN datasets. We train our network with the same datasets as used in DeMoN <ref type="bibr" target="#b36">[37]</ref>. The dataset consists of short sequences ranging from two to tens of images including real-world datasets (SUN3D <ref type="bibr" target="#b39">[40]</ref>, RGB-D SLAM <ref type="bibr" target="#b35">[36]</ref>, CITYWALL and ACHTECK-TURM <ref type="bibr" target="#b5">[6]</ref>) of outdoor and indoor scenes and a synthesized dataset (SCENES11 <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b1">2]</ref>) with random objects flying in the air. As suggested in <ref type="bibr" target="#b36">[37]</ref>, mixing real-world and synthetic datasets is important since each has its own limitations. The ground truth for real-world datasets contains measurement errors, whereas synthesized datasets have unrealistic appearance, and may not be capable of reflecting some characteristics of real imagery, such as illumination, depth of field, and noise. The image resolution of this dataset is 640 × 480 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MVS-Synth dataset.</head><p>To address the limitations of the DeMoN datasets, we introduce the MVS-SYNTH dataset, which consists of 120 sequences of urban scenes captured in the video game Grand Theft Auto V. <ref type="bibr" target="#b1">2</ref> Each sequence is composed of 100 RGB frames of size 1920×1080, ground truth disparity maps, and the extrinsic and intrinsic camera parameters. <ref type="figure" target="#fig_2">Figure 4</ref> shows examples from the MVS-SYNTH dataset.</p><p>Compared to existing synthetic datasets, the MVS-SYNTH dataset is more realistic in terms of context and shading. Compared to real-world datasets, MVS-SYNTH provides complete ground truth disparities which cover regions such as the sky, reflective surfaces, and thin structures, whose ground truths are usually missing in real-world datasets. Therefore, training with MVS-SYNTH allows us to predict disparities for these challenging regions. We train the network using both image resolution 1280×720 and 960×540 pixels as data augmentation.</p><p>ETH3D datasets. For evaluation, we use the high-res multi-view dataset in the recently introduced ETH3D benchmark datasets <ref type="bibr" target="#b33">[34]</ref>. It consists of 13 sequences of realworld outdoor and indoor scenes with ground truth point clouds captured by laser scanners. We project the point clouds back to each view to obtain a ground truth disparity map for each reference image. Note that ground truth data are not complete and contain holes in the sky, reflective surfaces, and thin objects. Nevertheless, we use it to validate the efficacy of our method for real-world scenes. We resize the images to 810×540 pixels for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Our training process consists of two stages. First, we train the network by replacing the intra-volume feature aggregation network with two simple 3×3 convolutional layers. Here, our goal in the first stage is to pre-train the network so it can be transferred to the second stage. Then, we add the intra-volume feature aggregation network back with weights initialized from the pre-trained network, and train the entire network using both DeMoN and the MVS-SYNTH datasets.</p><p>For both training stages, we use the Adam solver <ref type="bibr" target="#b19">[20]</ref> with learning rates 10 −5 and 10 −6 , respectively, for 320k iterations per stage. We apply gradient clipping to prevent gradient explosion by constraining the L2-norm of the gradients at each layer to be no more than 1.0 at the first stage and 0.1 at the second stage. We implement the network in PyTorch. Training the network with an NVIDIA P100 GPU with 16GB memory takes two days for each stage.</p><p>We use 64px×64px patches as our inputs so as to fit our network into the GPU memory at the training stages. We generate the semantic features by a feed-forward pass of a VGG-19 network using the entire image. We then take only the region of interest corresponding to the input patches from the intermediate features. At test time, we feed 128px×128px patches into the network, and take only the center 64px×64px of the output to reduce boundary artifacts. The 64px×64px output patches are then tiled to achieve full-resolution results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Metrics</head><p>Geometric errors. We compute geometric error by taking the L1 distance between the predicted disparity and the ground truth. Unavailable pixels are ignored.</p><p>Photometric errors. We also measure photometric rephotography error <ref type="bibr" target="#b37">[38]</ref> -the L1 distance between the reference and the rephotography image. We generate the rephotography using the predicted disparity map, warping the pixels to all other neighbor images, sampling colors from the neighbor images, and finally selecting the median among all color candidates for each pixel.</p><p>Completeness. Another important factor for evaluation is completeness. We measure completeness using the percentage of pixels whose errors are below a certain threshold. Plotting the relationship between different error thresholds and their corresponding completeness helps visualize the distributions of the errors. The curves lying in the lower right represent more pixels having lower errors and thus have better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation</head><p>COLMAP. Several conventional MVS algorithms have been proposed, including PMVS <ref type="bibr" target="#b9">[10]</ref>, MVE <ref type="bibr" target="#b5">[6]</ref>, and COLMAP <ref type="bibr" target="#b32">[33]</ref>. We choose to compare with COLMAP as it is the top performer on the ETH3D dataset <ref type="bibr" target="#b33">[34]</ref>.</p><p>We follow the default settings of COLMAP unless otherwise mentioned. COLMAP provides an option to filter out the predictions that are not geometrically consistent. However, the filtered disparity maps may significantly reduce completeness. We show both unfiltered and filtered maps for comparison.</p><p>Note that we do not use DenseCRF to refine COLMAP's noisy unfiltered maps since COLMAP predicts a deterministic disparity for each pixel, whereas DenseCRF requires pixel-wise distributions as inputs.</p><p>DeMoN. We compare our approach with DeMoN <ref type="bibr" target="#b36">[37]</ref> because it is the closest to ours among the existing learningbased stereopsis methods. However, as their network only works with image pairs, we propose two ways to extend their approach to multi-view stereo applications.</p><p>The first method is to choose the best result among all the disparity maps generated from the image pairs formed by the reference image and its neighbor images. This method is not practical in real applications since the ground truths are not available. Nevertheless, the method establishes the upper-bound performance of DeMoN. The second method is to compute the per-pixel median among all the generated disparity maps so as to aggregate information from all available image pairs. Since DeMoN is trained with images taken with fixed focal lengths and image resolutions, we crop and resize the images from ETH3D dataset before using them to evaluate DeMoN's performance. This leads to the incomplete reconstruction results in <ref type="figure">Figure 5</ref> and <ref type="figure">Figure 6</ref>. The cropped regions are ignored when the error is computed. In addition, DeMoN assumes that the translation between the input image pair is a unit vector. Therefore, we multiply the depth maps produced by DeMoN by the actual translational distance between the two views before comparing them with the ground truths.</p><p>Qualitative comparisons. <ref type="figure">Figure 5</ref> shows qualitative comparisons between DeMoN, COLMAP, and our approach. While DeMoN detects the overall structure of the scene, it fails to predict accurate scaling factors and thus results in inaccurate predictions. On the other hand, COLMAP and our approach give accurate predictions wherever the depth cues are sufficient. However, for textureless regions like the sky, the wall, and the surface of the white desks, the predictions made by COLMAP are very noisy, whereas our network is capable of assigning zero disparity to the sky, and interpolating or extrapolating disparities for poorly textured regions. <ref type="figure">Figure 6</ref> shows several rephotography results. The results from DeMoN are often blurry and distorted, indicating that the predictions are not accurate. COLMAP performs well in rephotography in the regions where the predictions are clean. However, for challenging regions, the results contain large holes. Our rephotography results generally recover the reference images with only small holes. However, edges appear to be jagged because of the disparity quantization in our approach.</p><p>Quantitative comparisons. <ref type="table" target="#tab_0">Table 1</ref> shows quantitative comparisons of the average errors over the entire ETH3D dataset between DeMoN, COLMAP, and our approach. First, DeMoN gives much larger errors than COLMAP and our approach with respect to both metrics. COLMAP's filtered predictions have significantly lower average errors, but it discards 29% of the pixels to achieve that. Finally, COLMAP's unfiltered maps and our results have similar errors. While COLMAP gives slightly lower photometric errors, our approach gives slightly lower geometric errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth</head><p>DeMoN <ref type="bibr" target="#b36">[37]</ref> (best) DeMoN <ref type="bibr" target="#b36">[37]</ref> (median)</p><p>COLMAP <ref type="bibr" target="#b32">[33]</ref> (filtered)</p><p>COLMAP <ref type="bibr" target="#b32">[33]</ref> (unfiltered)</p><p>Our result <ref type="figure">Figure 5</ref>. Qualitative comparisons between different algorithms on ETH3D dataset. <ref type="figure">Figure 7</ref> shows the distributions of the errors. We observe that COLMAP predicts 85% of the pixels with smaller geometric errors than our approach, whereas our approach gives more accurate results for the other 15% pixels. A possible reason is that for regions with sufficient depths cues, COLMAP produces accurate predictions. Our approach, on the other hand, suffers from the quantized disparity effects. However, for the challenging regions, COLMAP gives noisy predictions which lead to large errors, whereas our approach produces plausible predictions. As for the distributions of the photometric errors, our approach produces almost the same curve as COLMAP does.</p><p>Progressive improvement. <ref type="figure">Figure 8</ref> shows two examples of the progressive improvements by COLMAP and our approach for an increasing number of input images. When N is small, COLMAP tends to produce large geometric errors, whereas our network can still generate accurate predictions </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Studies</head><p>DenseCRF. As shown in <ref type="figure">Figure 9</ref>, applying DenseCRF removes a large portion of the noisy patches in lowconfidence regions such as the reflective wall, and encourages the disparity predictions to follow the color edges. As shown in <ref type="table" target="#tab_1">Table 2</ref>, DenseCRF improves the results with respect to both error metrics.</p><p>DeMoN <ref type="bibr" target="#b36">[37]</ref> (best)</p><p>COLMAP <ref type="bibr" target="#b32">[33]</ref> (unfiltered)</p><p>Our result <ref type="figure">Figure 6</ref>. Comparisons of rephotography results. See <ref type="figure">Figure 5</ref> for the ground truth reference images.</p><p>Geometric errors Photometric errors MVS-Synth dataset. <ref type="table" target="#tab_1">Table 2</ref> shows that removing MVS-SYNTH dataset from the training set results in slightly larger errors for both metrics. Qualitatively, we observe that the network trained without MVS-SYNTH dataset works very poorly for the sky and reflective surfaces, as <ref type="figure">Figure 10</ref> shows. These regions usually lack ground truth data, so Image Ours Ours w/o DenseCRF <ref type="figure">Figure 9</ref>. Example of the improvements from the DenseCRF refinement. Applying DenseCRF removes the noisy predictions.</p><p>Image Ours Ours w/o MVS-SYNTH <ref type="figure">Figure 10</ref>. Comparisons between networks trained with and without the MVS-SYNTH dataset. Without MVS-SYNTH dataset, the network has difficulty in handling regions such as the sky because real-world datasets do not cover these regions.</p><p>the errors do not reflect much on the quantitative errors. We suggest that the poor predictions result from the fact that the ground truths in DeMoN dataset does not cover such regions.</p><p>U-Net and VGG features. As <ref type="table" target="#tab_1">Table 2</ref> shows, adding the U-net and VGG features each provides improvements in both error metrics. This shows that allowing non-local information and providing semantic features both help the network in better disparity predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Limitations</head><p>Following are some limitations of our network. First, the quantization of disparity results in undesired geometric and photometric errors. Second, our network often fails to predict correct disparities for vegetation areas containing trees or grass. Finally, the computation speed of our algorithm is constrained by the time-consuming generation of the planesweep volumes and the deep and large network structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>With DeepMVS, we demonstrate the feasibility of learning Mulit-View Stereopsis with a convolutinoal neural network, and show that learning-based approaches can overcome the weaknesses of conventional algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. DeepMVS network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Network architecture of the intra-volume feature aggregation network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Samples from the proposed MVS-SYNTH dataset, which provides photorealistic images with ground truth disparities even for the sky, reflective surfaces, and thin structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Figure 7. The distributions of the errors of different approaches on ETH3D dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparisons between different algorithms on ETH3D dataset.</figDesc><table>Algorithm 
Completeness Geo. error Pho. error 

DeMoN (best) 
100% 
0.045 
0.288 
DeMoN (median) 
100% 
0.201 
0.367 
COLMAP (filtered) 
71% 
0.007 
0.178 
COLMAP (unfiltered) 
100% 
0.046 
0.218 
Ours 
100% 
0.036 
0.224 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Contributions of different components in our algorithm.</figDesc><table>Components 
Geo. error Pho. error 

Pretraining 
0.051 
0.242 
+ U-net 
0.043 
0.230 
+ U-net + VGG 
0.040 
0.226 
+ U-net + VGG + DenseCRF 
0.036 
0.224 
+ U-net + VGG + DenseCRF − MVS-SYNTH 
0.037 
0.225 

and hallucinate disparities for the regions lacking of good 
depth cues. 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Throughout this work, we always refer to "disparities" rather than "depths". Disparities are defined as the reciprocal of depths.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This academic article may contain images and/or data from sources that are not affiliated with the article submitter. Inclusion should not be construed as approval, endorsement or sponsorship of the submitter, article or its content by any such party.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used in this research and the grant from Office of Naval Research N00014-16-1-2314.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shadowcuts: Photometric stereo with shadows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazırbaş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deepstereo: Learning to predict new views from the world&apos;s imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Neulander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mve-a multiview reconstruction environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fuhrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Langguth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Workshop on Graphics and Cultural Heritage</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Manhattan-world stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards internet-scale multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multi-view stereo: A tutorial. Foundations and Trends R in Computer Graphics and Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hernández</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Accurate, dense, and robust multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Piecewise planar and non-planar stereo for urban scene reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-view stereo for community photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dense semantic 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Häne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1730" to="1743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learned multi-patch similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Havlena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning-based view synthesis for light field cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Kalantari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning a multi-view stereo machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Häne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02515</idno>
		<title level="m">Self-normalizing neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<editor>J. ShaweTaylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Shading-aware multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Langguth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient deep learning for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Playing for benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The SYNTHIA Dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pixelwise view selection for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A multi-view stereo benchmark with high-resolution images and multicamera videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schöps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A benchmark for the evaluation of rgb-d slam systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Engelhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Intelligent Robot Systems (IROS)</title>
		<meeting>of the International Conference on Intelligent Robot Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Demon: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Virtual rephotography: Novel view prediction error for 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Waechter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fuhrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Moehrle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Highquality shape from multi-view stereo and shading under general illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wilburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SUN3D: A database of big spaces reconstructed using sfm and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Physically-based rendering for indoor scene understanding using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
