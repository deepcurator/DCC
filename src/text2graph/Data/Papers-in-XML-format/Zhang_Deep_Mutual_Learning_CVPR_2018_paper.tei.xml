<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Mutual Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
							<email>t.xiang@qmul.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
							<email>t.hospedales@ed.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Mutual Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks achieve state of the art performance on many problems, but are often very large in depth and/or width, and contain large numbers of parameters <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28]</ref>. This has the drawback that they may be slow to execute or large to store, limiting their use in applications or platforms with low memory or fast execution requirements, e.g., mobile phones. This has led to a rapid growth of research in developing smaller and faster models. Achieving compact yet accurate models has been approached in a variety of ways including explicit frugal architecture design <ref type="bibr" target="#b8">[9]</ref>, model compression <ref type="bibr" target="#b21">[22]</ref>, pruning <ref type="bibr" target="#b13">[14]</ref>, binarisation <ref type="bibr" target="#b17">[18]</ref> and most interestingly model distillation <ref type="bibr" target="#b7">[8]</ref>.</p><p>Distillation-based model compression relates to the observation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1]</ref> that small networks often have the same representation capacity as large networks; but compared to large networks they are simply harder to train and find the right parameters that realise the desired function. That is, the limitation seems to lie in the difficulty of optimisation rather than in the network size <ref type="bibr" target="#b0">[1]</ref>. To better learn a small network, the distillation approach starts with a powerful (deep and/or wide) teacher network (or network ensemble), and then trains a smaller student network to mimic the teacher <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b2">3]</ref>. Mimicking the teacher's class probabilities <ref type="bibr" target="#b7">[8]</ref> and/or feature representation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19]</ref> conveys additional information beyond the conventional supervised learning target. The optimisation problem of learning to mimic the teacher turns out to be easier than learning the target function directly, and the student can match or even outperform <ref type="bibr" target="#b18">[19]</ref> the much larger teacher.</p><p>In this paper we aim to solve the same problem of learning small but powerful deep neural networks. However, we explore a different but related idea to model distillation -that of mutual learning. Distillation starts with a powerful large and pre-trained teacher network and performs one-way knowledge transfer to a small untrained student. In contrast, in mutual learning we start with a pool of untrained students who simultaneously learn to solve the task together. Specifically, each student is trained with two losses: a conventional supervised learning loss, and a mimicry loss that aligns each student's class posterior with the class probabilities of other students. Trained in this way, it turns out that each student in such a peer-teaching based scenario learns significantly better than when learning alone in a conventional supervised learning scenario. Moreover mutually learned student networks achieve better results than students trained by conventional distillation from a larger pre-trained teacher. Furthermore, while the conventional understanding of distillation requires a teacher larger and more powerful than the intended student, it turns out that in many cases mutual learning of several large networks also improves performance compared to independent learning. This makes the deep mutual learning strategy generally applicable, e.g., it can also be used in application scenarios where there is no constraint on the model size and the recognition accuracy is the only concern.</p><p>It is perhaps not obvious why the proposed learning strategy should work at all. Where does the additional knowledge come from, when the learning process starts out with all small and untrained student networks? Why does it converge to a good solution rather than being hamstrung by groupthink as 'the blind lead the blind'. Some intuition about these questions can be gained by considering the following: Each student is primarily directed by a conventional supervised learning loss, which means that their performance generally increases and they cannot drift arbitrarily into groupthink as a cohort. With supervised learning, all networks soon predict the same (true) labels for each training instance; but since each network starts from a different initial condition, they learn different representations, and consequently their estimates of the probabilities of the next most likely classes vary. It is these secondary quantities that provide the extra information in distillation <ref type="bibr" target="#b7">[8]</ref> as well as mutual learning. In mutual learning the student cohort effectively pools their collective estimate of the next most likely classes. Finding out -and matching -the other most likely classes for each training instance according to their peers increases each student's posterior entropy <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>, which helps them to converge to a more robust (flatter) minima with better generalisation to testing data. This is related to very recent work on the robustness of high posterior entropy solutions (network parameter settings) in deep learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>, but with a more informed choice of alternatives than blind entropy regularisation.</p><p>Overall, mutual learning provides a simple but effective way to improve the generalisation ability of a network by training collaboratively with a cohort of other networks. Extensive experiments are carried out on both object category recognition (image classification on CIFAR100 <ref type="bibr" target="#b11">[12]</ref>) and instance recognition problems (person re-identiciation on Market1501 <ref type="bibr" target="#b32">[33]</ref>). The results show that, compared with distillation by a pre-trained static large network, collaborative learning by small peers achieves better performance. In particular, on the person re-identification task, state-of-theart results can be obtained using a much smaller network trained with mutual learning, compared to the latest competitors. Furthermore we observe that: (i) it applies to a variety of network architectures, and to heterogeneous cohorts consisting of mixed big and small networks; (ii) The efficacy increases with the number of networks in the cohort -a nice property to have because by training on small networks only, more of them can fit on given GPU resources for more effective mutual learning; (iii) it also benefits semisupervised learning with the mimicry loss activated both on labelled and unlabelled data. Finally, we note that while our focus is on obtaining a single effective network, the entire cohort can also be used as a highly effective ensemble model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Model Distillation The distillation-based approach to model compression has been proposed over a decade ago <ref type="bibr" target="#b2">[3]</ref> but was recently re-popularised by <ref type="bibr" target="#b7">[8]</ref>, where some additional intuition about why it works -due to the additional supervision and regularisation of the higher entropy soft-targets -was presented. Initially, a common application was to distill the function approximated by a powerful model/ensemble teacher into a single neural network student <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>. But later, the idea has been applied to distill powerful and easy-to-train large networks into small but harder-to-train networks <ref type="bibr" target="#b18">[19]</ref> that can even outperform their teacher. Recently, distillation has been connected more systematically to information learning theory <ref type="bibr" target="#b14">[15]</ref> and SVM+ <ref type="bibr" target="#b24">[25]</ref> -an intelligent teacher provides privileged information to the student. This idea of using model distillation for learning with privileged information has been exploited by Zhang et al. <ref type="bibr" target="#b28">[29]</ref> for action recognision: the more expensive optical flow field is treated as privileged information and an optical flow CNN is used to teach a motion vector CNN. In terms of representation of the knowledge to be distilled from the teacher, existing models typically use teacher's class probabilities <ref type="bibr" target="#b7">[8]</ref> and/or feature representation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19]</ref>. Recently, Yim et al. <ref type="bibr" target="#b26">[27]</ref> exploited flow between layers computed as the inner product of feature maps between layers. In contrast to model distillation, we address dispensing with the teacher altogether, and allowing an ensemble of students to teach each other in mutual distillation. Collaborative Learning Other related ideas on collaborative learning include Dual Learning <ref type="bibr" target="#b5">[6]</ref> where two crosslingual translation models teach each other interactively. But this only applies in this special translation problem where an unconditional within-language model is available to be used to evaluate the quality of the predictions, and ultimately provides the supervision that drives the learning process. Furthermore, in dual learning different models have different learning tasks whilst in mutual learning the tasks are identical. Recently, Cooperative Learning <ref type="bibr" target="#b1">[2]</ref> has been proposed to learn multiple models jointly for the same task but in different domains. E.g. recognising the same set of object categories but with one model inputting RGB images and the other inputting depth images. The models communicate via object attributes which are domain invariant. Again this is different from mutual learning where all models address the same task and domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Mutual Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation</head><p>We formulate the proposed deep mutual learning (DML) approach with a cohort of two networks (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Extension to more networks is straightforward (see <ref type="bibr">Sec. 3.3)</ref>. x i given by a neural network Θ 1 is computed as</p><formula xml:id="formula_0">Given N samples X = {x i } N i=1 from M classes,</formula><formula xml:id="formula_1">p m 1 (x i ) = exp(z m 1 ) M m=1 exp(z m 1 ) ,<label>(1)</label></formula><p>where the logit z m is the output of the "softmax" layer in</p><formula xml:id="formula_2">Θ 1 .</formula><p>For multi-class classification, the objective function to train the network Θ 1 is defined as the cross entropy error between the predicted values and the correct labels,</p><formula xml:id="formula_3">L C1 = − N i=1 M m=1 I(y i , m) log(p m 1 (x i )),<label>(2)</label></formula><p>with an indicator function I defined as</p><formula xml:id="formula_4">I(y i , m) = 1 y i = m 0 y i = m<label>(3)</label></formula><p>The conventional supervised loss trains the network to predict the correct labels for the training instances. To improve the generalisation performance of Θ 1 on the testing instances, we use another peer network Θ 2 to provide training experience in the form of its posterior probability p 2 . To quantify the match of the two network's predictions p 1 and p 2 , we use the Kullback Leibler (KL) Divergence.</p><p>The KL distance from p 1 to p 2 is computed as</p><formula xml:id="formula_5">D KL (p 2 p 1 ) = N i=1 M m=1 p m 2 (x i ) log p m 2 (x i ) p m 1 (x i ) . (4)</formula><p>The overall loss functions L Θ1 and L Θ2 for networks Θ 1 and Θ 2 respectively are thus:</p><formula xml:id="formula_6">L Θ1 = L C1 + D KL (p 2 p 1 ). (5) L Θ2 = L C2 + D KL (p 1 p 2 ).<label>(6)</label></formula><p>In this way each network learns both to correctly predict the true label of training instances (supervised loss L C ) as well as to match the probability estimate of its peer (KL mimicry loss).</p><p>Our KL divergence based mimicry loss is asymmetric, thus different for the two networks. One can instead use a symmetric Jensen-Shannon Divergence loss:</p><formula xml:id="formula_7">1 2 (D KL (p 1 p 2 ) + D KL (p 1 p 2 )).<label>(7)</label></formula><p>However, we found empirically that whether a symmetric or asymmetric KL loss is used does not make any difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Optimisation</head><p>A key difference between model distillation and DML is that in DML, the two models are optimised jointly and collaboratively, with the optimisation processes for the two models being closely intervened. The mutual learning strategy is embedded in each mini-batch based model update step for both models and throughout the whole training process. The models are learned with the same mini-batches. At each iteration, we compute the predictions of the two models and update both networks' parameters according to the predictions of the other. The optimisation of Θ 1 and Θ 2 is conducted iteratively until convergence. The optimisation details are summarised in Algorithm 1. It consists of 4 sequential steps if running on a single GPU. When two GPUs are available, distributed training can be implemented by running Steps 1, 2 on one GPU and Steps 3,4 on another in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Extension to Larger Student Cohorts</head><p>The proposed DML approach naturally extends to more networks in the student cohort.</p><p>Given K networks</p><formula xml:id="formula_8">Θ 1 , Θ 2 , ..., Θ K (K ≥ 2), the objective function for opti- mising Θ k , (1 ≤ k ≤ K) becomes L Θ k = L C k + 1 K − 1 K l=1,l =k D KL (p l p k ).<label>(10)</label></formula><p>Equation <ref type="formula" target="#formula_1">(10)</ref> indicates that with K networks, DML for each student effectively takes the other K − 1 networks in the cohort as teachers to provide mimicry targets. Equation (6) is now a special case of (10) with K = 2. Note that </p><formula xml:id="formula_9">Θ 1 ← Θ 1 + γ t ∂L Θ1 ∂Θ 1<label>(8)</label></formula><p>2: Update the predictions p 1 of x by (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>Compute the stochastic gradient and update Θ 2 :</p><formula xml:id="formula_10">Θ 2 ← Θ 2 + γ t ∂L Θ2 ∂Θ 2<label>(9)</label></formula><p>4: Update the predictions p 2 of x by (1). Until : convergence we have added the coefficient 1 K−1 to make sure that the training is mainly directed by supervised learning of the true labels. The optimisation for DML with more than two networks is a straightforward extension of Algorithm 1. This learning strategy naturally suits distributed learning <ref type="bibr" target="#b20">[21]</ref>: It can be distributed by learning each network on one device and passing the small probability vectors between devices.</p><p>With more than two networks, an interesting alternative learning strategy for DML is to take the ensemble of all the other K − 1 networks as a single teacher to provide a combined mimicry target, which would be very similar to the distillation approach but performed at each mini-batch model update. Then the objective function of Θ k can be written as</p><formula xml:id="formula_11">L Θ k = L C k +D KL (p avg p k ), p avg = 1 K − 1 K l=1,l =k p l . (11)</formula><p>In our experiments (see Sec. 4.8), we find that this DML strategy with a single ensemble teacher (denoted DML e) leads to worse performance than DML with K − 1 teachers. This is because the model averaging step (Equation <ref type="formula" target="#formula_1">(11)</ref>) to build the teacher ensemble makes the teacher's posterior probabilities more peaked at the true class, thus reducing the posterior entropy over all classes. It is therefore contradictory to one of the objectives of DML which is to produce robust solutions with high posterior entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Extension to Semi-supervised Learning</head><p>The proposed DML extends straightforwardly to semisupervised learning. Under the semi-supervised learning setting, we only activate the cross-entropy loss for labelled data, while computing the KL distance based mimicry loss for all the training data. This is because the KL distance computation does not require class labels, so unlabelled data can also be used. Denote the labelled and unlabelled data as L and U , where we have X = L ∪ U , the objective function for learning network Θ 1 can be reformulated as</p><formula xml:id="formula_12">L Θ1 = L C1 x∈L + D KL x∈X (p 2 p 1 ).<label>(12)</label></formula><p>4. Experiments  <ref type="table">Table 2</ref>. Top-1 accuracy (%) on the CIFAR-10 and CIFAR-100 dataset. "DML-Ind" measures the difference in accuracy between the network learned with DML and the same network learned independently.</p><p>age padded by 4 pixels on each side, filling missing pixels with reflections of original image. For Market-1501, we use the Adam optimiser <ref type="bibr" target="#b10">[11]</ref>, with learning rate lr = 0.0002, β 1 = 0.5, β 2 = 0.999 and a mini-batch size of 16. For ImageNet, we use RMSProp with decay of 0.9, mini-batch size of 64, and initial learning rate of 0.1. The learning rate decayed every 20 epochs using an exponential rate of 0.16.   <ref type="table">(Table 1)</ref>, it still benefits from being trained together with a smaller peer. (iv) Training a cohort of large networks (WRN-28-10) is still beneficial compared to learning them independently. Thus in contrast to the conventional wisdom of model distillation, we see that a large pre-trained teacher does not necessary bring large benefits, and multiple large networks can still benefit from our distillation-like process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on CIFAR-100</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on Market-1501</head><p>In this experiment, we use MobileNet in a two-network DML cohort. <ref type="table">Table 3</ref> summarises the mAP (%) and rank-1 accuracy (%) of Market-1501 of MobileNets trained with/without DML, as well as the comparison against existing state of the art methods. We can see that on this more challenging instance recognition problem, DML greatly improves the performance of MobileNet compared to independent learning, both with and without pre-training on ImageNet. It can also be seen that our DML approach using two MobileNets significantly outperforms prior state-of-the-art deep re-id methods. This is noteworthy as MobileNet is  <ref type="table">Table 3</ref>. Comparative results on the Market-1501 dataset. Each MobileNet is trained in a two-network cohort and the averaged performance of the two networks in the cohort is reported. 'pre?' indicates whether ImageNet pretraining was carried out. a simple, small, and general-purpose network. In contrast many recently proposed deep re-id networks such as those in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34]</ref> have complicated and specially designed architectures to handle the drastic pose-changes and body-part mis-alignment when matching people across camera views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on ImageNet</head><p>Figure 2 (a) compares MobileNet and InceptionV1 accuracy on ImageNet with Independent and DML training. We can see that the DML variants of both architectures consistently performs better than their independently trained counterparts. These results show that DML is applicable to large-scale problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Distributed Training of DML</head><p>To investigate the impact of training strategy on DML, we compared two DML variants: 1) sequential: train two networks according to Algorithm 1 on one GPU; two networks are updated one after the other; 2) distributed: each network is trained on a separate GPU and CPU is used for KL divergence communication; in this way, the predictions and parameters of two networks are updated simultaneously. We experiment on Market-1501 with <ref type="bibr" target="#b1">2</ref>   and show the convergence and mAP results in <ref type="figure" target="#fig_2">Fig. 2 (b)</ref>. It is interesting to observe that our DML's performance is further boosted by distributed training. Comparing these two variants, the two networks are more 'equal' when trained distributed as they always have exactly the same number of training iterations. This result thus suggests that the students benefit the most from the DML peer-teaching when the discrepancy in their learning progress is minimised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Comparison with Model Distillation</head><p>As our method is closely related to model distillation, we next provide a focused comparison to Distillation <ref type="bibr" target="#b7">[8]</ref>. Table 4 compares our DML with model distillation where the teacher network (Net 1) is pre-trained and provides fixed posterior targets for the student network (Net 2). As expected the conventional distillation approach from a powerful pre-trained teacher does indeed improve the student performance compared to independently learning the student (1 distills 2 versus Net 2 Independent).</p><p>However, the results show that training both networks together in deep mutual learning provides a clear improvement compared to distillation (1 distills 2 versus DML Net 2). This implies that in the process of mutual learning, the network that would play the role of teacher actually becomes better than a pre-trained teacher, via learning from interactions with an a-priori untrained student.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">DML with Larger Student Cohorts</head><p>The prior experiments studied cohorts of 2 students. We next investigate how DML scales with more students in the cohort. <ref type="figure" target="#fig_3">Figure 3(a)</ref> shows the results on Market-1501 with DML training of increasing cohort sizes of MobileNets. The figure shows average mAP, as well as the standard deviation. From <ref type="figure" target="#fig_3">Fig. 3(a)</ref> we can see that the performance of the average single network increases with the number of networks in the DML cohort, hence its gap to the independently trained networks. This demonstrates that the generalisation ability of students is enhanced when learning together with increasing numbers of peers. The performance of different networks is also more consistent with larger cohort size, indicated by the smaller standard deviations.</p><p>A common technique when training multiple networks is to use them as an ensemble and make a combined prediction. In <ref type="figure" target="#fig_3">Fig. 3(b)</ref> we use the same models as <ref type="figure" target="#fig_3">Fig. 3(a)</ref> but make predictions based on the ensemble (matching based on concatenated feature of all members) instead of reporting the average prediction of each individual. From the results we can see that the ensemble prediction outperforms individual network predictions as expected <ref type="figure" target="#fig_3">(Fig. 3(b) vs. (a)</ref>). Moreover, the ensemble performance also benefits from training multiple networks as a DML cohort <ref type="figure" target="#fig_3">(Fig. 3(b)</ref> DML ensemble vs. Independent ensemble).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">How and Why does DML Work?</head><p>In this section we attempt to give some insights about how and why our deep mutual learning strategy works. There has been a wave of recent research on the subject of "Why Deep Nets Generalise" <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b9">10]</ref>, which have provided some insights such as: While there are often many solutions (deep network parameter settings) that generate zero training error, some of these generalise better than oth- ers due to being in wide valleys rather than narrow crevices <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref> -so that small perturbations do not change the prediction efficacy drastically; and that deep networks are better than might be expected at finding these good solutions <ref type="bibr" target="#b29">[30]</ref>, but that the tendency towards finding robust minima can be enhanced by biasing deep nets towards solutions with higher posterior entropy <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>. DML Leads to Better Quality Solutions with More Robust Minima With these insights in mind we make some observations about the DML process. Firstly we note that in our experiments, the networks typically fit the training data perfectly: Training accuracy goes to 100% and classification loss becomes minimal (e.g., <ref type="figure">Fig. 4(a)</ref>). However, as we saw earlier, DML performs better on test data. Therefore rather than helping to find a better (deeper) minimum of training loss, DML appears to be helping us to find a wider/more robust minimum that generalises better to test data. Inspired by <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref>, we perform a simple test to analyse the robustness of the discovered minima on CIFAR-100 using MobileNet. For the DML and independent models, we compare the training loss of the learned models before and after adding independent Gaussian noise with variable standard deviation σ to each model parameter. We see that the depths of the two minima were the same <ref type="figure">(Fig. 4(a)</ref>), but after adding this perturbation the training loss of the independent model jumps up while the loss of the DML model increases much less. This suggests that the DML model has found a much wider minimum, which is expected to provide better generalisation performance <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>. How is a Better Minimum Found? When asking each network to match its peer's probability estimates, mismatches where a given network predicts zero and its teacher/peer predicts non-zero are heavily penalised. Therefore the overall effect of DML is that, where each network independently would put a small mass on a small set of secondary probabilities, all networks in the DML tend to aggregate their prediction of secondary probabilities, and both (i) put more mass on the secondary probabilities altogether, and (ii) place non-zero mass on more distinct secondary probabilities. We illustrate this effect by comparing the probabilities assigned to the top-5 highest ranked classes obtained by a ResNet-32 on CIFAR-100 trained by DML vs. an independently trained ResNet-32 model in <ref type="figure">Fig. 4(c)</ref>.</p><p>For each training sample, the top 5 classes are ranked according to the posterior probabilities produced by the model (Class 1 being the true class and Class 2 the second most probable class, etc). Here we can see that the assignment of mass to probabilities below the Top-1 decays more quickly for Independent than DML. This can be quantified by the entropy values averaged over all training samples of the DML trained model and the independently trained model, which are 1.7099 and 0.2602 respectively. Thus our method has connection to entropy regularisation-based approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref> to finding wide minima, but by mutual probability matching on 'reasonable' alternatives, rather than a blind high-entropy preference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DML with Ensemble Teacher</head><p>In DML, each student is taught by all other students in the cohort individually, regardless how many students are in the cohort (Eq. <ref type="formula" target="#formula_1">(10)</ref>). In Sec. 3.3, an alternative DML strategy (DML e) is discussed, by which each student is asked to match the predictions of the ensemble of all other students in the cohort (Eq. <ref type="formula" target="#formula_1">(11)</ref>). One might reasonably expect this approach to be better: As the ensemble prediction is better than individual predictions, it should provide a cleaner and stronger teach- ing signal -more like conventional distillation. In practice the results of ensemble rather than peer teaching are worse (see <ref type="figure" target="#fig_3">Fig. 3</ref>). By analysing the teaching signal of the ensemble in comparison to peer teaching, the ensemble target is much more sharply peaked at the true label than the peer targets, resulting in larger prediction entropy value for DML (0.2805) than DML e (0.1562). Thus while the noiseaveraging property of ensembling is effective for making a correct prediction, it is actually detrimental to providing a teaching signal where the secondary class probabilities are the salient cue in the signal and having high-entropy posterior leads to more robust solutions to model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.">Does DML Makes Models More Similar?</head><p>We know that with the same training objective, the predicted class posterior would be similar for different models in a DML cohort. The question is do these model also produce similar features, especially when the models have identical architecture? <ref type="figure">Figure 5</ref> shows the t-SNE visualisation of feature distribution on the Market-1501 test set by two MobileNets. We can see that either with or without DML, the two MobileNets do indeed different features, indicating diverse models are obtained. This helps to explain why different models in a DML cohort can teach each other: Each has learned something that the others have not.</p><p>We note that in a number of model distillation studies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19]</ref>, a feature distance loss is added to force the student network to produce similar features to the teacher at corresponding layers. This makes sense when the teacher is pretrained and fixed and the student aims to imitate the teacher. However, in DML, aligning the internal representations different DML models would diminish the cohort diversity and thus damage the ability of each network to teach its peers. <ref type="table" target="#tab_5">Table 5</ref> shows that indeed, when a feature L2 loss is introduced, DML becomes less effective (DML vs. DML, L2). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10.">Semi-Supervised Learning</head><p>We finally explore semi-supervised learning in CIFAR-100 and Market-1501. For CIFAR-100 , we randomly select a subset (from 10% to 100%) of the training images per class as labeled, and treat the rest as unlabeled. For Market-1501, we randomly select M identities as labelled in the training set, varying M from 100 to 751. Experiments are performed with 3 different training strategies: 1) training on the labelled data only with single network; 2) training on labelled data only with DML (DML-labelled). 3) training on all data with DML, where the classification loss is computed for labelled data only, and KL loss is calculated for all the training data (DML-all).</p><p>From the results in <ref type="figure" target="#fig_5">Figure 6</ref>, we can see that: (1) Training two networks with DML consistently performs better than training a single network -as before, but now with varying amounts of labeled data. (2) Compared with adding DML to only labelled data (DML-labelled), DML-all further improves the performance by exploiting the unlabelled data using the KL-distance based mimicry loss. The improvement is bigger when the percentage of labelled data is smaller. This confirms that DML benefits both supervised and semi-supervised learning scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a simple and generally applicable approach to improving the performance of deep neural networks by training them in a cohort with peers and mutual distillation. With this approach we can obtain compact networks that perform better than those distilled from a strong but static teacher. One application of DML is to obtain compact, fast and effective networks. We also showed that this approach is also promising to improve the performance of large powerful networks, and that the network cohort trained in this manner can be combined as an ensemble to further improve performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Deep Mutual Learning (DML) schematic. Each network is trained with a supervised learning loss, and a Kullback Leibler Divergence based mimcry loss to match the probability estimates of its peers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>2 compares the Top-1 accuracy of the CIFAR-100 dataset obtained by various architectures in a two-network DML cohort. From the table we can make the follow- ing observations: (i) All the network combinations among ResNet-32, MobileNet and WRN-28-10 improve perfor- mance when learning in a cohort compared to learning inde- pendently, indicated by the all positive values in the "DML- Independent" columns. (ii) The networks with smaller ca- pacity (ResNet-32 and MobileNet) generally benefit more from DML. (iii) Although WRN-28-10 is a much larger net- work than MobileNet or ResNet-32</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>(a) Results on ImageNet. It shows top-1 acc. (%) w.r.t. training steps; (b) Convergence effect and test mAP (%) on Market-1501 with sequential and distributed training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Performance (mAP (%)) on Market-1501 with different cohort size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>change given parameter noise (c) Posterior certainty comparison Figure 4. Analysis on why DML works</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Semi-supervised learning with DML.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Algorithm 1: Deep Mutual Learning Input: Training set X , label set Y, learning rate γ t Initialize: Initialise Θ 1 and Θ 2 to different conditions; t = 0. Repeat : t = t + 1 Randomly sample data x from X . Compute predictions p 1 and p 2 by (1). 1: Compute the stochastic gradient and update Θ 1 :</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5</head><label>5</label><figDesc>further shows that this is more effective way to learn a more generalisation model when DML is compared with the entropy regularisation- based approach [4] (DML vs. Independent, Entropy).Table 5. Single-Query results on Market-1501 under different set- tings with MobileNets. L2 indicates decreasing the feature dis- tance of two networks.</figDesc><table>Settings 
mAP Rank-1 
Independent 
50.15 
76.87 
DML 
54.71 
79.12 
Independent, Entropy [4] 50.94 
76.34 
DML, L2 
51.01 
77.58 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>Figure 5. tSNE Visualisation of MobileNets trained with DML and independently on the Market-1501 dataset. Different numbers in- dicate different identities.</figDesc><table>1 

92 
94 
155 
156 
183 
227 
228 
260 
270 
521 
727 

MobileNet 2 

1 
92 
94 
155 
156 
183 
227 
228 
260 
270 
521 
727 

MobileNet 1 

(a) Independent 
(b) DML 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported by the Natural Science Foundation of China under Grant 61725202 , 61472060 and China Scholarship Council (CSC) and the European Union's Horizon 2020 research and innovation program (grant agreement no. 640891)</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Do deep nets really need to be deep? In NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.05512</idno>
		<title level="m">Cooperative learning with visual attributes</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bucila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Entropy-sgd: Biasing gradient descent into wide valleys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chaudhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Choromansk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baldass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zecchina</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Person Re-Identification. Advances in Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dual learning for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="820" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Distilling the knowl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T P</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cifar-10 (canadian institute for advanced research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person re-identificatio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unifying distillation and privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Actor-mimic: Deep multitask and transfer reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hinton. Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Xnornet: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Privacy-preserving deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Allerton Conference on Communication, Control, and Computing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="909" to="910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J D</forename><surname>Song Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Posedriven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning using privileged information: Similarity control and knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Izmailov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="791" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Realtime action recognition with enhanced motion vector cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2718" to="2726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spindle net: Person reidentification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yan Andshuai Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiaogangwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdongwang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by GAN improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
