<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-06-18">18 Jun 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wasserstein</forename><surname>Banach</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mathematics KTH -Royal institute of Technology Research and Physics Elekta</orgName>
								<orgName type="department" key="dep2">Department of Applied Mathematics and Theoretical Physics</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gan</surname></persName>
							<email>jonasadl@kth.se</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mathematics KTH -Royal institute of Technology Research and Physics Elekta</orgName>
								<orgName type="department" key="dep2">Department of Applied Mathematics and Theoretical Physics</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mathematics KTH -Royal institute of Technology Research and Physics Elekta</orgName>
								<orgName type="department" key="dep2">Department of Applied Mathematics and Theoretical Physics</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Lunz</surname></persName>
							<email>lunz@math.cam.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mathematics KTH -Royal institute of Technology Research and Physics Elekta</orgName>
								<orgName type="department" key="dep2">Department of Applied Mathematics and Theoretical Physics</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main"></title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-06-18">18 Jun 2018</date>
						</imprint>
					</monogr>
					<note>Preprint. Work in progress.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Wasserstein Generative Adversarial Networks (WGANs) can be used to generate realistic samples from complicated image distributions. The Wasserstein metric used in WGANs is based on a notion of distance between individual images, which induces a notion of distance between probability distributions of images. So far the community has considered 2 as the underlying distance. We generalize the theory of WGAN with gradient penalty to Banach spaces, allowing practitioners to select the features to emphasize in the generator. We further discuss the effect of some particular choices of underlying norms, focusing on Sobolev norms. Finally, we demonstrate the impact of the choice of norm on model performance and show state-of-the-art inception scores for non-progressive growing GANs on CIFAR-10.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative Adversarial Networks (GANs) are one of the most popular generative models <ref type="bibr" target="#b5">[6]</ref>. A neural network, the generator, learns a map that takes random input noise to samples from a given distribution. The training involves using a second neural network, the critic, to discriminate between real samples and the generator output.</p><p>In particular, <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref> introduces a critic built around the Wasserstein distance between the distribution of true images and generated images. The Wasserstein distance is inherently based on a notion of distance between images which in all implementations of Wasserstein GANs (WGAN) so far has been the 2 distance. On the other hand, the imaging literature contains a wide range of metrics used to compare images <ref type="bibr" target="#b3">[4]</ref> that each emphasize different features of interest, such as edges or to more accurately approximate human observer perception of the generated image.</p><p>There is hence an untapped potential in selecting a norm beyond simply the classical 2 norm. We could for example select an appropriate Sobolev space to either emphasize edges, or large scale behavior. In this work we extend the classical WGAN theory to work on these and more general Banach spaces.</p><p>Our contributions are as follows:</p><p>• We introduce Banach Wasserstein GAN (BWGAN), extending WGAN implemented via a gradient penalty (GP) term to any separable complete normed space.</p><p>• We describe how BWGAN can be efficiently implemented. The only practical difference from classical WGAN with gradient penalty is that 2 norm is replaced with a dual norm. We also give theoretically grounded heuristics for the choice of regularization parameters.</p><p>• We compare BWGAN with different norms on the CIFAR-10 dataset. Using the Sobolev space W − 3 2 , 2 , which emphasizes large scale behaviour, we achieve an unsupervised inception score of 8.26, state of the art for non-progressive growing GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background 2.1 Generative adversarial networks</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b5">[6]</ref> perform generative modeling by learning a map G : Z → B from a low-dimensional latent space Z to image space B, mapping a fixed noise distribution P Z to a distribution of generated images P G .</p><p>In order to train the generative model G, a second network D is used to discriminate between original images drawn from a distribution of real images P r and images drawn from P G . The generator is trained to output images that are conceived to be realistic by the critic D. The process is iterated, leading to the famous minimax game <ref type="bibr" target="#b5">[6]</ref> between generator G and critic D</p><formula xml:id="formula_0">min G max D E X∼Pr [log(D(X))] + E Z∼P Z [log(1 − D(G Θ (Z)))] .<label>(1)</label></formula><p>Assuming the discriminator is perfectly trained, this gives rise to the Jensen-Shannon divergence (JSD) as distance measure between the distributions P G and P r [6, Theorem 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Wasserstein metrics</head><p>To overcome undesirable behavior of the JSD in the presence of singular measures <ref type="bibr" target="#b0">[1]</ref>, in <ref type="bibr" target="#b1">[2]</ref> the Wasserstein metric is introduced to quantify the distance between the distributions P G and P r . While the JSD is a strong metric, measuring distances point-wise, the Wasserstein distance is a weak metric, measuring the cost of transporting one probability distribution to another. This allows it to stay finite and provide meaningful gradients to the generator even when the measures are mutually singular.</p><p>In a rather general form, the Wasserstein metric takes into account an underlying metric d B : B × B → R on a Polish (e.g. separable completely metrizable) space B. In its primal formulation, the Wasserstein-p, p ≥ 1, distance is defined as</p><formula xml:id="formula_1">Wass p (P G , P r ) := inf π∈Π(P G ,Pr) E (X1,X2)∼π d B (X 1 , X 2 ) p 1/p ,<label>(2)</label></formula><p>where Π(P G , P r ) denotes the set of distributions on B×B with marginals P G and P r . The Wasserstein distance is hence highly dependent on the choice of metric d B .</p><p>The Kantorovich-Rubinstein duality <ref type="bibr">[18, 5.10</ref>] provides a way of more efficiently computing the Wasserstein-1 distance (which we will henceforth simply call the Wasserstein distance, Wass = Wass 1 ) between measures on high dimensional spaces. The duality holds in the general setting of Polish spaces and states that</p><formula xml:id="formula_2">Wass(P G , P r ) = sup Lip(f )≤1 E X∼P G f (X) − E X∼Pr f (X).<label>(3)</label></formula><p>The supremum is taken over all Lipschitz continuous functions f : B → R with Lipschitz constant equal or less than one. We note that in this dual formulation, the dependence of f on the choice of metric is encoded in the condition of f being 1-Lipschitz and recall that a function f :</p><formula xml:id="formula_3">B → R is γ-Lipschitz if |f (x) − f (y)| ≤ γd B (x, y).</formula><p>In an abstract sense, the Wasserstein metric could be used in GAN training by training a critic D to approximate the supremum in (3). The generator uses the loss E Z∼P Z D(G(Z)). In the case of perfectly trained critic D, this is equivalent to using the Wasserstein loss Wass(P G , P r ) to train G [2, Theorem 3].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Wasserstein GAN</head><p>Implementing GANs with the Wasserstein metric requires to approximate the supremum in (3) with a neural network. In order to do so, the Lipschitz constraint has to be enforced on the network. In the paper Wasserstein GAN <ref type="bibr" target="#b1">[2]</ref> this was achieved by restricting all network parameters to lie within a predefined interval. This technique does guarantee that the network is γ Lipschitz for some γ for any metric space. However, it typically reduces the set of admissible functions to a proper subset of all γ Lipschitz functions, hence introducing an uncontrollable additional constraint on the network. This can lead to training instabilities and artifacts in practice <ref type="bibr" target="#b6">[7]</ref>.</p><p>In <ref type="bibr" target="#b6">[7]</ref> strong evidence was presented that the condition can better be enforced by working with another characterization of the 1−Lipschitz functions. In particular, they prove that if B = R n , d(x, y) B = x − y 2 we have the gradient characterization</p><formula xml:id="formula_4">f is 1 − Lipschitz ⇐⇒ ∇f (x) 2 ≤ 1 for all x ∈ R n .</formula><p>They softly enforce this condition by adding a penalty term to the loss function of D that takes the form</p><formula xml:id="formula_5">EX ∇D(X) 2 − 1 2 ,<label>(4)</label></formula><p>where the distribution ofX is taken to be the uniform distributions on lines connecting points drawn from P G and P r .</p><p>However, penalizing the 2 norm of the gradient corresponds specifically to choosing the 2 norm as underlying distance measure on image space. Some research has been done on generalizing GAN theory to other spaces <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b9">10]</ref>, but in its current form WGAN with gradient penalty does not extend to arbitrary choices of underlying spaces B. We shall give a generalization to a large class of spaces, the (separable) Banach spaces, but first we must introduce some notation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Banach spaces of images</head><p>A vector space is a collection of objects (vectors) that can be added together and scaled, and can be seen as a generalization of the Euclidean space R n . If a vector space B is equipped with a notion of length, a norm · B : B → R, we call it a normed space. The most commonly used norm is the 2 norm defined on R n , given by</p><formula xml:id="formula_6">x 2 = n i=1 x 2 i 1/2 .</formula><p>Such spaces can be used to model images in a very general fashion. In a pixelized model, the image space B is given by the discrete pixel values, B ∼ R n×n . Continuous image models that do not rely on the concept of pixel discretization include the space of square integrable functions over the unit square. The norm · B gives room for a choice on how distances between images are measured. The Euclidean distance is a common choice, but many other distance notions are possible that account for more specific image features, like the position of edges in Sobolev norms.</p><p>A normed space is called a Banach space if it is complete, that is, Cauchy sequences converge. Finally, a space is separable if there exists some countable dense subset. Completeness is required in order to ensure that the space is rich enough for us to define limits whereas separability is necessary for the usual notions of probability to hold. These technical requirements formally hold in finite dimensions but are needed in the infinite dimensional setting. We note that all separable Banach spaces are Polish spaces and we can hence define Wasserstein metrics on them using the induced</p><formula xml:id="formula_7">metric d B (x, y) = x − y B .</formula><p>For any Banach space B, we can consider the space of all bounded linear functionals B → R, which we will denote B * and call the (topological) dual of B. It can be shown <ref type="bibr" target="#b15">[16]</ref> that this space is itself a Banach space with norm · B * : B * → R given by</p><formula xml:id="formula_8">f B * = sup x∈B f (x) x B .<label>(5)</label></formula><p>In what follows, we will give some examples of Banach spaces along with explicit characterizations of their duals. We will give the characterizations in continuum, but they are also Banach spaces in their discretized (finite dimensional) forms.</p><formula xml:id="formula_9">L p -spaces.</formula><p>Let Ω be some domain, then the set of functions f : Ω → R with norm</p><formula xml:id="formula_10">f L p = Ω f (x) p dx 1/p is a Banach space with dual [L p ] * = L q where 1/p + 1/q = 1. In particular, we note that [L 2 ] * = L 2 .</formula><p>The parameter p controls the emphasis on outliers, with a higher values corresponding to a stronger focus on outliers. In the extreme case p = 1, the norm is known to induce sparsity, ensuring that all but a small amount of pixels are set to the correct values.</p><p>Sobolev spaces. Let Ω be some domain, then the set of functions f : Ω → R with norm</p><formula xml:id="formula_11">f W 1,2 = Ω f (x) 2 + |∇f (x)| 2 dx 1/2</formula><p>where ∇f is the spatial gradient, is an example of a Sobolev space. In this space, more emphasis is put on the edges than in e.g. L p spaces, since if f − g W 1,2 is small then not only are their absolute values close, but so are their edges.</p><p>Since taking the gradient is equivalent to multiplying with ξ in the Fourier space, the concept of Sobolev spaces can be generalized to arbitrary (real) derivative orders s if we use the norm</p><formula xml:id="formula_12">f W s,p = Ω F −1 (1 + |ξ| 2 ) s/2 Ff (x) p dx 1/p ,<label>(6)</label></formula><p>where F is the Fourier transform. The tuning parameter s allows to control which frequencies of an image are emphasized: A negative value of s corresponds to amplifying low frequencies, hence prioritizing the global structure of the image. On the other hand, high values of s amplify high frequencies, thus putting emphasis on sharp local structures, like the edges or ridges of an image.</p><p>The dual of the Sobolev space, [W s,p ] * , is W −s,q where q is as above <ref type="bibr" target="#b2">[3]</ref>. Under weak assumptions on Ω, all Sobolev spaces with 1 ≤ p &lt; ∞ are separable. We note that W 0,p = L p and in particular we recover as an important special case</p><formula xml:id="formula_13">W 0,2 = L 2 .</formula><p>There is a wide range of other norms that can be defined for images, see appendix A and <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3]</ref> for a further overview of norms and their respective duals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Banach Wasserstein GANs</head><p>In this section we generalize the loss (4) to separable Banach spaces, allowing us to effectively train a Wasserstein GAN using arbitrary norms.</p><p>We will show that the characterization of γ-Lipschitz functions via the norm of the differential can be extended from the 2 setting in (4) to arbitrary Banach spaces by considering the gradient as an element in the dual of B. In particular, for any Banach space B with norm · B , we will derive the loss function</p><formula xml:id="formula_14">L = 1 γ (E X∼PΘ D(X) − E X∼Pr D(X)) + λEX 1 γ ∂D(X) B * − 1 2 ,<label>(7)</label></formula><p>where λ, γ ∈ R are regularization parameters, and show that a minimizer of this this is an approximation to the Wasserstein distance on B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Enforcing the Lipschitz constraint in Banach spaces</head><p>Throughout this chapter, let B denote a Banach space with norm · B and f : B → R a continuous function. We require a more general notion of gradient: The function f is called Fréchet differentiable at x ∈ B if there is a bounded linear map ∂f (x) : B → R such that</p><formula xml:id="formula_15">lim h B →0 1 h B f (x + h) − f (x) − ∂f (x) (h) = 0.<label>(8)</label></formula><p>The differential ∂f (x) is hence an element of the dual space B * . We note that the usual notion of gradient ∇f (x) in R n with the standard inner product is connected to the Fréchet derivative via ∂f (x) (h) = ∇f (x) · h.</p><p>The following theorem allows us to characterize all Lipschitz continuous functions according to the dual norm of the Fréchet derivative. </p><p>Proof. Assume f is γ-Lipschitz. Then for all x, h ∈ B and &gt; 0</p><p>∂f (x) (h) = lim</p><formula xml:id="formula_17">→0 1 (f (x + h) − f (x)) ≤ lim →0 γ h B = γ h B ,</formula><p>hence by the definition of the dual norm, eq. <ref type="formula" target="#formula_8">(5)</ref>, we have</p><formula xml:id="formula_18">∂f (x) B * = sup h∈B ∂f (x) (h) h B ≤ sup h∈B γ h B h B ≤ γ.</formula><p>Now let f satisfy <ref type="formula" target="#formula_16">(9)</ref> and let x, y ∈ B. Define the function g : R → R by</p><formula xml:id="formula_19">g(t) = f (x(t))</formula><p>, where x(t) = tx + (1 − t)y, As x(t + ∆t) − x(t) = ∆t(x − y), we see that g is everywhere differentiable and</p><formula xml:id="formula_20">g (t) = ∂f x(t) (x − y). Hence |g (t)| = ∂f x(t) (x − y) ≤ ∂f (x(t)) B * x − y B ≤ γ x − y B ,</formula><p>which gives</p><formula xml:id="formula_21">|f (x) − f (y)| = |g(1) − g(0)| ≤ 1 0 |g (t)| dt ≤ γ x − y B ,</formula><p>thus finishing the proof.</p><p>By using lemma 1 we see that a γ-Lipschitz requirement in Banach spaces is equivalent to enforcing that the dual norm of the Fréchet derivative is less than γ everywhere. In order to enforce this we need to compute ∂f (x) B * . As shown in section 2.4, the dual norm can be readily computed for a range of interesting Banach spaces, but we also need to compute ∂f (x), preferably using readily available automatic differentiation software. However, such software can typically only compute derivatives in R n with the standard norm.</p><p>Consider a finite dimensional Banach space B equipped by any norm · B . By Lemma 1, gradient norm penalization requires characterizing (e.g. giving a basis for) the dual B * of B. This can be a difficult for infinite dimensional Banach spaces. In a finite dimensional however setting, there is an linear continuous bijection ι : R n → B given by</p><formula xml:id="formula_22">ι(x) i = x i .<label>(10)</label></formula><p>This isomorphism implicitly relies on the fact that a basis of B can be chosen and can be mapped to the corresponding dual basis. This does not generalize to the infinite dimensional setting, but we hope that this is not a very limiting assumption in practice.</p><p>We note that we can write f = g • ι where g : R n → R and automatic differentiation can be used to compute the derivative ∂g(x) efficiently. Further, note that the chain rule yields</p><formula xml:id="formula_23">∂f (x) = ι * (∂g(ι(x))) ,</formula><p>where ι * : R n → B * is the adjoint of ι which is readily shown to be as simple as ι, ι * (x) i = x i . This shows that computing derivatives in finite dimensional Banach spaces can be done using standard automatic differentiation libraries with only some formal mathematical corrections. In an implementation, the operators ι, ι * would be implicit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Regularization parameter choices</head><p>The network will be trained by adding the regularization term</p><formula xml:id="formula_24">λEX 1 γ ∂D(X) B * − 1 2 .</formula><p>Here, λ is a regularization constant and γ is a scaling factor controlling which norm we compute. In particular D will approximate γ times the Wasserstein distance. In the original WGAN-GP paper <ref type="bibr" target="#b6">[7]</ref> and most following work λ = 10 and γ = 1, while γ = 750 was used in Progressive GAN <ref type="bibr" target="#b7">[8]</ref>. However, it is easy to see that these values are specific to the 2 norm and that we would need to re-tune them if we change the norm. In order to avoid having to hand-tune these for every choice of norm, we will derive some heuristic parameter choice rules that work with any norm.</p><p>For our heuristic, we will start by assuming that the generator is the zero-generator, always returning zero. Assuming symmetry of the distribution of true images P r , the discriminator will then essentially be decided by a single constant f (x) = c x B , where c solves the optimization problem</p><formula xml:id="formula_25">min c∈R E X∼Pr − c X B γ + λ(c − γ) 2 γ 2 .</formula><p>By solving this explicitly we find</p><formula xml:id="formula_26">c = γ 1 + E X∼Pr X B 2λ .</formula><p>Since we are trying to approximate γ times the Wasserstein distance, and since the norm has Lipschitz constant 1, we want c ≈ γ. Hence to get a small relative error we need E X∼Pr X B 2λ. With this theory to guide us, we can make the heuristic rule</p><formula xml:id="formula_27">λ ≈ E X∼Pr X B .</formula><p>In the special case of CIFAR-10 with the 2 norm this gives λ ≈ 27, which agrees with earlier practice (λ = 10) reasonably well.</p><p>Further, in order to keep the training stable we assume that the network should be approximately scale preserving. Since the operation x → ∂D(x) is the deepest part of the network (twice the depth as the forward evaluation), we will enforce x B * ≈ ∂D(x) B * . Assuming λ was appropriately chosen, we find in general (by lemma 1) ∂D(x) B * ≈ γ. Hence we want γ ≈ x B * . We pick the expected value as a representative and hence we obtain the heuristic γ ≈ E X∼Pr X B * For CIFAR-10 with the 2 norm this gives γ = λ ≈ 27 and may explain the improved performance obtained in <ref type="bibr" target="#b7">[8]</ref>.</p><p>A nice property of the above parameter choice rules is that they can be used with any underlying norm. By using these parameter choice rules we avoid the issue of hand tuning further parameters when training using different norms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Computational results</head><p>To demonstrate computational feasibility and show how the choice of norm can impact the trained generator, we implemented Banach Wasserstein GAN with various Sobolev norms applied to CIFAR-10. The implementation was done in TensorFlow and the architecture used was a faithful re-implementation of the residual architecture used in <ref type="bibr" target="#b6">[7]</ref>, see appendix B. For the loss function, we used used the loss eq. <ref type="formula" target="#formula_14">(7)</ref> with parameters according to section 3.2 and the norm according to eq. <ref type="formula" target="#formula_12">(6)</ref>, where we selected units such that |ξ| ≤ 5. Following <ref type="bibr" target="#b7">[8]</ref>, we add a small 10</p><formula xml:id="formula_28">−5 E X∼Pr D(X) 2</formula><p>term to the discriminator loss to stop it from drifting during the training.</p><p>For training we used the Adam optimizer <ref type="bibr" target="#b8">[9]</ref> with learning rate decaying linearly from 2 · 10 −4 to 0 over 100 000 iterations with β 1 = 0, β 2 = 0.9. We used 5 discriminator updates per generator update. The batch size used was 64. In order to evaluate the reproducibility of the results, we followed this up by training an ensemble of 5 generators using SGD with warm restarts following <ref type="bibr" target="#b10">[11]</ref>. Each warm restart used 10 000 generator steps. Our implementation is available online <ref type="bibr" target="#b0">1</ref> .</p><p>Some representative samples from the generator can be seen in <ref type="figure" target="#fig_0">fig. 1</ref>. In our opinion, the s = 2 samples are visually most appealing, but might be lacking in variation as compared to s = −2, note for example the large number of red cars. We note that the Fréchet derivatives behave as expected, for    <ref type="bibr" target="#b19">[20]</ref> 7.07 ± .10 WGAN-GP <ref type="bibr" target="#b6">[7]</ref> 7.86 ± .07 CT GAN <ref type="bibr" target="#b18">[19]</ref> 8.12 ± .12 SNGAN <ref type="bibr" target="#b12">[13]</ref> 8.22 ± .05 W − 3 2 , 2 -BWGAN (ours) 8.26 ± .07 Progressive GAN <ref type="bibr" target="#b7">[8]</ref> 8.80 ± .05 high s they vary quickly, indicating that the discriminator puts emphasize on smaller features, while for low s they vary more slowly, indicating that the discriminator focuses more on the large scale behavior. See appendix C for samples from each of the W s,p space investigated as well as samples and inception scores using some other spaces.</p><p>We also display the computed inception values over the ensemble in <ref type="figure" target="#fig_2">fig. 2</ref> where we note that negative s gives higher inception scores. To the best of our knowledge, the inception score of 8.26 ± 0.07, achieved using the W − 3 2 , 2 space, is state of the art for non-progressive growing methods, see <ref type="table" target="#tab_0">table 1</ref> for an overview. We also note that our result for W 0,2 = 2 is slightly better than the reference implementation, despite using the same network. We suspect that this is due to our improved parameter choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">How about metric spaces?</head><p>Gradient norm penalization according to lemma 1 is only valid in Banach spaces but a natural alternative to penalizing gradient norms is to enforce the Lipschitz condition directly (see <ref type="bibr" target="#b13">[14]</ref>). This would potentially allow training Wasserstein GAN on general metric spaces by adding a penalty term of the form</p><formula xml:id="formula_29">E X,Y |f (X) − f (Y )| d B (X, Y ) − 1 2 + .<label>(11)</label></formula><p>While theoretically equivalent to gradient norm penalization when the distributions of X and Y are chosen appropriately, this term is very likely to have considerably higher variance in practice.</p><p>For example, if we assume that d is not bounded from below and consider two points x, y ∈ M that are sufficiently close then a penalty term of the Lipschitz quotient as in <ref type="formula" target="#formula_0">(11)</ref> imposes a condition on the differential around x and y in the direction (x−y) only, i.e. only |∂f (x)(x−y)| ≤ 1 is ensured. In the case of two distributions that are close, we will with high probability sample the difference quotient in a spatial direction that does not exhaust the Lipschitz constraint, i.e. |∂f (x)(x − y)| 1 . Difference quotient penalization (11) then does not effectively enforce the Lipschitz condition. Gradient norm penalization on the other hand ensures this condition in all spatial directions simultaneously by considering the dual norm of the differential.</p><p>On the other hand, if d is bounded from below the above argument fails. For example, Wasserstein GAN over a space equipped with the trivial metric</p><formula xml:id="formula_30">d trivial (x, y) = 0 if x = y 1 else</formula><p>using the regularizer eq. <ref type="formula" target="#formula_0">(11)</ref> is a slight variation of Least Squares GAN <ref type="bibr" target="#b11">[12]</ref>. We do not further investigate this line of reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We analyzed the dependence of Wasserstein GANs (WGANs) on the notion of distance between images and showed how choosing distances other than the 2 metric can be used to make WGANs focus on particular image features of interest. We introduced a generalization of WGANs with gradient norm penalization to Banach spaces, allowing to easily implement WGANs for a wide range of underlying norms on images. This opens up a new degree of freedom to design the algorithm to account for the image features relevant in a specific application.</p><p>On the CIFAR-10 dataset, we demonstrated the impact a change in norm has on model performance. In particular, we computed inception scores for Banach Wasserstein GANs using different Sobolev spaces W s,2 and found a correlation between the value s and model performance.</p><p>While this work was motivated by images, the theory is general and can be applied to data in any normed space. In the future, we hope that practitioners take a step back and ask themselves if the 2 metric is really the best measure of fit, or if some other metric better emphasize what they want to achieve with their generative model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Some further Banach spaces</head><p>There is some algebra for how to form new Banach spaces from known spaces. Specifically we have the following constructions that the reader might find useful. 1 is the adjoint of the inverse of A. These weighted spaces could be used to focus on some feature of interest, e.g. focus especially on the red color channel or on some spatial region of the image, the middle perhaps, that is more important.</p><p>Product spaces. Let B 1 , . . . B n be Banach spaces and let B = B 1 × · · · × B n be the product space with norm</p><formula xml:id="formula_31">(x 1 , . . . , x n ) B = n i=1 x i p Bi 1/p</formula><p>then the dual space has norm</p><formula xml:id="formula_32">(x * 1 , . . . , x * n ) B * = n i=1 x * i q B * i 1/q</formula><p>where 1/p + 1/q = 1. These spaces could be used to explicitly model the color channels or even to model multi-modal data such as a generator outputting both an image and a caption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Network details</head><p>The implementation faithfully follows the source code from <ref type="bibr" target="#b6">[7]</ref>. It uses of residual blocks consisting of "nonlinearity + conv + nonlinearity + conv + residual connection" and meanpooling/nearest neighbor interpolation as building blocks. The generator starts from a latent space of 128 normally distributed random numbers and applies a dense layer to 4x4 images and applies a residual block then an interpolation repeatedly until the resolution 32x32 is reached. Then, a nonlinearity followed by a 1x1 convolution with 3 output channels is applied in order to obtain the generated color images.</p><p>The discriminator goes the other way using pooling with a final spatial mean-pooling followed by a dense layer. We used ReLU nonlinearities, all convolutions uses 128 channels and we used batch normalization after the nonlinearities in the generator. Following, we used uniform He initialization for all convolutions except the residual connections which used uniform Xavier initialization.</p><p>See <ref type="bibr" target="#b6">[7]</ref> and/or our open source implementation for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Further samples</head><p>We here give further examples of generated samples for various spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Sobolev spaces</head><p>We give samples from each of the W s,2 investigated in <ref type="figure">fig. 3 and fig. 4</ref>. The qualitative results mirror those observed in section 4 with higher s indicating higher gradients in the discriminators Fréchet derivative, thus indicating a focus on higher frequency content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 L p spaces</head><p>We performed the same experiment as for the Sobolev spaces for some L p spaces. The training was exactly the same, and the parameter choices were according to section 3.2. Interestingly, we find that γ ≈ 1 for L 10 , while γ ≈ 1000 for L 1.1 , hence the parameter choices become really important in this case and without them we could not successfully train the networks at all. The resulting figures are given is <ref type="figure">fig. 6 and fig. 7</ref>. We found the p ≈ 4 samples to be the most visually appealing.</p><p>Once again we note that the derivatives behave as expected from theory. For low p values, the Fréchet derivative is approximately ±1, thus focusing equally much on each pixel. For high p values the Fréchet derivative is mostly close to zero except for a few outliers where it is very high, indicating that the discriminator is focusing on "the worst" pixels.</p><p>We also computed inception scores for the trained network as shown in <ref type="figure">fig. 5</ref>. We note that higher p (e.g. more focus on outliers) seems to correlate strongly with higher inception scores, but that the effect seems to peak around p = 5 (inception score 8.25 ± 0.07). For p → ∞ we expect the error to increase since L ∞ is not separable and hence our theory does not apply to it. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Lemma 1 .</head><label>1</label><figDesc>Assume f : B → R is Fréchet differentiable. Then f is γ-Lipschitz if and only if ∂f (x) B * ≤ γ ∀x ∈ B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Top: Generated CIFAR-10 samples for Sobolev spaces W s,2 . Bottom: corresponding Fréchet derivatives of the discriminator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Inception scores for Banach Wasserstein GAN for the Sobolev spaces W s,2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 : 0 Figure 4 :</head><label>504</label><figDesc>Figure 5: Inception scores for Banach Wasserstein GAN for some L p spaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Inception Scores on CIFAR-10</figDesc><table>Method 
Inception Score 

DCGAN [15] 
6.16 ± .07 
EBGAN </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Weighted spaces. Let B 1 be some separable Banach space with norm · B1 , then we can construct another space B 2 with norm f B2 := Af B1 where A : B 2 → B 1 is a continuous linear bijection. It is straightforward to show that the dual space B *</figDesc><table>2 has norm 
f 

 *  
B  *  

2 

= A 
− *  f 

 *  
B  *  

1 

where A 
− *  : B 

 *  

2 → B 

 *  

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/adler-j/bwgan</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to acknowledge Peter Maass for brining us together as well as important support from Ozan Öktem, Axel Ringh, Johan Karlsson, Jens Sjölund, Sam Power and Carola Schönlieb.</p><p>The work by J.A. was supported by the Swedish Foundation of Strategic Research grants AM13-0049, ID14-0055 and Elekta. The work by S.L. was supported by the EPSRC grant EP/L016516/1 for the University of Cambridge Centre for Doctoral Training, and the Cambridge Centre for Analysis. We also acknowledge the support of the Cantab Capital Institute for the Mathematics of Information.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<title level="m">Towards Principled Methods for Training Generative Adversarial Networks. International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Arjovsky</surname></persName>
		</author>
		<title level="m">Soumith Chintala, and Léon Bottou. Wasserstein Generative Adversarial Networks. International Conference on Machine Learning, ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Functional analysis, Sobolev spaces and partial differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haim</forename><surname>Brezis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Image processing and analysis: variational, PDE, wavelet, and stochastic methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhong Jackie</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">94</biblScope>
			<pubPlace>Siam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Encyclopedia of Distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Deza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Progressive Growing of GANs for Improved Quality, Stability, and Variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Approximation and Convergence Properties of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Generative Adversarial Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic Gradient descent with Warm Restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<title level="m">Spectral Normalization for Generative Adversarial Networks. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Henning Petzka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lukovnikov</surname></persName>
		</author>
		<title level="m">On the regularization of Wasserstein GANs. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Functional analysis. International series in pure and applied mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walter Rudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Calvin</forename><surname>Seward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urs</forename><surname>Bergmann</surname></persName>
		</author>
		<title level="m">Nikolay Jetchev, and Sepp Hochreiter. First Order Generative Adversarial Networks. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Optimal transport: old and new</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cédric</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixia</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Liqiang Wang, and Boqing Gong. Improving the Improved Training of Wasserstein GANs. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Energy-based Generative Adversarial Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Junbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Figure 3: Samples for all W s,2 -spaces investigated</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Figure 7: Fréchet derivatives for all L p -spaces investigated</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
