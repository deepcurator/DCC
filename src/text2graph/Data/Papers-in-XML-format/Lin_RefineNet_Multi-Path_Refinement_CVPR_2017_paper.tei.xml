<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is a crucial component in image understanding. The task here is to assign a unique label (or category) to every single pixel in the image, which can be considered as a dense classification problem. The related problem of so-called object parsing can usually be cast as semantic segmentation. Recently, deep learning methods, and in particular convolutional neural networks (CNNs), e.g., VGG <ref type="bibr" target="#b41">[42]</ref>, Residual Net <ref type="bibr" target="#b23">[24]</ref>, have shown remarkable results in recognition tasks. However, these approaches exhibit clear limitations when it comes to dense prediction in tasks like dense depth or normal estimation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> and semantic segmentation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b4">5]</ref>. Multiple stages of spatial pooling and convolution strides reduce the final image prediction typically by a factor of 32 in each dimension, thereby losing much of the finer image structure.</p><p>One way to address this limitation is to learn deconvolutional filters as an up-sampling operation <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b35">36]</ref> to generate high-resolution feature maps. The deconvolution operations are not able to recover the low-level visual features which are lost after the downsampling operation in the convolution forward stage. Therefore, they are unable to output accurate high-resolution prediction. Low-level visual information is essential for accurate prediction on the boundaries or details. The method DeepLab recently proposed by Chen et al. <ref type="bibr" target="#b5">[6]</ref> employs atrous (or dilated) convolutions to account for larger receptive fields without downscaling the image. DeepLab is widely applied and represents state-ofthe-art performance on semantic segmentation. This strategy, although successful, has at least two limitations. First, it needs to perform convolutions on a large number of detailed (high-resolution) feature maps that usually have highdimensional features, which are computational expensive. Moreover, a large number of high-dimensional and highresolution feature maps also require huge GPU memory resources, especially in the training stage. This hampers the computation of high-resolution predictions and usually limits the output size to 1/8 of the original input. Second, dilated convolutions introduce a coarse sub-sampling of features, which potentially leads to a loss of important details.</p><p>Another type of methods exploits features from interme-diate layers for generating high-resolution prediction, e.g., the FCN method in <ref type="bibr" target="#b35">[36]</ref> and Hypercolumns in <ref type="bibr" target="#b21">[22]</ref>. The intuition behind these works is that features from middle layers are expected to describe mid-level representations for object parts, while retaining spatial information. This information is thought to be complementary to the features from early convolution layers which encode low-level spatial visual information like edges, corners, circles, etc., and also complementary to high-level features from deeper layers which encode high-level semantic information, including object-or category-level evidence, but which lack strong spatial information. We argue that features from all levels are helpful for semantic segmentation. High-level semantic features help the category recognition of image regions, while low-level visual features help to generate sharp, detailed boundaries for high-resolution prediction. How to effectively exploit middle layer features remains an open question and deserves more attentions. To this end, we propose a novel network architecture which effectively exploits multi-level features for generating high-resolution predictions. Our main contributions are as follows:</p><p>1. We propose a multi-path refinement network (RefineNet) which exploits features at multiple levels of abstraction for high-resolution semantic segmentation. RefineNet refines low-resolution (coarse) semantic features with fine-grained low-level features in a recursive manner to generate high-resolution semantic feature maps. Our model is flexible in that it can be cascaded and modified easily.</p><p>2. Our cascaded RefineNets can be effectively trained end-to-end, which is crucial for best prediction performance. More specifically, all components in RefineNet employ residual connections <ref type="bibr" target="#b23">[24]</ref> with identity mappings <ref type="bibr" target="#b24">[25]</ref>, such that gradients can be directly propagated through short-range and long-range residual connections allowing for both effective and efficient end-to-end training.</p><p>3. We propose a new network component we call chained residual pooling which is able to capture background context from a large image region. It does so by efficiently pooling features with multiple window sizes and fusing them together with residual connections and learnable weights.</p><p>4. The proposed RefineNet achieves new state-of-theart performance on 7 public datasets, including PASCAL VOC 2012, PASCAL-Context, NYUDv2, SUN-RGBD, Cityscapes, ADE20K, and the object parsing Person-Parts dataset. In particular, we achieve an IoU score of 83.4 on the PASCAL VOC 2012 dataset, outperforming the currently best approach DeepLab by a large margin.</p><p>To facilitate future research, we release both source code and trained models for our RefineNet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>CNNs have become the most successful methods for semantic segmentation in recent years. The early methods in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref> are region-proposal-based methods which classify region proposals to generate segmentation results. Recently, fully convolution networks (FCNNs) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10]</ref> have shown effective feature generation and end-to-end training, and have thus become the most popular choice for semantic segmentation. FCNNs have also been widely applied in other dense-prediction tasks, e.g., depth estimation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33]</ref>, image restoration <ref type="bibr" target="#b13">[14]</ref>, and image superresolution <ref type="bibr" target="#b11">[12]</ref>. The proposed method here is also based on fully convolution-style networks.</p><p>FCNN-based methods usually have the limitation of lowresolution prediction. There are a number of proposed techniques which address this limitation and aim to generate high-resolution predictions. The atrous convolution approach DeepLab-CRF in <ref type="bibr" target="#b4">[5]</ref> directly output a middleresolution score map and then applies a dense CRF method <ref type="bibr" target="#b26">[27]</ref> to refine boundaries by leveraging color contrast information. CRF-RNN <ref type="bibr" target="#b46">[47]</ref> extends this approach by implementing recurrent layers for end-to-end learning of the dense CRF and FCNN. Deconvolution methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b1">2]</ref> learn deconvolution layers to upsample the low-resolution predictions. The depth estimation method in <ref type="bibr" target="#b33">[34]</ref> employs super-pixel pooling to output high-resolution prediction.</p><p>There exist several methods which exploit middle layer features for segmentation. The FCN method of Long et al. <ref type="bibr" target="#b35">[36]</ref> adds prediction layers to middle layers to generate prediction scores at multiple resolutions. They average the multi-resolution scores to generate the final prediction mask. Their system is trained in a stage-wise manner rather than end-to-end. The Hypercolumn approach <ref type="bibr" target="#b21">[22]</ref> merges features from middle layers and learns dense classification layers. That method also employs a stage-wise training strategy instead of end-to-end training. Both SegNet <ref type="bibr" target="#b1">[2]</ref> and U-Net <ref type="bibr" target="#b39">[40]</ref> apply skip-connections in the deconvolution architecture to exploit the features from middle layers.</p><p>Although several related works exist, it still remains an open question how to effectively exploit middle layer features. We propose a novel network architecture, RefineNet, to address this question. The network architecture of RefineNet is distinct from existing methods. It consists of a number of specially designed components which are able to refine the coarse high-level semantic features by exploiting low-level visual features. In particular, RefineNet employs short-range and long-range residual connections with identity mappings which enable effective end-to-end training of the whole system, and thus help to achieve superior performance. Comprehensive empirical results clearly verify the effectiveness of our novel network architecture for exploiting middle layer features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Before presenting our approach, we first review the structure of fully convolutional networks for semantic segmentation <ref type="bibr" target="#b35">[36]</ref> in more detail and also discuss the recent dilated convolution technique <ref type="bibr" target="#b5">[6]</ref> which is specifically designed to generate high-resolution predictions.</p><p>Very deep CNNs have shown outstanding performance on object recognition problems. Specifically, the recently proposed Residual Network (ResNet) <ref type="bibr" target="#b23">[24]</ref> has shown step-change improvements over earlier architectures, and ResNet models pre-trained for ImageNet recognition tasks are publicly available. Because of this, in the following we adopt ResNet as our fundamental building block for semantic segmentation. Note, however, that replacing it with any other deep network is straightforward.</p><p>Since semantic segmentation can be cast as a dense classification problem, the ResNet model can be easily modified for this task. This is achieved by replacing the single label prediction layer with a dense prediction layer that outputs the classification confidence for each class at every pixel. This approach is illustrated in <ref type="figure">Fig. 2(a)</ref>. As can be seen, during the forward pass in ResNet, the resolution of the feature maps (layer outputs) is decreased, while the feature depth, i.e. the number of feature maps per layer (or channels) is increased. The former is caused by striding during convolutional and pooling operations.</p><p>The ResNet layers can be naturally divided into 4 blocks according to the resolution of the output feature maps, as shown in <ref type="figure">Fig. 2(a)</ref>. Typically, the stride is set to 2, thus reducing the feature map resolution to one half when passing from one block to the next. This sequential sub-sampling has two effects: first it increases the receptive field of convolutions at deeper levels, enabling the filters to capture more global and contextual information which is essential for high quality classification; second it is necessary to keep the training efficient and tractable because each layer comprises a large number of filters and therefore produces an output which has a corresponding number of channels. Thus, there is a trade-off between the number of channels and resolution of the feature maps. Typically, the final feature map output ends up being 32 times smaller in each spatial dimension than the original image (but with thousands of channels). This low-resolution feature map loses important visual details captured by early low-level filters, resulting in a rather coarse segmentation map. This issue is a well-known limitation of deep CNN-based segmentation methods.</p><p>An alternative approach to avoid lowering the resolution while retaining a large receptive field is to use dilated (atrous) convolution. This method introduced in <ref type="bibr" target="#b5">[6]</ref> shows state-of-the-art performance on semantic segmentation. The sub-sampling operations are removed (the stride is changed from 2 to 1), and all convolution layers after the first block use dilated convolution. Such a dilated convolution, which is effectively a sub-sampled convolution kernel, has the effect of increasing the receptive field size of the filters without increasing the number of weights that must be learned (see illustration in <ref type="figure">Fig. 2(b)</ref>). Even so, there is a significant cost in memory, because unlike the image subsampling methods, one must retain very large numbers of feature maps at higher resolutions. For example, if we retain all channels in all layers to be at least 1/4 of the original image resolution, and consider a typical number of filter channels to be 1024, then we can see that the memory capacity of even high-end GPUs is quickly swamped by very deep networks. In practice, therefore, dilation convolution methods usually have a resolution prediction of no more than 1/8 size of the original rather than 1/4, when using a deep network.</p><p>In contrast to dilated convolution methods, in this paper we propose a means to enjoy both the memory and computational benefits of deresolving, while still able to produce effective and efficient high-resolution segmentation prediction, as described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>We propose a new framework that provides multiple paths over which information from different resolutions and via potentially long-range connections is assimilated using a generic building block, the RefineNet. <ref type="figure">Fig. 2(c)</ref> shows one possible arrangement of the building blocks to achieve our goal of high resolution semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-Path Refinement</head><p>As noted previously, we aim to exploit multi-level features for high-resolution prediction with long-range residual connections. RefineNet provides a generic means to fuse coarse high-level semantic features with finer-grained low-level features to generate high-resolution semantic feature maps. A crucial aspect of the design ensures that the gradient can be effortlessly propagated backwards through the network all the way to early low-level layers over longrange residual connections, ensuring that the entire network can be trained end-to-end.</p><p>For our standard multi-path architecture, we divide the pre-trained ResNet (trained with ImageNet) into 4 blocks according to the resolutions of the feature maps, and employ a 4-cascaded architecture with 4 RefineNet units, each of which directly connects to the output of one ResNet block as well as to the preceding RefineNet block in the cascade. Note, however, that such a design is not unique. In fact, our flexible architecture allows for a simple exploration of different variants. For example, a RefineNet block can accept input from multiple ResNet blocks. We will analyse a 2-cascaded version, a single-block approach as well as a 2-scale 7-path architecture later in Sec. 4.3. M u l t i -P a t h R e fi n e m e n t <ref type="figure">Figure 2</ref>. Comparison of fully convolutional approaches for dense classification. Standard multi-layer CNNs, such as ResNet (a) suffer from downscaling of the feature maps, thereby losing fine structures along the way. Dilated convolutions (b) remedy this shortcoming by introducing atrous filters, but are computationally expensive to train and quickly reach memory limits even on modern GPUs. Our proposed architecture that we call RefineNet (c) exploits various levels of detail at different stages of convolutions and fuses them to obtain a high-resolution prediction without the need to maintain large intermediate feature maps. See text and <ref type="figure">Fig. 3</ref> for details.  <ref type="figure">Figure 3</ref>. The individual components of our multi-path refinement network architecture RefineNet. Components in RefineNet employ residual connections with identity mappings, allowing gradients to directly propagate both locally through the network and also directly to the input paths via long-range residual connections, and thus enabling effective end-to-end training of the whole system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chained Residual Pooling</head><p>We denote RefineNet-m as the RefineNet block that connects to the output of block-m in ResNet. In practice, each ResNet output is passed through one convolutional layer to adapt the dimensionality. Although all RefineNets share the same internal architecture, their parameters are not tied, allowing for a more flexible adaptation for individual levels of detail. Following the illustration in <ref type="figure">Fig. 2(c)</ref> bottom up, we start from the last block in ResNet, and connect the output of ResNet block-4 to RefineNet-4. Here, there is only one input for RefineNet-4, and RefineNet-4 serves as an extra set of convolutions which adapt the pre-trained ResNet weights to the task at hand, in our case, semantic segmentation. In the next stage, the output of RefineNet-4 and the ResNet block-3 are fed to RefineNet-3 as 2-path inputs. The goal of RefineNet-3 is to use the high-resolution features from ResNet block-3 to refine the low-resolution feature map output by RefineNet-4 in the previous stage. Similarly, RefineNet-2 and RefineNet-1 repeat this stage-wise refinement by fusing high-level information from the later layers and high-resolution but low-level features from the earlier ones. As the last step, the final high-resolution feature maps are fed to a dense soft-max layer to make the final prediction in the form of a dense score map. This score map is then upsampled to match the original image using bilinear interpolation. Note that the entire network can be efficiently trained end-to-end.</p><p>It is important to note that we introduce long-range residual connections between the blocks in ResNet and the RefineNet modules. During the forward pass, these long-range residual connections convey the low-level features that encode visual details for refining the coarse high-level feature maps. In the training step, the long-range residual connections allow direct gradient propagation to early convolution layers, which helps effective end-to-end training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">RefineNet</head><p>The architecture of one RefineNet block is illustrated in <ref type="figure">Fig. 3(a)</ref>. In the multi-path overview shown in <ref type="figure">Fig 2(c)</ref></p><note type="other">, RefineNet-1 has one input path, while all other RefineNet blocks have two inputs. Note, however, that our architecture is generic and each Refine block can be easily modified to accept an arbitrary number of feature maps with arbitrary resolutions and depths.</note><p>Residual convolution unit. The first part of each RefineNet block consists of an adaptive convolution set that mainly fine-tunes the pretrained ResNet weights for our task. To that end, each input path is passed sequentially through two residual convolution units (RCU), which is a simplified version of the convolution unit in the original ResNet <ref type="bibr" target="#b23">[24]</ref>, where the batch-normalization layers are removed (cf . <ref type="figure">Fig. 3(b)</ref>). The filter number for each input path is set to 512 for RefineNet-4 and 256 for the remaining ones in our experiments.</p><p>Multi-resolution fusion. All path inputs are then fused into a high-resolution feature map by the multi-resolution fusion block, depicted in <ref type="figure">Fig. 3(c)</ref>. This block first applies convolutions for input adaptation, which generate feature maps of the same feature dimension (the smallest one among the inputs), and then upsamples all (smaller) feature maps to the largest resolution of the inputs. Finally, all features maps are fused by summation. The input adaptation in this block also helps to re-scale the feature values appropriately along different paths, which is important for the subsequent sumfusion. If there is only one input path (e.g., the case of RefineNet-4 in <ref type="figure">Fig. 2(c)</ref>), the input path will directly go through this block without changes.</p><p>Chained residual pooling. The output feature map then goes through the chained residual pooling block, schematically depicted in <ref type="figure">Fig. 3(d)</ref>. The proposed chained residual pooling aims to capture background context from a large image region. It is able to efficiently pool features with multiple window sizes and fuse them together using learnable weights. In particular, this component is built as a chain of multiple pooling blocks, each consisting of one max-pooling layer and one convolution layer. One pooling block takes the output of the previous pooling block as input. Therefore, the current pooling block is able to re-use the result from the previous pooling operation and thus access the features from a large region without using a large pooling window. If not further specified, we use two pooling blocks each with stride 1 in our experiments.</p><p>The output feature maps of all pooling blocks are fused together with the input feature map through summation of residual connections. Note that our choice to employ residual connections also persists in this building block, which once again facilitates gradient propagation during training. In one pooling block, each pooling operation is followed by convolutions which serve as a weighting layer for the summation fusion. It is expected that this convolution layer will learn to accommodate the importance of the pooling block during the training process.</p><p>Output convolutions. The final step of each RefineNet block is another residual convolution unit (RCU). This results in a sequence of three RCUs between each block. To reflect this behavior in the last RefineNet-1 block, we place two additional RCUs before the final softmax prediction step. The goal here is to employ non-linearity operations on the multi-path fused feature maps to generate features for further processing or for final prediction. The feature dimension remains the same after going through this block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Identity Mappings in RefineNet</head><p>Note that all convolutional components of the RefineNet have been carefully constructed inspired by the idea behind residual connections and follow the rule of identity mapping <ref type="bibr" target="#b24">[25]</ref>. This enables effective backward propagation of the gradient through RefineNet and facilitates end-to-end learning of cascaded multi-path refinement networks.</p><p>Employing residual connections with identity mappings allows the gradient to be directly propagated from one block to any other blocks, as was recently shown by He et al. <ref type="bibr" target="#b24">[25]</ref>. This concept encourages to maintain a clean information path for shortcut connections, so that these connections are not "blocked" by any non-linear layers or components. Instead, non-linear operations are placed on branches of the main information path. We follow this guideline for developing the individual components in RefineNet, including all convolution units. It is this particular strategy that allows the multi-cascaded RefineNet to be trained effectively. Note that we include one non-linear activation layer (ReLU) in the chained residual pooling block. We observed that this ReLU is important for the effectiveness of subsequent pooling operations and it also makes the model less sensitive to changes in the learning rate. We observed that one single ReLU in each RefineNet block does not noticeably reduce the effectiveness of gradient flow.</p><p>We have both short-range and long-range residual connections in RefineNet. Short-range residual connections refer to local shortcut connections in one RCU or the residual pooling component, while long-range residual connections refer to the connections between RefineNet modules and  the ResNet blocks. With long-range residual connections, the gradient can be directly propagated to early convolution layers in ResNet and thus enables end-to-end training of all network components. The fusion block fuses the information of multiple shortcut paths, which can be considered as performing summation fusion of multiple residual connections with necessary dimension or resolution adaptation. In this aspect, the role of the multi-resolution fusion block here is analogous to the role of the "summation" fusion in a conventional residual convolution unit in ResNet. There are certain layers in RefineNet, and in particular within the fusion block, that perform linear feature transformation operations, like linear feature dimension reduction or bilinear upsampling. These layers are placed on the shortcut paths, which is similar to the case in ResNet <ref type="bibr" target="#b23">[24]</ref>. As in ResNet, when a shortcut connection crosses two blocks, it will include a convolution layer in the shortcut path for linear feature dimension adaptation, which ensures that the feature dimension matches the subsequent summation in the next block. Since only linear transformation are employed in these layers, gradients still can be propagated through these layers effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To show the effectiveness of our approach, we carry out comprehensive experiments on seven public datasets, which include six popular datasets for semantic segmentation on indoors and outdoors scenes, and one dataset for object parsing called Person-Part. The segmentation quality is measured by the intersection-over-union (IoU) score <ref type="bibr" target="#b15">[16]</ref>, the pixel accuracy and the mean accuracy <ref type="bibr" target="#b35">[36]</ref> over all classes. As commonly done in the literature, we apply simple data augmentation during training. Specifically, we perform random scaling (ranging from 0.7 to 1.3), random cropping and horizontal flipping of the images. If not further specified, we apply test-time multi-scale evaluation, which is a common practice in segmentation methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b5">6]</ref>. For multi-scale evaluation, we average the predictions on the same image across different scales for the final prediction. We also present an ablation experiment to inspect the impact of components and several variants of our model. Our system is built on MatConvNet <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Object Parsing</head><p>We first present our results on the task of object parsing, which consists of recognizing and segmenting object parts. We carry out experiments on the Person-Part dataset <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref> which provides pixel-level labels for six person parts including Head, Torso, Upper/Lower Arms and Upper/Lower Legs. The rest of each image is considered background. There are 1717 training and 1818 test images. We use 4 chained pooling blocks for this dataset.</p><p>We compare our results to a number of state-of-the-art methods, listed in <ref type="table" target="#tab_1">Table 1</ref>. The results clearly demonstrate the improvement over previous works. In particular, we significantly outperform the the recent DeepLab-v2 approach <ref type="bibr" target="#b5">[6]</ref> which is based on dilated convolutions for highresolution segmentation, using the same ResNet as initialization. In <ref type="table">Table 2</ref>, we present an ablation experiment to quantify the influence of the following components: Network depth, chained residual pooling and multi-scale evaluation (Msc Eva), as described earlier. This experiment shows that each of these three factors can improve the overall performance. Qualitative examples of our object parsing on this dataset are shown in <ref type="figure" target="#fig_3">Fig.4.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Semantic Segmentation</head><p>We now describe our experiments on dense semantic labeling on six public benchmarks and show that our RefineNet outperforms previous methods on all datasets.  NYUDv2. The NYUDv2 dataset <ref type="bibr" target="#b40">[41]</ref> consists of 1449 RGB-D images showing interior scenes. We use the segmentation labels provided in <ref type="bibr" target="#b18">[19]</ref>, in which all labels are mapped to 40 classes. We use the standard training/test split with 795 and 654 images, respectively. We train our models only on RGB images without using the depth information. Quantitative results are shown in Tab. 3. Our RefineNet achieves state-of-the-art results on NYUDv2. Similar to the object parsing task above, we also perform ablation experiments on the NYUDv2 dataset to evaluate the effect of different settings. The results are presented in Tab. 2. Once again, this study demonstrates the benefits of adding the proposed chained residual pooling component and deeper networks, both of which consistently improve the performance as measured by IoU. <ref type="bibr" target="#b15">[16]</ref> is a well-known segmentation dataset which includes 20 object categories and one background class. This dataset is split into a training set, a validation set and a test set, with 1464, 1449 and 1456 images each. Since the test set labels are not publicly available, all reported results have been obtained from the VOC evaluation server. Following the common convention <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b34">35]</ref>, the training set is augmented by additional annotated VOC images provided in <ref type="bibr" target="#b20">[21]</ref> as well as with the training data from the MS COCO dataset <ref type="bibr" target="#b30">[31]</ref>. We compare our RefineNet on the PASCAL VOC 2012 test set with a number of competitive methods, showing superior performance. We use 4 pooling blocks for this dataset. We also use a dense CRF <ref type="bibr" target="#b26">[27]</ref> for further refinement for this  dataset, which yields a marginal improvement of 0.1% on the validation set. Since this dense CRF post-processing only brings very minor improvement on our high-resolution prediction, we do not apply it on other datasets. The detailed results for each category and the mean IoU scores are shown in Tab. 4. We achieve an IoU score of 83.4, which is the best reported result on this challenging dataset to date. <ref type="bibr" target="#b1">2</ref> We outperform competing methods in almost all categories. In particular, we significantly outperform DeepLab-v2 <ref type="bibr" target="#b5">[6]</ref>, which uses the same ResNet-101 network as initialization by nearly 3 percentage points. Selected prediction examples are shown in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PASCAL VOC 2012</head><p>Cityscapes <ref type="bibr" target="#b8">[9]</ref> is a very recent dataset on street scene images from 50 different European cities. This dataset provides fine-grained pixel-level annotations of roads, cars, pedestrians, bicycles, sky, etc. The provided training set has 2975 images and the validation set has 500 images. In total, 19 classes are considered for training and evaluation. The test set ground-truth is withheld by the organizers, and we evaluate our method on the their evaluation server. The test results are shown in <ref type="table" target="#tab_4">Table 5</ref>. In this challenging setting, our architecture again outperforms previous methods. A few test images along with ground truth and our predicted semantic maps are shown in <ref type="figure" target="#fig_5">Fig. 6</ref>.</p><p>PASCAL-Context. The PASCAL-Context <ref type="bibr" target="#b36">[37]</ref> dataset provides the segmentation labels of the whole scene for the PASCAL VOC images. We use the segmentation labels which contain 60 classes (59 object categories plus background) for evaluation as well as the provided training/test splits. The training set contains 4998 images and the test set has 5105 images. Results are shown in <ref type="table">Table 6</ref>. Even without additional training data and with the same underlying ResNet architecture with 101 layers, we outperform the previous state-of-the-art achieved by DeepLab. <ref type="bibr" target="#b42">[43]</ref> is a segmentation dataset that contains around 10, 000 RGB-D indoor images and provides pixel labeling masks for 37 classes. Results are shown in Tab. 7. Our method outperforms all existing methods by a large margin across all evaluation metrics, even though we do not make use of the depth information for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUN-RGBD</head><p>ADE20K MIT <ref type="bibr" target="#b47">[48]</ref> is a newly released dataset for scene parsing which provides dense labels of 150 classes on more than 20K scene images. The categories include a large variety of objects (e.g., person, car, etc.) and stuff (e.g., sky, road, etc.). The provided validation set consisting of 2000 images is used for quantitative evaluation. Results are shown in <ref type="table">Table 8</ref>. Our method clearly outperforms the baseline methods described in <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Variants of cascaded RefineNet</head><p>As discussed earlier, our RefineNet is flexible in that it can be cascaded in various manners for generating various architectures. Here, we discuss several variants of The evaluation in Tab. 9 demonstrates that the 4-cascaded version yields better performance than the 2-cascaded and 1-cascaded version, and using 2-scale image input with 2 ResNet is better than using 1-scale input. This is expected due to the larger capacity of the network. However, it also results in longer training time. Hence, we resort to using the single-scale 4-cascaded version as the standard architecture in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented RefineNet, a novel multi-path refinement network for semantic segmentation and object parsing. The cascaded architecture is able to effectively combine high-level semantics and low-level features to produce high-resolution segmentation maps. Our design choices are inspired by the idea of identity mapping which facilitates gradient propagation across long-range connections and thus enables effective end-to-end learning. Experiments show that our method sets a new mark for the state of the art in semantic labeling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Example results of our method on the task of object parsing (left) and semantic segmentation (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 1</head><label>1</label><figDesc>Code: https://github.com/guosheng/refinenet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Our prediction examples on Person-Parts dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Segmentation examples on PASCAL VOC 2012.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Our prediction examples on the Cityscapes dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Object parsing results on the Person-Part dataset. Our method achieves the best performance (bold).Table 2. Ablation experiments on NYUDv2 and Person-Part.</figDesc><table>Method IoU 
Method IoU 
Attention [7] 56.4 
HAZN [45] 57.5 
LG-LSTM [29] 58.0 
Graph-LSTM [28] 60.2 
DeepLab [5] 62.8 DeepLab-v2 (Res101) [6] 64.9 
RefineNet-Res101 (ours) 68.6 

Initialization Chained pool. Msc Eva NYUDv2 Person-Parts 
ResNet-50 
no 
no 
40.4 
64.1 
ResNet-50 
yes 
no 
42.5 
65.7 
ResNet-50 
yes 
yes 
43.8 
67.1 
ResNet-101 
yes 
no 
43.6 
67.6 
ResNet-101 
yes 
yes 
44.7 
68.6 
ResNet-152 
yes 
yes 
46.5 
68.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Segmentation results on NYUDv2 (40 classes). 

method training data pixel acc. mean acc. IoU 
Gupta et al. [20] 
RGB-D 
60.3 
-
28.6 
FCN-32s [36] 
RGB 
60.0 
42.2 
29.2 
FCN-HHA [36] 
RGB-D 
65.4 
46.1 
34.0 
Context [30] 
RGB 
70.0 
53.6 
40.6 
RefineNet-Res152 
RGB 
73.6 
58.9 
46.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Results on the PASCAL VOC 2012 test set (IoU scores). Our RefineNet achieves the best overall performance (IoU 83.4).</figDesc><table>Method 
a e r o 
b i k e 
b i r d 
b o a t 
b o t t l e 
b u s 
c a r 
c a t 
c h a i r 
c o w 
t a b l e 
d o g 
h o r s e 
m 
b i k e 
p e r s o n 
p o t t e d 
s h e e p 
s o f a 
t r a i n 
t v 
mean 
FCN-8s [36] 76.8 34.2 68.9 49.4 60.3 75.3 74.7 77.6 21.4 62.5 46.8 71.8 63.9 
76.5 
73.9 
45.2 
72.4 37.4 70.9 55.1 
62.2 
DeconvNet [38] 89.9 39.3 79.7 63.9 68.2 87.4 81.2 86.1 28.5 77.0 62.0 79.0 80.3 
83.6 
80.2 
58.8 
83.4 54.3 80.7 65.0 
72.5 
CRF-RNN [47] 90.4 55.3 88.7 68.4 69.8 88.3 82.4 85.1 32.6 78.5 64.4 79.6 81.9 
86.4 
81.8 
58.6 
82.4 53.5 77.4 70.1 
74.7 
BoxSup [10] 89.8 38.0 89.2 68.9 68.0 89.6 83.0 87.7 34.4 83.6 67.1 81.5 83.7 
85.2 
83.5 
58.6 
84.9 55.8 81.2 70.7 
75.2 
DPN [35] 89.0 61.6 87.7 66.8 74.7 91.2 84.3 87.6 36.5 86.3 66.1 84.4 87.8 
85.6 
85.4 
63.6 
87.3 61.3 79.4 66.4 
77.5 
Context [30] 94.1 40.7 84.1 67.8 75.9 93.4 84.3 88.4 42.5 86.4 64.7 85.4 89.0 
85.8 
86.0 
67.5 
90.2 63.8 80.9 73.0 
78.0 
DeepLab [5] 89.1 38.3 88.1 63.3 69.7 87.1 83.1 85.0 29.3 76.5 56.5 79.8 77.9 
85.8 
82.4 
57.4 
84.3 54.9 80.5 64.1 
72.7 
DeepLab2-Res101 [6] 92.6 60.4 91.6 63.4 76.3 95.0 88.4 92.6 32.7 88.5 67.6 89.6 92.1 
87.0 
87.4 
63.3 
88.3 60.0 86.8 74.5 
79.7 
CSupelec-Res101 [4] 92.9 61.2 91.0 66.3 77.7 95.3 88.9 92.4 33.8 88.4 69.1 89.8 92.9 
87.7 
87.5 
62.6 
89.9 59.2 87.1 74.2 
80.2 
RefineNet-Res101 94.9 60.2 92.8 77.5 81.5 95.0 87.4 93.3 39.6 89.3 73.0 92.7 92.4 
85.4 
88.3 
69.7 
92.2 65.3 84.2 78.7 
82.4 
RefineNet-Res152 94.7 64.3 94.9 74.9 82.9 95.1 88.5 94.7 45.5 91.4 76.3 90.6 91.8 
88.1 
88.0 
69.9 
92.3 65.9 88.7 76.8 
83.4 

(a) Test Image 
(b) Ground Truth 
(c) Prediction 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 .</head><label>5</label><figDesc>Segmentation results on the Cityscapes test set.</figDesc><table>Method IoU 
Method IoU 
FCN-8s [36] 65.3 
DPN [35] 66.8 
Dilation10 [46] 67.1 
Context [30] 71.6 
DeepLab [5] 63.1 
DeepLab-v2(Res101) [6] 70.4 
LRR-4x [17] 71.8 RefineNet-Res101 (ours) 73.6 

(a) Test Image 
(b) Ground Truth 
(c) Prediction 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Segmentation results on PASCAL-Context dataset (60 classes). DeepLab-v2 uses extra training data from COCO (∼100K images), while we only use the VOC training images.Res101 (ours) 47.1 RefineNet-Res152 (ours) 47.3Segmentation results on SUN-RGBD dataset (37 classes). Our RefineNet significantly outperforms the existing methods. Method Train data Pixel acc. Mean acc. IoU Liu et al.</figDesc><table>Method IoU 
Method IoU 
O2P [3] 18.1 
CFM [11] 34.4 
FCN-8s [36] 35.1 
BoxSup [10] 40.5 
HO-CRF [1] 41.3 
Context [30] 43.3 
DeepLab-v2(Res101) [6] 45.7 
− 
− 
RefineNet-[32] 
RGB-D 
− 
10.0 
− 
Ren et al. [39] 
RGB-D 
− 
36.3 
− 
Kendall et al. [26] 
RGB 
71.2 
45.9 
30.7 
Context [30] 
RGB 
78.4 
53.4 
42.3 
RefineNet-Res101 
RGB 
80.4 
57.8 
45.7 
RefineNet-Res152 
RGB 
80.6 
58.5 
45.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 8 .Table 9 .</head><label>89</label><figDesc>Segmentation results on the ADE20K dataset (150 classes) val set. our method achieves the best performance.Cascaded-DilatedNet [48] 34.9 − − RefineNet-Res101 (ours) 40.2 RefineNet-Res152 (ours) 40.7Evaluation of 4 types of cascaded RefineNet on NYUDv2.our RefineNet, as listed in Tab. 9. The single RefineNet model consists of only one single RefineNet block, which takes all four inputs from the four blocks of ResNet and fuses all-resolution feature maps in a single process. The 2-cascaded version is similar our main model (4-cascaded) from Fig. 2(c), but employs only two RefineNet modules instead of four. For the 2-scale model, the input image is scaled to a factor of 1.2 and 0.6 and fed into 2 independent 4-cascaded ResNets. The detailed architectures of these variants are described in the supplementary document.</figDesc><table>Method IoU 
Method IoU 
FCN-8s [36] 29.4 
SegNet [2] 21.6 
DilatedNet [5, 46] 32.3 
Cascaded-SegNet [48] 27.5 
Variant Initialization Msc Eva IoU 
single 
ResNet-50 
no 
40.3 
2-cascaded 
ResNet-50 
no 
40.9 
4-cascaded 
ResNet-50 
no 
42.5 
4-cascaded 2-scale 
ResNet-50 
no 
43.1 

3 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The result link to the VOC evaluation server: http://host. robots.ox.ac.uk:8080/anonymous/B3XPSK.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://arxiv.org/abs/1611.06612</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments We are extremely grateful to the Australian Research Council for funding this research through the ARC Centre for Robotic Vision CE140100016. C. Shen's participation was supported by an ARC Future Fellowship (FT120100969). I. Reid's participation was supported by an ARC Laureate Fellowship (FL130100102).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Higher order conditional random fields in deep neural networks. In European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic segmentation with second-order pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast, exact and multi-scale inference for semantic image segmentation with deep gaussian crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. CoRR, abs/1606.00915</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03339</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detect what you can: Detecting and representing objects using holistic models and body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1971" to="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BoxSup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Restoring an image taken through a window covered with dirt or rain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Bayesian segnet: Model uncertainty in deep convolutional encoderdecoder architectures for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>abs/1511.02680</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with Gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Semantic object parsing with graph lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07063</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Semantic object parsing with local-global long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04510</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sift flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rgb-(d) scene labeling: Features and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention</title>
		<editor>N. Navab, J. Hornegger, W. M. Wells, and A. F. Frangi, editors</editor>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="234" to="241" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">MatConvNet -convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Zoom better to see clearer: Human and object parsing with hierarchical auto-zoom net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06881</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In Proceedings of the IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Semantic understanding of scenes through the ADE20K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno>abs/1608.05442</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
