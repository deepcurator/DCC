<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Differentiable Learning of Logical Rules for Knowledge Base Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
							<email>fanyang1@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
							<email>zhiliny@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
							<email>wcohen@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Differentiable Learning of Logical Rules for Knowledge Base Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We study the problem of learning probabilistic first-order logical rules for knowledge base reasoning. This learning problem is difficult because it requires learning the parameters in a continuous space as well as the structure in a discrete space. We propose a framework, Neural Logic Programming, that combines the parameter and structure learning of first-order logical rules in an end-to-end differentiable model. This approach is inspired by a recently-developed differentiable logic called TensorLog [5], where inference tasks can be compiled into sequences of differentiable operations. We design a neural controller system that learns to compose these operations. Empirically, our method outperforms prior work on multiple knowledge base benchmark datasets, including Freebase and WikiMovies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the structure is a discrete optimization problem, and one that involves search over a potentially large problem space. Many past learning systems have thus used optimization methods that interleave moves in a discrete structure space with moves in parameter space <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>In this paper, we explore an alternative approach: a completely differentiable system for learning models defined by sets of first-order rules. This allows one to use modern gradient-based programming frameworks and optimization methods for the inductive logic programming task. Our approach is inspired by a differentiable probabilistic logic called TensorLog <ref type="bibr" target="#b4">[5]</ref>. TensorLog establishes a connection between inference using first-order rules and sparse matrix multiplication, which enables certain types of logical inference tasks to be compiled into sequences of differentiable numerical operations on matrices. However, TensorLog is limited as a learning system because it only learns parameters, not rules. In order to learn parameters and structure simultaneously in a differentiable framework, we design a neural controller system with an attention mechanism and memory to learn to sequentially compose the primitive differentiable operations used by TensorLog. At each stage of the computation, the controller uses attention to "softly" choose a subset of TensorLog's operations, and then performs the operations with contents selected from the memory. We call our approach neural logic programming, or Neural LP.</p><p>Experimentally, we show that Neural LP performs well on a number of tasks. It improves the performance in knowledge base completion on several benchmark datasets, such as WordNet18 and Freebase15K <ref type="bibr" target="#b2">[3]</ref>. And it obtains state-of-the-art performance on Freebase15KSelected <ref type="bibr" target="#b24">[25]</ref>, a recent and more challenging variant of Freebase15K. Neural LP also performs well on standard benchmark datasets for statistical relational learning, including datasets about biomedicine and kinship relationships <ref type="bibr" target="#b11">[12]</ref>. Since good performance on many of these datasets can be obtained using short rules, we also evaluate Neural LP on a synthetic task which requires longer rules. Finally, we show that Neural LP can perform well in answering partially structured queries, where the query is posed partially in natural language. In particular, Neural LP also obtains state-of-the-art results on the KB version of the WIKIMOVIES dataset <ref type="bibr" target="#b15">[16]</ref> for question-answering against a knowledge base. In addition, we show that logical rules can be recovered by executing the learned controller on examples and tracking the attention.</p><p>To summarize, the contributions of this paper include the following. First, we describe Neural LP, which is, to our knowledge, the first end-to-end differentiable approach to learning not only the parameters but also the structure of logical rules. Second, we experimentally evaluate Neural LP on several types of knowledge base reasoning tasks, illustrating that this new approach to inductive logic programming outperforms prior work. Third, we illustrate techniques for visualizing a Neural LP model as logical rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Structure embedding <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29]</ref> has been a popular approach to reasoning with a knowledge base. This approach usually learns a embedding that maps knowledge base relations (e.g CityInCountry) and entities (e.g. USA) to tensors or vectors in latent feature spaces. Though our Neural LP system can be used for similar tasks as structure embedding, the methods are quite different. Structure embedding focuses on learning representations of relations and entities, while Neural LP learns logical rules. In addition, logical rules learned by Neural LP can be applied to entities not seen at training time. This is not achievable by structure embedding, since its reasoning ability relies on entity-dependent representations.</p><p>Neural LP differs from prior work on logical rule learning in that the system is end-to-end differentiable, thus enabling gradient based optimization, while most prior work involves discrete search in the problem space. For instance, Kok and Domingos <ref type="bibr" target="#b11">[12]</ref> interleave beam search, using discrete operators to alter a rule set, with parameter learning via numeric methods for rule confidences. Lao and Cohen <ref type="bibr" target="#b12">[13]</ref> introduce all rules from a restricted set, then use lasso-style regression to select a subset of predictive rules. Wang et al. <ref type="bibr" target="#b26">[27]</ref> use an Iterative Structural Gradient algorithm that alternate gradient-based search for parameters of a probabilistic logic ProPPR <ref type="bibr" target="#b25">[26]</ref>, with structural additions suggested by the parameter gradients.</p><p>Recent work on neural program induction <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b7">8]</ref> have used attention mechanism to "softly choose" differentiable operators, where the attentions are simply approximations to binary choices. The main difference in our work is that attentions are treated as confidences of the logical rules and have semantic meanings. In other words, Neural LP learns a distribution over logical rules, instead of an approximation to a particular rule. Therefore, we do not use hardmax to replace softmax during inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Knowledge base reasoning</head><p>Knowledge bases are collections of relational data of the format Relation (head, tail), where head and tail are entities and Relation is a binary relation between entities. Examples of such data tuple are HasOfficeInCity (New York, Uber) and CityInCountry (USA, New York).</p><p>The knowledge base reasoning task we consider here consists of a query 1 , an entity tail that the query is about, and an entity head that is the answer to the query. The goal is to retrieve a ranked list of entities based on the query such that the desired answer (i.e. head) is ranked as high as possible.</p><p>To reason over knowledge base, for each query we are interested in learning weighted chain-like logical rules of the following form, similar to stochastic logic programs <ref type="bibr" target="#b18">[19]</ref>,</p><formula xml:id="formula_0">α query (Y, X)←R n (Y, Z n ) ∧ · · · ∧ R 1 (Z 1 , X)<label>(1)</label></formula><p>where α ∈ [0, 1] is the confidence associated with this rule, and R 1 , . . . , R n are relations in the knowledge base. During inference, given an entity x, the score of each y is defined as sum of the confidence of rules that imply query (y, x), and we will return a ranked list of entities where higher the score implies higher the ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TensorLog for KB reasoning</head><p>We next introduce TensorLog operators and then describe how they can be used for KB reasoning. Given a knowledge base, let E be the set of all entities and R be the set of all binary relations. We map all entities to integers, and each entity i is associated with a one-hot encoded vector v i ∈ {0, 1} |E| such that only the i-th entry is 1. TensorLog defines an operator M R for each relation R. Concretely, M R is a matrix in {0, 1} |E|×|E| such that its (i, j) entry is 1 if and only if R (i, j) is in the knowledge base, where i is the i-th entity and similarly for j.</p><p>We now draw the connection between TensorLog operations and a restricted case of logical rule inference. Using the operators described above, we can imitate logical rule inference R (Y, X)← P (Y, Z) ∧ Q (Z, X) for any entity X = x by performing matrix multiplications</p><formula xml:id="formula_1">M P · M Q · v x . = s.</formula><p>In other words, the non-zero entries of the vector s equals the set of y such that there exists z that P (y, z) and Q (z, x) are in the KB. Though we describe the case where rule length is two, it is straightforward to generalize this connection to rules of any length.</p><p>Using TensorLog operations, what we want to learn for each query is shown in Equation 2,</p><formula xml:id="formula_2">l α l Π k∈β l M Rk (2)</formula><p>where l indexes over all possible rules, α l is the confidence associated with rule l and β l is an ordered list of all relations in this particular rule. During inference, given an entity v x , the score of each retrieved entity is then equivalent to the entries in the vector s, as shown in Equation 3.</p><formula xml:id="formula_3">s = l (α l (Π k∈β l M Rk v x )) , score(y | x) = v T y s<label>(3)</label></formula><p>To summarize, we are interested in the following learning problem for each query.</p><formula xml:id="formula_4">max {α l ,β l } {x,y} score(y | x) = max {α l ,β l } {x,y} v T y l (α l (Π k∈β l M Rk v x ))<label>(4)</label></formula><p>where {x, y} are entity pairs that satisfy the query, and {α l , β l } are to be learned. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning the logical rules</head><p>We will now describe the differentiable rule learning process, including learnable parameters and the model architecture. As shown in Equation 2, for each query, we need to learn the set of rules that imply it and the confidences associated with these rules. However, it is difficult to formulate a differentiable process to directly learn the parameters and the structure {α l , β l }. This is because each parameter is associated with a particular rule, and enumerating rules is an inherently discrete task. To overcome this difficulty, we observe that a different way to write Equation 2 is to interchange the summation and product, resulting the following formula with a different parameterization,</p><formula xml:id="formula_5">T t=1 |R| k a k t M Rk<label>(5)</label></formula><p>where T is the max length of rules and |R| is the number of relations in the knowledge base. The key parameterization difference between Equation 2 and Equation 5 is that in the latter we associate each relation in the rule with a weight. This combines the rule enumeration and confidence assignment.</p><p>However, the parameterization in <ref type="table" target="#tab_4">Equation 5</ref> is not sufficiently expressive, as it assumes that all rules are of the same length. We address this limitation in Equation 6-8, where we introduce a recurrent formulation similar to Equation 3.</p><p>In the recurrent formulation, we use auxiliary memory vectors u t . Initially the memory vector is set to the given entity v x . At each step as described in <ref type="bibr">Equation 7</ref>, the model first computes a weighted average of previous memory vectors using the memory attention vector b t . Then the model "softly" applies the TensorLog operators using the operator attention vector a t . This formulation allows the model to apply the TensorLog operators on all previous partial inference results, instead of just the last step's.</p><formula xml:id="formula_6">u 0 = v x (6) u t = |R| k a k t M Rk t−1 τ =0 b τ t u τ for 1 ≤ t ≤ T (7) u T+1 = T τ =0 b τ T +1 u τ<label>(8)</label></formula><p>Finally, the model computes a weighted average of all memory vectors, thus using attention to select the proper rule length. Given the above recurrent formulation, the learnable parameters for each query are {a t | 1 ≤ t ≤ T } and {b t | 1 ≤ t ≤ T + 1}.</p><p>We now describe a neural controller system to learn the operator and memory attention vectors. We use recurrent neural networks not only because they fit with our recurrent formulation, but also because it is likely that current step's attentions are dependent on previous steps'. At every step t ∈ [1, T + 1], the network predicts operator and memory attention vectors using Equation 9, 10, and 11. The input is the query for 1 ≤ t ≤ T and a special END token when t = T + 1.</p><formula xml:id="formula_7">h t = update (h t−1 , input) (9) a t = softmax (W h t + b) (10) b t = softmax [h 0 , . . . , h t−1 ] T h t (11)</formula><p>The system then performs the computation in Equation 7 and stores u t into the memory. The memory holds each step's partial inference results, i.e. {u 0 , . . . , u t , . . . , u T+1 }. <ref type="figure" target="#fig_0">Figure 2</ref> shows an overview of the system. The final inference result u is just the last vector in memory, i.e. u T+1</p><note type="other">. As discussed in Equation 4, the objective is to maximize v T y u. In particular, we maximize log v T y u because the nonlinearity empirically improves the optimization performance. We also observe that normalizing the memory vectors (i.e. u t ) to have unit length sometimes improves the optimization. To recover logical rules from the neural controller system, for each query we can write rules and their confidences {α l , β l } in terms of the attention vectors {a t , b t }. Based on the relationship between Equation 3 and Equation 6-8, we can recover rules by following Equation 7 and keep track of the coefficients in front of each matrix M Rk . The detailed procedure is presented in Algorithm 1. Algorithm 1 Recover logical rules from attention vectors Input: attention vectors {a t | t = 1, . . . , T } and {b t | t = 1, . . . , T + 1} Notation: Let R t = {r 1 , . . . , r l } be the set of partial rules at step t. Each rule r l is represented by a pair of (α, β) as described in Equation 1, where α is the confidence and β is an ordered list of relation indexes. Initialize: R 0 = {r 0 } where r 0 = (1, ( )). for t ← 1 to T + 1 do</note><p>Initialize: R t = ∅, a placeholder for storing intermediate results.</p><formula xml:id="formula_8">for τ ← 0 to t − 1 do for rule (α, β) in R τ do Update α ← α · b τ t . Store the updated rule (α , β) in R t . if t ≤ T then Initialize: R t = ∅ for rule (α, β) in R t do for k ← 1 to |R| do Update α ← α · a k t , β ← β append k. Add the updated rule (α , β ) to R t . else R t = R t return R T +1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To test the reasoning ability of Neural LP, we conduct experiments on statistical relation learning, grid path finding, knowledge base completion, and question answering against a knowledge base. For all the tasks, the data used in the experiment are divided into three files: facts, train, and test. The facts file is used as the knowledge base to construct TensorLog operators {M Rk | R k ∈ R}. The train and test files contain query examples query (head, tail). Unlike in the case of learning embeddings, we do not require the entities in train and test to overlap, since our system learns rules that are entity independent.</p><p>Our system is implemented in TensorFlow and can be trained end-to-end using gradient methods. The recurrent neural network used in the neural controller is long short-term memory <ref type="bibr" target="#b8">[9]</ref>, and the hidden state dimension is 128. The optimization algorithm we use is mini-batch ADAM <ref type="bibr" target="#b10">[11]</ref> with batch size 64 and learning rate initially set to 0.001. The maximum number of training epochs is 10, and validation sets are used for early stopping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Statistical relation learning</head><p>We conduct experiments on two benchmark datasets <ref type="bibr" target="#b11">[12]</ref> in statistical relation learning. The first dataset, Unified Medical Language System (UMLS), is from biomedicine. The entities are biomedical concepts (e.g. disease, antibiotic) and relations are like treats and diagnoses. The second dataset, Kinship, contains kinship relationships among members of the Alyawarra tribe from Central Australia <ref type="bibr" target="#b5">[6]</ref>. Datasets statistics are shown in <ref type="table" target="#tab_0">Table 1</ref>. We randomly split the datasets into facts, train, test files as described above with ratio 6:2:1. The evaluation metric is Hits@10. Experiment results are shown in <ref type="table" target="#tab_1">Table 2</ref>. Comparing with Iterative Structural Gradient (ISG) <ref type="bibr" target="#b26">[27]</ref>, Neural LP achieves better performance on both datasets. <ref type="bibr" target="#b1">2</ref> We conjecture that this is mainly because of the optimization strategy used in Neural LP, which is end-to-end gradient-based, while ISG's optimization alternates between structure and parameter search.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Grid path finding</head><p>Since in the previous tasks the rules learned are of length at most three, we design a synthetic task to test if Neural LP can learn longer rules. The experiment setup includes a knowledge base that contains location information about a 16 by 16 grid, such as North ((1,2), (1,1)) and SouthEast ((0,2), (1,1)) The query is randomly generated by combining a series of directions, such as North_SouthWest. The train and test examples are pairs of start and end locations, which are generated by randomly choosing a location on the grid and then following the queries. We classify the queries into four classes based on the path length (i.e. Hamming distance between start and end), ranging from two to ten. <ref type="figure" target="#fig_1">Figure 3</ref> shows inference accuracy of this task for learning logical rules using ISG <ref type="bibr" target="#b26">[27]</ref> and Neural LP. As the path length and learning difficulty increase, the results show that Neural LP can accurately learn rules of length 6-8 for this task, and is more robust than ISG in terms of handling longer rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Knowledge base completion</head><p>We also conduct experiments on the canonical knowledge base completion task as described in <ref type="bibr" target="#b2">[3]</ref>. In this task, the query and tail are part of a missing data tuple, and the goal is to retrieve the related head. For example, if HasOfficeInCountry (USA, Uber) is missing from the knowledge base, then the goal is to reason over existing data tuples and retrieve USA when presented with query HasOfficeInCountry and Uber. To represent the query as a continuous input to the neural controller, we jointly learn an embedding lookup table for each query. The embedding has dimension 128 and is randomly initialized to unit norm vectors.</p><p>The knowledge bases in our experiments are from WordNet <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b9">10]</ref> and Freebase <ref type="bibr" target="#b1">[2]</ref>. We use the datasets WN18 and FB15K, which are introduced in <ref type="bibr" target="#b2">[3]</ref>. We also considered a more challenging dataset, FB15KSelected <ref type="bibr" target="#b24">[25]</ref>, which is constructed by removing near-duplicate and inverse relations from FB15K. We use the same train/validation/test split as in prior work and augment data files with reversed data tuples, i.e. for each relation, we add its inverse inv_relation. In order to create a facts file which will be used as the knowledge base, we further split the original train file into facts and train with ratio 3:1. <ref type="bibr" target="#b2">3</ref> The dataset statistics are summarized in <ref type="table" target="#tab_2">Table 3</ref>. The attention vector at each step is by default applied to all relations in the knowledge base. Sometimes this creates an unnecessarily large search space. In our experiment on FB15K, we use a subset of operators for each query. The subsets are chosen by including the top 128 relations that share common entities with the query. For all datasets, the max rule length T is 2.</p><p>The evaluation metrics we use are Mean Reciprocal Rank (MRR) and Hits@10. MRR computes an average of the reciprocal rank of the desired entities. Hits@10 computes the percentage of how many desired entities are ranked among top ten. Following the protocol in Bordes et al. <ref type="bibr" target="#b2">[3]</ref>, we also use filtered rankings. We compare the performance of Neural LP with several models, summarized in <ref type="table" target="#tab_3">Table 4</ref>. Neural LP gives state-of-the-art results on WN18, and results that are close to the state-of-the-art on FB15K. It has been noted <ref type="bibr" target="#b24">[25]</ref> that many relations in WN18 and FB15K have inverse also defined, which makes them easy to learn. FB15KSelected is a more challenging dataset, and on it, Neural LP substantially improves the performance over Node+LinkFeat <ref type="bibr" target="#b24">[25]</ref> and achieves similar performance as DISTMULT <ref type="bibr" target="#b28">[29]</ref> in terms of MRR. We note that in FB15KSelected, since the test entities are rarely directly linked in the knowledge base, the models need to reason explicitly about compositions of relations. The logical rules learned by Neural LP can very naturally capture such compositions.</p><p>Examples of rules learned by Neural LP are shown in <ref type="table" target="#tab_4">Table 5</ref>. The number in front each rule is the normalized confidence, which is computed by dividing by the maximum confidence of rules for each relation. From the examples we can see that Neural LP successfully combines structure learning and parameter learning. It not only induce multiple logical rules to capture the complex structure in the knowledge base, but also learn to distribute confidences on rules.</p><p>To demonstrate the inductive learning advantage of Neural LP, we conduct experiments where training and testing use disjoint sets of entities. To create such setting, we first randomly select a subset of the test tuples to be the test set. Secondly, we filter the train set by excluding any tuples that share entities with selected test tuples. <ref type="table" target="#tab_5">Table 6</ref> shows the experiment results in this inductive setting.   As expected, the inductive setting results in a huge decrease in performance for the TransE model <ref type="bibr" target="#b3">4</ref> , which uses a transductive learning approach; for all three datasets, Hits@10 drops to near zero, as one could expect. In contrast, Neural LP is much less affected by the amount of unseen entities and achieves performance at the same scale as the non-inductive setting. This emphasizes that our Neural LP model has the advantage of being able to transfer to unseen entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Question answering against knowledge base</head><p>We also conduct experiments on a knowledge reasoning task where the query is "partially structured", as the query is posed partially in natural language. An example of a partially structured query would be "in which country does x has an office" for a given entity x, instead of HasOfficeInCountry (Y, x). Neural LP handles queries of this sort very naturally, since the input to the neural controller is a vector which can encode either a structured query or natural language text.</p><p>We use the WIKIMOVIES dataset from Miller et al. <ref type="bibr" target="#b15">[16]</ref>. The dataset contains a knowledge base and question-answer pairs. Each question (i.e. the query) is about an entity and the answers are sets of entities in the knowledge base. There are 196,453 train examples and 10,000 test examples. The knowledge base has 43,230 movie related entities and nine relations. A subset of the dataset is shown in <ref type="table" target="#tab_6">Table 7</ref>. We process the dataset to match the input format of Neural LP. For each question, we identity the tail entity by checking which words match entities in the knowledge base. We also filter the words in the question, keeping only the top 100 frequent words. The length of each question is limited to six words. To represent the query in natural language as a continuous input for the neural controller, we jointly learn a embedding lookup table for all words appearing in the query. The query representation is computed as the arithmetic mean of the embeddings of the words in it.</p><p>We compare Neural LP with several embedding based QA models. The main difference between these methods and ours is that Neural LP does not embed the knowledge base, but instead learns to compose operators defined on the knowledge base. The comparison is summarized in <ref type="table" target="#tab_7">Table 8</ref>. Experiment results are extracted from Miller et al. <ref type="bibr" target="#b15">[16]</ref>.   <ref type="bibr" target="#b27">[28]</ref>. QA system is from <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Accuracy</head><p>Memory Network 78.5 QA system 93.5 Key-Value Memory Network <ref type="bibr" target="#b15">[16]</ref> 93.9 Neural LP 94.6</p><p>To visualize the learned model, we randomly sample 650 questions from the test dataset and compute the embeddings of each question. We use tSNE <ref type="bibr" target="#b14">[15]</ref> to reduce the embeddings to the two dimensional space and plot them in <ref type="figure" target="#fig_3">Figure 4</ref>. Most learned logical rules consist of one relation from the knowledge base, and we use different colors to indicate the different relations and label some clusters by relation.</p><p>The experiment results show that Neural LP can successfully handle queries that are posed in natural language by jointly learning word representations as well as the logical rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We present an end-to-end differentiable method for learning the parameters as well as the structure of logical rules for knowledge base reasoning. Our method, Neural LP, is inspired by a recent probabilistic differentiable logic TensorLog <ref type="bibr" target="#b4">[5]</ref>. Empirically Neural LP improves performance on several knowledge base reasoning datasets. In the future, we plan to work on more problems where logical rules are essential and complementary to pattern recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The neural controller system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Accuracy on grid path finding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>1.00 partially_contains(C, A) ← contains (B, A) ∧ contains (B, C) 0.45 partially_contains(C, A) ← contains (A, B) ∧ contains (B, C) 0.35 partially_contains(C, A) ← contains (C, B) ∧ contains (B, A) 1.00 marriage_location (C, A) ← nationality (C, B) ∧ contains (B, A) 0.35 marriage_location (B, A) ← nationality (B, A) 0.24 marriage_location (C, A) ← place_lived (C, B) ∧ contains (B, A) 1.00 film_edited_by (B, A)←nominated_for (A, B) 0.20 film_edited_by (C, A)←award_nominee (B, A) ∧ nominated_for (B, C)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of learned logical rules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Datasets statistics.</figDesc><table># Data # Relation # Entity 

UMLS 
5960 
46 
135 
Kinship 
9587 
25 
104 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Experiment results. T indicates the maxi- mum rule length.</figDesc><table>ISG 
Neural LP 

T = 2 T = 3 T = 2 T = 3 

UMLS 
43.5 
43.3 
92.0 
93.2 
Kinship 
59.2 
59.0 
90.2 
90.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Knowledge base completion datasets statistics.</figDesc><table>Dataset 
# Facts 
# Train 
# Test # Relation # Entity 

WN18 
106,088 
35,354 
5,000 
18 
40,943 
FB15K 
362,538 120,604 59,071 
1,345 
14,951 
FB15KSelected 204,087 
68,028 20,466 
237 
14,541 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Knowledge base completion performance comparison. TransE [4] and Neural Tensor 
Network [24] results are extracted from [29]. Results on FB15KSelected are from [25]. 

WN18 
FB15K 
FB15KSelected 

MRR Hits@10 MRR Hits@10 MRR Hits@10 

Neural Tensor Network 0.53 
66.1 
0.25 
41.4 
-
-
TransE 
0.38 
90.9 
0.32 
53.9 
-
-
DISTMULT [29] 
0.83 
94.2 
0.35 
57.7 
0.25 
40.8 
Node+LinkFeat [25] 
0.94 
94.3 
0.82 
87.0 
0.23 
34.7 
Implicit ReasoNets [23] -
95.3 
-
92.7 
-
-
Neural LP 
0.94 
94.5 
0.76 
83.7 
0.24 
36.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Examples of logical rules learned by Neural LP on FB15KSelected. The letters A,B,C are ungrounded logic variables.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 :</head><label>6</label><figDesc>Inductive knowledge base completion. The metric is Hits@10.</figDesc><table>WN18 FB15K FB15KSelected 

TransE 
0.01 
0.48 
0.53 
Neural LP 
94.49 
73.28 
27.97 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 7 :</head><label>7</label><figDesc>A subset of the WIKIMOVIES dataset. Knowledge base directed_by (Blade Runner, Ridley Scott) written_by (Blade Runner, Philip K. Dick) starred_actors (Blade Runner, Harrison Ford) starred_actors (Blade Runner, Sean Young) Questions What year was the movie Blade Runner released? Who is the writer of the film Blade Runner?</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 8 :</head><label>8</label><figDesc>Performance comparison. Memory Net- work is from</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this work, the notion of query refers to relations, which differs from conventional notion, where query usually contains relation and entity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use the implementation of ISG available at https://github.com/TeamCohen/ProPPR. In Wang et al. [27], ISG is compared with other statistical relational learning methods in a different experiment setup, and ISG is superior to several methods including Markov Logic Networks [12].</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We also make minimal adjustment to ensure that all query relations in test appear at least once in train and all entities in train and test are also in facts. For FB15KSelected, we also ensure that entities in train are not directly linked in facts.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use the implementation of TransE available at https://github.com/thunlp/KB2E.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was funded by NSF under IIS1250956 and by Google Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1545" to="1554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Question answering with subgraph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.3676</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William W Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06523</idno>
		<title level="m">Tensorlog: A differentiable deductive database</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The detection of patterns in Alyawara nonverbal behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Woodrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Denham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
		</imprint>
		<respStmt>
			<orgName>University of Washington, Seattle.</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Introduction to statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabskabarwińska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Wordnet: An electronic lexical database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical predicate invention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relational retrieval using a combination of path-constrained random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William W Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="67" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random walk inference and learning in a large scale knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Key-value memory networks for directly reading documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03126</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Muggleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Otero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Tamaddoni-Nezhad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stochastic logic programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Muggleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in inductive logic programming</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="254" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Neural programmer: Inducing latent programs with gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04834</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning a natural language interface with neural programmer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08945</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="107" to="136" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Implicit reasonet: Modeling large-scale structured relationships with shared memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04642</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Programming with personalized pagerank: a locally groundable first-order probabilistic logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM international conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2129" to="2138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Structure learning via parameter learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM 2014</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
