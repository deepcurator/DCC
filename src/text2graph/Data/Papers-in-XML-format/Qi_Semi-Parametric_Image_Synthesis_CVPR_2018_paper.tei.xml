<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-parametric Image Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Intel Labs</orgName>
								<orgName type="institution" key="instit2">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuhk</forename><forename type="middle">Qifeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Intel Labs</orgName>
								<orgName type="institution" key="instit2">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><forename type="middle">Jia</forename><surname>Cuhk</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Intel Labs</orgName>
								<orgName type="institution" key="instit2">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Intel Labs</orgName>
								<orgName type="institution" key="instit2">Intel Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-parametric Image Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We present a semi-parametric approach to photographic image synthesis from semantic layouts. </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zeuxis having painted a child carrying grapes, the birds came to peck at them; upon which [...] he expressed himself vexed with his work, and exclaimed -"I have surely painted the grapes better than the child, for if I had fully succeeded in the last, the birds would have been in fear of it."</head><p>-Pliny the Elder, The Natural History, 79 AD Photographic image synthesis by deep networks can open a new route to photorealism: a problem that has traditionally been approached via explicit manual modeling of three-dimensional surface layout and reflectance distributions <ref type="bibr" target="#b23">[24]</ref>. A deep network that is capable of synthesizing photorealistic images given a rough specification could become a new tool in the arsenal of digital artists. It could also prove useful in the creation of AI systems, by endowing them with a form of visual imagination <ref type="bibr" target="#b18">[19]</ref>.</p><p>Recent progress in photographic image synthesis has been driven by parametric models -deep networks that represent all data concerning photographic appearance in their weights <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b1">2]</ref>. This is in contrast to the practices of human photorealistic painters, who do not draw purely on memory but use external references as source material for reproducing detailed object appearance <ref type="bibr" target="#b16">[17]</ref>. It is also in contrast to earlier work on image synthesis, which was based on nonparametric techniques that could draw on large datasets of images at test time <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b9">10]</ref>. In switching from nonparametric approaches to parametric ones, the research community gained the advantages of end-to-end training of highly expressive models. But it relinquished the ability to draw on large databases of original photographic content at test time: a strength of earlier nonparametric techniques.</p><p>In this paper, we present a semi-parametric approach to photographic image synthesis from semantic layouts. The presented approach exemplifies a general family of methods that we call semi-parametric image synthesis (SIMS). Semi-parametric synthesis combines the complementary strengths of parametric and nonparametric techniques. In the presented approach, the nonparametric component is a database of segments drawn from a training set of photographs with corresponding semantic layouts. At test time, given a novel semantic layout, the system retrieves compatible segments from the database. These segments are used as raw material for synthesis. They are composited onto a canvas with the aid of deep networks that align the segments to the input layout and resolve occlusion relationships. The canvas is then processed by a deep network that produces a photographic image as output.</p><p>We conduct experiments on the Cityscapes, NYU, and ADE20K datasets. The experimental results indicate that images produced by SIMS are considerably more realistic than the output of purely parametric models for photographic image synthesis from semantic layouts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recent work on conditional image synthesis is predominantly based on parametric models <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b36">37]</ref>. Most related to ours are the works of Isola et al. <ref type="bibr" target="#b10">[11]</ref> and Chen and Koltun <ref type="bibr" target="#b1">[2]</ref>. Isola et al. propose a general framework for image-to-image translation based on adversarial training. This approach can be applied to synthesize images from semantic layouts. Chen and Koltun propose a direct approach to synthesizing high-resolution images conditioned on semantic layouts. Their method does not rely on adversarial training, but rather trains a convolutional network directly with a perceptual loss. Our approach differs from all of these in that a memory bank of object segments is utilized at test time as source material for synthesis. Synthesis is performed by a deep network, but is based on exemplars of object appearance retrieved from the memory bank. <ref type="figure" target="#fig_0">Figure 1</ref> provides a qualitative comparison.</p><p>Nonparametric methods for image synthesis have a long history and were dominant before the ascendance of purely parametric techniques. Hays and Efros <ref type="bibr" target="#b6">[7]</ref> used a collection of images as source material for image completion. At test time, similar images are retrieved via descriptor matching and are used to inpaint missing regions. Lalonde et al. <ref type="bibr" target="#b14">[15]</ref> developed an interactive system that retrieves object segments from a large library of images. The retrieved segments are interactively composited onto an image. Chen et al. <ref type="bibr" target="#b2">[3]</ref> described a system that synthesized an image from a freehand sketch with associated text labels. Given a sketch and associated text, their system retrieves relevant images from the Web, segments them, and composes an output image with interactive assistance by the user. Johnson et al. <ref type="bibr" target="#b12">[13]</ref> described a related system for post-processing computer-generated images. Isola and Liu <ref type="bibr" target="#b9">[10]</ref> presented an analysis-by-synthesis approach that retrieves object segments that match a query image and combines these segments to form a "scene collage" that explains the query. Our research is inspired by this line of work and aims to reintroduce these earlier ideas into the current stream of image synthesis research. Unlike the earlier work, our approach combines nonparametric use of a database of image segments with deep parametric models that assist composition and perform synthesis based on the retrieved material.</p><p>Zhu et al. <ref type="bibr" target="#b39">[40]</ref> train a convolutional network to predict the realism of image composites. (See also the earlier work of Lalonde and Efros <ref type="bibr" target="#b13">[14]</ref> and Xue et al. <ref type="bibr" target="#b32">[33]</ref>.) Tsai et al. <ref type="bibr" target="#b30">[31]</ref> train a convolutional network to harmonize the appearance of image composites. In these works, the composites were assumed to be given, typically generated interactively by a human user. In contrast, our work develops a complete automatic pipeline for semi-parametric image synthesis from semantic layouts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview</head><p>Our goal is to synthesize a photorealistic image based on a semantic layout L ∈ {0, 1} h×w×c , where h×w is the image size and c is the number of semantic classes. Our model is trained on a set of paired color images and corresponding semantic layouts. This set is used to generate a memory bank M of image segments from different semantic categories. Segments are extracted from training images by taking connected components in corresponding semantic layouts. Each segment P i in M is a segment from a training color image, associated with a semantic class. A number of segments are shown in <ref type="figure" target="#fig_1">Figure 2(a,b)</ref>.</p><p>At test time, we are given a semantic label map L that was not seen during training. This label map is decomposed into connected components {L i }. For each connected component, we retrieve a compatible segment from M based on shape, location, and context ( <ref type="figure" target="#fig_1">Figure 2(b)</ref>). This retrieved segment is aligned to L i by a spatial transformer network trained for this purpose <ref type="bibr" target="#b11">[12]</ref>  <ref type="figure" target="#fig_1">(Figure 2(c,d)</ref>). The transformed segments are composited onto a canvas C ∈ R w×h×3 <ref type="figure" target="#fig_1">(Figure 2(f)</ref>). Since the segments may not align perfectly with the masks {L i }, they may overlap. Relative front-back order is determined by an ordering network ( <ref type="figure" target="#fig_1">Figure 2</ref>(e)). Boundaries of retrieved segments are deliberately elided. The composition of the canvas C is described in detail in Section 4.</p><p>The canvas C and the input layout L are used as input to a synthesis network f . This network synthesizes the final output image and is illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>. It inpaints missing regions, harmonizes retrieved segments, blends boundaries, synthesizes shadows, and otherwise adjusts and synthesizes photographic appearance based on the raw material in the canvas C and the target layout L. The architecture and training of the network f are described in Section 5.</p><p>To apply the presented approach to coarse input layouts, such as ones shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we train a cascaded refinement network to convert coarse incomplete layouts to dense pixelwise layouts <ref type="bibr" target="#b1">[2]</ref>. The network is trained on pairs of coarse and fine semantic layouts. At test time, given a coarse incomplete layout, the trained network synthesizes a dense semantic layout, which is then provided to the presented image synthesis pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">External Memory</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Representation</head><p>The memory bank M is a set of image segments {P i } extracted from the training data. Each segment corresponds to a maximal connected component in the semantic label map of one of the training images. A segment P i is associated with a tuple (P</p><formula xml:id="formula_0">color i , P mask i , P cont i ), where P color i ∈ R</formula><p>h×w×3 is a color image that contains the segment (other pixels are zeroed out),</p><formula xml:id="formula_1">P mask i ∈ {0, 1}</formula><p>h×w×c is a binary mask that specifies the segment's footprint, and P cont i ∈ {0, 1} h×w×c is a semantic map representing the semantic context around P i within a bounding box, obtained from the semantic label map that originally contained the segment. The bounding box that encloses the context region is obtained by computing the bounding box of P color i</p><p>and enlarging it by 25% in each dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Retrieval</head><p>Given a novel semantic layout L at test time, we compute L mask j and L cont j for each semantic segment L j , by analogy with the definitions provided in Section 4.1. Then for each segment L j in the test image L, we select the most compatible segment P σ(j) in M based on a similarity score:  where IoU is the intersection-over-union score, and i iterates over segments in M that have the same semantic class as L j . The first term (mask IoU) measures the overlap of the segment shapes. The second term (context IoU) measures the similarity of the surrounding semantic layout. The use of semantic context helps retrieve compatible segments when surrounding context affects appearance.</p><formula xml:id="formula_2">σ(j) = arg max i IoU(P mask i , L mask j ) + IoU(P cont i , L cont j ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transformation</head><p>The transformation network T is designed to transform the selected object segment P σ(j) to match L j via translation, rotation, scaling, and clipping. The transformation aims to align P σ(j) to L j while preserving the integrity of the object's appearance.</p><formula xml:id="formula_3">T (L, L mask j , P color σ(j) ) takes L, L mask j</formula><p>, and P color σ(j) as input and produces a transformed imageP σ(j) by applying a 2D affine transformation to P color σ(j) <ref type="bibr" target="#b11">[12]</ref>. We use a deep network rather than an analytical approach because a network can learn to preserve properties such as symmetry and upright orientation as needed, without hard-coding such properties as rules.</p><p>To train the network T , we need to generate segment pairs that will simulate the inconsistencies in shape, scale, and location that T encounters at test time. For this reason, simply training T to transform P color i to match P mask i , for segments P i ∈ M, does not work: the requisite transformation is trivial. We therefore simulate misalignments by applying random affine transformations and cropping to P . The training loss for T is</p><formula xml:id="formula_4">L T (θ T ) = Pi∈M P color i − T (P, P mask i ,P color i ; θ T ) 1 .</formula><p>This loss is defined over the color images rather than the mask because information in the color image is more specific and better constrains the transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Canvas</head><p>After selecting and transforming object segments for the semantic layout L, we composite these segments onto a single canvas image. LetP σ(j) be the transformed segment for L j . If all pairs (P σ(i) ,P σ(j) ) are disjoint, the canvas composition is trivial. IfP σ(i) andP σ(j) overlap, we need to determine their order, since one of them will occlude the other. For example, when a building segment overlaps a sky segment, the sky segment should be occluded.</p><p>We train an ordering network to determine the front-back ordering of adjacent object segments. The architecture of the ordering network is based on VGG-19 <ref type="bibr" target="#b29">[30]</ref>. Its output is a c-dimensional one-hot vector that indicates the semantic label of the segment that should be in front. When two segments overlap, we query the ordering network to determine their front-back order on the canvas C.</p><p>To train the network, we use the relative depth of adjacent semantic segments in the training set. This relative depth can be estimated from depth or stereo data provided with some datasets, such as Cityscapes and NYU <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref>. For datasets without such auxiliary information, such as ADE20K <ref type="bibr" target="#b38">[39]</ref>, we generate approximate depth maps using a network trained for this purpose <ref type="bibr" target="#b3">[4]</ref>. The approximate depth maps are used to determine the relative depth order of adjacent segments in the training data, which are in turn used to train the ordering network. The ordering network is trained with a cross-entropy loss.</p><p>The boundaries of segments in the canvas are elided as described in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Image Synthesis</head><p>The image synthesis network f takes as input the semantic layout L, the canvas C, and a binary mask that indicates missing pixels in the canvas. The canvas C provides raw material for synthesis, but is inadequate in itself: regions are typically missing, different segments are inconsistently illuminated and color-balanced, and a variety of boundary artifacts are apparent. Missing regions could be filled using an inpainting network <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b8">9]</ref>, but this does not address other artifacts that are present in the canvas. We therefore design and train a dedicated network that takes both the canvas and the target semantic layout into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Network architecture</head><p>The architecture of the synthesis network f is shown in <ref type="figure" target="#fig_3">Figure 3</ref>. The network has an encoder-decoder structure with skip connections. The encoder constructs a multiscale representation of the input (C, L). The decoder uses this representation to synthesize progressively finer feature maps, culminating in full-resolution output.</p><p>Encoder. Our encoder is based on VGG-19 <ref type="bibr" target="#b29">[30]</ref>. The input is a tensor that collates L and C. The network consists of five modules. Each module contains a number of convolutional layers <ref type="bibr" target="#b15">[16]</ref> with layer normalization <ref type="bibr" target="#b0">[1]</ref>, ReLU <ref type="bibr" target="#b19">[20]</ref>, and average pooling. The first module has two convolutional layers, while each of the other modules have three. Each element in the encoder's output tensor has a receptive field of approximately 276×276. The encoder can thus capture long-range correlations that can help the decoder harmonize color, lighting, and texture.</p><p>Decoder. Our decoder is based on the cascaded refinement network (CRN) <ref type="bibr" target="#b1">[2]</ref>. The network is a cascade of refinement modules. The input to each module is a concatenation of feature maps produced at the corresponding resolution by the encoder, feature maps produced by the preceding refinement module (if any), the canvas C (appropriately resized), and the semantic layout L (resized). Each refinement module contains two convolutional layers with layer normalization and Leaky ReLU <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Training</head><p>The image synthesis network f is trained using simulated canvases that are generated to mimic artifacts that are encountered at test time. Given a semantic layout L and a corresponding color image I from the training set, we generate a simulated canvas C ′ by applying stenciling, color transfer, and boundary elision to segments in (I, L). The network f is trained to take the pair (C ′ , L) and recover the original image I. Following <ref type="bibr" target="#b1">[2]</ref>, the network is trained using a perceptual loss based on feature activations in a pretrained VGG-19 network <ref type="bibr" target="#b29">[30]</ref>. The loss is</p><formula xml:id="formula_5">L f (θ f ) = (I,L)∈D l λ l Φ l (I) − Φ l (f (C ′ , L); θ f ) 1 ,</formula><p>where Φ l is the feature tensor in layer l, and the weights {λ l } balance the terms. We use 'conv1 2', 'conv2 2', 'conv3 2', 'conv4 2', and 'conv5 2' layers in the loss. We now review the generation of the simulated canvas C ′ , organized into a number of steps.</p><p>Stenciling. It is inevitable that the test-time canvas C will contain missing regions. Thus the network f must be trained on simulated canvases with realistic missing regions. We simulate missing regions by stenciling each segment in (I, L) using a mask obtained from a different segment in the dataset. Specifically, for each segment P j , we use the retrieval procedure described in Section 4.2 to retrieve a segment from a different image in the training set. The mask of that segment is then used to stencil P j . This is illustrated in <ref type="figure" target="#fig_5">Figure 4</ref> (b-e).</p><p>Color transfer. At test time, different segments composited onto the canvas will generally have inconsistent tone and illumination. To simulate these artifacts in the training canvas C ′ , we select 20% of the segments at random and  <ref type="table">Table 1</ref>. Results of blind randomized A/B tests. Each entry reports the percentage of comparisons in which an image synthesized by our approach (SIMS) was judged more realistic than a corresponding image synthesized by Pix2pix <ref type="bibr" target="#b10">[11]</ref> or the CRN <ref type="bibr" target="#b1">[2]</ref>. Chance is at 50%.</p><p>apply color transfer <ref type="bibr" target="#b26">[27]</ref>. Specifically, to modify the color distribution of a segment P j in C ′ , we randomly retrieve a segment P i with the same semantic class from M and transfer the color distribution from P i to P j . This is illustrated in <ref type="figure" target="#fig_5">Figure 4(f)</ref>. Boundary elision. The network f should also be trained to naturally blend object boundaries. To encourage this, we randomly mask out 80% of pixels within a distance of 0.05h from a segment boundary. These are replaced by white pixels. The network is thus forced to learn to synthesize content near boundaries. This masking of interior boundary regions is illustrated in <ref type="figure" target="#fig_5">Figure 4</ref>(g).</p><p>Furthermore, inconsistencies along boundaries arise not only inside segments, but also outside. Consider a car composited onto a road. A typical salient artifact is the absence of shadow beneath the car. To encourage the network to learn to synthesize such shadows and other near-range interobject effects, we also mask out pixels in C ′ that lie outside an object segment within a distance of 0.05h from its boundary. These are replaced by black pixels. The network f is forced to inpaint these exterior regions.</p><p>The same interior and exterior boundary elision steps are also applied at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>Datasets. We conduct experiments on three semantic segmentation datasets: Cityscapes <ref type="bibr" target="#b4">[5]</ref>, NYU <ref type="bibr" target="#b28">[29]</ref>, and ADE20K <ref type="bibr" target="#b38">[39]</ref>. The Cityscapes dataset contains images of urban street scenes. It provides 3K images with fine pixelwise annotations and 20K images with coarse incomplete annotations for training. We train models separately for the fine and coarse regimes, and test on the 500 images in the validation set, which have both fine and coarse label maps. For the NYU dataset, we train on the first 1200 images and test on the remaining 249 images in the dataset. For ADE20K, we use outdoor images from the dataset; this yields 10K images for training and 1K images for testing. Perceptual experiments. We adopt the experimental protocol of Chen and Koltun <ref type="bibr" target="#b1">[2]</ref>. The protocol is based on large batches of blind randomized A/B tests deployed on the Amazon Mechanical Turk platform. We compare the presented approach to Pix2pix <ref type="bibr" target="#b10">[11]</ref> and the CRN <ref type="bibr" target="#b1">[2]</ref>. <ref type="table">Table 1</ref> reports the results. Each entry in the table reports the percentage of comparisons in which an image synthesized by our approach (SIMS) was judged more realistic than a corresponding image synthesized by Pix2pix or the CRN. Models trained on Cityscapes are tested in three conditions: 'Cityscapes-coarse' for models trained and tested on coarse input layouts, 'Cityscapes-fine' for models trained and tested on fine input layouts, and Cityscapes→GTA5 for models trained on fine Cityscapes layouts and then applied to semantic label maps from the GTA5 dataset <ref type="bibr" target="#b27">[28]</ref>. (We use the 6K semantic layouts in the GTA5 validation set.) Note that chance is at 50%.</p><p>In all conditions, the presented approach outperforms the baselines. Across the five datasets, images synthesized by our approach were rated more realistic than images synthesized by Pix2pix and the CRN in 94% and 86% of comparisons, respectively.</p><p>We have also conducted time-limited pairwise comparisons, again following the protocol of Chen and Koltun <ref type="bibr" target="#b1">[2]</ref>. The results are reported in <ref type="figure">Figure 6</ref>. Here each comparison pairs an image synthesized by one of the approaches versus the real reference image for the same semantic layout. In this case 50% is the equivalent of passing the visual Turing test. While none of the approaches achieves this, images synthesized by SIMS are more frequently mistaken for real ones. For example, after 1 second, the preference rate for SIMS&gt;Real in the Cityscapes-coarse condition is 25.2%, versus 4.0% for CRN&gt;Real and 3.8% for Pix2pix&gt;Real. After 1 second in the Cityscapes-fine condition, the preference rate for SIMS&gt;Real is 27.8%, versus 15.2% for CRN&gt;Real and 1.9% for Pix2pix&gt;Real.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic segmentation accuracy.</head><p>Next we analyze the realism of synthesized images using a different protocol. Given a semantic layout L, we use one of the evaluated approaches to synthesize an image I. This image is then given as input to a pretrained semantic segmentation network. (We use the PSPNet <ref type="bibr" target="#b37">[38]</ref>.) This network produces a semantic layoutL as output. We then compareL to the original layout L. In principle, the closer these are, the more realistic the intermediate synthesized image I can be assumed to be <ref type="bibr" target="#b10">[11]</ref>. We evaluate the similarity of L andL using two measures: intersection over union (IoU) and overall pixel accuracy. These measures are averaged over all images in the test set of each dataset.</p><p>The results are reported in <ref type="table">Table 2</ref>. Images synthesized by our approach can be more accurately parsed by the PSPNet than images synthesized by Pix2pix or the CRN. The differences on Cityscapes-coarse and ADE20K are dra-  <ref type="table">Table 2</ref>. Images synthesized by different approaches are given to a pretrained semantic segmentation network (PSPNet <ref type="bibr" target="#b37">[38]</ref>). Its output is compared to the semantic layout that was used as input for image synthesis. IoU and pixel accuracy are averaged across each dataset. Higher is better. 'Reference' is the value achieved by using the real images as input to the PSPNet. Cityscapes-coarse Cityscapes-fine <ref type="figure">Figure 6</ref>. Time-limited pairwise comparisons versus real images. 50% is the equivalent of passing the visual Turing test.</p><p>matic. Note that this experimental procedure also evaluates conformance with the input semantic layout: if the image synthesized by an approach is realistic but does not conform to the input layout, it will be penalized by this protocol.</p><p>Image statistics. We now analyze the realism of synthesized images in terms of low-level image statistics. We consider the mean power spectrum of synthesized images across a given dataset, versus corresponding real images from the dataset <ref type="bibr" target="#b35">[36]</ref>. <ref type="figure" target="#fig_6">Figure 5</ref> shows the mean power spectra of images synthesized by Pix2pix, CRN, and SIMS, averaged across the ADE20K dataset. The mean power spectrum of real ADE20K images is shown for reference. As can be seen in the figure, the mean power spectrum of images synthesized by our approach is virtually indistinguishable from the mean power spectrum of real images. In contrast, the mean power spectra of images synthesized by Pix2pix and CRN are clearly spiky, with many spurious local maxima that are not present in real images.</p><p>Qualitative results. <ref type="figure">Figure 7</ref> shows a number of images synthesized by Pix2pix, CRN, and SIMS, trained on the Cityscapes dataset. Results are shown in the three conditions summarized earlier: Cityscapes-coarse, Cityscapesfine, and Cityscapes→GTA5. <ref type="figure">Figure 8</ref> shows examples of synthesized images for the NYU and ADE20K datasets. Additional results are provided in the supplement.</p><p>Diversity. The presented approach can be easily extended to synthesize a diverse collection of images. To this end, the retrieval stage described in Section 4.2 can be modified to retrieve not a single segment that maximizes the presented score, but a random segment among the top k segments that maximize the score across the dataset. The retrieved segment for each semantic region L j can be randomized in this fashion. Given an input layout, the synthesis process can be repeated to synthesize as many corresponding images as desired. Results of this process are shown in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have presented a semi-parametric approach to photographic image synthesis from semantic layouts. Experiments demonstrate that the presented approach (SIMS) produces considerably more realistic images than recent purely parametric techniques. Note that the quality of SIMS is in a sense lower-bounded by the performance of parametric methods: if the memory bank is not useful, the network f can simply ignore the canvas and perform parametric synthesis based on the input semantic layout.</p><p>Many interesting problems are left open for future work. First, our implementation is significantly slower than purely parametric methods; more efficient data structures and algorithms should be explored. Second, other forms of input can be used, such as semantic instance segmentation or textual descriptions. Third, the presented pipeline is not trained end-to-end. Lastly, applying semi-parametric techniques to video synthesis is an exciting frontier.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Comparison to the approach of Chen and Koltun [2] on coarse semantic layouts from the Cityscapes dataset. Zoom in for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. First stage of the image synthesis pipeline. (a) Given a semantic layout L, the external memory M is queried to retrieve compatible segments (b), which are aligned to the input layout by a spatial transformer network (c,d). An ordering network (e) assists the composition of the canvas C (f). The synthesis process continues as illustrated in Figure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The target semantic layout L and the canvas C are given to a network f , which synthesizes the output image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Generation of a simulated canvas during training. (a) Training image I. (b) Object segment Pj extracted from the training image. (c) A corresponding segment retrieved from a different image in the training set. (d) Mask of the retrieved segment, used to stencil Pj. (e) The segment Pj is stenciled with the mask of the retrieved segment. (f) Color transfer is applied to further modify the appearance of Pj. (g) Boundaries are elided to force the synthesis network to learn to synthesize content near boundaries. (h) A complete canvas C ′</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Mean power spectra over the ADE20K dataset. Magnitude is on a logarithmic scale. We compare the mean power spectra of images synthesized by Pix2pix, CRN, and SIMS to the mean power spectrum of real images from the ADE20K test set. The mean power spectrum of images synthesized by SIMS is virtually indistinguishable from the mean power spectrum of real images, while the mean power spectra of images synthesized by Pix2pix and the CRN are characterized by spurious spikes. Zoom in for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Images synthesized by Pix2pix, CRN, and SIMS. This figure shows results produced by models trained on the Cityscapes dataset.Images synthesized by models trained on the NYU and ADE20K datasets.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sketch2Photo: Internet image montage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Single-image depth perception in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scene completion using millions of photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
		<idno>2017. 5</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scene collaging: Analysis and synthesis of natural images with semantic layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CG2Real: Improving the realism of computer generated images using a large collection of photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using color compatibility for assessing image realism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Photo clip art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
		<idno>1989. 5</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Photorealism: 50 Years of Hyperrealistic Painting. Hatje Cantz</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Letze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Handbook of Imagination and Mental Simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Markman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M P</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Suhr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Taylor &amp; Francis Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Synthesizing the preferred inputs for neurons in neural networks via deep generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Plug &amp; play generative networks: Conditional iterative generation of images in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Physically Based Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pharr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Humphreys</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
	<note>3rd edition</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning what and where to draw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Color transfer between images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ashikhmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep image harmonization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generative image modeling using style and structure adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Understanding and improving the realism of image composites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dorsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Rushmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attribute2Image: Conditional image generation from visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">High-resolution image inpainting using multi-scale neural patch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02688</idno>
		<title level="m">Statistics of deep generated images</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scene parsing through ADE20K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning a discriminative model for the perception of realism in composite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
