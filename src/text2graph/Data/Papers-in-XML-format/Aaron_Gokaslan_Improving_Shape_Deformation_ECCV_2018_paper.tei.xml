<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Shape Deformation in Unsupervised Image-to-Image Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Brown University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Ramanujan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Brown University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ritchie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Brown University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang</forename><forename type="middle">In</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Bath</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Tompkin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Brown University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Shape Deformation in Unsupervised Image-to-Image Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>Generative adversarial networks · Image translation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Unsupervised image-to-image translation techniques are able to map local texture between two domains, but they are typically unsuccessful when the domains require larger shape change. Inspired by semantic segmentation, we introduce a discriminator with dilated convolutions that is able to use information from across the entire image to train a more context-aware generator. This is coupled with a multi-scale perceptual loss that is better able to represent error in the underlying shape of objects. We demonstrate that this design is more capable of representing shape deformation in a challenging toy dataset, plus in complex mappings with significant dataset variation between humans, dolls, and anime faces, and between cats and dogs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unsupervised image-to-image translation is the process of learning an arbitrary mapping between image domains without labels or pairings. This can be accomplished via deep learning with generative adversarial networks (GANs), through the use of a discriminator network to provide instance-specific generator training, and the use of a cyclic loss to overcome the lack of supervised pairing. Prior works such as DiscoGAN <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr">CycleGAN [43]</ref> are able to transfer sophisticated local texture appearance between image domains, such as translating between paintings and photographs. However, these methods often have difficulty with objects that have both related appearance and shape changes; for instance, when translating between cats and dogs.</p><p>Coping with shape deformation in image translation tasks requires the ability to use spatial information from across the image. For instance, we cannot expect to transform a cat into a dog by simply changing the animals' local texture. From our experiments, networks with fully connected discriminators, such as DiscoGAN, are able to represent larger shape changes given sufficient network capacity, but train much slower <ref type="bibr" target="#b16">[17]</ref> and have trouble resolving smaller details. Patch-based discriminators, as used in CycleGAN, work well at resolving high frequency information and train relatively quickly <ref type="bibr" target="#b16">[17]</ref>, but have a limited 'receptive field' for each patch that only allows the network to consider spatially local content. These networks reduce the amount of information received by the generator. Further, the functions used to maintain the cyclic loss prior in both networks retains high frequency information in the cyclic reconstruction, which is often detrimental to shape change tasks.</p><p>We propose an image-to-image translation system, designated GANimorph, to address shortcomings present in current techniques. To allow for patch-based discriminators to use more image context, we use dilated convolutions in our discriminator architecture <ref type="bibr">[39]</ref>. This allows us to treat discrimination as a semantic segmentation problem: the discriminator outputs per-pixel real-vs.-fake decisions, each informed by global context. This per-pixel discriminator output facilitates more fine-grained information flow from the discriminator to the generator. We also use a multi-scale structure similarity perceptual reconstruction loss to help represent error over image areas rather than just over pixels. We demonstrate that our approach is more successful on a challenging shape deformation toy dataset than previous approaches. We also demonstrate example translations involving both appearance and shape variation by mapping human faces to dolls and anime characters, and mapping cats to dogs ( <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>The source code to our GANimorph system and all datasets are online: https://github.com/brownvc/ganimorph/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Image-to-image Translation. Image analogies provides one of the earliest examples of image-to-image translation <ref type="bibr" target="#b13">[14]</ref>. The approach relies on non-parametric texture synthesis and can handle transformations such as seasonal scene shifts <ref type="bibr" target="#b19">[20]</ref>, color and texture transformation, and painterly style transfer. Despite the ability of the model to learn texture transfer, the model cannot affect the shape of objects. Recent research has extended the model to perform visual attribute transfer using neural networks <ref type="bibr">[23,</ref><ref type="bibr" target="#b12">13]</ref>. However, despite these improvements, deep image analogies are unable to achieve shape deformation.</p><p>Neural Style Transfer. These techniques show transfer of more complex artistic styles than image analogies <ref type="bibr" target="#b9">[10]</ref>. They combine the style of one image with the content of another by matching the Gram matrix statistics of early-layer feature  <ref type="bibr" target="#b1">[2]</ref>. Reference images can affect brush strokes, color palette, and local geometry, but larger changes such as anime-style combined appearance and shape transformations do not propagate.</p><p>Generative Adversarial Networks. Generative adversarial networks (GANs) have produced promising results in image editing <ref type="bibr">[22]</ref>, image translation <ref type="bibr" target="#b16">[17]</ref>, and image synthesis <ref type="bibr" target="#b10">[11]</ref>. These networks learn an adversarial loss function to distinguish between real and generated samples. Isola et al. <ref type="bibr" target="#b16">[17]</ref> demonstrated with Pix2Pix that GANs are capable of learning texture mappings between complex domains. However, this technique requires a large number of explicitly-paired samples. Some such datasets are naturally available, e.g., registered map and satellite photos, or image colorization tasks. We show in our supplemental material that our approach is also able to solve these limited-shape-change problems.</p><p>Unsupervised Image Translation GANs. Pix2Pix-like architectures have been extended to work with unsupervised pairs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr">43]</ref>. Given image domains X and Y, these approaches work by learning a cyclic mapping from X→Y→X and Y→X→Y. This creates a bijective mapping that prevents mode collapse in the unsupervised case. We build upon the DiscoGAN <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr">CycleGAN [43]</ref> architectures, which themselves extend Coupled GANs for style transfer <ref type="bibr">[25]</ref>. We seek to overcome their shape change limitations through more efficient learning and expanded discriminator context via dilated convolutions, and by using a cyclic loss function that considers multi-scale frequency information ( use two autoencoders to create a cyclic loss through a shared latent space with additional constraints. Several layers are shared between the two generators and an identity loss ensures that both domains resolve to the same latent vector. This produces some shape transformation in faces; however, the network does not improve the discriminator architecture to provide greater context awareness.</p><p>One qualitatively different approach is to introduce object-level segmentation maps into the training set. Liang et al.'s <ref type="bibr">ContrastGAN [22]</ref> has demonstrated shape change by learning segmentation maps and combining multiple conditional cyclic generative adversarial networks. However, this additional input is often unavailable and time consuming to declare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>Crucial to the success of translation under shape deformation is the ability to maintain consistency over global shapes as well as local texture. Our algorithm adopts the cyclic image translation framework <ref type="bibr" target="#b18">[19,</ref><ref type="bibr">43]</ref> and achieves the required consistency by incorporating a new dilated discriminator, a generator with residual blocks and skip connections, and a multi-scale perceptual cyclic loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dilated Discriminator</head><p>Initial approaches used a global discriminator with a fully connected layer <ref type="bibr" target="#b18">[19]</ref>. Such a discriminator collapses an image to a single scalar value for determining image veracity. Later approaches [43,22] used a patch-based DCGAN [32] discriminator, initially developed for style transfer and texture synthesis <ref type="bibr">[21]</ref>. In this type of discriminator, each image patch is evaluated to determine a fake or real score. The patch-based approach allows for fast generator convergence by operating on each local patch independently. This approach has proven effective for texture transfer, segmentation, and similar tasks. However, this patch-based view limits the networks' awareness of global spatial information, which limits the generator's ability to perform coherent global shape change.</p><p>Reframing Discrimination as Semantic Segmentation. To solve this issue, we reframe the discrimination problem from determining real/fake images or subimages into the more general problem of finding real or fake regions of the image, i.e., a semantic segmentation task. Since the discriminator outputs a higherresolution segmentation map, the information flow between the generator and discriminator increases. This allows for faster convergence than using a fully connected discriminator, such as in DiscoGAN.</p><p>Current state-of-the-art networks for segmentation use dilated convolutions, and have been shown to require far fewer parameters than conventional convolutional networks to achieve similar levels of accuracy <ref type="bibr">[39]</ref>. Dilated convolutions provide advantages over both global and patch-based discriminator architectures. For the same parameter budget, they allow the prediction to incorporate data from a larger surrounding region. This increases the information flow between the generator and discriminator: by knowing which regions of the image contribute to making the image unrealistic, the generator can focus on that region of the image. An alternative way to think about dilated convolutions is that they allow the discriminator to implicitly learn context. While multi-scale discriminators have been shown to improve results and stability for high resolution image synthesis tasks [35], we will show that incorporating information from farther away in the image is useful in translation tasks as the discriminator can determine where a region should fit into an image based on surrounding data. For example, this increased spatial context helps localize the face of a dog relative to its body, which is difficult to learn from small patches or patches learned in isolation from their neighbors. <ref type="figure" target="#fig_1">Figure 2</ref> (right) illustrates our discriminator architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generator</head><p>Our generator architecture builds on those of DiscoGAN and CycleGAN. Disco-GAN uses a standard encoder-decoder architecture ( <ref type="figure" target="#fig_1">Figure 2</ref>, top left). However, its narrow bottleneck layer can lead to output images that do not preserve all the important visual details from the input image. Furthermore, due to the low capacity of the network, the approach remains limited to low resolution images of size 64×64. The CycleGAN architecture seeks to increase capacity over DiscoGAN by using a residual block to learn the image translation function <ref type="bibr" target="#b11">[12]</ref>.</p><p>Residual blocks have been shown to work in extremely deep networks, and they are able to represent low frequency information <ref type="bibr">[40,</ref><ref type="bibr" target="#b1">2]</ref>. However, using residual blocks at a single scale limits the information that can pass through the bottleneck and thus the functions that the network can learn. Our generator includes residual blocks at multiple layers of both the decoder and encoder, allowing the network to learn multi-scale transformations that work on both higher and lower spatial resolution features <ref type="figure" target="#fig_1">(Figure 2</ref>, bottom left).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Objective Function</head><p>Perceptual Cyclic Loss. As per prior unsupervised image-to-image translation work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr">22,</ref><ref type="bibr">24,</ref><ref type="bibr">43</ref>,38], we use a cyclic loss to learn a bijective mapping between two image domains. However, not all image translation functions can be perfectly bijective, e.g., when one domain has smaller appearance variation, like human face photos vs. anime drawings. When all information in the input image cannot be preserved in the translation, the cyclic loss term should aim to preserve the most important information. Since the network should focus on image attributes of importance to human viewers, we should choose a perceptual loss that emphasizes shape and appearance similarity between the generated and target images.</p><p>Defining an explicit shape loss is difficult, as any explicit term requires known image correspondences between domains. These do not exist for our examples and our unsupervised setting. Further, including a more-complex perceptual neural network into the loss calculation imparts a significant computational and memory overhead. While using pretrained image classification networks as a perceptual loss can speed up style transfer <ref type="bibr" target="#b17">[18]</ref>, these do not work on shape changes as the pretrained networks tend only to capture low-level texture information <ref type="bibr" target="#b1">[2]</ref>.</p><p>Instead, we use multi-scale structure similarity loss (MS-SSIM) <ref type="bibr">[36]</ref>. This loss better preserves features visible to humans instead of noisy high frequency information. MS-SSIM can also better cope with shape change since it can recognize geometric differences through area statistics. However, MS-SSIM alone can ignore smaller details, and does not capture color similarity well. Recent work has shown that mixing MS-SSIM with L1 or L2 losses is effective for super resolution and segmentation tasks <ref type="bibr">[41]</ref>. Thus, we also add a lightly-weighted L1 loss term, which helps increase the clarity of generated images.</p><p>Feature Matching Loss. To increase the stability of the model, our objective function uses a feature matching loss [33]:</p><formula xml:id="formula_0">L FM (G, D) = 1 n − 1 n−1 i=1 E x∼p data f i (x) − E z∼pz f i (G(z)) 2 2 .<label>(1)</label></formula><p>Where f i ∈ D(x) represents the raw activation potentials of the i th layer of the discriminator D, and n is the number of discriminator layers. This term encourages fake and real samples to produce similar activations in the discriminator, and so encourages the generator to create images that look more similar to the target domain. We have found this loss term to prevent generator mode collapse, to which GANs are often susceptible <ref type="bibr" target="#b18">[19,</ref><ref type="bibr">33,</ref><ref type="bibr">35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scheduled Loss Normalization (SLN).</head><p>In a multi-part loss function, linear weights are often used to normalize the terms with respect to one another, with previous works often optimizing a single set of weights. However, finding appropriatelybalanced weights can prove difficult without ground truth. Further, often a single set of weights is inappropriate because the magnitude of the loss terms changes over the course of training. Instead, we create a procedure to periodically renormalize each loss term and so control their relative values. This lets the user intuitively provide weights that sum to 1 to balance the loss terms in the model, without having knowledge of how their magnitudes will change over training.</p><p>Let L be a loss function, and let X n = {x t } bn t=1 be a sequence of n batches of training inputs, each b images large, such that L(x t ) is the training loss at iteration t. We compute an exponentially-weighted moving average of the loss:</p><formula xml:id="formula_1">L moavg (L, X n ) = (1 − β) xt∈Xn β bn−t L(x t ) 2<label>(2)</label></formula><p>where β is the decay rate. We can renormalize the loss function by dividing it by this moving average. If we do this on every training iteration, however, the loss stays at its normalized average and no training progress is made. Instead, we schedule the loss normalization:</p><formula xml:id="formula_2">SLN(L, X n , s) = L(X n )/(L moavg (L, X n ) + ǫ) if n (mod s) = 1 L(X n ) otherwise</formula><p>Here, s is the scheduling parameter such that we apply normalization every s training iterations. For all experiments, we use β = 0.99, ǫ = 10 −10 , and s = 200. One other normalization difference between CycleGAN/DiscoGAN and our approach is the use of instance normalization <ref type="bibr" target="#b14">[15]</ref> and batch normalization <ref type="bibr" target="#b15">[16]</ref>, respectively. We found that batch normalization caused excessive over-fitting to the training data, and so we used instance normalization.</p><p>Final Objective. Our final objective comprises three loss normalized terms: a standard GAN loss, a feature matching loss, and two cyclic reconstruction losses. Given image domains X and Y , let G : X → Y map from X to Y and F : Y → X map from Y to X. D X and D Y denote discriminators for G and F , respectively.</p><p>For GAN loss, we combine normal GAN loss terms from Goodfellow et al. <ref type="bibr" target="#b10">[11]</ref>:</p><formula xml:id="formula_3">L GAN = L GAN X (F, D X , Y, X) + L GAN Y (G, D Y , X, Y )<label>(3)</label></formula><p>For feature matching loss, we use Equation 1 for each domain:</p><formula xml:id="formula_4">L FM = L FM X (G, D X ) + L FM Y (F, D Y )<label>(4)</label></formula><p>For the two cyclic reconstruction losses, we consider structural similarity [36] and an L 1 loss. Let X ′ = F (G(X)) and Y ′ = G(F (Y )) be the cyclicallyreconstructed input images. Then:</p><formula xml:id="formula_5">L SS =(1 − MS-SSIM(X ′ , X)) + (1 − MS-SSIM(Y ′ , Y ))<label>(5)</label></formula><formula xml:id="formula_6">L L1 = X ′ − X 1 + Y ′ − Y 1<label>(6)</label></formula><p>where we compute MS-SSIM without discorrelation. Our total objective function with scheduled loss normalization (SLN) is:</p><formula xml:id="formula_7">L total =λ GAN SLN(L GAN ) + λ FM SLN(L FM )+ λ CYC SLN(λ SS L SS + λ L1 L L1 )<label>(7)</label></formula><p>with λ GAN + λ FM + λ CYC = 1, λ SS + λ L1 = 1, and all coefficients ≥ 0. We set λ GAN = 0.49, λ FM = 0.21, and λ CYC = 0.3, and λ SS = 0.7 and λ L1 = 0.3. Empirically, these helped to reduce mode collapse and worked across all datasets. For all training details, we refer the reader to our supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Toy Problem: Learning 2D Dot and Polygon Deformations</head><p>We created a challenging toy problem to evaluate the ability of our network design to learn shape-and texture-consistent deformation. We define two domains: the regular polygon domain X and its deformed equivalent Y <ref type="figure" target="#fig_2">(Figure 3</ref>). Each example X s,h,d ∈ X contains a centered regular polygon with s ∈ {3 . . . 7} sides, plus a deformed matrix of dots overlaid. The dot matrix is computed by taking a unit dot grid and transforming it via h, a Gaussian random normal 2×2 matrix, and a displacement vector d, a Gaussian normal vector in R 2 . The corresponding domain equivalent in Y is Y s,h,d , with instead the polygon transformed by h and the dot matrix remaining regular. This construction forms a bijection from X to Y , and so the translation problem is well-posed.</p><p>Learning a mapping from X to Y requires the network to use the large-scale cues present in the dot matrix to successfully deform the polygon, as local patches with a fixed image location cannot overcome the added displacement d. <ref type="table" target="#tab_2">Table 2</ref> shows that DiscoGAN is unable to learn to map between either domain, and produces an output that is close to the mean of the dataset (off-white). CycleGAN is able to learn only local deformation, which produces hue shifts towards the blue of the polygon when mapping from regular to deformed spaces, and which in most cases produces an undeformed dot matrix when mapping from deformed to regular spaces. In contrast, our approach is significantly more successful at learning the deformation as the dilated discriminator is able to incorporate information from across the image.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deformed to Regular</head><p>Quantitative Comparison. As our output is a highly-deformed image, we estimate the learned transform parameters by sampling. We compute a Hausdorff distance between 500 point samples on the ground truth polygon and on the image of the generated polygon after translation: for finite sets of points X and Y , d(X, Y ) = max y∈Y min x∈X x − y . We hand annotate 220 generated polygon boundaries for our network, sampled uniformly at random along the boundary. Samples exist in a unit square with bottom left corner at (0, 0). First, DiscoGAN fails to generate polygons at all, despite being able to reconstruct the original image. Second, for 'regular to deformed', CycleGAN fails to produce a polygon, whereas our approach produces average Hausdorff distance of 0.20 ± 0.01. Third, for 'deformed to regular', CycleGAN produces a polygon with distance of 0.21 ± 0.04, whereas our approach has distance of 0.10 ± 0.03. In the true dataset, note that regular polygons are centered, but CycleGAN only constructs polygons at the position of the original distorted polygon. Our network constructs a regular polygon at the center of the image as desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Real-world Datasets</head><p>We evaluate GANimorph on several image datasets. For human faces, we use the aligned version of the CelebFaces Attribute dataset <ref type="bibr">[26]</ref>, with 202,599 images.</p><p>Anime Faces. Previous works have noted that anime images are challenging for style transfer methods, since translating between photoreal and anime faces involves both shape and appearance changes. We create a large 966,777 image anime dataset crowdsourced from Danbooru <ref type="bibr" target="#b0">[1]</ref>. The Danbooru dataset has a wide variety of styles from super-deformed chibi-style faces, to realisticallyproportioned faces, to rough sketches. Since traditional face detectors yield poor results on drawn datasets, we ran the Animeface filter [29] on both datasets. When translating humans to anime, we see an improvement in our approach for head pose and accessories such as glasses <ref type="table" target="#tab_3">(Table 3</ref>, 3rd row, right), plus a larger degree of shape deformation such as reduced face vertical height. The final line of each group represents a particularly challenging example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Doll Faces.</head><p>Translating human faces to dolls provides an informative test case: both domains have similar photorealistic appearance, so the translation task focuses on shape more than texture. Similar to Morsita et al.</p><p>[28], we extracted 13,336 images from the Flickr100m dataset [30] using specific doll manufacturers as keywords. Then, we extract local binary patterns [31] using OpenCV <ref type="bibr" target="#b3">[4]</ref>, and use the Animeface filter for facial alignment <ref type="bibr">[29]</ref>. <ref type="table" target="#tab_3">Table 3</ref>, bottom, shows that our architecture handles local deformation and global shape change better than CycleGAN and DiscoGAN, while preserving local texture similarity. Either the shape is malformed (DiscoGAN), or the shape </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dog→Cat</head><p>shows artifacts from the original image or unnatural skin texture (CycleGAN). Our method matches skintones from the CelebA dataset, while capturing the overall facial structure and hair color of the doll. For a more difficult doll to human example in the bottom right-hand corner, while our transformation is not realistic, our method still creates more shape change than existing networks.</p><p>Pets in the Wild. To demonstrate our network on unaligned data, we evaluate on the Kaggle cat and dog dataset <ref type="bibr" target="#b8">[9]</ref>. This contains 12,500 images of each species, across many animal breeds at varying scales, lighting conditions, poses, backgrounds, and occlusion factors. When translating between cats and dogs <ref type="table" target="#tab_4">(Table 4)</ref>, the network is able to change both the local features such as the addition and removal of fur and whiskers, plus the larger shape deformation required to fool the discriminator, such as growing a snout. Most errors in this domain come from the generator failing to identify an animal from the background, such as forgetting the rear or tail of the animal. Sometimes the generator may fail to identify the animal at all.</p><p>We also translate between humans and cats. <ref type="table">Table 5</ref> demonstrates how our architecture handles large scale translation with these two variable data distributions. Our failure cases are approximately the same as that of the cats to dogs translation, with some promising results. Overall, we translate a surprising degree of shape deformation even when we might not expect this to be possible.  </p><note type="other">5. Human and Pet Faces (dataset details in supplemental): As a challenge, we map humans to cats and cats to humans. Pose is reliably translated; semantic appearance like hair color</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Quantitative Study</head><p>To quantify GANimorph's translation ability, we consider classification-based metrics to detect class change, e.g., whether a cat was successfully translated into a dog. Since there is no per pixel ground truth in this task for any real-world datasets, we cannot use Fully Convolution Score. Using Inception Score [33] is uninformative since simply outputting the original image would score highly. Further, similar to adversarial examples, CycleGAN is able to convince many classification networks that the image is translated even though to a human the image appears untranslated: all CycleGAN results from supplemental <ref type="table" target="#tab_3">Table 3</ref> convince both ResNet50 <ref type="bibr" target="#b11">[12]</ref> and the traditional segmentation network of Zheng et al. <ref type="bibr">[42]</ref>, even though the image is unsuccessfully translated.</p><p>However, semantic segmentation networks that use dilated convolutions can distinguish CycleGAN's 'adversarial examples' from true translations, such as DeepLabV3 <ref type="bibr" target="#b4">[5]</ref>. As such, we run each test image through the DeepLabV3 network to generate a segmentation mask. Then, we compute the percent of nonbackground-labeled pixels per class, and average across the test set <ref type="table" target="#tab_7">(Table 6</ref>). Our approach is able to more fully translate the image in the eyes of the classification network, with images also appearing translated to a human <ref type="table">(Table 7)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>We use these quantiative settings for an ablation study <ref type="table" target="#tab_7">(Table 6</ref>). First, we removed MS-SSIM to leave only L1 (L SS , Eq. 7), which causes our network to  <ref type="table">Table 7</ref>. Example segmentation masks from DeepLabV3 for <ref type="table" target="#tab_7">Table 6</ref> for Cat→Dog. Red denotes the cat class, and blue denotes the intended dog class.</p><p>mode collapse. Next, we removed feature match loss, but this decreases both our segmentation consistency and the stability of the network. Then, we replaced our dilated discriminator with a patch discriminator. However, the patch discriminator cannot use global context, and so the network confuses facial layouts. Finally, we replace our dilated discriminator with a fully connected discriminator. We see that our generator architecture and loss function allow our network to outperform DiscoGAN even with the same type of discriminator (fully connected). Qualitative ablation study results are shown in <ref type="table">Table 8</ref>. The patch based discriminator translates texture well, but fails to create globally-coherent images. Decreasing the information flow by using a fully-connected discriminator or removing feature match leads to better results. Maximizing the information flow ultimately leads to the best results (last column). Using L1 instead of a perceptual cyclic loss term leads to mode collapse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>There exists a trade off in the relative weighting of the cyclic loss. A higher cyclic loss term weight λ cyc will prevent significant shape change and weaken the generator's ability to adapt to the discriminator. Setting it too low will cause the collapse of the network and prevent any meaningful mapping from existing between domains. For instance, the network can easily hallucinate objects in the other domain if the reconstruction loss is too low. Likewise, setting it too high will prevent the network from deforming the shape properly. As such, an <ref type="table">Table 8</ref>. In qualitative comparisons, GANimorph outperforms all of its ablated versions. For instance, our approach better resolves fine details (e.g., second row, cat eyes) while also better translating the overall shape (e.g., last row, cat nose and ears).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>No FM Loss L1 Loss Patch Discr. FC Discr. Ours architecture that allowed modifying the weighting of this term at test time would prove valuable for allowing the user control over how much deformation to allow. One counter-intuitive result we discovered is that in domains with little variety, the mappings can lose semantic meaning (see supplemental material). One example of a failed mapping was from celebA to bitmoji faces <ref type="bibr">[34,</ref><ref type="bibr">37]</ref>. Many attributes were lost, including pose, and the mapping fell back to pseudo-steganographic encoding of the faces <ref type="bibr" target="#b6">[7]</ref>. For example, background information would be encoded in color gradients of hair styles, and minor variations in the width of the eyes were used similarly. As such, the cyclic loss limits the ability of the network to abstract relevant details. Approaches such as relying on mapping the variance within each dataset, similar to Benaim et al. <ref type="bibr" target="#b2">[3]</ref>, may prove an effective means of ensuring the variance in either domain is maintained. We found that this term over-constrained the amount of shape change in the target domain; however, this may be worth further investigation.</p><p>Finally, trying to learn each domain simultaneously may also prove an effective way to increase the accuracy of image translation. Doing so allows the discriminator(s) and generator to learn how to better determine and transform regions of interest for either network. Better results might be obtained by mapping between multiple domains using parameter-efficient networks (e.g., StarGAN <ref type="bibr" target="#b5">[6]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Repository:</head><p>The source code to our GANimorph system and all datasets are available online: https://github.com/brownvc/ganimorph/. 42. Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang, C., Torr, P. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Our approach translates texture appearance and complex head and body shape changes between the cat and dog domains (left: input; right: translation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (Left) Generators from different unsupervised image translation models. The skip connections and residual blocks are combined via concatenation as opposed to addition. (Right) Our discriminator network architecture is a fully-convolutional segmentation network. Each colored block represents a convolution layer; block labels indicate filter size. In addition to global context from the dilations, the skip connection bypassing the dilated convolution blocks preserves the network's view of local context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Toy Dataset (128×128). Left: X instance; a regular polygon with deformed dot matrix overlay. Right: Y instance; a deformed polygon and dot lattice. The dot lattice provides information from across the image to the true deformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, X., Zhang, H., Xing, E.P.: Generative semantic manipulation with contrasting gan. arXiv preprint arXiv:1708.00315 (2017) 23. Liao, J., Yao, Y., Yuan, L., Hua, G., Kang, S.B.: Visual attribute transfer through deep image analogy. ACM Trans. Graph. (2017) 24. Liu, M.Y., Breuel, T., Kautz, J.: Unsupervised image-to-image translation networks. In: Advances in Neural Information Processing Systems (2017) 25. Liu, M.Y., Tuzel, O.: Coupled generative adversarial networks. In: Advances in Neural Information Processing Systems (2016) 26. Liu, Z., Luo, P., Wang, X., Tang, X.: Deep learning face attributes in the wild. In: International Conference on Computer Vision (2015) 27. Ma, S., Fu, J., Chen, C.W., Mei, T.: DA-GAN: Instance-level image translation by deep attention generative adversarial networks. In: Conference on Computer Vision and Pattern Recognition (2018) 28. Morishita, M., Ueno, M., Isahara, H.: Classification of doll image dataset based on human experts and computational methods: A comparative analysis. In: Interna- tional Conference On Advanced Informatics: Concepts, Theory And Application (ICAICTA) (2016) 29. Nagadomi: lbpcascade animeface. https://github.com/nagadomi/ lbpcascade animeface (2017) 30. Ni, K., Pearce, R., Boakye, K., Van Essen, B., Borth, D., Chen, B., Wang, E.: Large- scale deep learning on the YFCC100M dataset. arXiv preprint arXiv:1502.03409 (2015) 31. Ojala, T., Pietikainen, M., Harwood, D.: Performance evaluation of texture measures with classification based on kullback discrimination of distributions. In: International Conference on Pattern Recognition (1994) 32. Radford, A., Metz, L., Chintala, S.: Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. ArXiv e-prints (Nov 2015) 33. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X.: Im- proved techniques for training GANs. In: Advances in Neural Information Processing Systems (2016) 34. Taigman, Y., Polyak, A., Wolf, L.: Unsupervised cross-domain image generation. arXiv preprint arXiv:1611.02200 (2016) 35. Wang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro, B.: High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs. In: Computer Vision and Pattern Recognition (2018) 36. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment: from error visibility to structural similarity. Transactions on Image Processing (2004) 37. Wolf, L., Taigman, Y., Polyak, A.: Unsupervised creation of parameterized avatars. In: International Conference on Computer Vision (2017) 38. Yi, Z., Zhang, H.R., Tan, P., Gong, M.: DualGAN: Unsupervised dual learning for image-to-image translation. In: International Conference on Computer Vision (2017) 39. Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. In: International Conference on Learning Representations (2015) 40. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In: European Conference on Computer Vision (2014) 41. Zhao, H., Gallo, O., Frosio, I., Kautz, J.: Loss functions for image restoration with neural networks. IEEE Transactions on Computational Imaging (2017)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 )</head><label>1</label><figDesc>. Other works tackle complementary problems. Yi et al. [38] focus on improving high frequency features over CycleGAN in image translation tasks, such as texture transfer and segmentation. Shuang et al. [27] examine adapting CycleGAN to wider variety in the domains-so-called instance-level translation. Liu et al. [24]</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Toy Dataset. When estimating complex deformation, DiscoGAN collapses to the mean dataset value (near white). CycleGAN approximates the deformation of the polygon but not the dot lattice (right-hand side). Our approach learns both.</figDesc><table>Regular to Deformed 

Input 
CycleGAN DiscoGAN 
Ours 
Input 
CycleGAN DiscoGAN 
Ours 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 .</head><label>3</label><figDesc>GANimorph is able to translate shape and style changes while retaining input attributes such as hair color, pose, glasses, headgear, and background.</figDesc><table>Photoreal to Anime 

Input 
CycleGAN 
DiscoGAN 
Ours 
Input 
CycleGAN 
DiscoGAN 
Ours 

Anime to Photoreal 

Human to Doll Face 

Input 
CycleGAN 
DiscoGAN 
Ours 
Input 
CycleGAN 
DiscoGAN 
Ours 

Doll Face to Human 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Pets in the Wild: Between dogs and cats, our approach is able to generate shape transforms across pose and appearance variation.</figDesc><table>Cat→Dog 

Input 
CycleGAN DiscoGAN 
Ours 
Input 
CycleGAN DiscoGAN 
Ours 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>is sometimes translated; some inputs still fail (bottom left).</figDesc><table>Input 
Output 
Input 
Output 
Input 
Output 
Input 
Output 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 6 .</head><label>6</label><figDesc>Percentage of pixels classified in translated images via CycleGAN, DiscoGAN, and our algorithm (with design choices). Target classes are in blue.</figDesc><table>Class (%) 
Cat→Dog 
Dog→Cat 

Networks 
Cat Dog Person Other 
Cat Dog Person Other 

Initial Domain 
100.00 0.00 
0.00 0.00 
0.00 98.49 
1.51 0.00 

CycleGAN 
99.99 0.01 
0.00 0.00 
2.67 97.27 
0.06 0.00 
DiscoGAN 
24.37 75.38 
0.25 0.00 96.95 0.00 
2.71 0.34 
Ours w/ L1 
100.00 0.00 
0.00 0.00 
0.00 0.00 
0.00 100.00 
Ours w/o feature match loss 5.03 93.64 
0.81 0.53 85.62 14.15 
0.00 0.23 
Ours w/ fully conn. discrim. 6.11 93.60 
0.29 0.00 91.41 8.45 
0.03 0.10 
Ours w/ patch discrim. 
46.02 42.90 
0.05 11.03 91.77 8.22 
0.00 0.01 
Ours (dilated discrim.) 
1.00 98.57 
0.41 0.02 100.00 0.00 
0.00 0.00 

Input DiscoGAN 
CycleGAN 
Ours 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head></head><label></label><figDesc>H.: Conditional random fields as recurrent neural networks. In: Interna- tional Conference on Computer Vision (2015) 43. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-consistent adversarial networkss. In: International Conference on Computer Vision (2017)</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: Kwang In Kim thanks RCUK EP/M023281/1.</p><p>Improving Shape Deformation in Unsupervised Image-to-Image Translation</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Danbooru2017: A large-scale crowdsourced and tagged anime illustration dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anonymous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Branwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gokaslan</surname></persName>
		</author>
		<ptr target="https://www.gwern.net/Danbooru2017" />
		<imprint>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Network dissection: Quantifying interpretability of deep visual representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">One-sided unsupervised domain mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<title level="m">The OpenCV Library. Dr. Dobb&apos;s Journal of Software Tools</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">StarGAN: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02950</idno>
		<title level="m">CycleGAN: a master of steganography</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Asirra: A captcha that exploits interestaligned manual image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Elson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Douceur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM Conference on Computer and Communications Security. CCS &apos;07</title>
		<meeting>the 14th ACM Conference on Computer and Communications Security. CCS &apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Sander</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.00756</idno>
		<title level="m">Neural color transfer between images</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transient attributes for high-level understanding and editing of outdoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Laffont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
