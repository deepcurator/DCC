<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convexified Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
						</author>
						<title level="a" type="main">Convexified Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We describe the class of convexified convolutional neural networks (CCNNs), which capture the parameter sharing of convolutional neural networks in a convex manner. By representing the nonlinear convolutional filters as vectors in a reproducing kernel Hilbert space, the CNN parameters can be represented in terms of a lowrank matrix, and the rank constraint can be relaxed so as to obtain a convex optimization problem. For learning two-layer convolutional neural networks, we prove that the generalization error obtained by a convexified CNN converges to that of the best possible CNN. For learning deeper networks, we train CCNNs in a layerwise manner. Empirically, we find that CCNNs achieve competitive or better performance than CNNs trained by backpropagation, SVMs, fully-connected neural networks, stacked denoising auto-encoders, and other baseline methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks (CNNs) <ref type="bibr" target="#b20">(LeCun et al., 1998)</ref> have proven successful across many tasks including image classification <ref type="bibr" target="#b20">(LeCun et al., 1998;</ref><ref type="bibr" target="#b17">Krizhevsky et al., 2012)</ref>, face recognition <ref type="bibr" target="#b18">(Lawrence et al., 1997)</ref>, speech recognition ), text classification <ref type="bibr" target="#b32">(Wang et al., 2012)</ref>, and game playing <ref type="bibr" target="#b23">(Mnih et al., 2015;</ref><ref type="bibr" target="#b26">Silver et al., 2016)</ref>. There are two principal advantages of a CNN over a fully-connected neural network: (i) sparsity-each nonlinear convolutional filter acts only on a local patch of the input, and (ii) parameter sharing-the same filter is applied to each patch.</p><p>However, as with most neural networks, the standard approach to training CNNs is based on solving a nonconvex optimization problem that is known to be NP-hard <ref type="bibr" target="#b4">(Blum &amp; Rivest, 1992)</ref>. In practice, researchers use some flavor of stochastic gradient method, in which gradients are computed via backpropagation <ref type="bibr" target="#b5">(Bottou, 1998)</ref>. This approach has two drawbacks: (i) the rate of convergence, which is at best only to a local optimum, can be slow due to nonconvexity (for instance, see the paper <ref type="bibr" target="#b14">(Fahlman, 1988)</ref>), and (ii) its statistical properties are very difficult to understand, as the actual performance is determined by some combination of the CNN architecture along with the optimization algorithm.</p><p>In this paper, with the goal of addressing these two challenges, we propose a new model class known as convexified convolutional neural networks (CCNNs). These models have two desirable features. First, training a CCNN corresponds to a convex optimization problem, which can be solved efficiently and optimally via a projected gradient algorithm. Second, the statistical properties of CCNN models can be studied in a precise and rigorous manner. We obtain CCNNs by convexifying two-layer CNNs; doing so requires overcoming two challenges. First, the activation function of a CNN is nonlinear. In order to address this issue, we relax the class of CNN filters to a reproducing kernel Hilbert space (RKHS). This approach is inspired by the paper <ref type="bibr" target="#b34">(Zhang et al., 2016a)</ref>, which put forth a relaxation for fully-connected neural networks. Second, the parameter sharing induced by CNNs is crucial to its effectiveness and must be preserved. We show that CNNs with RKHS filters can be parametrized by a low-rank matrix. Relaxing this low-rank constraint to a nuclear norm constraint leads to our final formulation of CCNNs.</p><p>On the theoretical front, we prove an oracle inequality on the generalization error achieved by our class of CCNNs, showing that it is upper bounded by the best possible performance achievable by a two-layer CNN given infinite data-a quantity to which we refer as the oracle risk-plus a model complexity term that decays to zero polynomially in the sample size. Our results suggest that the sample complexity for CCNNs is significantly lower than that of the convexified fully-connected neural network <ref type="bibr" target="#b34">(Zhang et al., 2016a)</ref>, highlighting the importance of parameter sharing. For models with more than one hidden layer, our theory does not apply, but we provide encouraging empirical results using a greedy layer-wise training heuristic. Finally, we apply CCNNs to the MNIST handwritten digit dataset as well as four variation datasets (VariationsMNIST), and find that it achieves state-of-the-art accuracy.</p><p>Related work. With the empirical success of deep neural networks, there has been an increasing interest in understanding its connection to convex optimization. <ref type="bibr" target="#b3">Bengio et al. (2005)</ref> showed how to formulate neural network training as a convex optimization problem involving an infinite number of parameters. <ref type="bibr" target="#b0">Aslan et al. (2013;</ref> propose a method for learning multi-layer latent-variable models. They showed that for certain activation functions, the proposed method is a convex relaxation for learning fullyconnected neural networks.</p><p>Past work has studied learning translation-invariant features without backpropagation. <ref type="bibr" target="#b22">Mairal et al. (2014)</ref> present convolutional kernel networks. They propose a translationinvariant kernel whose feature mapping can be approximated by a composition of the convolution, non-linearity and pooling operators, obtained through unsupervised learning. However, this method is not equipped with the optimality guarantees that we provide for CCNNs in this paper, even for learning one convolutional layer. The ScatNet method <ref type="bibr" target="#b6">(Bruna &amp; Mallat, 2013</ref>) uses translation and deformation-invariant filters constructed by wavelet analysis; however, these filters are independent of the data, in contrast to CCNNs. <ref type="bibr" target="#b10">Daniely et al. (2016)</ref> show that a randomly initialized CNN can extract features as powerful as kernel methods, but it is not clear how to provably improve the model from random initialization.</p><p>Notation. For any positive integer n, we use [n] as a shorthand for the discrete set {1, 2, . . . , n}. For a rectangular matrix A, let A * be its nuclear norm, A 2 be its spectral norm (i.e., maximal singular value), and A F be its Frobenius norm. We use 2 (N) to denote the set of countable dimensional</p><formula xml:id="formula_0">vectors v = (v 1 , v 2 , . . . ) such that ∞ =1 v 2 &lt; ∞.</formula><p>For any vectors u, v ∈ 2 (N), the inner product u, v := ∞ =1 u i v i and the 2 -norm u 2 := u, u are well defined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and problem setup</head><p>In this section, we formalize the class of convolutional neural networks to be learned and describe the associated nonconvex optimization problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Convolutional neural networks</head><p>At a high level, a two-layer CNN 1 is a function that maps an input vector x ∈ R d0 (e.g., an image) to an output vector in y ∈ R d2 (e.g., classification scores for d 2 classes). This mapping is formed in the following manner:</p><p>1 Average pooling and multiple channels are also an integral part of CNNs, but these do not present any new technical challenges, so that we defer these extensions to Section 4.</p><p>• First, we extract a collection of P vectors {z p (x)} P p=1</p><p>of the full input vector x. Each vector z p (x) ∈ R d1 is referred to as a patch, and these patches may depend on overlapping components of x.</p><p>• Second, given some choice of activation function σ : R → R and a collection of weight vectors {w j } r j=1</p><p>in R d1 , we define the functions</p><formula xml:id="formula_1">h j (z) := σ(w j z) for each patch z ∈ R d1 . (1) Each function h j (for j ∈ [r]</formula><p>) is known as a filter, and note that the same filters are applied to each patch-this corresponds to the parameter sharing of a CNN.</p><formula xml:id="formula_2">• Third, for each patch index p ∈ [P ], filter index j ∈ [r],</formula><p>and output coordinate k ∈ [d 2 ], we introduce a coefficient α k,j,p ∈ R that governs the contribution of the filter h j on patch z p (x) to output f k (x). The final form of the CNN is given by f (x) : = (f 1 (x), . . . , f d2 (x)), where the k th component is given by</p><formula xml:id="formula_3">f k (x) := r j=1 P p=1 α k,j,p h j (z p (x)).<label>(2)</label></formula><p>Taking the patch functions {z p } P p=1 and activation function σ as fixed, the parameters of the CNN are the filter vectors</p><formula xml:id="formula_4">w := {w j ∈ R d1 : j ∈ [r]} along with the collection of coefficient vectors α := {α k,j ∈ R P : k ∈ [d 2 ], j ∈ [r]}.</formula><p>We assume that all patch vectors z p (x) ∈ R d1 are contained in the unit 2 -ball. This assumption can be satisfied without loss of generality by normalization: By multiplying a constant γ &gt; 0 to every patch z p (x) and multiplying 1/γ to the filter vectors w, this assumption holds without changing the the output of the network.</p><p>Given some positive radii B 1 and B 2 , we consider the model class</p><formula xml:id="formula_5">F cnn (B 1 , B 2 ) := f of the form (2) : max j∈[r]</formula><p>w j 2 ≤ B 1 and max</p><formula xml:id="formula_6">k∈[d2],j∈[r] α k,j 2 ≤ B 2 .<label>(3)</label></formula><p>When the radii (B 1 , B 2 ) are clear from context, we adopt F cnn as a convenient shorthand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Empirical risk minimization.</head><p>Given an input-output pair (x, y) and a CNN f , we let L(f (x); y) denote the loss incurred when the output y is predicted via f (x). We assume that the loss function L is convex and L-Lipschitz in its first argument given any value of its second argument. As a concrete example, for multiclass classification with d 2 classes, the output vector y takes values in the dis-</p><formula xml:id="formula_7">crete set [d 2 ] = {1, 2, . . . , d 2 }. For example, given a vector f (x) = (f 1 (x), . . . , f d2 (y)) ∈ R d2</formula><p>of classification scores, the associated multiclass logistic loss for a pair (x, y) is given by L(f (x); y)</p><formula xml:id="formula_8">:= −f y (x) + log d2 y =1 exp(f y (x)) . Given n training examples {(x i , y i )} n i=1</formula><p>, we would like to compute an empirical risk minimizer:</p><formula xml:id="formula_9">f cnn ∈ arg min f ∈Fcnn n i=1 L(f (x i ); y i ).<label>(4)</label></formula><p>Recalling that functions f ∈ F cnn depend on the parameters w and α in a highly nonlinear way (2), this optimization problem is nonconvex. As mentioned earlier, heuristics based on stochastic gradient methods are used in practice, which makes it challenging to gain a theoretical understanding of their behavior. Thus, in the next section, we describe a relaxation of the class F cnn for which empirical risk minimization is convex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Convexifying CNNs</head><p>We now turn to the development of the class of convexified CNNs. We begin in Section 3.1 by illustrating the procedure for the special case of the linear activation function. Although the linear case is not of practical interest, it provides intuition for our more general convexification procedure, described in Section 3.2, which applies to nonlinear activation functions. In particular, we show how embedding the nonlinear problem into an appropriately chosen reproducing kernel Hilbert space (RKHS) allows us to again reduce to the linear setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Linear activation functions: low rank relaxations</head><p>In order to develop intuition for our approach, let us begin by considering the simple case of the linear activation function σ(t) = t. In this case, the filter h j when applied to the patch vector z p (x) outputs a Euclidean inner product of the form</p><formula xml:id="formula_10">h j (z p (x)) = z p (x), w j . For each x ∈ R d0 , we first define the P × d 1 -dimensional matrix Z(x) :=    z 1 (x)</formula><p>. . .</p><formula xml:id="formula_11">z P (x)    .<label>(5)</label></formula><p>We also define the P -dimensional vector α k,j := (α k,j,1 , . . . , α k,j,P ) . With this notation, we can rewrite equation <ref type="formula" target="#formula_3">(2)</ref> for the k th output as</p><formula xml:id="formula_12">f k (x) = r j=1 P p=1 α k,j,p z p (x), w j = r j=1 α k,j Z(x)w j = tr Z(x) r j=1 w j α k,j = tr(Z(x)A k ),<label>(6)</label></formula><p>where in the final step, we have defined the</p><formula xml:id="formula_13">d 1 × P - dimensional matrix A k := r j=1 w j α k,j .</formula><p>Observe that f k now depends linearly on the matrix parameter A k . Moreover, the matrix A k has rank at most r, due to the parameter sharing of CNNs. See <ref type="figure">Figure 1</ref> for a graphical illustration of this model structure.</p><p>Letting A := (A 1 , . . . , A d2 ) be a concatenation of these matrices across all d 2 output coordinates, we can then define a function</p><formula xml:id="formula_14">f A : R d1 → R d2 of the form f A (x) := (tr(Z(x)A 1 ), . . . , tr(Z(x)A d2 )).<label>(7)</label></formula><p>Note that these functions have a linear parameterization in terms of the underlying matrix A. Our model class corresponds to a collection of such functions based on imposing certain constraints on the underlying matrix A: in particular, we define F cnn (B 1 , B 2 ) to be the set of functions f A which satisfies:</p><formula xml:id="formula_15">(C1) max j∈[r] w j 2 ≤ B 1 , max k∈[d2],j∈[r] α k,j 2 ≤ B 2 ; and (C2) rank(A) = r.</formula><p>This is simply an alternative formulation of our original class of CNNs defined in equation <ref type="formula" target="#formula_6">(3)</ref>. Notice that if the filter weights w j are not shared across all patches, then the constraint (C1) still holds, but constraint (C2) no longer holds. Thus, the parameter sharing of CNNs is realized by the low-rank constraint (C2). The matrix A of rank r can be decomposed as A = U V , where both U and V have r columns. The column space of matrix A contains the convolution parameters {w j }, and the row space of A contains to the output parameters {α k,j }.</p><p>The matrices satisfying constraints (C1) and (C2) form a nonconvex set. A standard convex relaxation is based on the nuclear norm A * corresponding to the sum of the singular values of A. It is straightforward to verify that any matrix A satisfying the constraints (C1) and (C2) must have nuclear norm bounded as A * ≤ B 1 B 2 r √ d 2 . Consequently, if we define the function class</p><formula xml:id="formula_16">F ccnn := f A : A * ≤ B 1 B 2 r d 2 ,<label>(8)</label></formula><p>then we are guaranteed that F ccnn ⊇ F cnn .</p><p>We propose to minimize the empirical risk (4) over F ccnn instead of F cnn ; doing so defines a convex optimization problem over this richer class of functions</p><formula xml:id="formula_17">f ccnn := arg min f A ∈Fccnn n i=1 L(f A (x i ); y i ).<label>(9)</label></formula><p>In Section 3.3, we describe iterative algorithms that can be used to solve this convex problem in the more general setting of nonlinear activation functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Nonlinear activations: RKHS filters</head><p>For nonlinear activation functions σ, we relax the class of CNN filters to a reproducing kernel Hilbert space (RKHS). As we will show, this relaxation allows us to reduce the problem to the linear activation case.</p><p>Let K : R d1 × R d1 → R be a positive semidefinite kernel function. For particular choices of kernels (e.g., the Gaus-</p><formula xml:id="formula_18">f k (x) = tr Z(x) z 1 (x) z P (x) P d 1 × w 1 w r d 1 r (filters) × α k,1 α k,r r P (patches) A k Figure 1. The k th output of a CNN f k (x)</formula><p>∈ R can be expressed as the product between a matrix Z(x) ∈ R P ×d 1 whose rows are features of the input patches and a rank-r matrix A k ∈ R d 1 ×P , which is made up of the filter weights {wj} and coefficients {a k,j,p }, as illustrated. Due to the parameter sharing intrinsic to CNNs, the matrix A k inherits a low rank structure, which can be encouraged via convex relaxation using the nuclear norm.</p><p>sian RBF kernel) and a sufficiently smooth activation function σ, we are able to show that the filter h : z → σ( w, z ) is contained in the RKHS induced by the kernel function K. See Section 3.4 for the choice of the kernel function and the activation function.</p><formula xml:id="formula_19">Let S := {z p (x i ) : p ∈ [P ], i ∈ [n]}</formula><p>be the set of patches in the training dataset. The representer theorem then implies that for any patch z p (x i ) ∈ S, the function value can be represented by</p><formula xml:id="formula_20">h(z p (x i )) = (i ,p )∈[n]×[P ] c i ,p K(z p (x i ), z p (x i )) (10) for some coefficients {c i ,p } (i ,p )∈[n]×[P ]</formula><p>. Filters of the form (10) are members of the RKHS, because they are linear combinations of basis functions z → K(z, z p (x i )). Such filters are parametrized by a finite set of coefficients</p><formula xml:id="formula_21">{c i ,p } (i ,p )∈[n]×[P ]</formula><p>, which can be estimated via empirical risk minimization.</p><p>Let K ∈ R nP ×nP be the symmetric kernel matrix, where with rows and columns indexed by the example-patch index pair</p><formula xml:id="formula_22">(i, p) ∈ [n] × [P ]. The entry at row (i, p) and column (i , p ) of matrix K is equal to K(z p (x i ), z p (x i )).</formula><p>So as to avoid re-deriving everything in the kernelized setting, we perform a reduction to the linear setting of Section 3.1. Consider a factorization K = QQ of the kernel matrix, where Q ∈ R nP ×m ; one example is the Cholesky factorization with m = nP . We can interpret each row Q (i,p) ∈ R m as a feature vector in place of the original z p (x i ) ∈ R d1 , and rewrite equation <ref type="formula">(10)</ref> as</p><formula xml:id="formula_23">h(z p (x i )) = Q (i,p) , w where w := (i ,p ) c i ,p Q (i ,p ) .</formula><p>In order to learn the filter h, it suffices to learn the mdimensional vector w. To do this, define patch matrices Z(x i ) ∈ R P ×m for each i ∈ [n] so that its p-th row is Q <ref type="bibr">(i,p)</ref> . Then the problem reduces to learning a linear filter with coefficient vector w. Carrying out all of Section 3.1, solving the ERM gives us a parameter matrix A ∈ R m×P d2 . The only difference is that 2 -norm constraint (C1) needs to be adapted to the norm of the RKHS. See Appendix B for details.</p><p>At test time, given a new input x ∈ R d0 , we can compute a patch matrix Z(x) ∈ R P ×m as follows:</p><p>• The p-th row of this matrix is the feature vector for patch p, which is equal to Q † v(z p (x)) ∈ R m , where for any patch z, the vector v(z) is defined as a nPdimensional vector whose (i, p)-th coordinate is equal to K(z, z p (x i )). We note that if x is an instance x i in the training set, then the vector Q † v(z p (x)) is exactly equal to Q <ref type="bibr">(i,p)</ref> . Thus the mapping Z(x) applies to both training and testing.</p><p>• We can then compute the predictor f k (x) = tr(Z(x)A k ) via equation (6). Note that we do not explicitly need to compute the filter values h j (z p (x)) to compute the output under the CCNN.</p><p>Retrieving filters. When we learn multi-layer CCNNs (Section 4), we need to compute the filters h j explicitly in order to form the inputs to the next layer. Recall from Section 3.1 that the column space of matrix A corresponds to parameters of the convolutional layer, and the row space of A corresponds to parameters of the output layer. Thus, once we obtain the parameter matrix A, we compute a rankr approximation A ≈ U V . Then set the j-th filter h j to the mapping</p><formula xml:id="formula_24">z → U j , Q † v(z) for any patch z ∈ R d1 ,<label>(11)</label></formula><p>where U j ∈ R m is the j-th column of matrix U , and Q † v(z) represents the feature vector for patch z. 2 The matrix V encodes parameters of the output layer, thus Algorithm 1 Learning two-layer CCNNs</p><formula xml:id="formula_25">Input: Data {(xi, yi)} n i=1</formula><p>, kernel function K, regularization parameter R &gt; 0, number of filters r. 1. Construct a kernel matrix K ∈ R nP ×nP such that the entry at column (i, p) and row (i , p ) is equal to K(zp(xi), z p (x i )). Compute a factorization K = QQ or an approximation K ≈ QQ , where Q ∈ R nP ×m . 2. For each xi, construct patch matrix Z(xi) ∈ R P ×m whose p-th row is the (i, p)-th row of Q, where Z(·) is defined in Section 3.2. 3. Solve the following optimization problem to obtain a matrix A = ( A1, . . . , A d 2 ):</p><formula xml:id="formula_26">A ∈ argmin A * ≤R L(A) where (12) L(A) := n i=1 L tr(Z(xi)A1), . . . , tr(Z(xi)A d 2 ) ; yi .</formula><p>4. Compute a rank-r approximation U V ≈ A where U ∈ R m×r and V ∈ R P d 2 ×r .</p><p>Output: predictor fccnn(x) := tr(Z(x) A1), . . . , tr(Z(x) A d 2 ) and the convolutional layer output H(x) := U Z(x) .</p><p>doesn't appear in the filter expression (11). It is important to note that the filter retrieval is not unique, because the rank-r approximation of the matrix A is not unique. The heuristic we suggest is to form the singular value decomposition A = U ΛV , then define U to be the first r columns of U .</p><p>When we apply all of the r filters to all patches of an input x ∈ R d0 , the resulting output is H(x) := U Z(x) -this is an r × P matrix whose element at row j and column p is equal to h j (z p (x)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Algorithm</head><p>The algorithm for learning a two-layer CCNN is summarized in Algorithm 1; it is a formalization of the steps described in Section 3.2. In order to solve the optimization problem (12), the simplest approach is via projected gradient descent: At iteration t, using a step size η t &gt; 0, we form the new matrix A t+1 based on the previous iterate A t according to:</p><formula xml:id="formula_27">A t+1 = Π R A t − η t ∇ A L(A t ) .<label>(13)</label></formula><p>Here ∇ A L denotes the gradient of the objective function defined in (12), and Π R denotes the Euclidean projection onto the nuclear norm ball {A : A * ≤ R}. This nuclear norm projection can be obtained by first computing the singular value decomposition of A, and then projecting the vector of singular values onto the 1 -ball. This latter projection step can be carried out efficiently by the algorithm of <ref type="bibr" target="#b12">Duchi et al. (2008)</ref>. There are other efficient optimization algorithms <ref type="bibr" target="#b13">(Duchi et al., 2011;</ref><ref type="bibr" target="#b33">Xiao &amp; Zhang, 2014)</ref> for solving the problem (12). All these algorithms can be executed in a stochastic fashion, so that each gradient step processes a mini-batch of examples.</p><p>The computational complexity of each iteration depends on the width m of the matrix Q. Setting m = nP allows us to solve the exact kernelized problem, but to improve the computation efficiency, we can use Nyström approximation <ref type="bibr" target="#b11">(Drineas &amp; Mahoney, 2005)</ref> or random feature approximation <ref type="bibr" target="#b24">(Rahimi &amp; Recht, 2007)</ref>; both are randomized methods to obtain a tall-and-thin matrix Q ∈ R nP ×m such that K ≈ QQ . Typically, the parameter m is chosen to be much smaller than nP . In order to compute the matrix Q, the Nyström approximation method takes O(m 2 nP ) time. The random feature approximation takes O(mnP d 1 ) time, but can be improved to O(mnP log d 1 ) time using the fast Hadamard transform <ref type="bibr" target="#b19">(Le et al., 2013)</ref>. The time complexity of project gradient descent also scales with m rather than with nP .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Theoretical results</head><p>In this section, we upper bound the generalization error of Algorithm 1, proving that it converges to the best possible generalization error of CNN. We focus on the binary classification case where the output dimension is d 2 = 1. The learning of CCNN requires a kernel function K. We consider kernel functions whose associated RKHS is large enough to contain any function of the following form: z → q( w, z ), where q is an arbitrary polynomial function and w ∈ R d1 is an arbitrary vector. As a concrete example, we consider the inverse polynomial kernel:</p><formula xml:id="formula_28">K(z, z ) := 1 2 − z, z , z 2 ≤ 1, z 2 ≤ 1.<label>(14)</label></formula><p>This kernel was studied by Shalev-Shwartz et al. <ref type="formula" target="#formula_3">(2011)</ref> for learning halfspaces, and by <ref type="bibr" target="#b34">Zhang et al. (2016a)</ref> for learning fully-connected neural networks. We also consider the Gaussian RBF kernel:</p><formula xml:id="formula_29">K(z, z ) := e −γ z−z 2 2 , z 2 = z 2 = 1.<label>(15)</label></formula><p>As shown by Appendix A, the inverse polynomial kernel and the Gaussian kernel satisfy the above notion of richness. We focus on these two kernels for the theoretical analysis.</p><p>Let f ccnn be the CCNN that minimizes the empirical risk (12) using one of the two kernels above. Our main theoretical result is that for suitably chosen activation functions, the generalization error of f ccnn is comparable to that of the best CNN model. In particular, we consider the following types of activation functions σ:</p><p>(a) arbitrary polynomial functions (e.g., used by <ref type="bibr" target="#b8">Chen &amp; Manning (2014)</ref>; <ref type="bibr" target="#b21">Livni et al. (2014)</ref>).</p><p>(b) sinusoid activation function σ(t) := sin(t) (e.g., used by <ref type="bibr" target="#b28">Sopena et al. (1999)</ref>; <ref type="bibr" target="#b16">Isa et al. (2010)</ref>).</p><p>(c) erf function σ erf (t) := 2/ √ π t 0 e −z 2 dz, which represents a close approximation to the sigmoid function <ref type="bibr" target="#b34">(Zhang et al., 2016a)</ref>.</p><p>(d) a smoothed hinge loss σ sh (t) := t −∞ 1 2 (σ erf (z) + 1)dz, which represents a close approximation to the ReLU function <ref type="bibr" target="#b34">(Zhang et al., 2016a)</ref>.</p><p>To understand how these activation functions pair with our choice of kernels, we consider polynomial expansions of the above activation functions: σ(t) = ∞ j=0 a j t j , and note that the smoothness of these functions are characterized by the rate of their coefficients {a j } converge quickly enough to zero (the criterion depends on the concrete choice of the kernel). Concretely, the inverse polynomial kernel is shown to capture all of the four categories of activations: thus, (a), (b), (c), and (d) are all are referred as valid activation functions for the inverse polynomial kernel. The Gaussian kernel induces a smaller RKHS, so only (a) and (b) are valid activation functions for the Gaussian kernel. In contrast, the sigmoid function and the ReLU function are not valid for either kernel, because their polynomial expansions fail to converge quickly enough, or more intuitively speaking, because they are not smooth enough to be contained in the RKHS.</p><p>We are ready to state the main theoretical result. In the theorem statement, we use K(X) ∈ R P ×P to denote the random kernel matrix obtained from an input vector X ∈ R d0 drawn randomly from the population. More precisely, the (p, q)-th entry of K(X) is given by K(z p (X), z q (X)). Theorem 1. Assume that the loss function L(·; y) is LLipchitz continuous for every y ∈ [d 2 ] and that K is the inverse polynomial kernel or the Gaussian kernel. For any valid activation function σ, there is a constant C σ (B 1 ) such that by choosing hyper-parameter R := C σ (B 1 )B 2 r in Algorithm 1, the expected generalization error is at most</p><formula xml:id="formula_30">E X,Y [L( f ccnn (X); Y )] ≤ inf f ∈Fcnn E X,Y [L(f (X); Y )] + c LC σ (B 1 )B 2 r log(nP ) E X [ K(X) 2 ] √ n ,<label>(16)</label></formula><p>where c &gt; 0 is a universal constant.</p><p>Proof sketch The proof of Theorem 1 consists of two parts: First, we consider a larger function class that contains the class of CNNs. This function class is defined as:</p><formula xml:id="formula_31">F ccnn := x → r * j=1 P p=1 α j,p h j (z p (x)) : r * &lt; ∞ (17) and r * j=1 α j 2 h j H ≤ C σ (B 1 )B 2 d 2 .<label>(18)</label></formula><p>where · H is the norm of the RKHS associated with the kernel. This new function class relaxes the class of CNNs in two ways: 1) the filters are relaxed to belong to the RKHS, and 2) the 2 -norm bounds on the weight vectors are replaced by a single constraint on α j 2 and h j H . We prove the following property for the predictor f ccnn : it must be an empirical risk minimizer of F ccnn . This property holds even though equation <ref type="formula" target="#formula_16">(18)</ref> defines a non-parametric function class F ccnn , while Algorithm 1 optimizes f ccnn in a parametric function class.</p><p>Second, we characterize the Rademacher complexity of this new function class F ccnn , proving an upper bound for it based on the matrix concentration theory. Combining this bound with the classical Rademacher complexity theory <ref type="bibr" target="#b2">(Bartlett &amp; Mendelson, 2003)</ref>, we conclude that the generalization loss of f ccnn converges to the least possible generalization error of F ccnn . The latter loss is bounded by the generalization loss of CNNs (because F cnn ⊆ F ccnn ), which establishes the theorem. See the full version of this paper <ref type="bibr" target="#b35">(Zhang et al., 2016b</ref>) for a rigorous proof of Theorem 1.</p><p>Remark on activation functions. It is worth noting that the quantity C σ (B 1 ) depends on the activation function σ, and more precisely, depends on the convergence rate of the polynomial expansion of σ. Appendix A shows that if σ is a polynomial function of degree , then C σ (B 1 ) = O(B 1 ). If σ is the sinusoid function, the erf function or the smoothed hinge loss, then the quantity C σ (B 1 ) will be exponential in B 1 . From an algorithmic perspective, we don't need to know the activation function for executing Algorithm 1. From a theoretical perspective, however, the choice of σ is relevant from the point of Theorem 1 to compare f ccnn with the best CNN, whose representation power is characterized by the choice of σ. Therefore, if a CNN with a low-degree polynomial σ performs well on a given task, then CCNN also enjoys correspondingly strong generalization. Empirically, this is actually borne out: in Section 5, we show that the quadratic activation function performs almost as well as the ReLU function for digit classification.</p><p>Remark on parameter sharing. In order to demonstrate the importance of parameter sharing, consider a CNN without parameter sharing, so that we have filter weights w j,p for each filter index j and patch index p. With this change, the new CNN output (2) is</p><formula xml:id="formula_32">f (x) = r j=1 P p=1 α j,p σ(w j,p z p (x)),</formula><p>where α j,p ∈ R and w j,p ∈ R d1 . Note that the hidden layer of this new network has P times more parameters than that of the convolutional neural network with parameter sharing. These networks without parameter sharing can be learned by the recursive kernel method proposed by <ref type="bibr" target="#b34">Zhang et al. (2016a)</ref>. Their paper shows that under the norm constraints w j 2 ≤ B 1 and r j=1 P p=1 |α j,p | ≤ B 2 , the excess risk of the recursive kernel method is at most O(LC σ (B 1 )B 2 K max /n), where K max = max z: z 2 ≤1 K(z, z) is the maximal value of the kernel function. Plugging in the norm constraints of the function class F cnn , we have B 1 = B 1 and B 2 = B 2 r √ P . Thus, the expected risk of the estimated f is bounded by:</p><formula xml:id="formula_33">E X,Y [L( f (X); Y )] ≤ inf f ∈Fcnn E X,Y [L(f (X); Y )] + c LC σ (B 1 )B 2 r √ P K max √ n .<label>(19)</label></formula><p>Comparing this bound to Theorem 1, we see that (apart from the logarithmic terms) they differ in the multiplicative factors of</p><formula xml:id="formula_34">√ P K max versus E[ K(X) 2 ]. Since the matrix K(X) is P -dimensional, we have K(X) 2 ≤ max p∈[P ] q∈[P ] |K(z p (X), z q (X))| ≤ P K max .</formula><p>This demonstrates that √ P K max is always greater than E[ K(X) 2 ]. In general, the first term can be up to factor of √ P times greater, which implies that the sample complexity of the recursive kernel method is up to P times greater than that of the CCNN. This difference is intuitive given that the recursive kernel method learns a model with P times more parameters. Although comparing the upper bounds doesn't rigorously show that one method is better than the other, it gives intuition for understanding the importance of parameter sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning multi-layer CCNNs</head><p>In this section, we describe a heuristic method for learning CNNs with more layers. The idea is to estimate the parameters of the convolutional layers incrementally from bottom to top. Before presenting the multi-layer algorithm, we present two extensions, average pooling and multi-channel inputs.</p><p>Average pooling. Average pooling is a technique to reduce the output dimension of the convolutional layer from dimensions P × r to dimensions P × r with P &lt; P . For the CCNN model, if we apply average pooling after Algorithm 2 Learning multi-layer CCNNs</p><formula xml:id="formula_35">Input:Data {(xi, yi)} n i=1</formula><p>, kernel function K, number of layers m, regularization parameters R1, . . . , Rm, number of filters r1, . . . , rm. Define H1(x) = x. For each layer s = 2, . . . , m:</p><p>• Train a two-layer network by Algorithm 1, taking</p><formula xml:id="formula_36">{(Hs−1(xi), yi)} n i=1</formula><p>as training examples and Rs, rs as parameters. Let Hs be the output of the convolutional layer and fs be the predictor. Output: Predictor fm and the top layer output Hm.</p><p>the convolutional layer, then the k-th output of the CCNN model becomes tr(GZ(x)A k ) where G ∈ R P ×P is the pooling matrix. Thus, performing a pooling operation requires only replacing every matrix Z(x i ) in problem <ref type="formula" target="#formula_3">(12)</ref> by the pooled matrix GZ(x i ). Note that the linearity of the CCNN allows us to effectively pool before convolution, even though for the CNN, pooling must be done after applying the nonlinear filters. The resulting ERM problem is still convex, and the number of parameters have been reduced by P/P -fold.</p><p>Processing multi-channel inputs. If our input has C channels (corresponding to RGB colors, for example), then the input becomes a matrix x ∈ R C×d0 . The c-th row of matrix x, denoted by x[c] ∈ R d0 , is a vector representing the c-th channel. We define the multi-channel patch vector as a concatenation of patch vectors for each channel:</p><formula xml:id="formula_37">z p (x) := (z p (x[1]), . . . , z p (x[C])) ∈ R Cd1 .</formula><p>Then we construct the feature matrix Z(x) using the concatenated patch vectors {z p (x)} P p=1 . From here, everything else of Algorithm 1 remains the same. We note that this approach learns a convex relaxation of filters taking the form σ(</p><formula xml:id="formula_38">C c=1 w c , z p (x[c]) ), parametrized by the vectors {w c } C c=1 .</formula><p>Multi-layer CCNN. Given these extensions, we are ready to present the algorithm for learning multi-layer CCNNs, summarized in Algorithm 2. For each layer s, we call Algorithm 1 using the output of previous convolutional layers as input-note that this consists of r channels (one from each previous filter); thus we must use the multi-channel extension. Algorithm 2 outputs a new convolutional layer along with a prediction function, which is kept only at the last layer. We optionally use averaging pooling after each successive layer. to reduce the output dimension of the convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we compare the CCNN approach with other methods on the MNIST dataset and more challenging variations (VariationsMNIST), including adding white noise (rand), random rotation (rot), random image back-basic rand rot img img+rot SVM rbf <ref type="bibr" target="#b31">(Vincent et al., 2010)</ref> 3.03% 14.58% 11.11% 22.61% 55.18% NN-1 <ref type="bibr" target="#b31">(Vincent et al., 2010)</ref> 4  <ref type="table">Table 1</ref>. Classification error on the basic MNIST and its four variations. The best performance within each block is bolded. "ReLU" and "Quad" denote using the ReLU and quadratic activation functions, respectively. ground (img) or combining the last two (img+rot). For all datasets, we use 10,000 images for training, 2,000 images for validation and 50,000 images for testing. This 10k/2k/50k partitioning is standard for MNIST variations (VariationsMNIST).</p><p>For the CCNN method and the baseline CNN method, we train two-layer and three-layer models respectively. The models with k convolutional layers are denoted by CCNNk and CNN-k. Each convolutional layer is constructed on 5 × 5 patches with unit stride, followed by 2 × 2 average pooling. The first and the second convolutional layers contains 16 and 32 filters, respectively. The loss function is chosen as the 10-class logistic loss. We use Gaussian kernel for the CCNN. The feature matrix Z(x) is constructed via random feature approximation <ref type="bibr" target="#b24">(Rahimi &amp; Recht, 2007)</ref> with dimension m = 500 for the first convolutional layer and m = 1000 for the second. Before training each CCNN layer, we preprocess the input vectors z p (x i ) using local contrast normalization and ZCA whitening <ref type="bibr" target="#b9">(Coates et al., 2010)</ref>. The convex optimization problem is solved by projected SGD with mini-batches of size 50. Code and reproducible experiments are available on the CodaLab platform 4 .</p><p>As a baseline approach, the CNN models are activated by the ReLU function σ(t) = max{0, t} or the quadratic function σ(t) = t 2 . We train them using mini-batch SGD. The input images are preprocessed by global contrast normalization and ZCA whitening <ref type="bibr" target="#b29">(Srivastava et al., 2014)</ref>. We compare our method against several alternative baselines. The CCNN-1 model is compared against an SVM with the Gaussian RBF kernel (SVM rbf ) and a fully connected neural network with one hidden layer (NN-1). The CCNN-2 model is compared against methods that report the state-of-the-art results on these datasets, including the translation-invariant RBM model (TIRBM) <ref type="bibr" target="#b27">(Sohn &amp; Lee, 2012)</ref>, the stacked denoising auto-encoder with 4 http://worksheets.codalab.org/ worksheets/0x1468d91a878044fba86a5446f52aacde/ three hidden layers (SDAE-3) <ref type="bibr" target="#b31">(Vincent et al., 2010)</ref>, the ScatNet-2 model <ref type="bibr" target="#b6">(Bruna &amp; Mallat, 2013</ref>) and the PCANet-2 model <ref type="bibr" target="#b7">(Chan et al., 2015)</ref>. <ref type="table">Table 1</ref> shows the classification errors on the test set. The models are grouped with respect to the number of layers that they contain. For models with one convolutional layer, the errors of CNN-1 are significantly lower than that of NN-1, highlighting the benefits of local filters and parameter sharing. The CCNN-1 model outperforms CNN-1 on all datasets. For models with two or more hidden layers, the CCNN-2 model outperforms CNN-2 on all datasets, and is competitive against the state-of-the-art. In particular, it achieves the best accuracy on the rand, img and img+rot dataset, and is comparable to the state-of-the-art on the remaining two datasets. Further adding a third convolutional layer doesn't notibly improve the performance on these datasets.</p><p>In Section 3.4, we showed that if the activation function σ is a polynomial function, then the CCNN (which does not depend on σ) requires lower sample complexity to match the performance of the best possible CNN using σ. More precisely, if σ is a degree-polynomial, then C σ (B) in the upper bound will be controlled by O(B ). This motivates us to study the performance of low-degree polynomial activations. <ref type="table">Table 1</ref> shows that the CNN-2 model with a quadratic activation function achieves error rates comparable to that with a ReLU activation: CNN-2 (Quad) outperforms CNN-2 (ReLU) on the basic and rand datasets, and is only slightly worse on the rot and img dataset. Since the performance of CCNN matches that of the best possible CNN, the good performance of the quadratic activation in part explains why the CCNN is also good.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>zero. If σ is a polynomial in category (a), then the richness of the RKHS guarantees that it contains the class of filters activated by function σ. If σ is a non-polynomial function in categories (b),(c),(d), then as Appendix A shows, the RKHS contains the filter only if the coefficients {a j } ∞ j=0</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Stanford University, CA, USA</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">If z is a patch in the training set, namely z = zp(xi), then we have equation Q † v(z) = Q (i,p)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We can treat the multiclass case by performing a standard one-versus-all reduction to the binary case.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. MJW and YZ were partially supported by the Office of Naval Research Grant DOD ONR-N00014 and the NSF Grant NSF-DMS-1612948. PL and YZ were partially supported by the Microsoft Faculty Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Convex two-layer modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Özlem</forename><surname>Aslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2985" to="2993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convex deep learning via normalized kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Özlem</forename><surname>Aslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3275" to="3283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rademacher and Gaussian complexities: Risk bounds and structural results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahar</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convex neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Marcotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="123" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Training a 3-node neural network is NP-complete</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><forename type="middle">L</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="127" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Online learning and stochastic approximations. On-line learning in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">142</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pcanet: A simple deep learning baseline for image classification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsung-Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shenghua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zinan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5017" to="5032" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1001</biblScope>
			<pubPlace>Ann Arbor</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Daniely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.05897</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the Nyström method for approximating a Gram matrix for improved kernel-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2153" to="2175" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient projections onto the 1 -ball for learning in high dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="272" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An empirical study of learning speed in back-propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Fahlman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Heuristics</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Navdeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Suitable mlp network activation functions for breast cancer and thyroid disease detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Isa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Is</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Omar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Sakim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 Second International Conference on Computational Intelligence, Modelling and Simulation</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="39" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face recognition: A convolutional neuralnetwork approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Back</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="113" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fastfoodapproximating kernel expansions in loglinear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamás</forename><surname>Sarlós</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Léon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the computational efficiency of training neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Livni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="855" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Piotr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2627" to="2635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volodymyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning kernel-based halfspaces with the 0-1 loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1623" to="1646" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Julian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning invariant representations with local transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning (ICML-12)</title>
		<meeting>the 29th International Conference on Machine Learning (ICML-12)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1311" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural networks with periodic and monotonic activation functions: a comparative study in classification problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep</forename><forename type="middle">M</forename><surname>Sopena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Alquezar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN 99</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="323" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Variations on the MNIST digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Variationsmnist</surname></persName>
		</author>
		<ptr target="http://www.iro.umontreal.ca/˜lisa/twiki/bin/view.cgi/Public/MnistVariations" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hugo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Isabelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Andrew Y. End-to-end text recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2012 21st International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3304" to="3308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A proximal stochastic gradient method with progressive variance reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2057" to="2075" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">1 -regularized neural networks are improperly learnable in polynomial time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings on the 33rd International Conference on Machine Learning</title>
		<meeting>on the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Convexified convolutional neural networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<idno>abs/1609.01000</idno>
		<ptr target="http://arxiv.org/abs/1609.01000" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
