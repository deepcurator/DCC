<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Novelty Detection for Visual Object Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Min</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Google Brain</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Novelty Detection for Visual Object Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object recognition in large-scale image datasets has achieved impressive performance with deep convolutional neural networks (CNNs) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref>. The standard CNN architectures are learned to recognize a predefined set of classes seen during training. However, in practice, a new type of objects could emerge (e.g., a new kind of consumer product). Hence, it is desirable to extend the CNN architectures for detecting the novelty of an object (i.e., deciding if the object does not match any previously trained object classes). There have been recent efforts toward developing efficient novelty detection methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref>, but most of the existing methods measure only the model uncertainty, i.e., confidence score, which is often too ambiguous for practical use. For example, suppose one trains a classifier on an animal image dataset as in <ref type="figure">Figure 1</ref>. A standard novelty detection method can be applied to a cat-like image to evaluate its novelty, but such a method would not tell  <ref type="figure">Figure 1</ref>: An illustration of our proposed hierarchical novelty detection task. In contrast to prior novelty detection works, we aim to find the most specific class label of a novel data on the taxonomy built with known classes.</p><p>whether the novel object is a new species of cat unseen in the training set or a new animal species.</p><p>To address this issue, we design a new classification framework for more informative novelty detection by utilizing a hierarchical taxonomy, where the taxonomy can be extracted from the natural language information, e.g., WordNet hierarchy <ref type="bibr" target="#b21">[22]</ref>. Our approach is also motivated by a strong empirical correlation between hierarchical semantic relationships and the visual appearance of objects <ref type="bibr" target="#b4">[5]</ref>. Under our scheme, a taxonomy is built with the hypernymhyponym relationships between known classes such that objects from novel classes are expected to be classified into the most relevant label, i.e., the closest class in the taxonomy. For example, as illustrated in <ref type="figure">Figure 1</ref>, our goal is to distinguish "new cat," "new dog," and "new animal," which cannot be achieved in the standard novelty detection tasks. We call this problem hierarchical novelty detection task.</p><p>In contrast to standard object recognition tasks with a closed set of classes, our proposed framework can be useful for extending the domain of classes to an open set with taxonomy information (i.e., dealing with any objects unseen in training). In practical application scenarios, our framework can be potentially useful for automatically or interactively organizing a customized taxonomy (e.g., company's prod-uct catalog, wildlife monitoring, personal photo library) by suggesting closest categories for an image from novel categories (e.g., new consumer products, unregistered animal species, untagged scenes or places).</p><p>We propose two different approaches for hierarchical novelty detection: top-down and flatten methods. In the topdown method, each super class has a confidence-calibrated classifier which detects a novel class if the posterior categorical distribution is close to a uniform distribution. Such a classifier was recently studied for a standard novelty detection task <ref type="bibr" target="#b18">[19]</ref>, and we extend it for detecting novel classes under our hierarchical novelty detection framework. On the other hand, the flatten method computes a softmax probability distribution of all disjoint classes. Then, it predicts the most likely fine-grained label, either a known class or a novel class. Although the flatten method simplifies the full hierarchical structure, it outperforms the top-down method for datasets of a large hierarchical depth.</p><p>Furthermore, we combine two methods for utilizing their complementary benefits: top-down methods naturally leverage the hierarchical structure information, but the classification performance might be degraded due to the error aggregation. On the contrary, flatten methods have a single classification rule that avoids the error aggregation, but the classifier's flat structure does not utilize the full information of hierarchical taxonomy. We empirically show that combining the top-down and flatten models further improves hierarchical novelty detection performance.</p><p>Our method can also be useful for generalized zero-shot learning (GZSL) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33]</ref> tasks. GZSL is a classification task with classes both seen and unseen during training, given that semantic side-information for all test classes is provided. We show that our method can generate a hierarchical embedding that leads to improved GZSL performance in combination with other commonly-used semantic embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Novelty detection. For robust prediction, it is desirable to detect a test sample if it looks unusual or significantly differs from the representative training data. Novelty detection is a task recognizing such abnormality of data (see <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref> for a survey). Recent novelty detection approaches leverage the output of deep neural network classification models. A confidence score about novelty can be measured by taking the maximum predicted probability <ref type="bibr" target="#b12">[13]</ref>, ensembling such outputs from multiple models <ref type="bibr" target="#b16">[17]</ref>, or synthesizing a score based on the predicted categorical distribution <ref type="bibr" target="#b1">[2]</ref>. There have also been recent efforts toward confidence-calibrated novelty detection, i.e., calibrating how much the model is certain with its novelty detection, by postprocessing <ref type="bibr" target="#b20">[21]</ref> or learning with joint objective <ref type="bibr" target="#b18">[19]</ref>. Object recognition with taxonomy. Incorporating the hierarchical taxonomy for object classification has been investigated in the literature, either to improve classification performance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">34]</ref>, or to extend the classification tasks to obtain more informative results <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">36]</ref>. Specifically for the latter purpose, Deng et al. <ref type="bibr" target="#b7">[8]</ref> gave some reward to super class labels in a taxonomy and maximized the expected reward. Zhao et al. <ref type="bibr" target="#b35">[36]</ref> proposed an open set scene parsing framework, where the hierarchy of labels is used to estimate the similarity between the predicted label and the ground truth. In contemporary work, Simeone et al. <ref type="bibr" target="#b27">[28]</ref> proposed a hierarchical classification and novelty detection task for the music genre classification, but their settings are different from ours: in their task, novel classes do not belong to any node in the taxonomy. Thus, their method cannot detect classes which are novel but similar to existing classes. To the best of our knowledge, our work is the first to propose a unified framework for hierarchical novelty detection and visual object recognition. Generalized zero-shot learning (GZSL). We remark that GZSL <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33]</ref> can be thought as addressing a similar task as ours. While the standard ZSL tasks test classes unseen during training only, GZSL tasks test both seen and unseen classes such that the novelty is automatically detected if the predicted label is not a seen class. However, the primary focus of ZSL and GZSL tasks is on transfer learning for a new domain, and they assume that semantic information of all test classes is given, e.g., attributes <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref> or text description <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref> of the objects. Therefore, GZSL cannot recognize a novel class if prior knowledge about the specific novel class is not provided, i.e., it is limited to classifying objects with prior knowledge, regardless of their novelty. Compared to GZSL, the advantages of the proposed hierarchical novelty detection are that 1) it does not require any prior knowledge on novel classes but only utilizes the taxonomy of known classes, 2) a reliable super class label can be more useful and human-friendly than an error-prone prediction over excessively subdivided classes, and 3) high-quality taxonomies are available off-the-shelf and they are better interpretable than latent semantic embeddings. In Section 5, we also show that our models for hierarchical novelty detection can also generate a hierarchical embedding such that combination with other semantic embeddings improves the GZSL performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section, we define terminologies to describe hierarchical taxonomy and then propose models for hierarchical classification combined with novelty detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Taxonomy</head><p>A taxonomy represents a hierarchical relationship among classes, where each node in the taxonomy corresponds to a class or a set of indistinguishable classes.  <ref type="figure">Figure 2</ref>: Illustration of two proposed approaches. In the top-down method, classification starts from the root class, and propagates to one of its children until the prediction arrives at a known leaf class (blue) or stops if the prediction is not confident, which means that the prediction is a novel class whose closest super class is the predicted class. In the flatten method, we add a virtual novel class (red) under each super class as a representative of all novel classes, and then flatten the structure for classification.</p><p>We define three types of classes as follows: 1) known leaf classes are nodes with no child, which are known and seen during training, 2) super classes are ancestors of the leaf classes, which are also known, and 3) novel classes are unseen during training, so they do not explicitly appear in the taxonomy. <ref type="bibr" target="#b1">2</ref> We note that all known leaf and novel classes have no child and are disjoint, i.e., they are neither ancestor nor descendant of each other. In the example in <ref type="figure">Figure 1</ref>, four species of cats and dogs are leaf classes, "cat," "dog," and "animal" are super classes, and any other classes unseen during training, e.g., "Angola cat," "Dachshund," and "Pika" are novel classes.</p><p>In the proposed hierarchical novelty detection framework, we first build a taxonomy with known leaf classes and their super classes, and at test time, we aim at predicting in the most fine-grained way using the taxonomy. In other words, if an image is predicted as novel, then we try to assign one of the super classes, implying that the input is in a novel class whose closest known class in the taxonomy is that super class.</p><p>To represent the hierarchical relationship, let T be the taxonomy of known classes, and for a class y, P(y) be the set of parents, C(y) be the set of children, A(y) be the set of ancestors including itself, and N (y) be the set of novel classes whose closest known class is y. And let L(T ) be the set of all descendant leaves under a taxonomy T .</p><p>As no prior knowledge of N (y) is provided during training and testing, all classes in N (y) are indistinguishable in our hierarchical novelty detection framework. Thus, we treat N (y) as a single class in our analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Top-down method</head><p>A natural way to perform classification using a hierarchical taxonomy is following top-down classification decisions starting from the root class, as shown in the top of <ref type="figure">Figure 2</ref>. Let (x, y) ∼ P r(x, y|s) be a pair of an image and its label sampled from data distribution at a super class s, where y ∈ C(s) ∪ N (s). Then, the classification rule is defined aŝ</p><formula xml:id="formula_0">y =    arg max y ′ P r(y ′ |x, s; θ s ) if confident, N (s) otherwise,</formula><p>where θ s and P r( · |x, s; θ s ) are the model parameters of C(s) ∪ N (s) and the posterior categorical distribution for an image x, respectively. The top-down classification stops at s if the prediction is a known leaf class or the classifier is not confident with the prediction (i.e., the predicted class is in N (s)). We measure the prediction confidence using the KL divergence with respect to the uniform distribution: intuitively, a confidence-calibrated classifier generates nearuniform posterior probability vector if the classifier is not confident about its prediction. Hence, we interpret that the prediction is confident at a super class s if</p><formula xml:id="formula_1">D KL (U (·|s) P r(·|x, s; θ s )) ≥ λ s ,</formula><p>where λ s is a threshold, D KL denotes the KL divergence, and U (·|s) is the uniform distribution when the classification is made under a super class s. To train such confidencecalibrated classifiers, we leverage classes disjoint from the class s. Let O(s) be such a set of all known classes except for s and its descendents. Then, the objective function of our top-down classification model at a super class s is</p><formula xml:id="formula_2">min θ E P r(x,y|s) [− log P r(y|x, s; θ s )] + E P r(x,y|O(s)) [D KL (U (·|s) P r(·|x, s; θ s ))] ,<label>(1)</label></formula><p>where P r(x, y|O(s)) denotes the data distribution of O(s). However, under the above top-down scheme, the classification error might aggregate as the hierarchy goes deeper. For example, if one of the classifier has poor performance, then the overall classification performance of all descendent classes should be low. In addition, the taxonomy is not necessarily a tree but a directed-acyclic graph (DAG), i.e., a class could belong to multiple parents, which could lead to incorrect classification. <ref type="bibr" target="#b2">3</ref> In the next section, we propose flatten approaches, which overcome the error aggregation issue. Nevertheless, the top-down method can be used for extracting good visual features for boosting the performance of the flatten method, as we show in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Flatten method</head><p>We now propose to enumerate all probability of known leaf and novel classes in a single probability vector, i.e., we flatten the hierarchy, as described on the bottom of <ref type="figure">Figure 2</ref>. The key idea is that a probability of super class s can be represented as P r(s|x) = y ′ ∈C(s) P r(y ′ |x) + P r(N (s)|x),</p><formula xml:id="formula_3">and l ′ ∈L(T ) P r(l ′ |x) + s ′ ∈T \L(T ) P r(N (s ′ )|x) = 1,</formula><p>where l ′ and s ′ are summed over all known leaf classes and super classes, respectively. Note that N (s) is considered as a single novel class under the super class s, as discussed in Section 3.1. Thus, as described in <ref type="figure">Figure 2</ref>, one can virtually add an extra child for each super class to denote all novel classes under it. Let (x, y) ∼ P r(x, y) be a pair of an image and its most fine-grained label sampled from data distribution. Then, the classification rule iŝ y = arg max</p><formula xml:id="formula_4">y ′ P r(y ′ |x; θ),</formula><p>where y ′ is either a known leaf or novel class. Here, a problem is that we have no training data from novel classes. To address this, we propose two approaches to model the score (i.e., posterior probability) of novel classes. Data relabeling. A naive strategy is to relabel some training samples to its ancestors in hierarchy. Then, the images relabeled to a super class are considered as novel class images under the super class. This can be viewed as a supervised learning with both fine-grained and coarse-grained classes where they are considered to be disjoint, and one can optimize an objective function of a simple cross entropy function over all known leaf classes and novel classes:</p><formula xml:id="formula_5">min θ E P r(x,y) [− log P r(y|x; θ T )] .<label>(2)</label></formula><p>In our experiments, an image is randomly relabeled recursively in a bottom-up manner with a probability of r, where 0 &lt; r &lt; 1 is termed a relabeling rate. An example of relabeling is illustrated in <ref type="figure" target="#fig_1">Figure 3 (b)</ref>. Leave-one-out strategy. A more sophisticated way to model novel classes is to temporarily remove a portion of taxonomy during training: specifically, for a training label y, we recursively remove one of its ancestor a ∈ A(y) from the taxonomy T in a hierarchical manner. To represent a deficient taxonomy, we define T \a as a taxonomy where a and its descendants are removed from the original taxonomy T . At each stage of removal, a training label y becomes a novel class of the parent of a in T \a, i.e., N (P(a)). <ref type="figure" target="#fig_1">Figure 3</ref> (a, c-d) illustrates this idea with an example: in <ref type="figure" target="#fig_1">Figure 3</ref> (a), when y is "Persian cat," the set of its ancestor is A(y) ={ "Persian cat," "cat," "animal" }.</p><p>In <ref type="figure" target="#fig_1">Figure 3</ref> (c), images under a ="Persian cat" belong to N (P(a)) ="novel cat" in T \a. Similarly, in <ref type="figure" target="#fig_1">Figure 3 (d)</ref>, images under a ="cat" belong to N (P(a)) ="novel animal" in T \a. As we leave a class out to learn a novel class,  (c-d) shows leave-one-out (LOO) strategy. To learn a novel class score under a super class, one of its child is temporarily removed such that its descendant known leaf classes are treated as novel during training. we call this leave-one-out (LOO) method. With some notation abuse for simplicity, the objective function of the LOO model is then</p><formula xml:id="formula_6">min θ E P r(x,y) − log P r(y|x; θ L(T ) ) + a∈A(y) − log P r(N (P(a))|x; θ T \a ) ,<label>(3)</label></formula><p>where the first term is the standard cross entropy loss with the known leaf classes, and the second term is the summation of losses with N (P(a)) and the leaves under T \a.</p><p>We provide further implementation details in Supplementary material.</p><p>As we mentioned earlier, the flatten methods can be combined with the top-down one in sequence: the top-down method first extracts multiple softmax probability vectors from visual features, and then the concatenation of all probabilities can be used as an input of the LOO model. We name the combined method TD+LOO for conciseness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation: Hierarchical novelty detection</head><p>We present the hierarchical novelty detection performance of our proposed methods combined with CNNs on ImageNet <ref type="bibr" target="#b6">[7]</ref>, Animals with Attributes 2 (AwA2) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33]</ref>, and Caltech-UCSD Birds (CUB) <ref type="bibr" target="#b31">[32]</ref>, where they represent visual object datasets with deep, coarse-grained, and finegrained taxonomy, respectively. Experimental results on CIFAR-100 <ref type="bibr" target="#b15">[16]</ref> can be found in Supplementary material, where the overall trends of results are similar to others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation setups</head><p>Compared algorithms. As a baseline, we modify the dual accuracy reward trade-off search (DARTS) algorithm <ref type="bibr" target="#b7">[8]</ref> for our purpose. Note that DARTS gives some rewards to labels in hierarchy, where fine-grained prediction gets higher reward. Under this algorithm, for a novel class, its closest super class in the taxonomy would give the maximum reward. At test time, the modified DARTS generates expected rewards for all known leaf and novel classes, so prediction can be done in the same way as the flatten methods.</p><p>As our proposed methods, Relabel, LOO, and TD+LOO are compared. For a fair comparison in terms of the model capacity, deep Relabel and LOO models are also experimented, where a deep model is a stack of fully connected layers followed by rectified linear units (ReLU). We do not report the performance of the pure top-down method since 1) one can combine it with LOO methods for better performance as mentioned in Section 3.2, and 2) fair comparisons between the pure top-down method and others are not easy: intuitively, the confidence threshold λ s in Section 3.2 can be tuned. For example, the novel class score bias in the flatten method would improve the novel class detection accuracy, but large λ s does not guarantee the best novel class performance in the top-down method because hierarchical classification results would tend to stop at the root class. Datasets. ImageNet <ref type="bibr" target="#b6">[7]</ref> consists of of 22k object classes where the taxonomy of the classes is built with the hypernym-hyponym relationships in WordNet <ref type="bibr" target="#b21">[22]</ref>. We take 1k mutually exclusive classes in ILSVRC 2012 as known leaf classes, which are a subset of the ImageNet. Based on the hypernym-hyponym relationships in WordNet, we initially obtained 860 super classes of 1k known leaf classes, and then merged indistinguishable super classes. Specifically, if a super class has only one child or shares exactly the same descendant leaf classes, it is merged with classes connected to the class. After merging, the resultant taxonomy is a DAG and has 396 super classes where all super classes have at least two children and have different set of descendant leaf classes. On the other hand, the rest of 21k classes can be used as novel classes for testing. Among them, we discarded super classes, classes under 1k known leaf classes, and classes with less than 50 images for reliable performance measure. After filtering classes, we obtain about 16k novel classes. ILSVRC 2012 has about 1.3M training images and another 50k images in 1k known leaf classes. We put the 50k images aside from training and used for test, and we sampled another 50k images from 1.3M training images for validation. For novel classes, we sampled 50 images from each class. In summary, we have about 1.2M training images, 50k validation images, and 50k test images from known leaf classes, and 800k test images from novel classes.</p><p>AwA2 <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33]</ref> consists of 40 known leaf classes and 10 novel classes with 37k images, and CUB <ref type="bibr" target="#b31">[32]</ref> consists of 150 known leaf classes and 50 novel classes with 12k images. Similar to ImageNet, the taxonomy of each dataset is built based on the hypernym-hyponym relationships in WordNet. The resultant taxonomy is a tree and has 21 and 43 super classes for AwA2 and CUB, respectively. Training. We take ResNet-101 <ref type="bibr" target="#b11">[12]</ref> as a visual feature extractor (i.e., the penultimate layer of the CNN before the classification layer) for all compared methods. The CNNs are pretrained with ILSVRC 2012 1k classes, where they do not contain any novel classes of datasets experimented. Then, the final classification layer of the CNNs is replaced with our proposed models. Note that CNNs and our proposed models can be trained in an end-to-end manner, but we take and freeze the pretrained parameters in all layers except for the final layer for the sake of faster training.</p><p>For ImageNet, we use mini-batch SGD with 5k centercropped data per batch. As a regularization, L2 norm weight decay with parameter 10 −2 is applied. The initial learning rate is 10 −2 and it decays at most two times when loss improvement is less than 2 % compared to the last epoch.</p><p>For AwA2 and CUB, the experiments are done in the same environment with the above except that the models are trained with the full-batch GD and Adam optimizer <ref type="bibr" target="#b14">[15]</ref>. Metrics. We first consider the top-1 accuracy by counting the number of predicted labels exactly matching the ground truth. Note that we have two types of classes in test datasets, i.e., known and novel classes. Performances on two types of classes are in trade-off relation, i.e., if one tunes model parameters for favoring novel classes, the accuracy of known   <ref type="figure" target="#fig_2">Figure 4</ref>: Qualitative results of hierarchical novelty detection on ImageNet. "GT" is the closest known ancestor (super class) of the novel class, which is the expected prediction, "DARTS" is the baseline method proposed in <ref type="bibr" target="#b7">[8]</ref> where we adapt their method to our task, and the others are our proposed methods. "ǫ" stands for the distance between the prediction and GT, and "A" indicates whether the prediction is an ancestor of GT. Dashed edges represent multi-hop connection, where the number indicates the number of edges between classes. If the prediction is on a super class (marked with * and rounded), then the test image is classified as a novel class whose closest class in the taxonomy is the super class.</p><p>classes would be decreased. Specifically, by adding some positive bias to the novel class scores (e.g., logits in the softmax), one can decrease known class accuracy while increasing novel class accuracy, or vice versa. Hence, for a fair comparison, we measure the novel class accuracy with respect to some fixed known class accuracy, e.g., 50 %. As a more informative evaluation metric, we also measure the area under known-novel class accuracy curve (AUC). Varying the novel class score bias, a curve of known class accuracy versus novel class accuracy can be drawn, which depicts the relationship between the known class accuracy and the novel class accuracy. The AUC is the area under this curve, which is independent of the novel class score bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental results</head><p>We first compare the hierarchical novelty detection results of the baseline method and our proposed methods qualitatively with test images on ImageNet in <ref type="figure" target="#fig_2">Figure 4</ref>. We remark that our proposed methods can provide informative prediction results by utilizing the taxonomy of the dataset. In <ref type="figure" target="#fig_2">Figure 4</ref> (a), LOO and TD+LOO find the ground truth label (the most fine-grained label in taxonomy), while DARTS classifies it as "beagle," which is in fact visually similar to "American foxhound." In <ref type="figure" target="#fig_2">Figure 4 (b)</ref>, none of the method finds the ground truth, but the prediction of <ref type="table">Table 1</ref>: Hierarchical novelty detection results on ImageNet, AwA2, and CUB. For a fair comparison, 50 % of known class accuracy is guaranteed by adding a bias to all novel class scores (logits). The AUC is obtained by varying the bias. Known-novel class accuracy curve is shown in <ref type="figure" target="#fig_4">Figure 5</ref>. Values in bold indicate the best performance. TD+LOO is the most informative, as it is the closest label in the hierarchy. In <ref type="figure" target="#fig_2">Figure 4 (c-d)</ref>, only the prediction of TD+LOO is correct, but the rest of the methods also give a reasonable amount of information. More qualitative results can be found in Supplementary material. <ref type="table">Table 1</ref> shows the hierarchical novelty detection performance on ImageNet, AwA2, and CUB. One can note that the proposed methods significantly outperform the baseline method in most cases, except the case of Relabel on CUB, because validation could not find the best relabeling rate for test. Also, we remark that LOO outperforms Relabel. In most regions, our proposed methods outperform the baseline method.</p><p>The main difference of two methods is that Relabel gives a penalty to the original label if it is relabeled during training, which turns out to be harmful for the performance. Finally, TD+LOO exhibits the best performance, which implies that the multiple softmax probability vectors extracted from the top-down method is more useful than the vanilla visual features extracted from the state-of-the-art CNNs in the hierarchical novelty detection tasks. <ref type="figure" target="#fig_4">Figure 5</ref> shows the knownnovel class accuracy curves by varying the bias added to the novel class scores. Our proposed methods have higher novel class accuracy than the baseline in most regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation: Generalized zero-shot learning</head><p>We present the GZSL performance of the combination of the hierarchical embedding obtained by the top-down method and other semantic embeddings on Animals with Attributes (AwA1 and AwA2) 5 <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33]</ref> and Caltech-UCSD Birds (CUB) <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation setups</head><p>Hierarchical embeddings for GZSL. GZSL requires an output semantic embedding built with side information, e.g., attributes labeled by human, or word embedding trained with a large text corpus. In addition to those two commonly used semantic embeddings, Akata et al. <ref type="bibr" target="#b0">[1]</ref> proposed to use hierarchical relationships of all classes, including classes unseen during training. Specifically, they measured the shortest path distance between classes in the taxonomy built with both known and novel classes, and take the vector of distance values as output embedding. We refer to this embedding as Path.</p><p>Motivated by the effectiveness of the features extracted from the top-down method shown in Section 4.2, we set the enumeration of the ideal multiple softmax probability vectors as the semantic embedding: let C(s)[i] be the i-th 5 AwA1 is similar to AwA2, but images in AwA1 are no longer available due to the public copyright license issue. We used precomputed CNN features for AwA1, which is available at http://datasets.d2. mpi-inf.mpg.de/xian/xlsa17.zip.</p><p>child of a super class s. Then, for a label y and a super class s, the i-th element of an ideal output probability vector</p><formula xml:id="formula_7">t (y,s) ∈ [0, 1] |C(s)| is t (y,s) [i] =      1 if y belongs to C(s)[i] 0 if y belongs to C(s)[j] where i = j 1 |C(s)|</formula><p>if y is novel or does not belong to s where |C(s)| is the number of known child classes under s. The final visual embedding is the concatenation of them with respect to the super classes, i.e., the ground truth semantic vector of a class y is t y = [. . . , t (y,s) , .</p><p>. . ], and we call this embedding TD. See Supplementary material for an example of the ideal output probability vector t y . Since classes who share the same closest super class have exactly the same desired output probability vector, we made random guess for fine-grained classification in the experiment only with the hierarchical embedding. Datasets. AwA1 and AwA2 <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33]</ref> consists of 40 seen classes and 10 unseen classes with 37k images, and CUB <ref type="bibr" target="#b31">[32]</ref> consists of 150 seen classes and 50 unseen classes with 12k images, <ref type="bibr" target="#b5">6</ref> where the taxonomy can be built in the same way with Section 4. Training. We note that the performance of combined models is reported in <ref type="bibr" target="#b0">[1]</ref>, but the numbers are outdated, due to the old CNNs and ZSL models. Thus, instead of making direct comparison with theirs, we construct the environment following the state-of-the-art setting and compared the performance gain obtained by ensembling different hierarchical embedding models to other semantic embeddings. We take ResNet-101 as a pretrained visual feature extractor, and we apply deep embedding model proposed in <ref type="bibr" target="#b34">[35]</ref> for training attribute embedding and word embedding models, where it learns to map semantic embeddings to the visual feature embedding with two fully connected layers with ReLU between them. As a combination strategy, we calculate prediction scores of each model and then used their Figure 6: Seen-unseen class accuracy curves of the best combined models obtained by varying the unseen class score bias on AwA1, AwA2, and CUB. "Path" is the hierarchical embedding proposed in <ref type="bibr" target="#b0">[1]</ref>, and "TD" is the embedding of the multiple softmax probability vector obtained from the proposed top-down method. In most regions, TD outperforms Path. <ref type="table">Table 2</ref>: ZSL and GZSL performance of semantic embedding models and their combinations on AwA1, AwA2, and CUB. "Att" stands for continuous attributes labeled by human, "Word" stands for word embedding trained with the GloVe objective <ref type="bibr" target="#b23">[24]</ref>, and "Hier" stands for the hierarchical embedding, where "Path" is proposed in <ref type="bibr" target="#b0">[1]</ref>, and "TD" is output of the proposed top-down method. "Unseen" is the accuracy when only unseen classes are tested, and "AUC" is the area under the seen-unseen curve where the unseen class score bias is varied for computation. The curve used to obtain AUC is shown in <ref type="figure">Figure 6</ref>. Values in bold indicate the best performance among the combined models. weighted sum for final classification, where the weights are cross-validated. See <ref type="bibr" target="#b0">[1]</ref> for more details about the combination strategy as well as the semantic embeddings. Metrics. The ZSL performance is measured by testing unseen classes only, and the GZSL performance is measured by the area under seen-unseen curve (AUC) following the idea in <ref type="bibr" target="#b3">[4]</ref>. We measure the class-wise accuracy rather than the sample-wise accuracy to avoid the effect of imbalanced test dataset, as suggested in <ref type="bibr" target="#b32">[33]</ref>. <ref type="table">Table 2</ref> shows the performance of the attribute, word, and path embedding model, the hierarchical embedding model derived from the proposed top-down method, and their combinations on AwA1, AwA2, and CUB. In <ref type="table">Table 2</ref>, the standalone performance of top-down method is not better than the path embedding, as it does not distinguish unseen classes sharing the same closest super class. In the same reason, the improvement on ZSL performance with the combined models is fairly small. However, in the GZSL task, the top-down hierarchical embedding shows significantly better performance in the combined models, which means that the top-down embedding is better when distinguishing seen classes and unseen classes together. Compared to the best single semantic embedding model (with attributes), the combination with the top-down embedding leads to absolute improvement of AUC by 7.65%, 7.97%, and 6.71% on AwA1, AwA2 and CUB, respectively, which is significantly better than that of the path embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose a new hierarchical novelty detection framework, which performs object classification and hierarchical novelty detection by predicting the closest super class in a taxonomy. We propose several methods for the hierarchical novelty detection task and show that our models achieve significantly better performance over prior work. In addition, the hierarchical embedding learned with our model can be combined with other semantic embeddings such as attributes and words to improve generalized zero-shot learning performance. As future work, augmenting textual information about labels for hierarchical novelty detection would be an interesting extension of this work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of strategies to train novel class scores in flatten methods. (a) shows the training images in the taxonomy. (b) shows relabeling strategy. Some training images are relabeled to super classes in a bottom-up manner. (c-d) shows leave-one-out (LOO) strategy. To learn a novel class score under a super class, one of its child is temporarily removed such that its descendant known leaf classes are treated as novel during training. we call this leave-one-out (LOO) method. With some notation abuse for simplicity, the objective function of the LOO model is then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4 4</head><label>4</label><figDesc>Except "teddy bear," all classes in ILSVRC 2012 are in ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Known-novel class accuracy curves obtained by varying the novel class score bias on ImageNet, AwA2, and CUB. In most regions, our proposed methods outperform the baseline method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>Att Word Hier Unseen AUC Unseen AUC Unseen AUC 65.29 50.02 63.87 51.27 50.05 23.60 51.87 39.67 54.77 42.21 27.28 11.47 67.80 52.84 65.76 53.18 49.83 24.13 Path 42.57 30.58 44.34 33.44 24.22 8.38 Path 67.09 51.45 66.58 53.50 50.25 23.70 Path 52.89 40.66 55.28 42.86 27.72 11.65 Path 68.04 53.21 67.28 54.31 50.87 24.20 TD 33.86 25.56 31.84 24.97 13.09 7.20 TD 66.13 54.66 66.86 57.49 50.17 30.31 TD 56.14 46.28 59.67 49.39 29.05 16.73 TD 69.23 57.67 68.80 59.24 50.17 30.31</figDesc><table>Embedding 
AwA1 
AwA2 
CUB 
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For example, if a class has only one known child class, these two classes are indistinguishable as they are trained with exactly the same data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We note that "novel" in our task is similar but different from "unseen" commonly referred in ZSL works; while class-specific semantic information for unseen classes must be provided in ZSL, such information for novel classes is not required in our task.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For example, if there are multiple paths to a class in a taxonomy, then the class may belong to (i.e., be a descendant of) multiple children at some super class s, which may lead to low KL divergence from the uniform distribution and the image could be incorrectly classified as N (s).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">In GZSL, we have semantic information of unseen classes. In this sense, although unseen classes are not used for training, they are known as such a class-specific semantic information is required.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by Software R&amp;D Center, Samsung Electronics Co., Ltd., Kwanjeong Educational Foundation Scholarship, Sloan Research Fellowship, and DARPA Explainable AI (XAI) program #313498. We also thank Zeynep Akata, Yongqin Xian, Junhyuk Oh, Lajanugen Logeswaran, Sungryull Sohn, Jongwook Choi, and Yijie Guo for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2927" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards open set deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Synthesized classifiers for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An empirical study and analysis of generalized zero-shot learning for object recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="52" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">What does classifying more than 10,000 image categories tell us? In ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="71" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-scale object classification using label relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="48" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised vocabulary-informed learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey of outlier detection methodologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence review</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="126" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Attributebased classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Training confidencecalibrated classifiers for detecting out-of-distribution samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout inference in bayesian neural networks with alpha-divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A review of novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tarassenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="249" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning deep representations of fine-grained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transfer learning in a transductive setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simeone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Santos-Rodríguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcvicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lijffijt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>De Bie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Intelligent Data Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="310" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Zeroshot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Caltech-ucsd birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Zeroshot learning-a comprehensive evaluation of the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00600</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hd-cnn: hierarchical deep convolutional neural networks for large scale visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2740" to="2748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning a deep embedding model for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Open vocabulary scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
