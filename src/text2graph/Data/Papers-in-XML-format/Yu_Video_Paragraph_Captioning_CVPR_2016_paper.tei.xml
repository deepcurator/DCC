<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Yu</surname></persName>
							<email>haonanu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
							<email>wangjiang03@baidu.com</email>
							<affiliation key="aff2">
								<orgName type="department">Baidu Research -Institute of Deep Learning</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
							<email>zhiheng@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yangyi05@baidu.com</email>
							<affiliation key="aff2">
								<orgName type="department">Baidu Research -Institute of Deep Learning</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
							<email>wei.xu@baidu.com</email>
							<affiliation key="aff2">
								<orgName type="department">Baidu Research -Institute of Deep Learning</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We present an approach that exploits hierarchical Recurrent Neural Networks (RNNs)   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper, we consider the problem of video captioning, i.e. generating one or multiple sentences to describe the content of a video. The given video could be as general as those uploaded to YouTube, or it could be as specific as cooking videos with fine-grained activities. This ability to generate linguistic descriptions for unconstrained video is important because not only it is a critical step towards machine intelligence, but also it has many applications in daily scenarios such as video retrieval, automatic video subtitling, blind navigation, etc. <ref type="figure" target="#fig_0">Figure 1</ref> shows some example sentences generated by our approach.</p><p>The video captioning problem has been studied for over one decade ever since the first rule-based system on describing human activities with natural language <ref type="bibr" target="#b22">[23]</ref>. In a very limited setting, Kojima et al. designed some simple heuris- * This work was done while the authors were at Baidu. tics for identifying video objects and a set of rules for producing verbs and prepositions. A sentence is then generated by filling predefined templates with the recognized parts of speech. Following their work, several succeeding approaches <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b2">3]</ref> applied similar rule-based systems to datasets with larger numbers of objects and events, in different tasks and scenarios. With ad hoc rules, they manually establish the correspondence between linguistic terms and visual elements, and analyze the relations among the visual elements to generate sentences. Among them, the most complex rule-based system <ref type="bibr" target="#b2">[3]</ref> supports a vocabulary of 118 lexical entries (including 48 verbs and 24 nouns).</p><p>To eliminate the tedious effort of rule engineering when the problem scales, some recent methods train statistical models for lexical entries, either in a fully <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42]</ref> or weakly <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b54">55]</ref> supervised fashion. The statistical models of different parts of speech usually have different mathematical representations and training strategies (e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24]</ref>). With most of the manual effort gone, the training process exposes these methods to even larger datasets (e.g., YouTubeClips <ref type="bibr" target="#b5">[6]</ref> and TACoS-MultiLevel <ref type="bibr" target="#b35">[36]</ref>) which contain thousands of lexical entries and dozens of hours of videos. As a result, the video captioning task becomes much more challenging, and the generation performance of these methods is usually low on these large-scale datasets.</p><p>Since then, inspiring results have been achieved by a recent line of work <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b55">56]</ref> which benefits from the rapid development of deep neural networks, especially Recurrent Neural Network (RNN). Applying RNN to translating visual sequence to natural language is largely inspired by the recent advances in Neural Machine Translation (NMT) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b42">43]</ref> in the natural language processing community. The idea is to treat the image sequence of a video as the "source text" and the corresponding caption as the target text. Given a sequence of deep convolutional features (e.g., VggNet <ref type="bibr" target="#b39">[40]</ref> and C3D <ref type="bibr" target="#b44">[45]</ref>) extracted from video frames, a compact representation of the video is obtained by: average pooling <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b31">32]</ref>, weighted average pooling with an attention model <ref type="bibr" target="#b55">[56]</ref>, or taking the last output from an RNN en-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A man is pouring oil into a pot.</head><p>A dog is playing in a bowl.  coder which summarizes the feature sequence <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b53">54]</ref>. Then an RNN decoder accepts this compact representation and outputs a sentence of a variable length.</p><p>While promising results were achieved by these RNN methods, they only focus on generating a single sentence for a short video clip. So far the problem of generating multiple sentences or a paragraph for a long video has not been attempted by deep learning approaches. Some graphicalmodel methods, such as Rohrbach et al. <ref type="bibr" target="#b35">[36]</ref>, are able to generate multiple sentences, but their results are still far from perfect. The motivation of generating a paragraph is that most videos depict far more than just one event. Using only one short sentence to describe a semantically rich video usually yields uninformative and even boring results. For example, instead of saying the person sliced the potatoes, cut the onions into pieces, and put the onions and potatoes into the pot, a method that is only able to produce one short sentence would probably say the person is cooking.</p><p>Inspired by the recent progress of document modeling <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> in natural language processing, we propose a hierarchical-RNN framework for describing a long video with a paragraph consisting of multiple sentences. The idea behind our hierarchical framework is that we want to exploit the temporal dependency among sentences in a paragraph, so that when producing the paragraph, the sentences are not generated independently. Instead, the generation of one sentence might be affected by the semantic context provided by the previous sentences. For example, in a video of cooking dishes, a sentence the person peeled the potatoes is more likely to occur, than the sentence the person turned on the stove, after the sentence the person took out some potatoes from the fridge. Towards this end, our hierarchical framework consists of two generators, i.e. a sentence generator and a paragraph generator, both of which use recurrent layers for language modeling. At the low level, the sentence generator produces single short sentences that describe specific time intervals and video regions. We exploit both temporal-and spatial-attention mechanisms to selectively focus on visual elements when generating a sentence. The embedding of the generated sentence is encoded by the output of the recurrent layer. At the high level, the paragraph generator takes the sentential embedding as input, and uses another recurrent layer to output the paragraph state, which is then used as the new initial state of the sentence generator (see Section 3). <ref type="figure">Figure 2</ref> illustrates our overall framework. We evaluate our approach on two public datasets: YouTubeClips <ref type="bibr" target="#b5">[6]</ref> and TACoS-MultiLevel <ref type="bibr" target="#b35">[36]</ref>. We show that our approach significantly outperforms other state-of-the-art methods. To our knowledge, this is the first application of hierarchical RNN to video captioning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Neural Machine Translation. The methods for NMT <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> in computational linguistics generally follow the encoder-decoder paradigm. An encoder maps the source sentence to a fixed-length feature vector in the embedding space. A decoder then conditions on this vector to generate a translated sentence in the target language. On top of this paradigm, several improvements were proposed. Bahdanau et al. <ref type="bibr" target="#b0">[1]</ref> proposed a soft attention model to do alignment during translation, so that their approach is able to focus on different parts of the source sentence when generating different translated words. Li et al. <ref type="bibr" target="#b26">[27]</ref> and Lin et al. <ref type="bibr" target="#b27">[28]</ref> employed hierarchical RNN to model the hierarchy of a document. Our approach is much similar to a neural machine translator with a simplified attention model and a hierarchical architecture. Image captioning with RNNs. The first attempt of visualto-text translation using RNNs was seen in the work of image captioning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b7">8]</ref>, which can be treated as a special case of video captioning when each video has a single frame and no temporal structure. As a result, image captioning only requires computing object appearance features, but not action/motion features. The amount of data handled by an image captioning method is much (dozens of times) less than that handled by a video captioning method. The overall structure of an image captioner (instance-tosequence) is also usually simpler than that of a video captioner (sequence-to-sequence). Some other methods, such as Park and Kim <ref type="bibr" target="#b33">[34]</ref>, addressed the problem of retrieving sentences from training database to describe a sequence of images. They proposed a local coherence model for fluent sentence transitions, which serves a similar purpose of our paragraph generator. Video captioning with RNNs. The very early video captioning method <ref type="bibr" target="#b47">[48]</ref> based on RNNs extends the image captioning methods by simply average pooling the video frames. Then the problem becomes exactly the same as image captioning. However, this strategy works only for short video clips where there is only one major event, usually appearing in one video shot from the beginning to the end. To avoid this issue, more sophisticated ways of encoding video features were proposed in later work, using either a recurrent encoder <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b53">54]</ref> or an attention model <ref type="bibr" target="#b55">[56]</ref>. Our sentence generator is closely related to Yao et al. <ref type="bibr" target="#b55">[56]</ref>, in that we also use attention mechanism to selectively focus on video features. One difference between our framework and theirs is that we additionally exploit spatial attention. The other difference is that after weighing video features with attention weights, we do not condition the hidden state of our recurrent layer on the weighted features (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Hierarchical RNN for Video Captioning</head><p>Our approach stacks a paragraph generator on top of a sentence generator. The sentence generator is built upon 1) a Recurrent Neural Network (RNN) for language modeling, 2) a multimodal layer <ref type="bibr" target="#b28">[29]</ref> for integrating information from different sources, and 3) an attention model <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b0">1]</ref> for selectively focusing on the input video features. The paragraph generator is simply another RNN which models the inter-sentence dependency. It receives the compact sentential representation encoded by the sentence generator, combines it with the paragraph history, and outputs a new initial state for the sentence generator. The RNNs exploited by the two generators incorporate the Gated Recurrent Unit (GRU) <ref type="bibr" target="#b8">[9]</ref> which is a simplification of the Long Short-Term Memory (LSTM) architecture <ref type="bibr" target="#b15">[16]</ref>. In the following, we first briefly review the RNN with the GRU (or the gated RNN), and then describe our framework in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Gated Recurrent Unit</head><p>A simple RNN <ref type="bibr" target="#b11">[12]</ref> can be constructed by adding feedback connections to a feedforward network that consists of three layers: the input layer x, the hidden layer h, and the output layer y. The network is updated by both the input and the previous recurrent hidden state as follows:</p><formula xml:id="formula_0">h t = φ W h x t + U h h t−1 + b h (hidden state) y t = φ (U y h t + b y ) (output)</formula><p>where W, U and b are weight matrices and biases to be learned, and φ(·) are element-wise activation functions. While the simple RNN is able to model temporal dependency for a small time gap, it usually fails to capture long-term temporal information. To address this issue, the GRU <ref type="bibr" target="#b8">[9]</ref> is designed to adaptively remember and forget the past. Inside the unit, the hidden state is modulated by nonlinear gates. Specifically, let ⊙ denote the element-wise multiplication of two vectors, the GRU computes the hidden state h as:</p><formula xml:id="formula_1">r t = σ(W r x t + U r h t−1 + b r ) (reset gate) z t = σ(W z x t + U z h t−1 + b z ) (update gate) h t = φ W h x t + U h (r t ⊙ h t−1 ) + b h h t = z t ⊙ h t−1 + (1 − z t ) ⊙ h t (hidden state)</formula><p>where σ(·) are element-wise Sigmoid functions. The reset gate r determines whether the hidden state wants to drop any information that will be irrelevant in the future. The update gate z controls how much information from the previous hidden state will be preserved for the current state. During the training of a gated RNN, the parameters can be estimated by Backpropagation Through Time (BPTT) <ref type="bibr" target="#b52">[53]</ref> as in traditional RNN architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sentence Generator</head><p>The overall structure of our hierarchical RNN is illustrated in <ref type="figure">Figure 2</ref>. The sentence generator operates at every time step when a one-hot input (1-of-N encoding, where N is the vocabulary size) arrives at the embedding layer. The embedding layer converts the one-hot vector to a dense representation in a lower dimensional space by multiplying it with an embedding table (512 × N ), of which each row is a word embedding to be learned. The resulting word embedding is then input to our first RNN, i.e., the recurrent layer I. This gated recurrent layer has 512 dimensions and acts similarly to those that are commonly employed by a variety of image/video captioning methods (e.g., <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b55">56]</ref>), i.e., modeling the syntax of a language. It updates its hidden state every time a new word arrives, and encodes the sentence semantics in a compact form up to the words that have been fed in. We set the activation function φ of this recurrent layer to be the Rectified Linear Unit (ReLU) <ref type="bibr" target="#b30">[31]</ref>, since it performs better than non-linear activation functions such as Sigmoid according to our observation.</p><p>As one branch, the output of the recurrent layer I is directed to the attention layers to compute attention weights for the features in the video feature pool. Our attention model is inspired by the recent soft-alignment method that  <ref type="figure">Figure 2</ref>. Our hierarchical RNN for video captioning. Green denotes the input to the framework, blue denotes the output, and red denotes the recurrent components. The orange arrow represents the reinitialization of the sentence generator with the current paragraph state. For simplicity, we only draw a single video feature pool in the figure. In fact, both appearance and action features go through a similar attention process before they are fed into the multimodal layer.</p><p>has been successfully applied in the context of Neural Machine Translation (NMT) <ref type="bibr" target="#b0">[1]</ref>, and was later adapted to video captioning by Yao et al. <ref type="bibr" target="#b55">[56]</ref>. The difference between our model and the one used by Yao et al. is that their model only focuses on temporal attention. We additionally include spatial attention by computing features for multiple image patches at different locations on a video frame and pool the features together. This simple improvement is important when objects are small and difficult to be localized on some datasets (e.g., TACoS-MultiLevel <ref type="bibr" target="#b35">[36]</ref>). In this case, whole-frame-based video features will fail to capture the object information and multiple object proposals are needed for good performance (see Section 5 for details). Let the features in the pool be denoted as {v 1 , v 2 , . . . , v KM }, where M is the video length and K is the number of patches on each frame. We want to compute a set of weights {β </p><formula xml:id="formula_2">q t m = w ⊤ φ(W q v m + U q h t−1 + b q )</formula><p>where w, W q , U q , and b q are the parameters shared by all the features at all the time steps, and φ is set to the element-wise Scaled Hyperbolic Tangent (stanh) function <ref type="bibr" target="#b24">[25]</ref>: 1.7159 · tanh( 2x 3 ). The above computation is performed by the attention layers I and II in <ref type="figure">Figure 2(a)</ref>, where the attention layer I projects the feature v and the hidden state h into a lower dimensional space whose dimension can range from 32 to 256. The attention layer II then further compresses the activation of the projected vector into a scalar, one for each feature. After this, we set up a sequential softmax layer to get the attention weights:</p><formula xml:id="formula_3">β t m = exp q t m KM m ′ =1 exp q t m ′</formula><p>Finally, a single feature vector is obtained by weighted averaging:</p><formula xml:id="formula_4">u t = KM m=1 β t m v m .</formula><p>The above process is a sophisticated version of the temporal mean pooling. It allows the sentence generator to selectively focus on a subset of the features during generation. Note that while only one feature channel is shown in <ref type="figure">Figure 2</ref>(a), our sentence generator in fact pumps features of several channels through the same attention process. Each feature channel has a different set of weights and biases to be learned. In our experiments, we employ two feature channels, one for object appearance and the other for action/motion. (Section 5).</p><p>After the attention process, the weighted sums of the video features are fed into the multimodal layer which has 1024 dimensions. The multimodal layer also receives the output of the recurrent layer I, thus connecting the vision component with the language model. Suppose we have two video feature channels, of which the weighted features output by the attention model are u t o and u t a respectively. The multimodal layer maps the two features, together with the hidden state h t of the recurrent layer I, into a 1024 dimensional feature space and add them up:</p><formula xml:id="formula_5">m t = φ(W m,o u t o + W m,a u t a + U m h t + b m )</formula><p>where φ is set to the element-wise stanh function. To reduce overfitting, we add dropout <ref type="bibr" target="#b40">[41]</ref> with a drop rate of 0.5 to this layer. The multimodal layer is followed by a hidden layer and a softmax layer (see <ref type="figure">Figure 2</ref>(a)), both with the elementwise stanh function as their activation functions. The hidden layer has exactly the same dimension 512 with the word embedding layer, and the softmax layer has a dimension that is equal to the size of the vocabulary which is dataset-dependent. Inspired by the transposed weight sharing scheme recently proposed by Mao et al. <ref type="bibr" target="#b29">[30]</ref>, we set the projection matrix from the hidden layer to the softmax layer as the transpose of the word embedding table. It has been shown that this strategy allows the use of a word embedding layer with a much larger dimension due to the parameter sharing, and helps regularize the word embedding table because of the matrix transpose. As the final step of the sentence generator, the maxid layer picks the index that points to the maximal value in the output of the softmax layer. The index is then treated as the predicted word id. Note that during test, the predicted word will be fed back to the sentence generator again as the next input word. While in the training, the next input word is always provided by the annotated sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Paragraph Generator</head><p>The sentence generator above only handles one single sentence at a time. For the first sentence in the paragraph, the initial state of the recurrent layer I is set to all zeros, i.e., h 0 = 0. However, any sentence after that will have its initial state conditioned on the semantic context of all its preceding sentences. This semantic context is encoded by our paragraph generator.</p><p>During the generation of a sentence, an embedding average layer (see <ref type="figure">Figure 2(b)</ref>) accumulates all the word embeddings of the sentence and takes the average to get a compact embedding vector. The average strategy is inspired by the QA embedding <ref type="bibr" target="#b4">[5]</ref> in which questions and answers are both represented as a combination of the embeddings of their individual words and/or symbols. We also take the last state of the recurrent layer I as a compact representation for the sentence, following the idea behind the Encoder-Decoder framework <ref type="bibr" target="#b8">[9]</ref> in NMT. After that, the averaged embedding and the last recurrent state are concatenated together, and fully connected to the sentence embedding layer (512 dimensions) with stanh as the activation function. We treat the output of the sentence embedding layer as the final sentence representation.</p><p>The sentence embedding layer is linked to our second gated RNN (see <ref type="figure">Figure 2(b)</ref>). The recurrent layer II operates whenever a full sentence goes through the sentence generator and the sentence embedding is produced by the sentence embedding layer. Thus the two recurrent layers are asynchronous: while the recurrent layer I keeps updating its hidden state at every time step, the recurrent layer II only updates its hidden state when a full sentence has been processed. The recurrent layer II encodes the paragraph semantics in a compact form up to the sentences that have been fed in. Finally, we set up a paragraph state layer to combine the hidden state of the recurrent layer II and the sentence embedding. This paragraph state is used as the initial hidden state when the recurrent layer I is reinitialized for the next sentence. It essentially provides the sentence generator with the paragraph history so that the next sentence is produced in the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training and Generation</head><p>We train all the components in our hierarchical framework together from scratch with randomly initialized parameters. We treat the activation value indexed by a training word w n t in the softmax layer of our sentence generator as the likelihood of generating that word: P w n t |s 1:n−1 , w n 1:t−1 , V given 1) all the preceding sentences s 1:n−1 in the paragraph, 2) all the previous words w n 1:t−1 in the same sentence n, and 3) the corresponding video V. The cost of generating that training word is then defined as the negative logarithm of the likelihood. We further define the cost of generating the whole paragraph s 1:N (N is the number of sentences in the paragraph) as:</p><formula xml:id="formula_6">PPL(s 1:N |V) = − N n=1 Tn t=1</formula><p>log P w n t |s 1:n−1 , w</p><formula xml:id="formula_7">n 1:t−1 , V N n=1</formula><p>T n where T n is the number of words in the sentence n. The above cost is in fact the perplexity of the paragraph given the video. Finally, the cost function over the entire training set is defined as:</p><formula xml:id="formula_8">PPL = Y y=1   PPL(s y 1:Ny |V y ) · Ny n=1 T y n   Y y=1 Ny n=1 T y n (1)</formula><p>where Y is the total number of paragraphs in the training set. To reduce overfitting, L2 and L1 regularization terms are added to the above cost function. We use Backpropagation Through Time (BPTT) <ref type="bibr" target="#b52">[53]</ref> to compute the gradients of the parameters and Stochastic Gradient Descent (SGD) to find the optimum. For better convergence, we divide the gradient by a running average of its recent magnitude according to the RMSPROP algorithm <ref type="bibr" target="#b43">[44]</ref>. We set a small learning rate 10 −4 to avoid the gradient explosion problem that is common in the training process of RNNs.</p><p>After the parameters are learned, we perform the generation with Beam Search. Suppose that we use a beam width of L. The beam search process starts with the BOS (begin-of-sentence) symbol w BOS (i.e., w 0 ) which is treated as a 1-word sequence with zero cost at t = 0. Assume that at any time step t, there are at most L t-word sequences that were previously selected with the lowest sequence costs (a sequence cost is the sum of the word costs in that sequence). For each of the t-word sequences, given its last word as input, the sentence generator calculates the cost of the next word − log P (w t |w 1:t−1 , V) and the sequence cost if the word is appended to the sequence. Then from all the t+1-word sequences expanded from the existing t-word sequences, we pick the top L with the lowest sequence costs.</p><p>Of the new t + 1-word sequences, any one that is a complete sentence (i.e., the last word w t+1 is the EOS (end-ofsentence) symbol w EOS ) will be removed from the search tree. It will be put into our sentence pool if 1) there are less than J (J ≤ L) sentences in the pool or, 2) its sequence cost is lower than one of the J sentences in the pool. In the second case, the sentence with the highest cost will be removed from the pool, replaced by the new added sentence. Also of the new t + 1-word sequences, any one that has a higher sequence cost than all of the J sentences in the pool will be removed from the search tree. The reason is that expanding a word sequence monotonically increases its cost. The beam search process stops when there is no word sequence to be expanded in the next time step. In the end, J candidate sentences will be generated for post-processing and evaluation.</p><p>After this, the generation process goes on by picking the sentence with the lowest cost from the J candidate sentences. This sentence is fed into our paragraph generator which reinitializes the sentence generator. The sentence generator then accepts a new BOS and again produces J candidate sentences. This whole process stops when the sentence received by the paragraph generator is the EOP (end-of-paragraph) which consists of only the BOS and the EOS. Finally, we will have a paragraph that is a sequence of lists, each list with J sentences. In our experiments, we set L = J = 5. Excluding the calculation of visual features, the average computational time for the sentence generator to produce top 5 candidate sentences with a beam width of 5 is 0.15 seconds, on a single thread with CPU Intel(R) Core(TM) i7-5960X @ 3.00GHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate our approach on two benchmark datasets: YouTubeClips <ref type="bibr" target="#b5">[6]</ref> and TACoS-MultiLevel <ref type="bibr" target="#b35">[36]</ref>. YouTubeClips This dataset consists of 1, 967 short video clips (9 seconds on average) downloaded from YouTube. The video clips are open-domain, containing different people, animals, actions, scenarios, landscapes, etc. Each video clip is annotated with multiple parallel sentences by different turkers. There are 80, 839 sentences in total, with about 41 annotated sentences per clip. Each sentence on average contains about 8 words. The words contained in all the sentences constitute a vocabulary of 12, 766 unique lexical entries. We adopt the train and test splits provided by Guadarrama et al. <ref type="bibr" target="#b13">[14]</ref>, where 1, 297 and 670 videos are used for training and testing respectively. It should be noted that while multiple sentences are annotated for each video clip, they are parallel and independent in the temporal extent, i.e., the sentences describe exactly the same video interval, from the beginning to the end of the video. As a result, we use this dataset as a special test case for our approach, when the paragraph length N = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TACoS-MultiLevel</head><p>This dataset consists of 185 long videos (6 minutes on average) filmed in an indoor environment. The videos are closed-domain, containing different actors, fine-grained activities, and small interacting objects in daily cooking scenarios. Each video is annotated by multiple turkers. A turker annotates a sequence of temporal intervals across the video, pairing every interval with a single short sentence. There are 16, 145 distinct intervals and 52, 478 sentences in total, with about 87 intervals and 284 sentences per video. The sentences were originally preprocessed so that they all have the past tense, and different gender specific identifiers were substituted with "the person". Each sentence on average contains about 8 words.</p><p>The words contained in all the sentences constitute a vocabulary of 2, 864 unique lexical entries. We adopt the train and test splits used by Rohrbach et al. <ref type="bibr" target="#b35">[36]</ref>, where 143 and 42 videos are used for training and testing respectively. Note that the cooking activities in this dataset have strong temporal dependencies. Such dependency in a video is implied by the sequence of intervals annotated by the same turker on that video. Following Donahue et al. <ref type="bibr" target="#b10">[11]</ref> and Rohrbach et al. <ref type="bibr" target="#b35">[36]</ref>, we employ the interval information to align our sentences in the paragraph during both training and generation. This dataset is used as a general test case for our approach, when the paragraph length N &gt; 1.</p><p>To model video object appearance, we use the pretrained VggNet <ref type="bibr" target="#b39">[40]</ref> (on the ImageNet dataset <ref type="bibr" target="#b37">[38]</ref>) for both datasets. Since the objects in YouTubeClips are usually prominent, we only extract one VggNet feature for each entire frame. This results in only temporal attention in our sentence generator (i.e., K = 1 in Section 3.2). For TACoSMultiLevel, the interacting objects are usually quite small and difficult to be localized. To solve this problem, both Donahue et al. <ref type="bibr" target="#b10">[11]</ref> and Rohrbach et al. <ref type="bibr" target="#b35">[36]</ref> designed a specialized hand detector. Once the hand regions are detected, they extract features in the neighborhood to represent the interacting objects. Instead of trying to accurately locate hands which requires a lot of engineering effort as in their case, we rely on a simple routine to obtain multiple object proposals. We first use Optical Flow <ref type="bibr" target="#b12">[13]</ref> to roughly detect a bounding box for the actor in each frame. We then extract K image patches of size 220 × 220 along the lower part of the box border, where every two neighboring patches have an overlap of half their size. Our simple observation is that these patches together have a high recall of containing the interacting objects while the actor is cooking. Finally, we</p><formula xml:id="formula_9">B@1 B@2 B@3 B@4 M C LSTM-YT [48] - - - 0.333 0.291 - S2VT [47] - - - - 0.298 - MM-VDN [54]</formula><p>---0.376 0.290 -TA <ref type="bibr" target="#b55">[56]</ref> 0.800 0.647 0.526 0.419 0.296 0.517 LSTM-E <ref type="bibr" target="#b31">[32]</ref> 0  <ref type="table">Table 1</ref>. Results on YouTubeClips, where B, M, and C are short for BLEU, METEOR, and CIDEr respectively.</p><p>compute the VggNet feature for each patch and pool all the patch features. When K &gt; 1, the above routine leads to both temporal and spatial attention in our sentence generator. In practice, we find that a small value of K (e.g., 3 ∼ 5) is enough to yield good performance.</p><p>To model video motion and activities, we use the pretrained C3D <ref type="bibr" target="#b44">[45]</ref> (on the Sports-1M dataset <ref type="bibr" target="#b18">[19]</ref>) for YouTubeClips. The C3D net reads in a video and outputs a fixed-length feature vector every 16 frames. Thus when applying the attention model to the C3D feature pool, we set K = 1 and divide M by 16 (Section 3.2). For the TACoS-MultiLevel dataset, since the cooking activities are fine-grained, the same model trained on sports videos does not work well. Alternatively we compute the Dense Trajectories <ref type="bibr" target="#b50">[51]</ref> for each video interval and encode them with the Fisher vector <ref type="bibr" target="#b16">[17]</ref>. For the attention model, we set K = 1 and M = 1.</p><p>We employ three different evaluation metrics: BLEU <ref type="bibr" target="#b32">[33]</ref>, METEOR <ref type="bibr" target="#b1">[2]</ref>, and CIDEr <ref type="bibr" target="#b45">[46]</ref>.</p><p>Because the YouTubeClips dataset was tested on by most existing videocaptioning methods, the prior results of all the three metrics have been reported. The TACoS-MultiLevel dataset is relatively new and only the BLEU scores were reported in the previous work. We compute the other metrics for the comparison methods based on the generated sentences that come with the dataset. Generally, the higher the metric scores are, the better the generated sentence correlates with human judgment. We use the evaluation script provided by Chen et al. <ref type="bibr" target="#b6">[7]</ref> to compute scores on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results</head><p>We compare our approach (h-RNN) on YouTubeClips with six state-of-the-art methods: LSTM-YT <ref type="bibr" target="#b47">[48]</ref>, S2VT <ref type="bibr" target="#b46">[47]</ref>, MM-VDN <ref type="bibr" target="#b53">[54]</ref>, TA <ref type="bibr" target="#b55">[56]</ref>, and LSTM-E <ref type="bibr" target="#b31">[32]</ref>. Note that in this experiment a single sentence is generated for each video. Thus only our sentence generator is evaluated in comparison to others. To evaluate the importance of our video features, we also report the results of two baseline methods: h-RNN-Vgg and h-RNN-C3D. The former uses only the object appearance feature and the latter uses only the motion feature, with other components of our  <ref type="table" target="#tab_2">Table 2</ref>. Results on TACoS-MultiLevel, where B, M, and C are short for BLEU, METEOR, and CIDEr respectively. framework unchanged. The evaluation results are shown in <ref type="table">Table 1</ref>. We can see that our approach performs much better than the comparison methods, in all the three metrics. The improvements on the most recent state-of-the-art method (i.e., LSTM-E <ref type="bibr" target="#b31">[32]</ref> = 5.16% in the ME-TEOR score. Since LSTM-E also exploits VggNet and C3D features, this demonstrates that our sentence generator framework is superior to their joint embedding framework. Moreover, although TA <ref type="bibr" target="#b55">[56]</ref> also employs temporal attention, our approach produces much better results due to the fact that the hidden state of our RNN is not conditioned on the video features. Instead, the video features are directly input to our multimodal layer. Our approach also outperforms the two baseline methods by large margins, indicating that both video features are indeed crucial in the video captioning task.</p><p>We compare our approach on TACoS-MultiLevel with three state-of-the-art methods: CRF-T <ref type="bibr" target="#b36">[37]</ref>, CRF-M <ref type="bibr" target="#b35">[36]</ref>, and LRCN <ref type="bibr" target="#b10">[11]</ref>. Like above, we have two baseline methods h-RNN-Vgg and h-RNN-DT which use only the appearance and motion features respectively. We also add another two baseline methods RNN-sent and RNN-cat that have no hierarchy (i.e., with only the sentence generator, but not the paragraph generator). RNN-sent is trained and tested on individual video clips that are segmented from the original 185 long videos according to the annotated intervals. The initial state of the sentence generator is set to zero for each sentence. As a result, sentences are trained and generated independently. RNN-cat initializes the sentence generator with zero only for the first sentence in a paragraph. Then the sentence generator maintains its state for the following sentences until the end of the paragraph. This concatenation strategy for training a paragraph has been exploited in a recent neural conversational model <ref type="bibr" target="#b48">[49]</ref>. We use RNN-send and RNN-cat to evaluate the importance of our hierarchical structure.</p><p>The results on TACoS-MultiLevel are shown in  score. Given that our strategy of extracting object regions is relatively simple compared to the sophisticated hand detector <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">36]</ref>, we expect to have even better performance if our object localization is improved. Our method is also superior to all the baseline methods. Although RNN-cat models temporal dependency among sentences by sentencelevel concatenation, it performs worse than our hierarchical architecture. Again, it shows that both the video features and the hierarchical structure are crucial in our task. <ref type="figure" target="#fig_5">Figure 3</ref> illustrates some example paragraphs generated by our approach on TACoS-MultiLevel. To further demonstrate that our method h-RNN generates better sentences than RNN-cat in general, we perform human evaluation to compare these two methods on TACoS-MultiLevel. Specifically, we discard 1, 166 test video intervals, each of which has exactly the same sentence generated by RNN-cat and h-RNN. This results in a total number of 4, 314 − 1, 166 = 3, 148 video intervals for human evaluation. We then put the video intervals and the generated sentences on Amazon Mechanical Turk (AMT). Each video interval is paired with one sentence generated by RNN-cat and the other by h-RNN, side by side. For each video interval, we ask one turker to select the sentence that better describes the video content. The turker also has a third choice if he believes that both sentences are equally good or bad. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Discussions and Limitations</head><p>Although our approach is able to produce paragraphs for video and has achieved encouraging results, it is subject to several limitations. First, our object detection routine has difficulty handling very small objects. Most of our failure cases on TACoS-MultiLevel produce incorrect object names in the sentences, e.g., confusing small objects that have similar shapes or appearances (cucumber vs. carrot, mango vs. orange, kiwi vs. avocado, etc.). See <ref type="figure" target="#fig_0">Figure 1</ref> for a concrete example: sliced the orange should really be sliced the mango. Accurately detecting small objects (sometimes with occlusion) in complex video scenarios still remains an open problem. Second, the sentential information flows unidirectionally through the paragraph recurrent layer, from the beginning of the paragraph to the end, but not also in the reverse way. Misleading information will be potentially passed down when the first several sentences in a paragraph are generated incorrectly. Using bidirectional RNN <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b51">52]</ref> for sentence generation is still an open problem. Lastly, our approach suffers from a known problem as in most other image/video captioning methods, namely, there is discrepancy between the objective function used by training and the one used by generation. The training process predicts the next word given the previous words from groundtruth, while the generation process conditions the prediction on the ones previously generated by itself. This problem is amplified in our hierarchical framework where the paragraph generator conditions on groundtruth sentences during training but on generated ones during generation. A potential cure for this would be adding Scheduled Sampling <ref type="bibr" target="#b3">[4]</ref> to the training process, where one randomly selects between the true previous words and the words generated by the model. Another solution might be to directly optimize the metric (e.g., BLEU) used at test time <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have proposed a hierarchical-RNN framework for video paragraph captioning. The framework models intersentence dependency to generate a sequence of sentences given video data. The experiments show that our approach is able to generate a paragraph for a long video and achieves the state-of-the-art results on two large-scale datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Some example sentences generated by our approach. The first row shows examples trained on YouTubeClips, where only one sentence is generated for each video. The second row shows examples trained on TACoS-MultiLevel, where paragraphs are generated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>To do so, we first compute an attention score q t m for each frame m, conditioning on the previous hidden state h t−1 :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Examples of generated paragraphs. Red indicates incorrect sentences produced by RNN-sent and green shows the ones generated by our h-RNN in the corresponding time intervals. In the first example, our hierarchical model successfully captures the high likelihood of the event walk to the sink after the event open the refrigerator. In the second example, RNN-sent generates the event take the cutting board twice due to the fact that the sentences in the paragraph are produced independently. In contrast, our hierarchical model avoids this mistake.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>The person opened the drawer. The person took out a pot. The person went to the sink. The person washed the pot. The person turned on the stove. The person peeled the fruit. The person put the fruit in the bowl. The person sliced the orange. The person put the pieces in the plate. The person rinsed the plate in the sink.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Our approach outperforms the state-of-the-art methods, in- cluding the very recently proposed one (i.e., LRCN) with an improvement ofThe person entered the kitchen. The person went to the refrigerator. The person placed the cucumber on the cutting board. The person rinsed the cutting board.The person walked into the kitchen. The person went to the refrigerator. The person walked over to the sink. The person rinsed the carrot in the sink.The person took out a cutting board from the drawer. The person got a knife and a cutting board from the drawer. The person cut the ends off the cutting board.The person took out a cutting board. The person got a knife from the drawer. The person cut the cucumber on the cutting board.</figDesc><table>0.305−0.292 
0.292 

= 4.45% in the BLEU@4 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>In the end, we obtained 773 selections for h- RNN and 472 selections for RNN-cat, with a gap of 301 selections. Thus h-RNN has at least472+3069 = 8.50% improvement over RNN-cat. h-RNN RNN-cat Equally good or bad Total</figDesc><table>301 

773 
472 
3069 
4314 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The primary author would like to thank Baidu Research for providing the summer internship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</title>
		<meeting>the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</meeting>
		<imprint>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video in sentences out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Burchill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Coroian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mussman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Waggoner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="102" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Question answering with subgraph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="615" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL-2011)</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics (ACL-2011)<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Microsoft COCO captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1504.00325</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Doell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2634" to="2641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COGNITIVE SCI-ENCE</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farnebäck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Scandinavian Conference on Image Analysis</title>
		<meeting>the 13th Scandinavian Conference on Image Analysis</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;13 Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automated textual descriptions for a wide range of video events with 48 human actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanckmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schutte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Burghouts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision Workshops and Demonstrations</title>
		<meeting>the European Conference on Computer Vision Workshops and Demonstrations</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="372" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>S&amp;amp;#x00e1;Nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1704" to="1716" />
			<date type="published" when="2012-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human focused video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">U G</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gotoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1480" to="1487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards coherent natural language description of video streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">U G</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gotoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="664" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Natural language description of human activities from video images based on concept hierarchy of actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generating natural-language video descriptions using text-mined knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="541" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page">546</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SAVE: A framework for semantic annotation of visual events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hakeem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Haering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A hierarchical neural autoencoder for paragraphs and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1106" to="1115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for document modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="899" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<title level="m">Deep captioning with multimodal recurrent neural networks (m-rnn). ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning like a child: Fast novel visual concept learning from sentence descriptions of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Jointly modeling embedding and translation to bridge video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<idno>abs/1505.01861</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jing Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Expressing an image stream with a sequence of natural sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="73" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<idno>abs/1511.06732</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Coherent multi-sentence video description with variable level of detail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition (GCPR)</title>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Translating video content to natural language descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semantic aware video transcription using random forest classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="772" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">C3D: generic features for video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sequence to sequence -video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4534" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Translating videos to natural language using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1494" to="1504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A neural conversational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Action Recognition by Dense Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what does it do and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">A multi-scale multiple instance video description network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanishka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno>abs/1505.05914</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Jointly modeling deep video and compositional text to bridge vision and language in a unified framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4507" to="4515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning to describe video with weak supervision by exploiting negative sentential information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015-01" />
			<biblScope unit="page" from="3855" to="3863" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
