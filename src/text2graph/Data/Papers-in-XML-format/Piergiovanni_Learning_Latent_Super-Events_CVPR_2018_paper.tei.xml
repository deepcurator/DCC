<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Latent Super-Events to Detect Multiple Activities in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<postCode>47408</postCode>
									<settlement>Bloomington</settlement>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
							<email>mryoo@indiana.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<postCode>47408</postCode>
									<settlement>Bloomington</settlement>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Latent Super-Events to Detect Multiple Activities in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In this paper, we introduce the concept of learning latent super-events from activity videos, and present   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Activity detection is an important computer vision problem with many societal applications, including smart surveillance, monitoring of patients or elderly (e.g., for quality-of-life systems), online video retrieval, and robot perception. Given a continuous video, the task is to find the frames corresponding to every event occurring in the video. This is more challenging compared to the activity classification problem of categorizing a pre-segmented video or localizing a single activity in a trimmed video. Although activity detection is an important area to study as almost all real-world videos contain multiple activities and are rarely segmented (e.g., surveillance systems), it has been investigated much less, particularly for multi-event videos.</p><p>In the past years, end-to-end learning methods using convolutional neural networks (CNNs) obtained a great amount  of success in video analysis. These approaches successfully modeled per-frame (or per-local-segment) information in activity videos, such as a single RGB frame or optical flows over a small number of frames <ref type="bibr" target="#b6">[7]</ref>. Recently, models such as I3D <ref type="bibr" target="#b2">[3]</ref> have been developed to capture longer-term dynamics (e.g., 64 frames). However, because such end-to-end models are optimized for capturing per-segment information, the primary focus of existing works has been mainly on activity classification and not detection. There are recent works on activity detection using end-to-end models (e.g., the detection task in <ref type="bibr" target="#b24">[25]</ref>), but many of these works also focus on making better per-segment decisions (and their postprocessing) rather than learning details of temporal structure/context over the entire (variable length) video.</p><p>A video contains multiple activities (in a sequence or in parallel) and they are correlated. This means that detecting the frames of one activity in the video should benefit from information in the frames corresponding to another activity, which are often temporally very separated. Existing approaches of representing/classifying video segments without regard to contextual information is thus limited; continuous videos contain rich temporal structure which can be exploited to improve activity detection. For example, in a   <ref type="figure">Figure 2</ref>. Overview of our approach. The temporal structure filters are applied to the entire video and soft-attention is applied to form a super-event representation for each video. We concatenate per-frame (or per-segment) video features with the super-event representation to make per-frame classifications, annotating each frame with its activity class (or no-activity).</p><p>video of a basketball game, shooting and blocking events must occur near-by, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. A block or rebound event cannot occur without a shot event.</p><p>In this paper, we introduce the concept of super-events and present how its representation learning can benefit activity detection. We define a super-event as a set of multiple events occurring together in videos with a particular temporal pattern (i.e., structure). More specifically, if the event we are interested in is a part of a longer-term event, we call the longer-term event its super-event. This is the opposite concept of 'sub-events'. For example, the events of shooting and blocking mentioned above forms a super-event, which may be named as a blocked-shoot. Learning such latent super-events allows the model to capture how the events are temporally related in their videos. Once learned, when making a prediction (i.e., testing), the super-events can serve as temporal context to better detect the events. This enables the detection decision at each frame to be made while considering longer-term temporal structure. Note that such super-events are 'latent', meaning that no super-event annotations are provided.</p><p>We newly design temporal structure filters, and convolve it with the video representation to obtain a super-event representation. Temporal structure filters allow the model to focus on particular sub-intervals. Such temporal structure filters are learned for each event, optimized based on the training data for the best super-event representation construction. For each frame, we combine the super-event representation with the per-frame or per-segment CNN representation for its binary classification per event. Our method is fully-differentiable and can be learned end-to-end using back propagation, making it suitable for any length video. Our experimental results with three different datasets confirm the benefits of our latent super-event learning, obtaining the state-of-the-art results on MultiTHUMOS and Charades.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Activity recognition has been a popular research topic in computer vision <ref type="bibr" target="#b0">[1]</ref>. Hand-crafted features, such as improved dense trajectories <ref type="bibr" target="#b28">[29]</ref> gave excellent results on many benchmark datasets in the past. Recently, there have been more works on learning features for action recognition using convolutional models. Two-stream CNN approaches take both RGB frames and optical flow as input <ref type="bibr" target="#b25">[26]</ref> to capture motion and image features. 3D spatio-temporal (XYT) convolutional filters also have been learned and applied to many activity recognition tasks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3]</ref>. Large scale datasets and challenges such as THUMOS <ref type="bibr" target="#b11">[12]</ref>, ActivityNet <ref type="bibr" target="#b5">[6]</ref> and Charades <ref type="bibr" target="#b24">[25]</ref> provided these approaches training videos to learn the model. Improving activity recognition by temporally aggregat-ing such per-frame CNN features has also been studied. Ng et al. <ref type="bibr" target="#b15">[16]</ref> compared several different forms of temporal pooling over per-frame CNN features. Karpathy et al. <ref type="bibr" target="#b12">[13]</ref> compared various methods to combine temporal information within a CNN. Piergiovanni et al. <ref type="bibr" target="#b17">[18]</ref> studied learning multiple sub-intervals to improve activity recognition. These works focused mostly on pooling short-term temporal information to classify a single interval as an activity. They did not detect multiple activity instances or explore long-term structure between multiple activities. Recurrent neural networks (RNNs) such as long shortterm memory (LSTM) have been popularly used to model temporal event transitions between frames <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b30">31]</ref>. These approaches all relied on a single RNN to capture temporal information in different frames, only implicitly capturing relationships between multiple activities. Our approach allows for more explicit modeling of activity relationships as their super-events, which leads to better performance and more insights into what the network learns.</p><p>Recently, segment-based 3D CNNs have been used to capture spatio-temporal information simultaneously for the activity detection <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30]</ref>. Shou et al. <ref type="bibr" target="#b20">[21]</ref> uses convolutional upsampling to make dense predictions to better localize start and end times. Zhao et al. <ref type="bibr" target="#b32">[33]</ref> use a binary actionness classifier to propose many action segments per video then classify each segment individually. However, these approaches all treated the segments as individual instances and do not exploit the longer-term relationships between actions.</p><p>Sigurdsson et al. <ref type="bibr" target="#b23">[24]</ref> found that using intent, which they defined as the clustering of similar activities, is helpful to activity detection. Fully-connected CRFs were applied as a post-processing of per-frame CNN features as well as object features. While such approach learns some global context, they do not learn the explicit temporal structure, nor are they learned in an end-to-end fashion.</p><p>To our knowledge, this is the first work exploring an endto-end model for super-event representation learning, capturing temporal structure and relationships between activities. Learning hierarchical structures of activities has been studied in many traditional works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b18">19</ref>], but they were not learned end-to-end or required additional labels for intervals. Our model is fully differentiable, enabling joint end-to-end learning of latent super-events and the activity detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Activity detection with latent super-events</head><p>The objective of our model is to annotate each frame as its corresponding activity class (including no-activity) given a continuous video. We present an end-to-end learning model that does such labeling not only by looking at each frame or each local segment but also by considering the overall temporal structure. Our idea is to allow the model to learn the representations of super-events summarizing much longer-term temporal intervals, and take advantage of them for the activity detection. A super-event is defined as a longer-term event containing the event of interest, which is the opposite concept of 'sub-events'. Our approach, shown in <ref type="figure">Fig. 2</ref>, effectively represents super-events using temporal structure filters. We learn per-class soft-attention weights over the filters to create a super-event representation, and take advantage of it for the frame classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Per-frame representation</head><p>The base component of our detection is a CNN providing per-frame (or per-local-segment) representation. This is obtained by learning standard video CNN models (e.g., <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b2">3]</ref>). We train the model to learn binary per-frame classifiers by optimizing:</p><formula xml:id="formula_0">L(v) = t,c z t,c log(p(c|v t )) + (1 − z t,c ) log(1 − p(c|v t ))</formula><p>(1) where v t is the per-frame or per-segment CNN feature at frame t and z t,c is the ground truth label for class c and time t. This gives a sequence of probabilities for each class which can be used to find activity intervals. Using a fully-connected network to model p(c|v t ) captures minimal temporal information, using just a single frame or segment. RNN models have been used to compute p(c|v t ) which captures some implicit temporal information. The learned CNN producing v t serves as our base component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temporal structure filter</head><p>The temporal structure filter we introduce in this paper is a filter designed to capture temporal context formed by multiple activities. It is an extension/generalization of the spatial attention model proposed in <ref type="bibr" target="#b8">[9]</ref>. The idea is to represent a variable length video with a fixed dimensional vector, by only focusing on the 'learned' frame locations. The previous attention model repeats a single Gaussian distribution several times with a fixed stride, while a temporal structure filter learns several independent distributions.</p><p>In particular, each temporal structure filter is modeled as a set of N Cauchy distributions: we found that the Cauchy distribution was easier to train than the Gaussian distribution commonly used (i.e., it converges faster). Each distri-bution learns a center, x n and γ n which controls the width. Given T , the length of the video, each filter is constructed by:</p><formula xml:id="formula_1">x n = (T − 1) · (tanh (x n ) + 1) 2 γ n = exp(1 − 2 · | tanh (γ n ) |) F [t, n] = 1 Z n πγ n (t −x n ) γ n 2<label>(2)</label></formula><p>where Z n is a normalization constant, t ∈ {1, 2, . . . , T } and n ∈ {1, 2, . . . , N }. <ref type="figure" target="#fig_2">Figure 3</ref> shows an example. When trained for the super-event representation learning, our temporal structure filter allows the model to explicitly learn which temporal intervals in the entire video are relevant for the frame-level event detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Super-event representation learning</head><p>In its elementary form, we represent the super-event of each event c by applying one temporal structure filter F c over the entire video representation v. This essentially is a matrix product between each of the N distributions in F c and v. The super-event representation S c is obtained as:</p><formula xml:id="formula_2">S c [n] = T t F c [t, n] · v t .<label>(3)</label></formula><p>This operation applies F c (an T × N filter) to v (the T × D video features) and returns an N · D-dimension vector, which we call the super-event representation. This has an effect of summarizing the entire video representation v into a much smaller representation S c by focusing on the frames specified with the learned parameters of F c .</p><p>Super-event representation with soft attention: As several activities can share the same super-event, it makes sense to learn a set of M different temporal structure filters and share these filters across the classes. Here, M is less than the number of classes C. In order to represent the superevent of each activity class c using such M filters, we learn a set of per-class soft-attention weights allowing each activity class to select some of the M structure filters to use. For a set of C classes, we learn weights W c,m and compute the soft-attention as:</p><formula xml:id="formula_3">A c,m = exp(W c,m ) M k exp(W c,k ) .<label>(4)</label></formula><p>We then create a super-event representation by applying these weights to the M temporal structure filters:</p><formula xml:id="formula_4">S c = M m A c,m · T t F m [t] · v t<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Detection with super-events</head><p>We perform per-frame binary classification for each class by concatenating the super-event representation with the CNN frame representation:</p><formula xml:id="formula_5">p(c|[v t , S c ]) = σ(W [v t , S c ])<label>(6)</label></formula><p>where W is a learnable parameter and σ is the sigmoid function. Unlike standard per-frame classification approaches, each frame now depends on the abstracts from the entire video features (i.e., our super-event representation) instead of a single frame.</p><p>To learn parameters, we optimize the multi-label binary classification loss:</p><formula xml:id="formula_6">L(v) = t,c z t,c log(σ(W [v t , S c ])) + (1 − z t,c ) log(1 − σ(W [v t , S c ]) (7)</formula><p>where z t,c is the ground truth label for class c at time t. We minimize this loss using stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Relative super-events</head><p>We also propose a relative super-event model, where instead of computing the super-event representation for the entire video, we compute a super-event representation relative to the current frame location. This allows us to capture relative relationships like in basketball, shooting and blocking must occur together, regardless of the other content in the video. This works well even when a video contains multiple, unrelated activity classes, for example a highlight video of various sports. This approach is identical to the one above, except the length L of the temporal structure filter is fixed. We use the temporal structure filters F as a convolutional kernel and convovle with the video features, applying F centered at each feature t with a length of L frames. The per-class attention weighting is applied to the structure filters to form a per-frame super-event representation used for the classification of the current frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In order to evaluate the effectiveness of our proposed super-event representation learning, we conducted a set of experiments comparing our method to conventional approaches across various datasets and feature types. We particularly focused on activity detection datasets with videos containing multiple actions/activities, including MultiTHU-MOS <ref type="bibr" target="#b30">[31]</ref>, Charades <ref type="bibr" target="#b24">[25]</ref>, and AVA <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Settings</head><p>Features: We extracted I3D <ref type="bibr" target="#b2">[3]</ref> features from the videos at 25fps with a stride of 8 frames. We are using I3D as the base per-segment CNN mentioned in Section 3.1. I3D is a two-stream 3D CNN that achieved state-of-the-art performance on several action recognition tasks. Using both RGB and optical flow as input, it is able to capture relatively longer-term temporal relationships using a temporal resolution of up to 99 frames. We obtained pre-trained weights for I3D as well as code to fine-tune I3D from the authors, which is identical to what they used for the Charades Challenge 2017. We also tested VGG RGB <ref type="bibr" target="#b26">[27]</ref>, VGG optical flow, and standard two-stream CNNs <ref type="bibr" target="#b6">[7]</ref> at 8fps as our base per-frame CNNs. The optical flow and two-stream features have a temporal resolution of 10 frames and RGB features have a temporal resolution of 1 frame.</p><p>Implementation Details: We implemented the temporal structure filters in PyTorch. The learning rate was set to 0.1 and reduced by a factor of 10 every 1000 iterations. We trained the network using a batch size of 32 videos for 5000 iterations using the Adam <ref type="bibr" target="#b13">[14]</ref> optimizer with default parameters. We applied dropout with a probability of 0.5 to the input features. We set N = 3 (i.e., 3 Cauchy distributions per temporal structure filter), and M = 5 (i.e., 5 temporal structure filters). Per-frame CNN representation dimensionality is D = 1024 for I3D features and D = 4096 for VGG features. In both cases, we used the output of the layer before the final fully-connected layer as the features. In order to make the final per-frame decisions based on per-segment CNN features and our super-event representations, we trained a single fully-connected layer with input size D + D · N and output size of the number of classes. Our code and pretrained models are available at https://github.com/piergiaj/super-events-cvpr18.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">MultiTHUMOS</head><p>Dataset: MultiTHUMOS <ref type="bibr" target="#b30">[31]</ref> is an extension of the THUMOS <ref type="bibr" target="#b11">[12]</ref> dataset with the untrimmed videos densely annotated for 65 different action classes. Unlike ActivityNet and THUMOS, MultiTHUMOS has on average 10.5 activity classes per video, 1.5 labels per frame, and up to 25 different activity instances in each video. This allows us to confirm the value of learning super-event representations in complex videos. MultiTHUMOS contains YouTube videos of various sport activities, such as basketball games (shoot, guard, dribble, and block), volleyball games (serve, set, block, and spike), types of weightlifting, throwing, etc.</p><p>We followed the standard activity detection evaluation setting of MultiTHUMOS, which is measuring the mean average precision (mAP) by annotating each frame in test videos. We actually used fewer training samples than provided: We only used the 1010 continuous videos for our training, and test on the full 1574 test videos. We did not use the segmented training videos from THUMOS, since superevent learning is not meaningful with trimmed videos. Even without using the full dataset, we were able to outperform  Results: <ref type="table" target="#tab_2">Table 1</ref> shows the performance (mAP) of our approach compared against previously reported results of the state-of-the-art methods. We are able to observe that our method outperforms previous approaches by a significant margin, achieving a mAP of 36.4%. This is meaningfully higher than our baseline (i.e., I3D) and the previous best approach <ref type="bibr" target="#b3">[4]</ref>: 29.7% vs. 36.4%. Using the pre-trained I3D <ref type="bibr" target="#b2">[3]</ref> model provided the accuracy identical to the previous best reported performance, and our approach of learning and using latent super-events on top of I3D outperformed it by the margin of 6.7%. Note that this is also higher than using a sequential recurrent model like LSTM on top of the same feature, also by a margin of more than 6%. In addition, we also tested the method of using a temporal pyramid commonly used in the previous works (e.g., <ref type="bibr" target="#b19">[20]</ref>) as a super-event representation. Using the temporal pyramid (of level 3) gave us the performance of 31.2 on MultiTHUMOS (while ours was 36.4).</p><p>We conducted more experiments to compare different super-event representations in our approach. <ref type="table" target="#tab_3">Table 2</ref> compares (1) the baseline per-frame classifier, (2) the method of using global mean/max pooling as a super-event representation, (3) temporal pyramid pooling, (4) the elementary single super-event representation mentioned in Section 3.3, (5) our soft-attention-based super-event representation, and (6) the relative super-event approach. We found that while the global max/mean pooled representation and the single super-event improves performance (compared to the baseline), using the soft-attention and shared temporal structure filters gave the best performance. Relative super-events performed similarly to non-relative super-events. <ref type="figure">Figure 4</ref> illustrates a qualitative analysis of our results. We can observe that super-events improve the detection of related basketball events, especially the co-occurrence of the shooting and blocking actions. The learned temporal structure filters and super-event representations are shown in <ref type="figure">Figure 6</ref>. The filters are visualized by combining our learned temporal structure filters using their soft-attention weights. We obtain 5 TxN filters, and added them based on the learned attention weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Charades dataset</head><p>Dataset: Charades <ref type="bibr" target="#b24">[25]</ref> is a large scale dataset consisting of 9848 videos across 157 activities. The videos were recorded by people in their own homes based on a provided script. Each video contains on an average of 6.8 activity instances, often with complex co-occurring activities, making it a suitable dataset to test our super-event learning. The activities were mainly performed at home, and their classes include 'preparing a meal', 'eating', 'sitting', 'cleaning', etc.</p><p>In our experiments, we follow the original Charades test setting (i.e., Charades v1 localize evaluation). This is a bit different from the newer test set released for the Charades Challenge 2017. The ground truth labels of the challenge test videos are not publicly available and its evaluation server is not approving any new account access. Instead, we followed the original Charades localization test setting (v1) from the dataset website. The original setting is more challenging than the competition setting in the aspect that the competition allowed more training data to be used than the original setting: the participants were able to use the videos of the original test set as a part of the training in the competition. Note that the exact same I3D code (with fine-tuning) that provided 20.72 mAP in the competition setting is only providing 17.22 mAP in the original test setting.</p><p>Similar to MultiTHUMOS, the performances are measured in terms of mAP by evaluating per-frame annotations.</p><p>Results: We compared the effectiveness of our approach of using super-event representations with multiple different base per-frame/per-segment CNNs. More specifically, we used the I3D features across RGB frames, flow frames, and two-stream with and without fine-tuning on the Charades dataset. We also tested our approach with the standard two-stream CNN using VGG models. These features capture various amounts of temporal information, from a single RGB frames (VGG RGB), to 10 optical flow frames (VGG Flow and VGG two-stream) and up to 99 frames for I3D. <ref type="table">Table 3</ref> shows the results describing how much our latent <ref type="table">Table 3</ref>. Results comparing our approach using super-events with the baselines and LSTMs on the Charades dataset v1. These numbers are raw results without the post-processing method of <ref type="bibr" target="#b22">[23]</ref>.  <ref type="table">Table 4</ref>. Results on Charades original dataset (i.e., Charades v1 localize setting). Note that this setting is a bit different from the Charades Challenge 2017 competition setting, whose evaluation server is not approving any new account access. This setting uses less training data. mAP Random <ref type="bibr" target="#b22">[23]</ref> 2.42 RGB <ref type="bibr" target="#b22">[23]</ref> 7.89 Predictive-corrective <ref type="bibr" target="#b3">[4]</ref> 8.9 Two-Stream <ref type="bibr" target="#b22">[23]</ref> 8.94 Two-stream+LSTM <ref type="bibr" target="#b22">[23]</ref> 9.6 R-C3D <ref type="bibr" target="#b29">[30]</ref> 12.7 Sigurdsson et al. <ref type="bibr" target="#b22">[23]</ref> 12.8 I3D <ref type="bibr" target="#b2">[3]</ref> 17 super-event learning approach improves the activity detection performance for each per-frame/per-segment baseline. We are reporting the performances of our method using the super-event representations with soft attention. Furthermore, we implemented the standard LSTM method over the same baseline CNNs. The idea was to directly compare the abilities of our approach and the LSTM in capturing longterm temporal dynamics/relations. We found that superevents yield meaningful performance increases regardless of the feature type. We also outperform the LSTM models, confirming that super-events are better able to capture and use temporal structure than LSTMs. We compare our results with the state-of-the-art in Table 4. Our method is obtaining the best known performance in the localization setting of the Charades dataset. Notably, it is performing better than I3D which obtained the best competition performance in 2017, while using the same feature. <ref type="figure" target="#fig_3">Figures 5 and 7</ref> show example detections.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">AVA dataset:</head><p>Dataset: AVA <ref type="bibr" target="#b9">[10]</ref> is a new large-scale video dataset containing 80 action classes in 57,600 video clips drawn from 192 movies. Unlike Charades, which has individual activity classes per objects, the activities in AVA are very generic, such as sit, stand, walk, carry, etc. A 15 minute segment is selected from each movie and annotated in 3 second intervals for spatial and temporal activity detection. Since we are interested in temporal activity detection, we designed a new setting using AVA in which we label each frame with the activities that are occurring in it regardless of spatial location. Identical to Charades (and MultiTHUMOS), we evaluate our models using per-frame mean average precision (mAP). Because each 3 second interval contains the same labels, we average our predictions and produce one probability vector per 3 second interval. We only use temporal annotations in this experiment. <ref type="table" target="#tab_7">Table 5</ref> compares our approach with random, I3D baseline, and I3D + LSTM. We used RGB, Flow, and Two-stream versions of I3D. Again, we are reporting the performances of our method using super-event representations with soft attention. The results are very consistent with the results we obtained from MultiTHUMOS and Charades. Learning (latent) super-event representations with our method and using them for the activity detection always achieved the higher performance. Using LSTM was also able to improve the vanilla baseline, but our approach always outperformed the LSTM by a meaningful margin. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduced the concept of learning latent superevents in activity videos. We defined a super-event as a set of multiple events occurring together in videos with a particular temporal structure (i.e., the opposite concept of subevents). We newly designed the temporal structure filters, and presented how they can be used to capture temporal dynamics in multi-activity videos for the super-event representation learning. We provided a fully differentiable endto-end architecture to jointly learn the latent super-events and the activity detector using them. We were able to confirm that our method performs superior to the state-of-theart methods in multiple different activity detection datasets, including MultiTHUMOS and Charades.  <ref type="figure">Figure 6</ref>. Illustration of the learned temporal structure filters with soft-attention for the block action in MultiTHUMOS. When applied to two different videos, we can observe that the temporal structure filters capture the temporal relationships between the frames corresponding to shooting and blocking action, as well as the relationship between those corresponding to dribble/pass and blocking action.  . Illustration of the learned temporal structure filters with soft-attention for the stand-up action in Charades. The filters are applied to two different videos. It shows that our temporal structure filters are able to capture the relations that the frames of a sitting action come before the frames of standing up, regardless of the other actions that may occur in the video.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Top: In a basketball video, the block event cannot occur without a shot event. Similarly, the dribble event provides temporal context as to where a shot event can occur. Bottom: In a video of a person eating, 'eating a sandwich' must occur near 'holding sandwich'. The drinking action is also related to eating.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. An illustration of a temporal structure filter. Each filter learns independent centers and widths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Results from a video in Charades. Super-events improve the detection of related events such as taking, holding and eating a sandwich.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Example video of the block action with passing and dribbling. (b) Example video of the block action with dribbling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Example video of the stand-up action with the dressing and holding towel actions. (b) Example video of the stand-up action with holding shoes and reading book actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7</head><label>7</label><figDesc>Figure 7. Illustration of the learned temporal structure filters with soft-attention for the stand-up action in Charades. The filters are applied to two different videos. It shows that our temporal structure filters are able to capture the relations that the frames of a sitting action come before the frames of standing up, regardless of the other actions that may occur in the video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>t</figDesc><table>TxD 

MxDxN 
DxN 

Per-segment 
CNN 
Temporal 
structure filters 

Super-event 
representation 

Soft-
attention 

Video 
frames 

c t+1 

Activity detection 

Continuous 
video 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Performances of the state-of-the-art methods and our ap- proach on MultiTHUMOS. Our approach meaningfully outper- forms all previous results.</figDesc><table>mAP 
Two-stream [31] 
27.6 
Two-stream + LSTM [31] 28.1 
Multi-LSTM [31] 
29.6 
Predictive-corrective [4] 
29.7 
I3D baseline 
29.7 
I3D + LSTM 
29.9 
I3D + Temporal Pyramid 
31.2 
I3D + our super-event 
36.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Performances of different super-event representations, evaluated using MultiTHUMOS. I3D was used as the base CNN.</figDesc><table>mAP 
Baseline 
29.7 
Global max pooling 
30.0 
Global mean pooling 
30.8 
Temporal pyramid pooling 
31.2 
Single super-event (per-class) 31.2 
With soft attention 
36.4 
With soft attention + relative 
36.2 

the previous approaches. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>Figure 4. Results from a video in MultiTHUMOS. Super-events especially help the detection of the shooting and blocking events.</figDesc><table>Guard 
Shot 
Block 
Dribble 
Dunk 

Ground Truth 
Baseline 
Super-Events 

Ground Truth 
Baseline 
Super-Events 

Take Dishes 
Take Sandwich 
Put Sandwich 
Hold Sandwich 
Eat Sandwich 
Take Food 
Put Food 
Hold Food 
Sit in Chair 
Open Door 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 5 .</head><label>5</label><figDesc>Results on Ava Test Set (mAP), with the temporal anno- tation setting. That is, we evaluated the accuracy solely based on frame-level annotations.</figDesc><table>Baseline LSTM Ours 
Random 
2.65 
2.65 
2.65 
I3D RGB 
6.8 
7.0 
8.3 
I3D Flow 
7.1 
7.2 
9.1 
I3D Two-Stream 
7.5 
7.8 
9.8 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2011-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal graphs of human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="778" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Predictivecorrective networks for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03615</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Actom sequence models for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3201" to="3208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04623</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">AVA: A video dataset of spatiotemporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08421</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Understanding videos, constructing plots learning a visually grounded storyline model from annotated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2012" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">THUMOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Leveraging temporal, contextual and ordering constraints for recognizing complex activities in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Laxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling temporal structure of decomposable motion segments for activity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="392" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning latent sub-events in activity videos using temporal attention filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the American Association for Artificial Intelligence (AAAI</title>
		<meeting>the American Association for Artificial Intelligence (AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">First-person activity recognition: What are they doing to me?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pooled motion features for first-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="896" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01515</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Asynchronous temporal fields for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">What actions are needed for understanding human actions in videos?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02696</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">C3d: generic features for video analysis. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">0767</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07814</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Endto-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06228</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
