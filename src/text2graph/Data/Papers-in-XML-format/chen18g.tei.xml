<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning K-way D-dimensional Discrete Codes for Compact Embedding Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Renqiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
						</author>
						<title level="a" type="main">Learning K-way D-dimensional Discrete Codes for Compact Embedding Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Conventional embedding methods directly associate each symbol with a continuous embedding vector, which is equivalent to applying a linear transformation based on a "one-hot" encoding of the discrete symbols. Despite its simplicity, such approach yields the number of parameters that grows linearly with the vocabulary size and can lead to overfitting. In this work, we propose a much more compact K-way D-dimensional discrete encoding scheme to replace the "one-hot" encoding. In the proposed "KD encoding", each symbol is represented by a D-dimensional code with a cardinality of K, and the final symbol embedding vector is generated by composing the code embedding vectors. To end-to-end learn semantically meaningful codes, we derive a relaxed discrete optimization approach based on stochastic gradient descent, which can be generally applied to any differentiable computational graph with an embedding layer. In our experiments with various applications from natural language processing to graph convolutional networks, the total size of the embedding layer can be reduced up to 98% while achieving similar or better performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Embedding methods, such as word embedding <ref type="bibr" target="#b24">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b25">Pennington et al., 2014)</ref>, have become pillars in many applications when learning from discrete structures. The examples include language modeling <ref type="bibr" target="#b17">(Kim et al., 2016)</ref>, machine translation <ref type="bibr" target="#b27">(Sennrich et al., 2015)</ref>, text classification <ref type="bibr" target="#b33">(Zhang et al., 2015b)</ref>, knowledge graph and social network modeling <ref type="bibr" target="#b2">(Bordes et al., 2013;</ref> Proceedings of the 35 th International Conference on Machine Learning, <ref type="bibr">Stockholm, Sweden, PMLR 80, 2018</ref><ref type="bibr">. Copyright 2018</ref> by the author(s). , and many others <ref type="bibr" target="#b19">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b4">Chen et al., 2016)</ref>. The objective of the embedding module in neural networks is to represent a discrete symbol, such as a word or an entity, with some continuous embedding vector v ∈ R d . This seems to be a trivial problem, at the first glance, in which we can directly associate each symbol with a learnable embedding vector, as is done in existing work. To retrieve the embedding vector of a specific symbol, an embedding table lookup operation can be performed. This is equivalent to the following: first we encode each symbol with an "one-hot" encoding vector b ∈ [0, 1] N where j b j = 1 (N is the total number of symbols), and then generate the embedding vector v by simply multiplying the "one-hot" vector b with the embedding matrix W ∈ R N ×d , i.e. v = W T b.</p><p>Despite the simplicity of this "one-hot" encoding based embedding approach, it has several issues. The major issue is that the number of parameters grows linearly with the number of symbols. This becomes very challenging when we have millions or billions of entities in the database, or when there are lots of symbols with only a few observations (e.g. Zipf's law). There also exists redundancy in the O(N ) parameterization, considering that many symbols are actually similar to each other. This over-parameterization can further lead to overfitting; and it also requires a lot of memory, which prevents the model from being deployed to mobile devices. Another issue is purely from the code space utilization perspective, where we find "one-hot" encoding is extremely inefficient. Its code space utilization rate is almost zero as N/2 N → 0 when N → ∞, while N dimensional discrete coding system can effectively represent 2 N symbols.</p><p>To address these issues, we propose a novel and much more compact coding scheme that replaces the "one-hot" encoding. In the proposed approach, we use a K-way Ddimensional code to represent each symbol, where each code has D dimensions, and each dimension has a cardinality of K. For example, a concept of cat may be encoded as (5-1-3-7), and a concept of dog may be encoded as (5-1-3-9). The code allocation for each symbol is based on data and specific tasks such that the codes can capture semantics of symbols, and similar codes should reflect similar mean-ings. While we mainly focus on the encoding of symbols in this work, the learned discrete codes can have larger applications, such as information retrieval. We dub the proposed encoding scheme as "KD encoding".</p><p>The KD code system is much more compact than its "onehot" counterpart. To represent a set of symbols of size N , the "KD encoding" only requires K D ≥ N . Increasing K or D by a small amount, we can easily achieve K D N , in which case it will still be much more compact and keep D = O( log N log K ). Consider K = 2, the utilization rate of "KD encoding" is N/2 D , which is 2 N −D times more compact than its "one-hot" counterpart 1 .</p><p>The compactness of the code can be translated into compactness of the parametrization. Dropping the giant embedding matrix W ∈ R N ×d that stores symbol embeddings and leveraging semantic similarities between symbols, the symbol embedding vector is generated by composing much fewer code embedding vectors. This can be achieved as follows: first we embed each "KD code" into a sequence of code embedding vectors in R D×d , and then apply embedding transformation function f (·) to generate the final symbol embedding. By adopting the new approach, we can reduce the number of embedding parameters from</p><formula xml:id="formula_0">O(N d) to O( K log K d log N + C)</formula><p>, where d is the code embedding size, and C is the number of neural network parameters.</p><p>Due to the the discreteness of the code allocation problem, it is very challenging to learn the meaningful discrete codes that can exploit the similarities among symbols according to a target task in an end-to-end fashion. A compromise is to learn the code given a trained embedding matrix, and then fix the code during the stage of task-specific training. While this has been shown working relatively well in previous work <ref type="bibr" target="#b28">Shu &amp; Nakayama, 2017)</ref>, it produces a sub-optimal solution, and requires a multi-stage procedure that is hard to tune. In this work, we derive a relaxed discrete optimization approach based on stochastic gradient descent (SGD), and propose two guided methods to assist the end-to-end code learning. To validate our idea, we conduct experiments on three different tasks from natural language processing to graph convolutional networks for semi-supervised node classification. We achieve 95% of embedding model size reduction in the language modeling task and 98% in text classification with similar or better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The K-way D-dimensional Discrete Encoding Framework</head><p>In this section, we introduce the "KD encoding" framework in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Problem Formulation</head><p>Symbols are represented with a vocabulary V = {s 1 , s 2 , · · · , s N } where s i corresponds to the i-th symbol. Here we aim to learn a transformation function that maps a symbol s i to a continuous embedding vector v i , i.e. T : V → R d . In the case of conventional embedding method, T is a linear transformation of "one-hot" code of a symbol.</p><p>To measure the fitness of T , we consider a differentiable computational graph G that takes discrete symbols as input x and outputs the predictions y, such as text classification model based on word embeddings. We also assume a taskspecific loss function L(x, y) is given. The task-oriented learning of T is to learn T such that L(x, y) is minimized, i.e. T = arg min T L(x, y|T , Θ) where Θ are task-specific parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The "KD Encoding" Framework</head><p>In the proposed framework, each symbol is associated with a K-way D-dimensional discrete code. We denote the discrete code for the i-th symbol as c i = (c</p><formula xml:id="formula_1">1 i , c 2 i , · · · , c D i ) ∈ B</formula><p>D , where B is the set of code bits with cardinality K. To connect symbols with discrete codes, a code allocation function φ(·) : V → B D is used. The learning of this mapping function will be introduced later, and once fixed it can be stored as a hash table for fast lookup. Since a discrete code c i has D dimensions, we do not directly use embedding lookup to find the symbol embedding as used in "onehot" encoding. Instead, we want to learn an adaptive code composition function that takes a discrete code and generates a continuous embedding vector, i.e. f : B D → R d . The details of f will be introduced in the next subsection. In sum, the "KD encoding" framework we have T = f • φ with a "KD code" allocation function φ and a composition function f as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>(a) and 1(b).</p><p>In order to uniquely identify every symbol, we only need to set K D = N , as we can assign a unique code to each symbol in this case. When this holds, the code space is fully utilized, and none of the symbol can change its code without affecting other symbols. We call this type of code system compact code. The optimization problem for compact code can be very difficult, and usually requires approximated combinatorial algorithms such as graph matching <ref type="bibr" target="#b20">(Li et al., 2016)</ref>. Realizing the difficulties in optimization, we propose to adopt the redundant code system, where K D N , namely, there are a lot of "empty" codes with no symbol associated. Changing the code of one symbol may not affect other symbols under this scheme, since the random collision probability can be very small 2 , which makes it easier to optimize. The redundant code can be achieved by slightly increasing the size of K or D thanks to the exponential nature of their relations to N . Therefore, in both compact code or redundant code, it only requires</p><formula xml:id="formula_2">D = O( log N log K ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Discrete Code Embedding</head><p>As mentioned above, given learned φ(·) and the i-th symbol s i , we can retrieve its code via a code lookup, i.e. c i = φ(s i ). In order to generate the composite embedding vector v i , we adopt an adaptive code composition function</p><formula xml:id="formula_3">v i = f (c i ).</formula><p>To do so, we first embed the code c i to a sequence of code embedding vectors</p><formula xml:id="formula_4">(W 1 c 1 i , W 2 c 2 i , · · · , W D c D i</formula><p>), and then apply another transfor-</p><formula xml:id="formula_5">mation v = f e (W 1 c 1 i , W 2 c 2 i , · · · , W D c D i</formula><p>; θ e ) to generate v.</p><p>Here W j ∈ R K×d is the code embedding matrix for the j-th code dimension, and f e is the embedding transformation function that maps the code embedding vectors to the symbol embedding vector. The choice of f e is very flexible and varies from task to task. In this work, we consider two types of embedding transformation functions.</p><p>The first one is based on a linear transformation:</p><formula xml:id="formula_6">v i = H j W j c j i T ,</formula><p>where H ∈ R d×d is a transformation matrix for matching the dimensions. While this is simple and efficient, due to its linear nature, the capacity of the generated symbol embedding may be limited when the size of K, D or the code embedding dimension d is small.</p><p>Another type of embedding transformation functions are nonlinear, and here we introduce one that is based on a recurrent neural network, LSTM <ref type="bibr" target="#b10">(Hochreiter &amp; Schmidhuber, 1997)</ref>, in particular. That is, we have</p><formula xml:id="formula_7">(h 1 , · · · , h j ) = LSTM(W 1 c 1 , · · · , W j c j ) (see supplementary for details).</formula><p>collision at all is 99.5%.</p><p>The final symbol embedding can be computed by summing over LSTM outputs at all code dimensions (and using a linear layer to match dimension if</p><formula xml:id="formula_8">d = d ), i.e. v = H( j h j )</formula><p>T . <ref type="figure" target="#fig_0">Figure 1</ref>(c) and 1(d) illustrate the above two embedding transformation functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Analysis of the Proposed "KD Encoding"</head><p>To measure the parameter and model size reduction, we first introduce two definitions as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1. (Embedding parameters)</head><p>The embedding parameters are the parameters θ that are used in code composition function f . Specifically, it includes code embedding matrices {W}, as well as other parameters θ e used in the embedding transformation function f e .</p><p>It is worth noting that we do not explicitly include the code as embedding parameters. This is due to the fact that we do not count "one-hot" codes as parameters. Also in some cases the codes are not adaptively learned, such as hashed from symbols <ref type="bibr" target="#b29">(Svenstrup et al., 2017)</ref>. However, when we export the model to embedded devices, the storage of discrete codes does occupy space. Hence, we introduce another concept below to take it into consideration as well.</p><p>Definition 2. (Embedding layer's size) The embedding layer's size is the number of bits used to store both embedding parameters as well as the discrete codes.</p><p>Lemma 1. The number of embedding parameters used in KD encoding is O(</p><formula xml:id="formula_9">K log K d log N +C)</formula><p>, where C is the number of parameters of neural nets.</p><p>The proof is given in the supplementary material.</p><p>For the analysis of the embedding layer's size under "KD encoding", we assume that 32-bits floating point number is used. The total bits used by the "KD encoding" is N D log 2 K + 32(KDd + C) consisting both code size as well as the size of embedding parameters. Comparing to the total model size by conventional full embedding, which is 32N (1 + d), it can still be a huge saving of model space, especially when N, d are large.</p><p>Here we provide a theoretical connection between the proposed "KD encoding" and the SVD or low-rank factorization of the embedding matrix. We consider the scenario where the composition function f is a linear function with no hidden layer, that is</p><formula xml:id="formula_10">v i = ( j W j c j i ) T .</formula><p>Proposition 1. A linear composition function f with no hidden layer is equivalent to a sparse binary low-rank factorization of the embedding matrix.</p><p>The proof is also provided in the supplementary material. But the overall idea is that the "KD code" mimics an 1-out-of-K selection within each of the D groups.</p><p>The computation overhead brought by linear composition is very small compared to the downstream neural network computation (without hidden layer in linear composition function, we only need to sum up D vectors). However, the expressiveness of the linear factorization is limited by the number of bases or rank of the factorization, which is determined by K and D. And the use of non-linear composition function can largely increase the expressiveness of the composite embedding matrix and may be an appealing alternative, this is shown by the proposition 2 in supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">End-to-End Learning of the Discrete Code</head><p>In this section, we propose methods for learning taskspecific "KD codes".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Continuous Relaxation for Discrete Code Learning</head><p>As mentioned before, we want to learn the symbol-toembedding-vector mapping function, T , to minimize the target task loss, i.e. T = arg min T L(x, y|T , Θ). This includes optimizing both code allocation function φ(·) and code composition function f (·). While f (·) is differentiable w.r.t. its parameters θ, φ(·) is very challenging to learn due to the discreteness and non-differentiability of the codes.</p><p>Specifically, we are interested in solving the following optimization problem,</p><formula xml:id="formula_11">min {c},θ,Θ i L x i , y i |f e W 1 c 1 i , W 2 c 2 i , · · · , W D c D i , Θ</formula><p>(1) where f e is the embedding transformation function mapping code embedding to the symbol embedding, θ = {W, θ e } contains code embeddings and the composition parameters, and Θ denotes other task-specific parameters.</p><p>We assume the above loss function is differentiable w.r.t. to the continuous parameters including embedding parameters θ and other task-specific parameters Θ, so they can Figure 2. The effects of temperature τ on output probability of Softmax and its entropy (when K = 2). As τ decreases, the probabilistic output approximates step function when K = 2, and generally "one-hot" vector when K &gt; 2.</p><p>be optimized by following standard stochastic gradient descent and its variants <ref type="bibr" target="#b18">(Kingma &amp; Ba, 2014)</ref>. However, each c i is a discrete code, it cannot be directly optimized via SGD as other parameters. In order to adopt gradient based approach to simplify the learning of discrete codes in an end-to-end fashion, we derive a continuous relaxation of the discrete code to approximate the gradient effectively.</p><p>We start by making the observation that each code c i can be seen as a concatenation of D "one-hot" vectors, i.e.</p><formula xml:id="formula_12">c i = (o 1 i , o 2 i , · · · , o D i ), where ∀j, o j i ∈ [0, 1] K and k o jk i = 1, where o jk i is the k-th component of o j i .</formula><p>To make it differentiable, we relax the o i from a "one-hot" vector to a continuous relaxed vectorô i by applying tempering Softmax:</p><formula xml:id="formula_13">o jk i ≈ô jk i = exp(π jk i /τ ) k exp(π jk i /τ )</formula><p>Where τ is a temperature term, as τ → 0, this approximation becomes exact (except for the case of ties). We show this approximation effects for K = 2 with y = 1/(1 + exp(−x/τ )) in <ref type="figure">Figure 2a</ref>. Similar techniques have been introduced in Gumbel-Softmax trick <ref type="bibr" target="#b13">(Jang et al., 2016;</ref><ref type="bibr" target="#b21">Maddison et al., 2016)</ref>.</p><p>Sinceô i is continuous (given τ is not approaching 0), instead of learning the discrete code assignment directly, we learnô i as an approximation to o i . To do so, we can adjust the code logits π i using SGD and gradually decrease the temperature τ during the training. Since the indexing operator for retrieval of code embedding vectors, i.e. W j c j i , is non-differentiable, to generate the embedding vector for j-th code dimension, we instead use an affine transformation operator, i.e. (W j )</p><p>Tô j i , which enables the gradient to flow backwards normally.</p><p>It is easy to see that control of temperature τ can be important. When τ is too large, the outputô i is close to uniform, which is too far away from the desired "one-hot" vector o i . When τ is too small, the slight differences between different logits π j i and π j i will be largely magnified. Also, the gradient vanishes when the Softmax output approaches "one-hot" vector, i.e. when it is too confident. A "right" schedule of temperature can thus be crucial. While we can handcraft a good schedule of temperature, we also observe that the temperature τ is closely related to the entropy of the output probabilistic vector, as shown in <ref type="figure">Figure 2b</ref>, where a same set of random logits can produce probabilities of different entropies when τ varies. This motivates us to implicitly control the temperature via regularizing the entropy of the model. To do so, we add the following entropy regularization term: H = − i,j,kô jk i logô jk . A large penalty for this regularization term encourages a small entropy for the relaxed codes, i.e. a more spiky distribution.</p><p>Up to this point, we still use the continuous relaxationô i to approximate o i during the training. In inference, we will only use discrete codes. The discrepancy of the continuous and discrete codes used in training and inference is undesirable. To close the gap, we take inspiration from StraightThrough Estimator <ref type="bibr" target="#b1">(Bengio et al., 2013)</ref>. In the forward pass, instead of using the relaxed tempering Softmax outputô i , which is likely a smooth continuous vector, we take its arg max and turn it into a "one-hot" vector as follows, which recovers a discrete code.</p><formula xml:id="formula_14">o j i = one_hot arg max kô jk i ≈ Softmax π j i τ , τ → 0</formula><p>We interpret the use of straight-through estimator as using different temperatures during the forward and backward pass. In forward pass, τ → 0 is used, for which we simply apply the arg max operator. In the backward pass (to compute the gradient), it pretends that a larger τ was used. Compared to using the same temperature in both passes, this always outputs "one-hot" discrete code o j i , which closes the previous gap between training and inference.</p><p>The training procedure is summarized in Algorithm 1, in which the stop_gradient operator will prevent the gradient from back-propagating through it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Code Learning with Guidances</head><p>It is not surprising the optimization problem is more challenging for learning discrete codes than learning conventional continuous embedding vectors, due to the discreteness of the problem (which can be NP-hard). This could lead to a suboptimal solution where discrete codes are not as competitive. Therefore, we propose to use guidances from the continuous embedding vectors to mitigate the problem. The basic idea is that instead of adjusting codes according to noisy gradients from the end task as shown above, we also require the composite embedding vectors from codes to mimic continuous embedding vectors, which can be either jointly trained (online distillation guidance),</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 An epoch of code learning via Straightthrough Estimator with Tempering Softmax.</head><p>Parameters: code logits {πi}, code embedding matrices {W j }, transformation parameters θe, and other task specific parameters Θ.</p><formula xml:id="formula_15">for i ← 1 to N do for j ← 1 to D dô o j i = Softmax(π j i /τ ) o j i = one_hot(arg max kô jk i ) o j i = stop_gradient(o j i −ô j i ) +ô j i end for A step of SGD on πi, {W j }, θe, Θ to reduce L xi, yi, fe (o 1 i ) T W 1 , · · · , (o D i ) T W D ; θe , Θ end for</formula><p>or pre-trained (pre-train distillation guidance). The continuous embedding can provide better signals for both code learning as well as the rest parts of the neural network, improve the training subsequently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Online Distillation Guidance (ODG).</head><p>A good learning progress in code allocation function φ(·) can be important for the rest of the neural network to learn. For example, it is hard to imagine we can train a good model based on "KD codes" if we have φ("table") = φ("cat"). However, the learning of the φ(·) also depends on the rest of network to provide good signals.</p><p>Based on the observation, we propose to associate a regular continuous embedding vector u i with each symbol during the training, and we want the "KD encoding" function T (·) to mimic the continuous embedding vectors, while both of them are simultaneously optimized for the end task. More specifically, during the training, instead of using the embedding vector generated from the code, i.e. f (c i ), we use a dropout average of them, i.e.</p><formula xml:id="formula_16">v i = m u i + (1 − m) f (c i ).</formula><p>Here m is a Bernoulli random variable for selecting between the regular embedding vector or the KD embedding vector. When m is turned on with a relatively high probability (e.g. 0.7), even if f (c i ) is difficult to learn, u i can still be learned to assist the improvement of the taskspecific parameters Θ, which in turn helps code learning. During the inference, we only use f (c i ) as output embedding. This choice can lead to a gap between training and generalization errors. Hence, we add a regularization loss λ u i −f (c i ) 2 during the training that encourages the match between u i and f (c i ) 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symbol</head><p>Code logits</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KD Symbol Embedding</head><p>Pre-trained Symbol Embedding Pre-trained Distillation Guidance (PDG). It is important to close the gap between training and inference in the online distillation guidance process, unfortunately this can still be difficult. Alternatively, we can also adopt pretrained continuous embedding vectors as guidance. Instead of training the continuous embedding vectors alongside the discrete codes, we utilize a pre-trained continuous embedding matrix U produced from the same model with conventional continuous embedding vectors. During the end-toend training of the codes (as well as other parameters), we ask the composite embedding vector v i generated by "KD encoding" to mimic the the given embedding vector u i by minimizing the l 2 distance. Furthermore, we can build an auto-encoder of the pretrained continuous embedding vectors, and use both continuous embedding vectors as well as the code logits as guidances. In the encoding pass, a transformation function g(·) is used to map u i to the code logits π i . In its decoding pass, it utilizes the same transformation function f (·) that is used in "KD encoding" to reconstruct u i . The loss function for the auto-encoders is</p><formula xml:id="formula_17">L auto−encoder = i f (g(u i ); τ ) − u i 2</formula><p>To follow the guidance of the pre-trained embedding matrix U, we ask the code logits π i and composite symbol embedding</p><formula xml:id="formula_18">v i = f (π i ; τ )</formula><p>4 to mimic the ones in the auto-encoder as follows</p><formula xml:id="formula_19">L distillation = i α f (π i ; τ ) − u i 2 + β π i − g(u i ) 2</formula><p>During the training, both L auto−encoder and L distillation will be added to the task-specific loss function to train jointly. The method is illustrated in the <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Here we also make a distinction between pre-trained distillation guidance (PDG) and pre-training of codes. Firstly, PDG can learn codes end-to-end to optimize the task's loss, while the pre-trained codes will be fixed during the task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct experiments to validate the proposed approach. Since the proposed "KD Encoding" can be applied to various tasks and applications with embedding layers involved. We choose three important tasks for evaluation, they are (1) language modeling, (2) text classification, and (3) graph convolutional networks for semisupervised node classification. For the detailed descriptions of these tasks and other applications of our method, we refer readers to the supplementary material.</p><p>For the language modeling task, we test on the widely used English Penn Treebank <ref type="bibr" target="#b22">(Marcus et al., 1993)</ref> dataset, which contains 1M words with vocabulary size of 10K. The training/validation/test split is provided by convention according to <ref type="bibr" target="#b23">(Mikolov et al., 2010)</ref>. Since we only focus on the embedding layer, we simply adopt a previous state-of-theart model <ref type="bibr" target="#b30">(Zaremba et al., 2014)</ref>, in which they provide three different variants of LSTMs <ref type="bibr" target="#b10">(Hochreiter &amp; Schmidhuber, 1997</ref>) of different sizes: The larger model has word embedding size and LSTM hidden size of 1500, while the number is 650 and 200 for the medium and small models. By default, we use K = 32, D = 32 and pre-trained distillation guidance for the proposed method, and linear embedding transformation function with 1 hidden layer of 300 hidden units.</p><p>For the text classification task, we utilize five different datasets from <ref type="bibr" target="#b33">(Zhang et al., 2015b)</ref>, namely Yahoo! news, AG's news, DBpedia, Yelp review polarity ratings as well Yelp review full-scale ratings 5 . We adopt network architecture used in FastText <ref type="bibr" target="#b16">(Joulin et al., 2016b;</ref><ref type="bibr">a)</ref>, where a SoftMax is stacked on top of the averaged word embedding vectors of the text. For simplicity, we only use unigram word information but not sub-words or bi-grams, as used in their work. The word embedding dimension is chosen to be 300 as it yields a good balance between size and performance. By default, we use K = 32, D = 32 for the proposed method, and linear transformation with no hidden layer. That is to add code embedding vectors together to generate symbol embedding vector, and the dimension of code embedding is the same as word embedding.</p><p>For the application with graph convolutional networks, we follow the same setting and hyper-parameters as in <ref type="bibr" target="#b19">(Kipf &amp; Welling, 2016)</ref>. Three datasets are used for comparison, namely Cora, Citeseer, Pubmed. Since both the number of symbols (1433, 3703, and 500 respectively) as well as its embedding dimension (16) are small, the compressible space is actually quite small. Nevertheless, we perform the proposed method with K = 64, D = 8 for Cora and Citeseer, and K = 32, D = 4 for Pubmed. Again, a linear embedding transformation function is used with one hidden layer of size 16. We do not use guidances for text classification and graph node classification tasks since the direct optimization is already satisfying enough.</p><p>We mainly compare the proposed "KD encoding" approach with the conventional continuous (full) embedding counterpart, and also compare with low-rank factorization <ref type="bibr" target="#b26">(Sainath et al., 2013)</ref> with different compression ratios. The results for three tasks are shown in <ref type="table" target="#tab_0">Table 1</ref>, 2, 3, respectively. In these tables, three types of metrics are shown: (1) the performance metric, perplexity for language modeling and accuracy for the others, (2) the number of embedding parameters θ used in f , and (3) the total embedding layer's size includes θ as well as the codes. From these tables, we observe that the proposed "KD encoding" with end-toend code learning perform similarly, or even better in many cases, while consistently saving more than 90% of embe- ding parameter and model size, 98% in the text classification case. In order to achieve similar level of compression, we note that low-rank factorization baseline will reduce the performance significantly.</p><p>We further compare with broader baselines on language modeling tasks (with medium sized language model for convenience): (1) directly using first 10 chars of a word as its code (padding when necessary), (2) training aware quantization <ref type="bibr" target="#b12">(Jacob et al., 2017)</ref>, and (3) product quantization <ref type="bibr" target="#b14">(Jegou et al., 2011;</ref><ref type="bibr" target="#b15">Joulin et al., 2016a)</ref>. The results are shown in <ref type="table">Table 4</ref>. We can see that our methods significantly outperform these baselines, in terms of both PPL as well as model size (bits) reduction.</p><p>In the following, we scrutinize different components of the proposed model based on PTB language modeling. To start with, we test various code learning methods, and demonstrate the impact of training with guidance. The results are shown in <ref type="table" target="#tab_2">Table 5</ref>. First, we note that both random codes as well as pre-trained codes are suboptimal, which is understandable as they are not (fully) adaptive to the target tasks. Then, we see that end-to-end training without guidance suffers serious performance loss, especially when the task specific networks increase its complexity (with larger hidden size and use of dropout). Finally, by adopting the proposed continuous guidances (especially distillation guidance), the performance loss can be overcame. We further vary the size of K or D and see how they affect the performance. As shown in <ref type="figure" target="#fig_3">Figure 4a</ref> and 4b, small K or D may harm the performance (even though that K D N is satisfied),   behind quickly starts 3-1-0-4 week tuesday wednesday monday thursday friday sunday saturday 3-1-0-5 by were after before while past ago close soon recently continued meanwhile 3-1-1-1 year month months record fall annual target cuts which suggests that the redundant code can be easier to learn. The size of D seems to have higher impact on the performance compared to K. Also, when D is small, nonlinear encoder such as RNN performs much better than the linear counterpart, which verifies our Proposition 2. To examine the learned codes, we apply our method on the pretrained embedding vectors from Glove <ref type="bibr" target="#b25">(Pennington et al., 2014)</ref>, which has better coverage and quality. We force the model to assign multiple words to the same code by setting K = 6, D = 4 (code space is 1296) for vocabulary size of 10K. <ref type="table" target="#tab_3">Table 6</ref> show a snippet of the learned codes, which shows that semantically similar words are assigned to the same or close-by discrete codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>The idea of using more efficient coding system traces to information theory, such as error correction code <ref type="bibr" target="#b7">(Hamming, 1950)</ref>, and Hoffman code <ref type="bibr" target="#b11">(Huffman, 1952)</ref>. However, in most embedding techniques such as word embedding <ref type="bibr" target="#b24">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b25">Pennington et al., 2014)</ref>, entity embedding <ref type="bibr" target="#b4">(Chen et al., 2016;</ref>, "one-hot" encoding is used along with a usually large embedding matrix. Recent work <ref type="bibr" target="#b17">(Kim et al., 2016;</ref><ref type="bibr" target="#b27">Sennrich et al., 2015;</ref><ref type="bibr" target="#b33">Zhang et al., 2015b)</ref> explores character or subword based embedding model instead of the word embedding model and show some promising results. <ref type="bibr" target="#b29">(Svenstrup et al., 2017)</ref> proposes using hash functions to automatically map texts to pre-defined bases with a smaller vocabulary size, according to which vectors are composed. However, in their cases, the chars, sub-words and hash functions are fixed and given a priori dependent on language, thus may have few semantic meanings attached and may not be available for other type of data. In contrast, we learn the code assignment function from data and tasks, and our method is language independent.</p><p>The compression of neural networks <ref type="bibr" target="#b8">(Han et al., 2015a;</ref><ref type="bibr" target="#b6">Chen et al., 2015)</ref> has become more and more important in order to deploy large networks to small mobile devices. Our work can be seen as a way to compress the embedding layer in neural networks. Most existing network compression techniques focus on dense/convolutional layers that are shared/amortized by all data instances, while one data instance only utilizes a fraction of embedding layer weights associated with the given symbols. To compress these types of weights, some efforts have been made, such as product quantization <ref type="bibr" target="#b14">(Jegou et al., 2011;</ref><ref type="bibr" target="#b15">Joulin et al., 2016a;</ref><ref type="bibr" target="#b31">Zhang;</ref><ref type="bibr" target="#b32">Zhang et al., 2015a;</ref><ref type="bibr" target="#b0">Babenko &amp; Lempitsky, 2014)</ref>. Compared to their methods, our framework is more general. Many of these methods can be seen as a special case of "KD encoding" using a linear embedding transformation function without hidden layer. Also, under our framework, both the codes and the transformation functions can be learned jointly by minimizing task-specific losses.</p><p>Our work is also related to LightRNN <ref type="bibr" target="#b20">(Li et al., 2016)</ref>, which can be seen as a special case of our proposed KD code with K = √ N and D = 2. Due to the use of a more compact code, its code learning is harder and more expensive. This work is an extension of our previous workshop paper  with guided end-to-end code learning. In parallel to , <ref type="bibr" target="#b28">(Shu &amp; Nakayama, 2017)</ref> explores similar ideas with linear composition functions and pre-trained codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we propose a novel K-way D-dimensional discrete encoding scheme to replace the "one-hot" encoding, which significantly improves the efficiency of the parameterization of models with embedding layers. To learn semantically meaningful codes, we derive a relaxed discrete optimization technique based on SGD enabling endto-end code learning. We demonstrate the effectiveness of our work with applications in language modeling, text classification and graph convolutional networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (a) The conventional symbol embedding based on "one-hot" encoding. (b) The proposed KD encoding scheme. (c) and (d) are examples of embedding transformation functions by DNN and RNN used in the "KD encoding" when generating the symbol embedding from its code.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Online Distillation Guidance. Dashed lines denotes regularization, doted line in the middle denotes sharing of transformation function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The effects of various K and D under different instantiation of embedding transformation function f (·).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Language modeling (PTB). Compared with Conven- tional full embedding, and low-rank (denoted with Lr) with dif- ferent compression rates.</figDesc><table>Model 
Full 
Lr(5X) Lr(10X) 
Ours 

Perplexity 

Small 
114.53 134.01 
134.89 
107.77 
Medi. 
83.38 
84.84 
85.53 
83.11 
Large 
78.71 
81.23 
81.85 
77.72 
# of emb. 
params. 
(M) 

Small 
2.00 
0.40 
0.19 
0.37 
Medi. 
6.50 
1.30 
0.65 
0.50 
Large 
15.00 
2.99 
1.50 
0.76 

# of bits 
(M) 

Small 
64.00 
12.73 
6.20 
13.39 
Medi. 
208.00 
41.58 
20.79 
17.75 
Large 
480.00 
95.68 
47.84 
26.00 

Table 2. Text classification. Lr denotes low-rank. 
Model 
Full 
Lr(10X) Lr(20X) Ours 

Accuracy 

Yahoo! 
0.698 
0.695 
0.691 
0.695 
AG N. 
0.914 
0.914 
0.915 
0.916 
Yelp P. 
0.932 
0.924 
0.923 
0.931 
Yelp F. 
0.592 
0.578 
0.573 
0.590 
DBpedia 
0.977 
0.977 
0.979 
0.980 

# of emb. 
params. 
(M) 

Yahoo! 
143.26 
13.857 
6.690 
0.308 
AG N. 
20.797 
2.019 
0.975 
0.308 
Yelp P. 
74.022 
7.164 
3.459 
0.308 
Yelp F. 
80.524 
7.793 
3.762 
0.308 
DBpedia 183.76 
17.772 
8.580 
0.308 

# of bits 
(G) 

Yahoo! 
4.584 
0.443 
0.214 
0.086 
AG N. 
0.665 
0.065 
0.031 
0.021 
Yelp P. 
2.369 
0.229 
0.111 
0.049 
Yelp F. 
2.577 
0.249 
0.120 
0.053 
DBpedia 
5.880 
0.569 
0.275 
0.108 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Graph Convolutional Networks. Lr denotes low-rank.</figDesc><table>Dataset 
Full 
Lr(2X) Lr(4X) Ours 

Accuracy 

Cora 
0.814 
0.789 
0.767 
0.823 
Citese. 0.721 
0.710 
0.685 
0.723 
Pubm. 
0.795 
0.773 
0.780 
0.797 
# of emb. 
params. 
(K) 

Cora 
22.93 
10.14 
5.8 
8.22 
Citese. 59.25 
26.03 
14.88 
8.22 
Pubm. 
8.00 
3.61 
2.06 
2.69 

# of bits 
(M) 

Cora 
0.73 
0.32 
0.19 
0.33 
Citese. 
1.90 
0.83 
0.48 
0.44 
Pubm. 
0.26 
0.12 
0.07 
0.10 

Table 4. Comparisons with more baselines in Language Modeling 
(Medium sized model). 
Methods 
PPL 
Bits saved 
Char-as-codes 
108.14 
96% 
Scalar quantization (8 bits) 
84.06 
75% 
Scalar quantization (6 bits) 
87.73 
81% 
Scalar quantization (4 bits) 
92.86 
88% 
Product quantization(64x325) 
84.03 
88% 
Product quantization(128x325) 
83.71 
85% 
Product quantization(256x325) 
83.66 
81% 
Ours 
83.11 
92% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 5 .</head><label>5</label><figDesc>Comparisons of different code learning methods.</figDesc><table>Small 
Medium Large 
Full embedding 
114.53 
83.38 
78.71 
Random code 
115.79 
104.12 
98.38 
Pre-trained code 
107.95 
84.92 
80.69 
Ours (no guidance) 108.50 
89.03 
86.41 
Ours (ODG) 
108.19 
85.50 
83.00 
Ours (PDG) 
107.77 
83.11 
77.72 

4 
8 
16 
32 
64 

D 

4 

8 

16 

32 

64 

K 

124.2 99.0 87.9 81.9 78.6 

103.8 90.0 82.8 80.0 78.0 

94.7 85.7 80.6 78.4 77.5 

89.0 82.3 79.5 77.8 77.5 

85.0 81.0 78.2 77.7 78.4 

75 

90 

105 

120 

135 

(a) Linear instantiation. 

4 
8 
16 
32 
64 

D 

4 

8 

16 

32 

64 

K 

109.7 88.3 81.3 79.2 78.7 

91.8 83.1 80.0 78.8 78.9 

84.9 81.5 79.2 78.3 78.3 

83.4 79.9 78.7 78.6 78.5 

82.7 79.7 78.7 78.7 79.3 

75 

90 

105 

120 

135 

(b) RNN instantiation. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 6 .</head><label>6</label><figDesc>Learned codes for 10K Glove embeddings (K=6, D=4). Code Words 3-1-0-3 up when over into time back off set left open half</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Assuming we have vocabulary size N = 10, 000 and the dimensionality D = 100, it is 2 9900 times more efficient.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For example, we can set K = 100, D = 10 for a billion symbols, in a random code assignment, the probability of the NO</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Here we use stop_gradient(ui) to prevent embedding vectors u being dragged to f (ci) as it has too much freedom.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Here we overload the function f (ci) by considering that code ci can be turned into "one-hot" oi, and oi ≈ Sof tmax(πi/τ ). learning. Secondly, the PDG training procedure is much easier, especially for the tuning of discrete code learning, while pre-training of codes requires three stages and is unfriendly for parameter tuning.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">YahooAnswers has 477K unique words and 131M tokens, and Yelp has 268K unique words and 94M tokens. More details available in (Zhang et al., 2015b).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank anonymous reviewers for their constructive comments. We would also like to thank Chong Wang, Denny Zhou, Lihong Li for some helpful discussions. This work is partially supported by NSF III-1705169, NSF CAREER Award 1741634, and Snapchat gift funds.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Additive quantization for extreme vector compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="931" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yakhnenko</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Task-guided and path-augmented heterogeneous network embedding for author identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="295" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Entity embedding-based anomaly detection for heterogeneous categorical events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentyFifth International Joint Conference on Artificial Intelligence</title>
		<meeting>the TwentyFifth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1396" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning k-way ddimensional discrete code for compact embedding representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1711.03067</idno>
		<ptr target="http://arxiv.org/abs/1711.03067" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compressing neural networks with the hashing trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Error detecting and error correcting codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Hamming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Labs Technical Journal</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="160" />
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<title level="m">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A method for the construction of minimum-redundancy codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Huffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IRE</title>
		<meeting>the IRE</meeting>
		<imprint>
			<date type="published" when="1952" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1098" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05877</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fasttext</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03651</idno>
		<title level="m">zip: Compressing text classification models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Characteraware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lightrnn: Memory and computation-efficient recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4385" to="4393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00712</idno>
		<title level="m">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Černockỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
		<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Low-rank matrix factorization for deep neural network training with high-dimensional output targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arisoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6655" to="6659" />
		</imprint>
	</monogr>
	<note>2013 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Compressing word embeddings via deep compositional code learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nakayama</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1711.01068" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hash embeddings for efficient word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Svenstrup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4935" to="4943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Composite quantization for approximate nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sparse composite quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4548" to="4556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
