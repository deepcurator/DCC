<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Light field intrinsics with a deep encoder-decoder network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Alperovich</surname></persName>
							<email>anna.alperovich@uni-konstanz.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Konstanz</orgName>
								<address>
									<settlement>Konstanz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Johannsen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Konstanz</orgName>
								<address>
									<settlement>Konstanz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strecke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Konstanz</orgName>
								<address>
									<settlement>Konstanz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Goldluecke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Konstanz</orgName>
								<address>
									<settlement>Konstanz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Light field intrinsics with a deep encoder-decoder network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Light fields have a complex, heavily redundant structure. In their two-plane parametrization <ref type="bibr" target="#b23">[24]</ref>, they are given as a dense, regularly sampled 2D grid of so-called subaperture views of a scene. When fixing a single vertical or horizontal line in the image plane and moving through the space of view points in the same direction, one obtains 2D slices in this four-dimensional space, which are called epipolar plane images (EPIs), see <ref type="figure">Figure 5</ref>. For scenes with purely diffuse reflection, these exhibit patterns of oriented lines of constant color. Each of these lines corresponds to the projection of a single 3D point in space, and its slope, called the disparity, is inversely proportional to the point's distance to the observer. Discontinuities in the pattern are caused by occlusions, as they cause transitions between multiple orientations at the occlusion edge <ref type="bibr" target="#b39">[40]</ref>, see <ref type="figure">Figure 2</ref>.</p><p>The situation also becomes less straightforward when reflection or glossy, non-Lambertian surfaces come into play, center view disparity diffuse specular <ref type="figure">Figure 1</ref>. Our network jointly separates an input light field into diffuse and specular components, and computes a disparity map for the center view. This figure shows output on a previously unseen light field rendered with Blender.</p><p>as the EPIs then show superimposed patterns <ref type="bibr" target="#b18">[19]</ref>. The orientation of the patterns corresponding to specular reflection does not correspond to disparity, but the specular flow direction, which depends on the intrinsic surface geometry.</p><p>To distinguish between those two cases, one must know if a point exhibits diffuse or specular reflection. On the other hand, with known geometry, the specular flow can be directly estimated and reflection components can be separated <ref type="bibr" target="#b33">[34]</ref>. In case that both shape and reflectance are unknown, it is hardly possible to tell which phenomena gave rise to a particular EPI.</p><p>Nevertheless, EPIs from natural light fields exhibit an overall regular structure, and it seems likely that they form <ref type="bibr">Figure 2</ref>. The four images to the left show, from left to right, the center view of the input light field, the diffuse component, the specular component (scaled for better visibility) and the disparity. The EPIs to the right are all taken from the same scan line in the light field, marked white. From top to bottom, they again show the input, the diffuse component, the specular component and the disparity. Since the diffuse component and the disparity correspond to the same projections of the same 3D points, they share the same pattern. However, the specular component behaves differently, as it follows the specular flow <ref type="bibr" target="#b33">[34]</ref>, which depends on the local surface geometry and view point change in a complex way. In particular, the orientation of the specular lobe in the EPI is different from that of the diffuse texture.</p><p>a comparatively low-dimensional manifold within all of epipolar plane image space. Furthermore, encoding an EPI well with only a few parameters is related to the difficult interrelated tasks, such as disparity estimation or separation of diffuse and specular components. Intuition suggests that if you learn how to do compression well, you will be able to better succeed at the other tasks. The idea of this paper is therefore to learn a low-dimensional representation of EPIs from arbitrary example light fields, but in a way that the latent variables can be used jointly to accurately solve various supervised tasks in light field analysis. For this, we propose an encoder-decoder neural network based on the concept of deep auto-encoders <ref type="bibr" target="#b13">[14]</ref>, which recently have been highly successful in finding meaningful manifold representations <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Contributions. We introduce the first network architecture to jointly solve disparity regression and reflectance separation in light fields. Our fully-convolutional encoderdecoder network can be trained both unsupervised to just learn representations, as well as supervised to solve the above tasks based on the latent space. We employ 3D convolutions to compute features integrated over the whole range of both vertical and horizontal stacks to deal with complex occlusions and reflections. The network is trained on datasets rendered with Blender taken from the benchmark <ref type="bibr" target="#b15">[16]</ref>, as well as a custom random light field generator which in theory can synthesize an arbitrary amount of training data for reflection separation as well as disparity estimation. Currently we use dataset of 175 light fields, and will share rendering scripts and network code. We demonstrate in extensive comparisons that our method quantitatively and qualitatively outperforms existing light-field methods for diffuse and specular separation, and can robustly compute depth for highly specular scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Encoding light fields. From the first introduction of light fields for image-based rendering <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25]</ref>, light field compression has been an important topic due to the huge amount of data which needs to be stored. Early on, it has been noted that estimating disparity is necessary to exploit the redundancies in the different viewpoints <ref type="bibr" target="#b25">[26]</ref>. This can be turned around, and sparse coding actually been used as a tool for disparity estimation -similar in spirit to what we are proposing here. In <ref type="bibr" target="#b10">[11]</ref> they use the idea of redundancy of sub-aperture views and used sparsity of the RPCA as a new matching term. Likewise, <ref type="bibr" target="#b28">[29]</ref> employ sparsity ideas to model light field patches as Gaussian random variables conditioned on its disparity value. They construct a patch prior and can estimate disparity by finding the nearest PCA subspace. In <ref type="bibr" target="#b18">[19]</ref>, EPI patches are encoded with a dictionary of patches with known slope, such that the coding coefficients give a disparity estimate. Notably, this method can recover disparity for multiple layers of a scene. Sparse coding is also used for compressive light field photography <ref type="bibr" target="#b26">[27]</ref>, which reduces the amount of data to be captured. Both sparse coding and low-rank constraints are also key to modern light field compression schemes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>. However, the idea of an auto-encoder we employ in this work is in some sense the exact opposite to sparse coding: instead of finding an overcomplete basis and represent patches with a sparse vector in a high-dimensional space, we want to find the best low-dimensional coding directly.</p><p>Reflection separation. The dichromatic reflection model proposed by Shafer <ref type="bibr" target="#b29">[30]</ref> decomposes an input scene into diffuse and specular components. Based on this, <ref type="bibr" target="#b45">[46]</ref> considers specularity removal as an image denoising problem and solves it with bilateral filtering. In <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36]</ref>, Tan and Ikeuchi devise a method based on pure chromaticity analysis without any geometrical information. Kim et al. <ref type="bibr" target="#b20">[21]</ref> used the fact that the dark channel can provide an approximately specular free image. In <ref type="bibr" target="#b0">[1]</ref>, Akashi and Okatani use sparse non-negative matrix factorization to jointly estimate body color and separate reflection components.</p><p>What makes reflection separation from a single image particularly difficult is that specularity is a view dependent phenomenon, and can hardly be recognized from a single view point. With multiple views available, changes in ob-  <ref type="figure">Figure 3</ref>. A single residual block of the network. After batch normalization, a first path leads through a (possibly strided) convolution layer and a leaky ReLU. A second path either keeps the input, or passes it through a strided convolution in case it needs to be resampled. Both paths are added together to produce the final output. The idea is that it is much easier for such blocks to learn the identiy transformation, or perform only small modifications to the input <ref type="bibr" target="#b9">[10]</ref>, which helps the encoder-decoder paths to gradually add details.</p><p>ject appearance can be tracked with respect to the viewing angle, which significantly simplifies the task of reflection separation. The behavior of specularity in static scenes with a moving camera is described by Swaminathan et al. <ref type="bibr" target="#b34">[35]</ref>. They show how motion of specularity depends on object geometry and light source position, and propose a technique for specularity extraction from an image sequence.</p><p>Recent works by Gryaditskaya et al. <ref type="bibr" target="#b7">[8]</ref> and Sulc et al. <ref type="bibr" target="#b33">[34]</ref> explore the light field structure to edit appearance of specularity and estimate diffuse and specular components. Tao et al. <ref type="bibr" target="#b37">[38]</ref> adapt the dichromatic reflection model to light fields and propose a depth estimation and specularity removal algorithm. Criminisi <ref type="bibr" target="#b3">[4]</ref> studies the behavior of diffuse and specular components in EPIs and proposes several reflection separation techniques.</p><p>Neural networks for light field analysis. Deep neural networks are employed for all of the above tasks including light field analysis. Wang et al. <ref type="bibr" target="#b40">[41]</ref> aim at material classification. They explore different light field representations that can be used to train a convolutional neural network. Heber and Pock <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> apply an encoder-decoder architecture on 2D EPIs and later 3D EPI stacks to estimate depth. Kalantari et al. <ref type="bibr" target="#b19">[20]</ref> and Srinivasan et al. <ref type="bibr" target="#b32">[33]</ref> introduce view synthesis algorithms, which recover light fields from a sparse set of images or a single view. In a similar vein, <ref type="bibr" target="#b8">[9]</ref> obtain compressive light field reconstructions from single coded 2D images using a joint autoencoder and 4D-CNN architecture. Recently, deep networks were also successfully applied for inverse rendering and intrinsic image problems <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref>. In contrast to the above approaches, our architecture is not limited to a single task, but can be trained to perform several of these jointly by implementing different decoder chains. The pathways of our deep encoder-decoder network are organized in six groups of three residual blocks each. The first two blocks in each encoder group keep depth and resolution the same, the last block reduces resolution (shown on bottom, viewpoint × spatial coordinates), while increasing feature depth (shown on top) by 32. The decoder paths are exact mirrors of this chain. Disparity is only a 2D decoder, where the view point dimension of the shape is removed. To not overly clutter the figure, the visualization does not show that the encoder and 3D decoders actually operate on two EPI stacks in parallel, the horizontal and vertical one. The feature output of these is briefly joined on the bottom layer, and then decoded again into two separate chains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed network architecture</head><p>The key idea is to build the network around an autoencoder, so it can be trained unsupervised using just raw light fields. However, we add multiple pathways to decode the latent representation, which can be trained jointly with the autoencoder in a supervised manner, depending on which data is available in the current training example. Due to the combination of supervised and unsupervised training, we can make sure that the latent representation is both tailored to the desired tasks, such as depth reconstruction or intrinsic component representation, but can also generalize well to datasets for which no training information is available for these tasks. When the network is deployed, all decoder chains can be evaluated using just the light field data.</p><p>Encoder pathway. The input to the network is a pair of epipolar volumes, one sliced horizontally, the other one vertically, see <ref type="figure">Figure 5</ref>. Input patches are 48×48 RGB with a depth of nine views, larger light fields are segmented into these patches, so that our network can deal with lightfields of any shape.</p><p>The basic ingredient for the encoders and decoders are residual blocks. To decrease resolution, we employ strided convolutions instead of max-pooling, so the network is fully convolutional. See <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32]</ref> for justifications of this architecture. The residual blocks have a very simple structure and allow direct pass-through of the (batch normalized) input, see <ref type="figure">Figure 3</ref>.</p><p>In the encoder pathway, 18 residual blocks are chained <ref type="figure">Figure 5</ref>. Visualization of horizontal (left) and vertical (right) EPI stacks used as input to our network. To achieve the actual spatial input resolution of 48 × 48, they need to be cut out from the above epipolar volumes. Note that although both stacks are three dimensional, they use images along different directions of view points. In effect, those two stacks assemble a crosshair of views around the center view, which is thus the only view present in both stacks.</p><p>together. Every third one reduces the patch resolution via strided convolution while increasing feature depth, with the overall goal of gradually reducing dimensionality. The final output has shape 3 × 3 × 3 × 192, for an overall reduction of the input to around 8.3% of its original size, see <ref type="figure" target="#fig_1">Figure 4</ref>. Horizontal and vertical epipolar volumes are encoded separately. As they have the exact same structure, we have them share the same filter kernels to reduce the number of network parameters. Since pathways like depth reconstruction require information from both horizontal as well as vertical epipolar volumes, their feature output at the representation level is concatenated. This is the final output of the encoder, and the bottleneck of the network.</p><p>Decoder pathways and output. After passing the bottleneck, the low-dimensional representation is decoded again by a chain of residual layers. The latent variables enter different decoder pathways. In this paper, we implement the auto-encoder path to reconstruct the input, two decoders for the diffuse and specular components, and a separate decoder for the disparity map. All decoder pathways use transpose convolutions to exactly revert the encoder on the corresponding level. However, the only link between them is through the latent representation, see <ref type="figure" target="#fig_1">Figure 4</ref>.</p><p>Lightfield, diffuse and specular components are reconstructed for the 17 = 9 + 9 − 1 views in a crosshair around the center view, see <ref type="figure">Figure 5</ref>. The disparity map is computed for the center view only. We employ the dichromatic reflection model <ref type="bibr" target="#b29">[30]</ref>, whose adaptation to lightfields was discussed in detail in <ref type="bibr" target="#b37">[38]</ref>. According to this model, the specular component is assumed to be independent from the diffuse one, which justifies the use of two separate decoder chains. However, they should also sum up to the input light field. To let the network better cope with this constraint, we append specular features to the diffuse ones and vice versa, but only for the input to the final layer. As disparity output is only 2D, we reduce the filter shape by the respective dimension. When tiling the output back together, we use overlapping patches and extract only the central 16 × 16 pixels, as data closer to the center is more accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Network training 4.1. Training data</head><p>As input data for our algorithm we use a variety of publicly available datasets <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b42">43]</ref> as well as scenes specifically created for the purpose of reflection separation.</p><p>4D light field benchmark <ref type="bibr" target="#b15">[16]</ref>. The light field benchmark <ref type="bibr" target="#b15">[16]</ref> offers 28 light fields rendered with Blender with ground truth disparity available. Their composition varies substantially, with many different materials, lighting conditions, and fine structures with complex occlusions. Their center view resolution is 512×512, but here and for all other datasets, we use only completely valid patches for training, in the sense that pixels shifted by their disparity always lie within all of the views. We use 48 × 48 pixel patches for training with 16 pixels of overlap, skipping a 16 pixel border region. In effect, this gives 900 training patches per light field for a total of around 25,200 from the benchmark.</p><p>New light fields rendered with Blender. We generate data for specular and diffuse separation using the Blender addon provided with <ref type="bibr" target="#b15">[16]</ref>. By randomizing scenes, we can generate a (theoretically) infinite amount of different light fields to ensure a large variety of data. We designed multiple scenes containing up to five objects of different scales and geometric complexity. Texture, the reflective properties and the environment map for lighting are chosen at random. Additionally, we randomly change the position and rotation of all objects and rotate the environment map, to prevent overfiting to certain geometries and lighting conditions. To ensure that the network can also deal with purely Lambertian materials, a certain percentage of objects have purely diffuse material. In total we used 36 pre-built scenes, 321 textures and 109 environment maps collected from different public sources. The 3D models we use are selected from Chocofur 1 and The British Museum 2 . We adapted the material properties to fit our needs and only used the mesh data.</p><p>Lightfields are rendered with the Cycles engine, and we  <ref type="figure">Figure 6</ref>. Network losses for different groups of datasets at convergence. The datasets most difficult to fit for the autoencoder are the ones from gantries, perhaps due to minimally uneven sampling of viewpoints which has not been properly corrected. Depth reconstruction on our own synthetic dataset is surprisingly easier than for the benchmark datasets, although it has much stronger specularity. However, the geometry of our objects is also substantially simpler, and the datasets have large regions of easy to fit planes. Overall, disparity MSE on the benchmark validation is around the current benchmark average, which is 6.29. However, our model is not specifically optimized for depth reconstruction, and in particular trained for non-Lambertian scenes, on which it can perform much more robustly than competing methods, see <ref type="figure">Figure 7</ref>.</p><p>adapted the addon <ref type="bibr" target="#b15">[16]</ref> such that it can output the intrinsic components. For both diffuse and specular passes, Cycles outputs the three different components color, direct lighting, and indirect lighting. Adding the direct and indirect light and multiplying it by the color yields the desired ground truth separation. Data is stored in high dynamic range to circumvent problems with saturated specularities. The size of these light fields is also 9 × 9 × 512 × 512. The 175 light fields we use for training contain around 160,000 patches. Real-world light fields. We have four sources for real world light fields for which no ground truth data is available. First, we use light fields captured with the Lytro Illum light field camera, calibrated and rectified using the light field toolbox from <ref type="bibr" target="#b4">[5]</ref>. The size of the light fields is 9×9×434× 625. We used 11 light fields for training and two for testing, which results in 10,175 training examples. Second, we have a dataset built from the Stanford Light Field Archive <ref type="bibr" target="#b38">[39]</ref> with six training data sets which is 6,816 patches, and with two light fields held back for testing. Third, we captured a light field using an industrial camera mounted on a gantry we assembled ourselves. The size of the light field is 9 × 9 × 497 × 710 with a disparity range of [−1.5, 1]. The light field illustrates a non-Lambertian object, illuminated with approximately white light. Fourth, we use five real world light fields from the HCI benchmark <ref type="bibr" target="#b42">[43]</ref> and we keep one for testing, which results in 16,016 more training patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Network implementation and training strategy</head><p>From the training data, we set aside 5% for a validation set. Several light fields are also completely held back, and used only for testing, see above for details. We implement the network using Tensorflow in Python3, and train on an Intel Core i9 system with four nVidia Titan Xp, with the encoder/decoder chains distributed to different GPUs to satisfy memory requirements for training. All decoders are trained with an L 2 -loss. In case a dataset does not provide ground truth for a certain pathway, that path is disabled during training. The autoencoder path can always be trained. Weights are initialized using the same strategy as for residual networks <ref type="bibr" target="#b9">[10]</ref>. Stochastic optimization using the Adam optimizer <ref type="bibr" target="#b21">[22]</ref> for twenty epochs of training data took roughly five days, after which loss for all pathways remained stable. The final losses over training and validation set are shown in <ref type="figure">Figure 6</ref>. While there is of course a slight gap between training and validation, performance on unseen data is not significantly worse, so overfitting does not seem to be an issue here.</p><p>Reconstruction of a single pathway during evaluation requires roughly 7 seconds on the above system for a light field with a center view resolution of 512 × 512, including tiling of the input light field, all transfers from CPU to GPU and back, and reassembling the output from the patches. The complete specular/diffuse decomposition with disparity estimation takes 19 seconds. We verify the quality of reflection separation and disparity estimation in detail in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We compare our reflection separation with two algorithms designed for light fields. The first one by Sulc et al. <ref type="bibr" target="#b33">[34]</ref> performs reflection separation based on specular flow. The second one is by Alperovich et al. <ref type="bibr" target="#b1">[2]</ref> and performs intrinsic light field decomposition. In addition, we compare to the network proposed by Shi et al. <ref type="bibr" target="#b30">[31]</ref>, which uses a deep autoencoder for intrinsic images. However, it only works for standard 2D images. To compare to the full decomposition <ref type="bibr" target="#b1">[2]</ref>, where the authors decompose the input light field into albedo, shading and specularity, we compute the diffuse component by multiplying albedo and  <ref type="figure">Figure 9</ref>. Ablation study: Quantitative comparison of separation over nine previously unseen test datasets, and depth estimation for the two scenes from <ref type="figure">Figure 7</ref>. Note that we compute error for the whole center view, without object mask.</p><p>shading <ref type="bibr" target="#b6">[7]</ref>.</p><p>For quantitative results, we evaluate reflection separation on synthetic scenes and report the local mean-squared error (LMSE) <ref type="bibr" target="#b6">[7]</ref> which we compute patch-wise. This error is scale invariant, since the brightness of the patches is adjusted to the ground truth. In our experiments, we use rectangular overlapping patches with a size of 20% of the total image size. To evaluate the errors that might me canceled by LMSE, we also compute global mean squared error (GMSE) that adjusts the brightness value for the whole image. We also measure the structural similarity index (SSIM). See <ref type="figure">Figure 8</ref> for an overview of all numerical results, and <ref type="figure" target="#fig_2">Figures 10 and 11</ref> for a visual comparison. We also compare performance of disparity map estimation for specular scenes to different other algorithms in <ref type="figure">Figure 7</ref>.</p><p>As an ablation study, we performed two experiments. In the first case we trained network only for center view without any disparity information from sub-aperture views, in the second case we have reduced spatial patch size to 24 × 24. Both experiments lead to decrease in performance compared to the original network, see <ref type="figure">Figure 9</ref> for the comparisons on the same data sets that are used in <ref type="figure">Figures 10, 8, 7</ref> . Finally, results of our method on different datasets that are commonly used in the light field community <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b15">16]</ref> can be found in <ref type="figure">Figure 12</ref>. We refer to the supplementary material for more results for the real and synthetic scenes, and videos for diffuse and specular components that show angular consistency of the decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we propose a generative encoder-decoder architecture for patches taken from light field epipolar volumes. Using different decoder paths, we can achieve both intrinsic decomposition as well as disparity estimation with a unified network. Thanks to joint training of autoencoder and the supervised pathways, we can transform the input light field into a latent representation which is both much smaller and well adapted to the desired tasks.</p><p>Our method outperforms recent light field based methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b1">2]</ref>, and a single image deep network approach for intrinsic image decomposition <ref type="bibr" target="#b30">[31]</ref>. Although we have only</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Alperovich et al. <ref type="bibr" target="#b1">[2]</ref> Sulc et al. <ref type="bibr" target="#b33">[34]</ref> Shi et al. <ref type="bibr" target="#b30">[31]</ref> diffuse specular <ref type="figure">Figure 10</ref>. Comparison for a synthetic data set with two non-Lambertian objects with almost no texture, which is typically challenging for reflection separation. Both modeling approaches <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b33">[34]</ref> fail to separate the specular component from the diffuse one. The CNNbased approach <ref type="bibr" target="#b30">[31]</ref> successfully separates reflection components, but the diffuse one has some artifacts. In addition, the method requires an object mask, thus its application is limited to objects well separated from the background, which are rarely found in real world scenes. HCI Cube <ref type="bibr" target="#b43">[44]</ref> center view disparity diffuse specular <ref type="figure">Figure 12</ref>. Results on unseen light fields from various sources. We show center views of the light fields with diffuse and specular components and estimated disparities. Top: lightfield from the Stanford data set <ref type="bibr" target="#b38">[39]</ref>, where we have chosen the most challenging case with respect to reflection separation and disparity estimation. Our network, while being trained on synthetic scenes, is able to generalize to real world examples with complicated geometry and reflection. Middle: synthetic scene from light field benchmark <ref type="bibr" target="#b15">[16]</ref>, where we have selected an object with small specular regions, to evaluate how the network will cope with it. Specularity is successfully from the diffuse part, while preserving texture. Bottom: an example data set from HCI benchmark <ref type="bibr" target="#b42">[43]</ref>.</p><p>average performance in depth reconstruction on datasets from the benchmark <ref type="bibr" target="#b15">[16]</ref>, in contrast to other methods, we still recover reliable depth in the presence of strong specularity. We also generalize well to real-world light fields captured with the Lytro Illum plenoptic camera or a gantry, although we do not have ground truth training data available for these. Despite being trained only on soft reflections, experiments with highly specular light fields show that we are robust against strong non-Lambertian effects. As the structures in epipolar volumes are both relatively characteristic and contain more information, we require only relatively few training examples (around 200 light fields), compared to single image approaches which use several millions of images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The pathways of our deep encoder-decoder network are organized in six groups of three residual blocks each. The first two blocks in each encoder group keep depth and resolution the same, the last block reduces resolution (shown on bottom, viewpoint × spatial coordinates), while increasing feature depth (shown on top) by 32. The decoder paths are exact mirrors of this chain. Disparity is only a 2D decoder, where the view point dimension of the shape is removed. To not overly clutter the figure, the visualization does not show that the encoder and 3D decoders actually operate on two EPI stacks in parallel, the horizontal and vertical one. The feature output of these is briefly joined on the bottom layer, and then decoded again into two separate chains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Two light fields captured with the Lytro Illum plenoptic camera. The first scene consist of a highly specular saxophone and an almost Lambertian koala. Our network successfully detects more specular parts of the saxophone compared to the other methods. While we mis-detect the koala as a specular object similar to [31], our method is the only one where the diffuse part behind the large specular spot on saxophone is not blurred. The second scene has two objects with very small saturated specularity, and only our method is the only one able to separate it. For all other methods, the specularity is still present in the diffuse component. Note that the single image CNN [31] does not perform decomposition for the background, thus it appears black in the visualization.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.chocofur.com 2 https://sketchfab.com/britishmuseum</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">almost Lambertian koala. Our network successfully detects more specular parts of the saxophone compared to the other methods. While we mis-detect the koala as a specular object similar to [31], our method is the only one where the diffuse part behind the large specular spot on saxophone is not blurred. The second scene has two objects with very small saturated specularity, and only our method is the only one able to separate it. For all other methods, the specularity is still present in the diffuse component. Note that the single image CNN [31] does not perform decomposition for the background, thus it appears black in the visualization.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the ERC Starting Grant "Light Field Imaging and Analysis" (LIA 336978, FP7-2014).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Separation of reflection components by sparse non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Akashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="77" to="85" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Goldluecke. Shadow and specularity priors for intrinsic light field decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alperovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Johannsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Strecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Energy Minimization Methods for Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Light field compression with disparity guided sparse coding based on structural key views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="page">2750413</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Extracting layers and analyzing their specular properties using epipolar-plane-image analysis. Computer vision and image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="51" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Decoding, calibration and rectification for lenselet-based plenoptic cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dansereau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pizarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1027" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Lumigraph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH</title>
		<meeting>SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Freeman. Ground truth dataset and baseline evaluations for intrinsic image algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gloss editing in light fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gryaditskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Masia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Didyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Myszkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision, Modelling and Visualization (VMV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Compressive light field reconstructions using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jauhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasuriya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Molnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shape from light field meets robust PCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Heber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional networks for shape from light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Heber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural epi-volume networks for shape from light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Heber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A deep learning framework for character motion synthesis and editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A dataset and evaluation methodology for depth estimation on 4D light fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Honauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Johannsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kondermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goldluecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Accurate depth map estimation from a lenslet light field camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Light field compression with homography-based low-rank approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pendu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrugia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1132" to="1145" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What sparse light field coding reveals about scene structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Johannsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sulc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goldluecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning-based view synthesis for light field cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Kalantari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH Asia</title>
		<meeting>SIGGRAPH Asia</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Specular reflection separation using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep convolutional inverse graphics network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2539" to="2547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Light fields and computational imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="46" to="55" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Light field rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH</title>
		<meeting>SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Data compression for light field rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="338" to="343" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Compressive light field photography using overcomplete dictionaries and optimized projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marwah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH)</title>
		<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Light field denoising, light field superresolution and stereo camera based refocussing using a GMM light field patch prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="22" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Using color to separate reflection components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shafer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Color Research &amp; Application</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="210" to="218" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning non-lambertian object intrinsics across shapenet categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (workshop track)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to synthesize a 4D RGBD light field from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sreelal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reflection separation in light fields based on sparse coding and specular flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sulc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alperovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Marniok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goldluecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision, Modelling and Visualization (VMV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the motion and appearance of specularities in image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002-05" />
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="508" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Separating reflection components of textured surfaces using a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="178" to="193" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Separating reflection components based on chromaticity and noise analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1373" to="1379" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Depth estimation and specular removal for glossy surfaces using point and line consistency with light-field cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1155" to="1169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<ptr target="http://lightfield.stanford.edu" />
		<title level="m">The (New) Stanford Light Field Archive</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Occlusion-aware depth estimation using light-field cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3487" to="3495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A 4D light-field dataset and CNN architectures for material recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hiroaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Globally consistent depth labeling of 4D light fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goldluecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Reconstructing reflective and transparent surfaces from epipolar plane images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goldluecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition (Proc. GCPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Datasets and benchmarks for densely sampled 4D light fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goldluecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision, Modelling and Visualization (VMV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">High performance imaging using large camera arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wilburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-V</forename><surname>Talvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antunez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="765" to="776" />
			<date type="published" when="2005-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">efficient and robust specular highlight removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1304" to="1311" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
