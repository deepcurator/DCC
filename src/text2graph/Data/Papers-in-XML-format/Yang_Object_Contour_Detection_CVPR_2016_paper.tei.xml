<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Object Contour Detection with a Fully Convolutional Encoder-Decoder Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
							<email>jimyang@adobe.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor, Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adobe</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor, Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
							<email>bprice@adobe.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor, Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adobe</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor, Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
							<email>scohen@adobe.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor, Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adobe</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor, Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
							<email>honglak@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor, Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<email>mhyang@ucmerced.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor, Merced</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Object Contour Detection with a Fully Convolutional Encoder-Decoder Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We develop a deep learning algorithm for contour detection   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object contour detection is fundamental for numerous vision tasks. For example, it can be used for image segmentation <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b2">3]</ref>, for object detection <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19]</ref>, and for occlusion and depth reasoning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b1">2]</ref>. Given its axiomatic importance, however, we find that object contour detection is relatively under-explored in the literature <ref type="bibr" target="#b48">[49]</ref>. At the same time, many works have been devoted to edge detection that responds to both foreground objects and background boundaries ( <ref type="figure" target="#fig_0">Figure 1 (b)</ref>). In this paper, we address "object-only" contour detection that is expected to suppress background boundaries <ref type="figure" target="#fig_0">(Figure 1(c)</ref>).</p><p>Edge detection has a long history. Early research focused on designing simple filters to detect pixels with highest gradients in their local neighborhood, e.g. Sobel <ref type="bibr" target="#b16">[17]</ref> and Canny <ref type="bibr" target="#b8">[9]</ref>. The main problem with filter based methods is that they only look at the color or brightness differences between adjacent pixels but cannot tell the texture differences in a larger receptive field. With the advance of texture descriptors <ref type="bibr" target="#b36">[37]</ref>, Martin et al. <ref type="bibr" target="#b38">[39]</ref> combined color, brightness and texture gradients in their probabilistic boundary detector. Arbelaez et al. <ref type="bibr" target="#b2">[3]</ref> further improved upon this by computing local cues from multiscale and spectral clustering, known as gPb, which yields state-of-the-art accuracy. However, the globalization step of gPb significantly increases the computational load. Lim and Dollar <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b12">13]</ref> analyzed the clustering structure of local contour maps and developed efficient supervised learning algorithms for fast edge detection <ref type="bibr" target="#b12">[13]</ref>. These efforts lift edge detection to a higher abstract level, but still fall below human perception due to their lack of object-level knowledge.</p><p>Recently deep convolutional networks <ref type="bibr" target="#b30">[31]</ref> have demonstrated remarkable ability of learning high-level representations for object recognition <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b10">11]</ref>. These learned features have been adopted to detect natural image edges <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51]</ref> and yield a new state-of-the-art performance <ref type="bibr" target="#b50">[51]</ref>. All these methods require training on ground truth contour annotations. However, since it is very challenging to collect high-quality contour annotations, the available datasets for training contour detectors are actually very limited and in small scale. For example, the standard benchmarks, Berkeley segmentation (BSDS500) <ref type="bibr" target="#b37">[38]</ref> and NYU depth v2 (NYUDv2) <ref type="bibr" target="#b46">[47]</ref> datasets only include 200 and 381 training images, respectively. Therefore, the representation power of deep convolutional networks has not been entirely harnessed for contour detection. In this paper, we scale up the training set of deep learning based contour detection to more than 10k images on PASCAL VOC <ref type="bibr" target="#b14">[15]</ref>. To address the quality issue of ground truth contour annotations, we develop a method based on dense CRF <ref type="bibr" target="#b27">[28]</ref> to refine the object segmentation masks from polygons.</p><p>Given image-contour pairs, we formulate object contour detection as an image labeling problem. Inspired by the success of fully convolutional networks <ref type="bibr" target="#b35">[36]</ref> and deconvolutional networks <ref type="bibr" target="#b39">[40]</ref> on semantic segmentation, we develop a fully convolutional encoder-decoder network (CEDN). Being fully convolutional, our CEDN network can operate on arbitrary image size and the encoder-decoder network emphasizes its asymmetric structure that differs from deconvolutional network <ref type="bibr" target="#b39">[40]</ref>. We initialize our encoder with VGG-16 net <ref type="bibr" target="#b47">[48]</ref> (up to the "fc6" layer) and to achieve dense prediction of image size our decoder is constructed by alternating unpooling and convolution layers where unpooling layers re-use the switches from max-pooling layers of encoder to upscale the feature maps. During training, we fix the encoder parameters (VGG-16) and only optimize decoder parameters. This allows the encoder to maintain its generalization ability so that the learned decoder network can be easily combined with other tasks, such as bounding box regression or semantic segmentation.</p><p>We evaluate the trained network on unseen object categories from BSDS500 and MS COCO datasets <ref type="bibr" target="#b32">[33]</ref>, and find the network generalizes well to objects in similar "super-categories" to those in the training set, e.g. it generalizes to objects like "bear" in the "animal" super-category since "dog" and "cat" are in the training set. We also show the trained network can be easily adapted to detect natural image edges through a few iterations of fine-tuning and yields comparable results with the state-of-the-art <ref type="bibr" target="#b50">[51]</ref>.</p><p>An immediate application of contour detection is generating object proposals. Previous literature has investigated various methods of generating bounding box or segmented object proposals by scoring edge features <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b11">12]</ref> and combinatorial grouping <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b3">4]</ref> and etc. In this paper, we use a multiscale combinatorial grouping (MCG) algorithm <ref type="bibr" target="#b3">[4]</ref> to generate segmented object proposals from our detected contour maps. As a result, our method significantly improves the quality of segmented object proposals on the PASCAL VOC 2012 validation set, achieving 0.67 average recall from overlap 0.5 to 1.0 with only about 1660 candidates per image, compared to the 0.62 average recall by original MCG algorithm with near 5140 candidates per image. We also evaluate object proposals on the MS COCO dataset with 80 object classes and analyze the average recalls from different object classes and their supercategories. Our key contributions are summarized below:</p><p>• We develop a simple yet effective fully convolutional encoder-decoder network for object contour detection and the trained model generalizes well to unseen object classes from the same super-categories, yielding significantly higher precision than previous methods.</p><p>• We show we can fine tune our network for edge detection and match the state-of-the-art in terms of precision and recall.</p><p>• We develop a method to generate accurate object contours from imperfect polygon based segmentation annotations, which makes training easier.</p><p>• Our method significantly improves the state-of-the-art results on segmented object proposals by integrating with combinatorial grouping <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic contour detection. Hariharan et al. <ref type="bibr" target="#b19">[20]</ref> study the problem of detecting semantic boundaries between different object classes without considering the occlusion boundaries of two adjacent object instances from the same class, e.g. a mom hugging her daughter. Although their method can be extended to detect object instance contours, it might encounter challenges of generalizing to unseen object classes due to the use of object detector output. Bertasius et al. <ref type="bibr" target="#b6">[7]</ref> improve semantic contour detection with convolutional features and shows its application to semantic segmentation. Our object contour detector can be potentially used to improve a more challenging and practical problem of instance-level semantic segmentation <ref type="bibr" target="#b20">[21]</ref>.</p><p>Occlusion boundary detection. Hoiem et al. <ref type="bibr" target="#b21">[22]</ref> study the problem of recovering occlusion boundaries from a single image. It is a very challenging ill-posed problem due to the partial observability while projecting 3D scenes onto 2D image planes. They formulate a CRF model to integrate various cues: color, position, edges, surface orientation and depth estimates. We believe our instance-level object contours will provide another strong cue for addressing this problem that is worth investigating in the future.</p><p>Object proposal generation. There is a large body of works on generating bounding box or segmented object proposals. Hosang et al. <ref type="bibr" target="#b22">[23]</ref> and Pont-Tuset et al. <ref type="bibr" target="#b41">[42]</ref>   <ref type="figure">Figure 2</ref>. Architecture of the proposed fully convolutional encoder-decoder network.</p><p>motivated by efficient object detection. One of their drawbacks is that bounding boxes usually cannot provide accurate object localization. More related to our work is generating segmented object proposals <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43]</ref>. At the core of segmented object proposal algorithms is contour detection and superpixel segmentation. We experiment with a state-of-the-art method of multiscale combinatorial grouping <ref type="bibr" target="#b3">[4]</ref> to generate proposals and believe our object contour detector can be directly plugged into most of these algorithms. In addition, Pinheiro et al. <ref type="bibr" target="#b40">[41]</ref> propose a network that learns to generate proposals directly from the test image without the grouping stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Object Contour Detection</head><p>In this section, we introduce the proposed fully convolutional encoder-decoder network for object contour detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fully Convolutional Encoder-Decoder Network</head><p>We formulate contour detection as a binary image labeling problem where "1" and "0" indicates "contour" and "non-contour", respectively. Image labeling is a task that requires both high-level knowledge and low-level cues. Given the success of deep convolutional networks <ref type="bibr" target="#b30">[31]</ref> for learning rich feature hierarchies, image labeling has been greatly advanced, especially on the task of semantic segmentation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b34">35]</ref>. Among those end-to-end methods, fully convolutional networks <ref type="bibr" target="#b35">[36]</ref> scale well up to the image size but cannot produce very accurate labeling boundaries; unpooling layers help deconvolutional networks <ref type="bibr" target="#b39">[40]</ref> to generate better label localization but their symmetric structure introduces a heavy decoder network which is difficult to train with limited samples.</p><p>We borrow the ideas of full convolution and unpooling from above two works and develop a fully convolutional encoder-decoder network for object contour detection. The network architecture is demonstrated in <ref type="figure">Figure 2</ref>. We use the layers up to "fc6" from VGG-16 net <ref type="bibr" target="#b47">[48]</ref> as our encoder. Since we convert the "fc6" to be convolutional, so we name it "conv6" in our decoder. Due to the asymmetric nature of image labeling problems (image input and mask output), we break the symmetric structure of deconvolutional networks and introduce a light-weighted decoder. The first layer of decoder "deconv6" is designed for dimension reduction that projects 4096-d "conv6" to 512-d with 1×1 kernel so that we can re-use the pooling switches from "conv5" to upscale the feature maps by twice in the following "deconv5" layer. The number of channels of every decoder layer is properly designed to allow unpooling from its corresponding maxpooling layer. All the decoder convolution layers except "deconv6" use 5 × 5 kernels. All the decoder convolution layers except the one next to the output label are followed by relu activation function. We also add a dropout layer after each relu layer in the decoder. A complete decoder network setup is listed in <ref type="table" target="#tab_1">Table 1</ref> and the loss function is simply the pixel-wise logistic loss. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Contour Ground Truth Refinement</head><p>Drawing detailed and accurate contours of objects is a challenging task for human beings. This is why many large scale segmentation datasets <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33]</ref> provide contour annotations with polygons as they are less expensive to collect at scale. However, because of unpredictable behaviors of human annotators and limitations of polygon representation, the annotated contours usually do not align well with the true image boundaries and thus cannot be directly used as ground truth for training. Among all, the PASCAL VOC dataset is a widely-accepted benchmark with high-quality annotation for object segmentation. VOC 2012 release includes 11540 images from 20 classes covering a majority of common objects from categories such as "person", "vehicle", "animal" and "household", where 1464 and 1449 images are annotated with object instance contours for training and validation. Hariharan et al. <ref type="bibr" target="#b19">[20]</ref> further contribute cannot be directly used for training due to its inaccurate boundaries (thin white area reflects unlabeled pixels between objects). We align them to image boundaries by re-labeling the uncertain areas with dense CRF (d), compared to Graph Cut (c).</p><p>more than 10000 high-quality annotations to the remaining images. Together there are 10582 images for training and 1449 images for validation (the exact 2012 validation set). We choose this dataset for training our object contour detector with the proposed fully convolutional encoder-decoder network.</p><p>The original PASCAL VOC annotations leave a thin unlabeled (or uncertain) area between occluded objects <ref type="figure" target="#fig_1">(Figure 3(b)</ref>). To find the high-fidelity contour ground truth for training, we need to align the annotated contours with the true image boundaries. We consider contour alignment as a multi-class labeling problem and introduce a dense CRF model <ref type="bibr" target="#b27">[28]</ref> where every instance (or background) is assigned with one unique label. The dense CRF optimization then fills the uncertain area with neighboring instance labels so that we obtain refined contours at the labeling boundaries <ref type="figure" target="#fig_1">(Figure 3(d)</ref>). We also experimented with the Graph Cut method <ref type="bibr" target="#b7">[8]</ref> but find it usually produces jaggy contours due to its shortcutting bias <ref type="figure" target="#fig_1">(Figure 3(c)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training</head><p>We train the network using Caffe <ref type="bibr" target="#b24">[25]</ref>. For each training image, we randomly crop four 224×224×3 patches and together with their mirrored ones compose a 224×224×3×8 minibatch. The ground truth contour mask is processed in the same way. We initialize the encoder with pre-trained VGG-16 net and the decoder with random values. During training, we fix the encoder parameters and only optimize the decoder parameters. This allows our model to be easily integrated with bounding box regression <ref type="bibr" target="#b17">[18]</ref> and other decoders such as semantic segmentation <ref type="bibr" target="#b39">[40]</ref> for joint training. As the "contour" and "non-contour" pixels are extremely imbalanced in each minibatch, the penalty for being "contour" is set to be 10 times the penalty for being "noncontour". We use the Adam method <ref type="bibr" target="#b4">[5]</ref> to optimize the network parameters and find it is more efficient than standard stochastic gradient descent. We set the learning rate to 10</p><formula xml:id="formula_0">−4</formula><p>and train the network with 30 epochs with all the training images being processed each epoch. Note that we fix the training patch to 224 × 224 for memory efficiency and the learned parameters can be used on images of arbitrary size because of its fully convolutional nature. Our CEDN network can be trained easily and efficiently in a single stage without batch-normalization due to the much smaller "deconv6" layer, which only contains 4096×1×1×512 parameters in comparison with 4096 × 7 × 7 × 512 parameters of "deconv-fc6" layer of DeconvNet <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>In this section, we evaluate our method on contour detection and proposal generation using three datasets: PASCAL VOC 2012, BSDS500 and MS COCO. We will explain the details of generating object proposals using our method after the contour detection evaluation. More evaluation results are in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Contour Detection</head><p>Given trained models, all the test images are fed-forward through our CEDN network in their original sizes to produce contour detection maps. The detection accuracies are evaluated by precision-recall curves and F-measure (F). Note that a standard non-maximum suppression is used to clean up the predicted contour maps (thinning the contours) before evaluation. PASCAL val2012. We present quantitative results on the PASCAL VOC 2012 validation set, shortly "PAS-CAL val2012", with comparisons to three baselines, structured edge detection (SE) <ref type="bibr" target="#b12">[13]</ref>, singlescale combinatorial grouping (SCG) and multiscale combinatorial grouping (MCG) <ref type="bibr" target="#b3">[4]</ref>. We also compare with the latest holisticallynested edge detection (HED) algorithm <ref type="bibr" target="#b50">[51]</ref>. Note that the HED model was originally trained on the BSDS dataset <ref type="bibr" target="#b37">[38]</ref>. We refer the results of applying the pretrained HED model to PASCAL val2012 as "HED-pretrain". To have a fair comparison, we further re-trained the HED model on PASCAL VOC using the same training data as our model with 30000 iterations. The contour prediction precision-recall curves from our CEDN model, baselines, HED-pretrain and HED are illustrated in <ref type="figure" target="#fig_3">Figure 5</ref>. It can be seen that the F-score of HED is improved (from 0.42 to 0.44) by training on PASCAL VOC but still significantly lower than CEDN (0.57). Note that we use the originally annotated contours instead of our refined ones as ground truth for unbiased evaluation. Accordingly we consider the   refined contours as the upper bound since our network is learned from them. Its precision-recall value is referred as "GT-DenseCRF" with a green spot in <ref type="figure" target="#fig_3">Figure 5</ref>. Compared to the baselines, our method (CEDN) yields very high precisions, which means it generates visually cleaner contour maps with background clutters well suppressed (the third column in <ref type="figure" target="#fig_2">Figure 4</ref>). Note that the occlusion boundaries between two instances from the same class are also well recovered by our method (the second example in <ref type="figure" target="#fig_2">Figure 4</ref>). We also note that there is still a big performance gap between our current method (F=0.57) and the upper bound (F=0.74), which requires further research for improvement.</p><formula xml:id="formula_1">(a) (b) (c) (d) (e)</formula><p>BSDS500 with fine-tuning. BSDS500 <ref type="bibr" target="#b37">[38]</ref> is a standard benchmark for contour detection. Different from our objectcentric goal, this dataset is designed for evaluating natural edge detection that includes not only object contours but also object interior boundaries and background boundaries (examples in <ref type="figure" target="#fig_4">Figure 6(b)</ref>). It includes 500 natural images with carefully annotated boundaries collected from multiple users. The dataset is divided into three parts: 200 for training, 100 for validation and the rest 200 for test. We first examine how well our CEDN model trained on PAS-CAL VOC can generalize to unseen object categories in this dataset. Interestingly, as shown in the <ref type="figure" target="#fig_4">Figure 6</ref>(c), most of wild animal contours, e.g. elephants and fish are accurately detected and meanwhile the background boundaries, e.g. building and mountains are clearly suppressed. We further fine-tune our CEDN model on the 200 training images from BSDS500 with a small learning rate (10 −5 ) for 100 epochs. As a result, the boundaries suppressed by pretrained CEDN model ("CEDN-pretrain") re-surface from the scenes. Quantitatively, we evaluate both the pretrained and fine-tuned models on the test set in comparisons with previous methods. <ref type="figure" target="#fig_5">Figure 7</ref> shows that 1) the  pretrained CEDN model yields a high precision but a low recall due to its object-selective nature and 2) the fine-tuned CEDN model achieves comparable performance (F=0.79) with the state-of-the-art method (HED) <ref type="bibr" target="#b50">[51]</ref>. Note that our model is not deliberately designed for natural edge detection on BSDS500, and we believe that the techniques used in HED <ref type="bibr" target="#b50">[51]</ref> such as multiscale fusion, carefully designed upsampling layers and data augmentation could further improve the performance of our model.</p><formula xml:id="formula_2">(a) (b) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object Proposal Generation</head><p>Object proposals are important mid-level representations in computer vision. Most of existing methods use boundary detection as cues for proposal generation. Thus the im- provements on contour detection will immediately boost the performance of object proposals. We choose the MCG algorithm to generate segmented object proposals from our detected contours. The MCG algorithm is based on low-level edge detectors, e.g. gPb and SE. It first computes ultrametric contour maps from multiscale edge maps and then aligns them into a single hierarchical segmentation. To obtain object proposals, a multi-objective optimization is designed to reduce the redundancy of combinatorial grouping of adjacent regions. The reduced set of grouping candidates are then ranked as the final segmented object proposals. If built on singlescale edge maps, the algorithm is referred as singlescale combinatorial grouping (SCG). Based on the procedure above, we simply replace the low-level edge detector with our CEDN contour detector to generate proposals. The multiscale and singlescale versions are referred to as "CEDN-MCG" and "CEDN-SCG", respectively.</p><p>We evaluate the quality of object proposals by two measures: Average Recall (AR) and Average Best Overlap (ABO). Both measures are based on the overlap (Jaccard index or Intersection-over-Union) between a proposal and a ground truth mask. AR is measured by 1) counting the percentage of objects with their best Jaccard above a certain threshold T and then 2) averaging them within a range of thresholds T ∈ [0.5, 1.0]. It is established in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b41">42]</ref> to benchmark the quality of bounding box and segmented object proposals. ABO is measured by calculating the best proposal's Jaccard for every ground truth object and then 2) averaging them over all the objects. We compare with state-of-the-art algorithms: MCG, SCG, Category Indepen- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average Recall per Class on pascal2012 val2012</head><p>Number of candidates = 1000 CEDN-MCG CEDN-SCG MCG SCG <ref type="figure">Figure 9</ref>. Per-class average recall on PASCAL val2012. dent object proposals (CI) <ref type="bibr" target="#b13">[14]</ref>, Constraint Parametric Min Cuts (CPMC) <ref type="bibr" target="#b9">[10]</ref>, Global and Local Search (GLS) <ref type="bibr" target="#b42">[43]</ref>, Geodesic Object Proposals (GOP) <ref type="bibr" target="#b28">[29]</ref>, Learning to Propose Objects (LPO) <ref type="bibr" target="#b29">[30]</ref>, Recycling Inference in Graph Cuts (RIGOR) <ref type="bibr" target="#b23">[24]</ref>, Selective Search (SeSe) <ref type="bibr" target="#b49">[50]</ref> and Shape Sharing (ShSh) <ref type="bibr" target="#b25">[26]</ref>. Note that these abbreviated names are inherited from <ref type="bibr" target="#b3">[4]</ref>. PASCAL val2012. We feed the HED edge maps into MCG for generating proposals and compare with others. We refer the results from the BSDS-trained HED model as HEDB-MCG and the ones from the PASCAL-trained HED model as HED-MCG. <ref type="figure" target="#fig_6">Figure 8</ref> shows that CEDN-MCG achieves 0.67 AR and 0.83 ABO with ∼ 1660 proposals per image, which improves the original MCG by 5% in AR and by 3% in ABO with a third as many proposals. At 1000 proposals, CEDN-MCG outperforms the second best HEDB-MCG by 8% in AR and by 2.5% in ABO, respectively. It takes 0.1 second to compute the CEDN contour map for a PASCAL image on a high-end GPU and 18 seconds to generate proposals with MCG on a standard CPU. We notice that the CEDN-SCG achieves similar accuracies with CEDN-MCG, but it only takes less than 3 seconds to run SCG. We also plot the per-class ARs in <ref type="figure">Figure 9</ref> and find that CEDN-MCG and CEDN-SCG improves MCG and SCG for all of the 20 classes. Notably, the bicycle class has the worst AR and we guess it is likely because of its incomplete annotations. Some examples of object proposals are demonstrated in <ref type="figure" target="#fig_2">Figure 4(d)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MS COCO val2014.</head><p>We present results in the MS COCO 2014 validation set, shortly "COCO val2014" that includes 40504 images annotated by polygons from 80 object classes. This dataset is more challenging due to its large variations of object categories, contexts and scales. Compared to PASCAL VOC, there are 60 unseen object classes for our CEDN contour detector. Note that we did not train CEDN on MS COCO. We report the AR and ABO results in <ref type="figure" target="#fig_0">Figure 10</ref>. It turns out that the CEDN-MCG achieves a competitive AR to MCG with a slightly lower recall from fewer proposals, but a weaker ABO than LPO, MCG and SeSe. Taking a closer look at the results, we find that our CEDN-MCG algorithm can still perform well on known objects (first and third examples in <ref type="figure" target="#fig_0">Figure 12</ref>) but less effectively on certain unknown object classes, such as food (second example in <ref type="figure" target="#fig_0">Figure 12</ref>). It is likely because those novel classes, although seen in our training set (PASCAL VOC), are actually annotated as background. For example, there is a "dining table" class but no "food" class in the PASCAL VOC dataset. Quantitatively, we present per-class ARs in <ref type="figure" target="#fig_0">Figure 11</ref> and have following observations: 1) CEDN obtains good results on those classes that share common supercategories with PASCAL classes, such as "vehicle", "animal" and "furniture"; 2) CEDN fails to detect the objects labeled as "background" in PASCAL VOC, such as "food" and "applicance"; 3) CEDN works well on unseen classes that are not prevalent in PASCAL VOC, such as "sports". These observations urge training on COCO, but we also observe that the polygon annotations in MS COCO are less reliable than the ones in PASCAL VOC (third example in <ref type="figure" target="#fig_0">Figure 12</ref>(b)). We will need more sophisticated methods for refining the COCO annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have developed an object-centric contour detection method using a simple yet efficient fully convolutional encoder-decoder network. Concerned with the imperfect contour annotations from polygons, we have developed a  <ref type="figure" target="#fig_0">Figure 11</ref>. Average recall per class on the MS COCO 2014 validation set.</p><p>(a) (b) (c) (d) (e) <ref type="figure" target="#fig_0">Figure 12</ref>. Example results on MS COCO val2014. In each row from left to right we present (a) input image, (b) ground truth annotation, (c) edge detection <ref type="bibr" target="#b12">[13]</ref>, (d) our object contour detection and (e) our best object proposals.</p><p>refinement method based on dense CRF so that the proposed network has been trained in a fully-supervised manner. As a result, the trained model yielded high precision on PASCAL VOC and BSDS500, and achieved comparable performance with the state-of-the-art on BSDS500 after fine-tuning. We have combined the proposed contour detector with MCG algorithm for generating segmented object proposals, which significantly advances the state-ofthe-art on PASCAL VOC. We also found that the proposed model generalized well to unseen object classes from the known super-categories and demonstrated competitive performance on MS COCO without re-training the network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Object contour detection. Given input images (a), our model can effectively learn to detect contours of foreground objects (c) in contrast to traditional edge detection (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Contour refinement. The polygon based annotations (a) cannot be directly used for training due to its inaccurate boundaries (thin white area reflects unlabeled pixels between objects). We align them to image boundaries by re-labeling the uncertain areas with dense CRF (d), compared to Graph Cut (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Example results on PASCAL VOC val2012. In each row from left to right we present (a) input image, (b) ground truth annotation, (c) edge detection [13], (d) our object contour detection and (e) our best object proposals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. PR curve for contour detection on PASCAL val2012.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .[</head><label>6</label><figDesc>Figure 6. Example results on BSDS500 test set. In each row from left to right we present (a) input image, (b) ground truth contour, (c) contour detection with pretrained CEDN and (d) contour detection with fine-tuned CEDN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. PR curve for contour detection on the BSDS500 set set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Average best overlap (ABO) and average recall (AR) on PASCAL val2012.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Average best overlap (ABO) and average recall (AR) on the MS COCO 2014 validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Decoder network setup.</figDesc><table>name 
deconv6 
deconv5 
deconv4 
setup 
conv 
unpool-conv unpool-conv 
kernel 1×1×512 5×5×512 5×5×256 
acti 
relu 
relu 
relu 
name 
deconv3 
deconv2 
deconv1 
pred 
setup unpool-conv unpool-conv unpool-conv conv 
kernel 5×5×128 5×5×64 5×5×32 5×5×1 
activation 
relu 
relu 
relu 
sigmoid 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Average Recall per Class on COCO val2014</figDesc><table>person 
bicycle 
car 
motorcycle 
airplane 
bus 
train 
truck 
boat 

traffic light 

fire hydrant 
stop sign 
parking meter 

bench 
bird 
cat 
dog 
horse 
sheep 
cow 
elephant 
bear 
zebra 
giraffe 
backpack 
umbrella 
handbag 
tie 
suitcase 
frisbee 
skis 
snowboard 

sports ball 

kite 

baseball bat 
baseball glove 
skateboard 
surfboard 

tennis racket 

bottle 

wine glass 

cup 
fork 
knife 
spoon 
bowl 
banana 
apple 
sandwich 
orange 
broccoli 
carrot 

hot dog 
pizza 
donut 
cake 
chair 
couch 

potted plant 

bed 

dining table 

toilet 
tv 
laptop 
mouse 
remote 
keyboard 

cell phone 
microwave 
oven 
toaster 
sink 
refrigerator 
book 
clock 
vase 
scissors 

teddy bear 
hair drier 
toothbrush 
0 

0.2 

0.4 

0.6 

0.8 

1 

Number of candidates = 1000 
vehicle 
outdoor 
animal 
accessory 
sports 
kitchen 
food 
furniture 
electronic appliance 
indoor 

CEDN-SCG 
SCG 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments J. Yang and M.-H. Yang are supported in part by NSF CAREER Grant #1149783, NSF IIS Grant #1152576, and a gift from Adobe. H. Lee is supported in part by NSF CAREER Grant IIS-1453651.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Monocular extraction of 2.1 D sketch using constrained convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yousefi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="42" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deepedge: A multiscale bifurcated deep network for top-down contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High-for-low and lowfor-high: Efficient boundary detection from deep object features and its applications to high-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interactive graph cuts for optimal boundary &amp; region segmentation of objects in n-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-P</forename><surname>Jolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Constrained parametric min-cuts for automatic object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BING: Binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structured forests for fast edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Category independent object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Groups of adjacent contour segments for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fevrier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="36" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Computer Vision: A Modern Approach. Pearson Education</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recovering occlusion boundaries from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">What makes for effective detection proposals?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">RIGOR: Reusing inference in graph cuts for generating object regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shape sharing for object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visual boundary prediction: A deep neural prediction network and quality dissection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Kivinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Geodesic object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to propose objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sketch tokens: A learned mid-level representation for contour and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-objective convolutional learning for face labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Contour and texture analysis for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Boosting object proposals: From Pascal to COCO</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generating object segmentation proposals using global and local search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rantalankila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Grabcutinteractive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">LabelMe: a database and web-based tool for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deepcontour: A deep convolutional feature learned by positivesharing loss for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Situational object boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlingsy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
