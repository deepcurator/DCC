<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Spectral Clustering Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">T</forename><surname>Law</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
						</author>
						<title level="a" type="main">Deep Spectral Clustering Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Clustering is the task of grouping a set of examples so that similar examples are grouped into the same cluster while dissimilar examples are in different clusters. The quality of a clustering depends on two problem-dependent factors which are i) the chosen similarity metric and ii) the data representation. Supervised clustering approaches, which exploit labeled partitioned datasets have thus been proposed, for instance to learn a metric optimized to perform clustering. However, most of these approaches assume that the representation of the data is fixed and then learn an appropriate linear transformation. Some deep supervised clustering learning approaches have also been proposed. However, they rely on iterative methods to compute gradients resulting in high algorithmic complexity. In this paper, we propose a deep supervised clustering metric learning method that formulates a novel loss function. We derive a closed-form expression for the gradient that is efficient to compute: the complexity to compute the gradient is linear in the size of the training mini-batch and quadratic in the representation dimensionality. We further reveal how our approach can be seen as learning spectral clustering. Experiments on standard real-world datasets confirm state-of-the-art Recall@K performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Clustering is a widely used technique with applications in machine learning, statistics, speech processing, computer vision. It consists in grouping a set of examples so that "similar" examples are in the same cluster while "dissimilar" examples are in different clusters. In most applications, the vector representation of examples is given as input and 1 Department of Computer Science, University of Toronto, Toronto, Canada 2 CIFAR Senior Fellow. Correspondence to: Marc T. Law &lt;law@cs.toronto.edu&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the 34</head><p>th International Conference on Machine Learning, Sydney, <ref type="bibr">Australia, PMLR 70, 2017</ref><ref type="bibr">. Copyright 2017</ref> by the author(s). a key step is then to determine an appropriate similarity metric so that similar and dissimilar objects can be easily identified. In some cases, experts with domain knowledge may help determine an appropriate distance metric. However, in high-dimensional problems, determining an effective metric becomes increasingly difficult even for an expert, and standard metrics such as the Euclidean distance can lead to very poor results.</p><p>Many approaches that learn an appropriate similarity metric <ref type="bibr" target="#b38">(Xing et al., 2002;</ref><ref type="bibr" target="#b15">Lajugie et al., 2014;</ref><ref type="bibr" target="#b16">Law et al., 2016)</ref> or a nonlinear embedding function <ref type="bibr" target="#b25">(Schroff et al., 2015;</ref><ref type="bibr" target="#b28">Sohn, 2016;</ref><ref type="bibr" target="#b30">Song et al., 2017)</ref> in a supervised way have thus been proposed. They assume the availability of a training dataset that "shares the same metric" as the test dataset that they want to partition (e.g. the datasets represent the same concepts such as birds species or car models). As both datasets share the same metric, the model learned from training examples is expected to correctly compare test examples. The approaches can roughly be divided into four groups based on two criteria: semisupervised/supervised setting and shallow/deep architecture. In the semi-supervised setting <ref type="bibr" target="#b38">(Xing et al., 2002;</ref><ref type="bibr" target="#b6">Chopra et al., 2005)</ref>, the training data is given as a small set of example pairs that are expected to be in the same or different clusters. On the other hand, supervised approaches <ref type="bibr" target="#b15">(Lajugie et al., 2014;</ref><ref type="bibr" target="#b16">Law et al., 2016)</ref> assume the availability of labeled datasets for which the ground truth partitions are provided during training. A model is then learned so that some clustering algorithm will produce a partition similar to the ground truth partition on the training dataset. The supervised clustering setting can be seen as the spe-cial case of semi-supervised setting where all the pairwise similarity relations between training examples are given. In particular, supervised clustering is a specific classification problem where the model is learned so that the representations of training examples are closer to the representative vector of their category than to the representative vector of any other category. In this paper, we propose a novel metric learning approach whose rationale is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. In particular, we leverage deep nonlinear and hierarchical architectures to learn complex representations that better reflect similarity relations among examples.</p><p>Many deep metric learning approaches <ref type="bibr" target="#b25">(Schroff et al., 2015;</ref><ref type="bibr" target="#b29">Song et al., 2016)</ref> extend the ideas introduced in the shallow metric learning literature <ref type="bibr" target="#b38">(Xing et al., 2002;</ref><ref type="bibr" target="#b37">Weinberger et al., 2006)</ref> to learn deep neural networks on large datasets. The main difficulty is how to implement these ideas so that they are scalable and avoid memory bottleneck. For instance, many approaches have proposed hard negative mining strategies to limit the number of training constraints. A deep supervised clustering approach was proposed in <ref type="bibr" target="#b30">(Song et al., 2017)</ref>: to compute its gradient, the approach uses an iterative greedy algorithm whose algorithmic complexity is high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions:</head><p>In this paper, we propose a metric learning framework that optimizes an embedding function so that the learned representations of similar examples are grouped into the same cluster, and dissimilar examples are in different clusters. To this end, we relax the problem of partitioning a dataset with Bregman divergences <ref type="bibr" target="#b3">(Banerjee et al., 2005)</ref>, and we formulate our problem so that the gradient is efficient to compute. In particular, the gradient can be expressed in closed-form, and the algorithmic complexity to compute it is linear in the size of the training mini-batch and quadratic in the representation dimensionality, which is better than the complexity of existing iterative methods. Our method is also simple to implement and obtains stateof-the-art performance on standard datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>In this section, we provide some technical background about clustering, and set up the notation throughout. We reformulate in matrix form the problem of clustering a set of examples with Bregman divergences <ref type="bibr" target="#b3">(Banerjee et al., 2005)</ref> and write it as an optimization problem w.r.t. one variable in Eq. (2).</p><p>Notation: We note A, B := tr(AB ), the Frobenius inner product where A and B are real-valued matrices; and A := tr(AA ), the Frobenius norm of A. 1 is the vector of all ones with appropriate dimensionality. A † is the Moore-Penrose pseudoinverse of A. ri(F) is the relative interior of the set F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data:</head><p>We consider that we are given a set of n examples f 1 , · · · , f n ∈ F which may be represented as a single matrix</p><formula xml:id="formula_0">F = [f 1 , · · · , f n ] ∈ F n .</formula><p>In the following, we consider that F ⊆ R d , and thus</p><formula xml:id="formula_1">F n ⊆ R n×d . Bregman divergence: Any Bregman divergence d φ : F × ri(F) → [0, +∞) is defined as d φ (x, y) = φ(x) − φ(y) − x − y, ∇φ(y)</formula><p>where φ : F → R is a continuouslydifferentiable and strictly convex function, ∇φ(y) ∈ R d represents the gradient vector of φ at y. The most commonly used Bregman divergences for clustering are the squared Euclidean distance defined with φ(x) = x 2 2 , the KL-divergence <ref type="bibr" target="#b7">(Dhillon et al., 2003)</ref> or the Itakura-Saito distance <ref type="bibr" target="#b5">(Buzo et al., 1980)</ref>. We refer the reader to <ref type="bibr" target="#b3">(Banerjee et al., 2005;</ref><ref type="bibr" target="#b21">Nielsen &amp; Nock, 2009</ref>) for details.</p><p>Clustering: Partitioning the n observations in</p><formula xml:id="formula_2">F = [f 1 , · · · , f n ] ∈ F n</formula><p>into k clusters is equivalent to determining an assignment matrixŶ ∈ {0, 1} n×k such that Y ic = 1 if f i is assigned to cluster c and 0 otherwise. In this paper, we assume that each example is assigned to one and only one cluster (i.e. the sum of each row ofŶ is 1) and that there is no empty cluster (which corresponds to adding the constraint rank(Ŷ ) = k). Therefore,Ŷ is in the following set of assignment matrices:</p><formula xml:id="formula_3">Y n×k := {Ŷ ∈ {0, 1} n×k :Ŷ 1 = 1, rank(Ŷ ) = k}</formula><p>We assume as in <ref type="bibr" target="#b3">(Banerjee et al., 2005)</ref> that each cluster c is represented by a single representative vector z c ∈ ri(F) and that an example f i is assigned to cluster c only if</p><formula xml:id="formula_4">∀b = c, d φ (f i , z c ) ≤ d φ (f i , z b ) where d φ is the chosen Bregman divergence.</formula><p>To simplify the notations, we consider that z c ∈ F, and we concatenate the k representative vectors into a single matrix</p><formula xml:id="formula_5">Z = [z 1 , · · · , z k ] ∈ F k</formula><p>. The problem of partitioning the n examples in F ∈ F n with d φ can then be formulated as minimizing the energy function:</p><formula xml:id="formula_6">min Y ∈Y n×k ,Z∈F k n i=1 k c=1Ŷ ic · d φ (f i , z c )<label>(1)</label></formula><p>= min</p><formula xml:id="formula_7">Y ∈Y n×k ,Z∈F k 1 Φ(F )−1 Φ(Ŷ Z)− F −Ŷ Z, ∇Φ(Ŷ Z)</formula><p>where we have used the definition of Bregman divergences and we note:</p><formula xml:id="formula_8">∀A = [a 1 , · · · , a n ] ∈ F n , Φ(A) = [φ(a 1 ), · · · , φ(a n )] ∈ R n</formula><p>is the concatenation into a single vector of the different φ(a i ) ∈ R, and ∇Φ(A) = [∇φ(a 1 ), · · · , ∇φ(a n )] ∈ R n×d is the concatenation into a single matrix of the different gradients ∇φ(a i ) ∈ R </p><formula xml:id="formula_9">Z = (Ŷ Ŷ ) −1Ŷ F =Ŷ † F</formula><p>is the unique global minimizer of Z in Eq. (1). We then define the set P = {ŶŶ † :Ŷ ∈ Y n×k } and rewrite Eq.</p><p>(1) as a minimization problem w.r.t. one variable:</p><formula xml:id="formula_10">min C∈P 1 Φ(F ) − 1 Φ(ĈF ) − F −ĈF, ∇Φ(ĈF ) (2)</formula><p>As an illustration, if d φ is the squared Euclidean distance, then Eq. (2) reduces to:</p><formula xml:id="formula_11">F 2 − Ĉ F 2 − F − CF, 2ĈF = F 2 + Ĉ F 2 − 2 F,ĈF = F −ĈF 2 .</formula><p>We then obtain the formulation mentioned in <ref type="bibr" target="#b15">(Lajugie et al., 2014)</ref>[Section 2.2] of the usual kmeans algorithm:</p><formula xml:id="formula_12">min C∈P F −ĈF 2 ⇔ max C∈P Ĉ , F F<label>(3)</label></formula><p>by using the fact that all the matrices in P are orthogonal projection matrices (i.e. symmetric and idempotent): we then have ∀Ĉ ∈ P, Ĉ F 2 = tr(Ĉ 2 F F ) = tr(ĈF F ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Spectral Clustering Learning</head><p>In this section, we introduce our method that we call Deep Spectral Clustering Learning (DSCL). We first relax the clustering problem in Eq. (2) and consider the set of solutions of the relaxed problem as the prediction function of our model. Then, we present our large margin supervised clustering problem and its efficient solver. Finally, we explain the connection of our approach with spectral clustering learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Constraint Relaxation</head><p>Optimizing Eq. (2) is a NP-hard problem <ref type="bibr" target="#b1">(Aloise et al., 2009)</ref>, we then approximate it. Following <ref type="bibr" target="#b27">(Shi &amp; Malik, 2000;</ref><ref type="bibr" target="#b39">Zha et al., 2001;</ref><ref type="bibr" target="#b20">Ng et al., 2002;</ref><ref type="bibr" target="#b22">Peng &amp; Wei, 2007)</ref>, we now present a spectral relaxation of this problem. In particular, we extend the domain of Eq. (2) so that the set of solutions of the resulting problem can be formulated in a convenient way.</p><p>We propose to relax the constraintĈ ∈ P in Eq. (2) with the constraintĈ ∈ C n,k</p><p>where the set C n,k includes P and is defined as the set of n × n rank-k orthogonal projection matrices C n,k := {Ĉ ∈ R n×n :Ĉ 2 =Ĉ,Ĉ = C, rank(Ĉ) = k}. The set of solutions of the resulting relaxed version of Eq. (2) can then be formulated:</p><formula xml:id="formula_13">f (F ) := arg max C∈C n,k 1 Φ(ĈF ) + F −ĈF, ∇Φ(ĈF ) (4)</formula><p>where the terms that do not depend onĈ are omitted.</p><p>The solutions in Eq. (4) can be found in closed-form. Let us note s = rank(F ), we show in the supplementary material that if s ≤ k, then the set of solutions of Eq. <ref type="formula">(4)</ref> </p><formula xml:id="formula_14">is f (F ) = {Ĉ ∈ C n,k :ĈF = F } = {Ĉ ∈ C n,k :Ĉ = F F † + V V , V V ∈ C n,(k−s) , V V F = 0}. In particular, if s = k, then V V = 0 and we have f (F ) = {F F † }.</formula><p>In the following, we consider only the case where the dimensionality d of training examples is not greater than k so that the property s ≤ k is satisfied. Nonetheless, if s &gt; k, the set of solutions can be considered as f (F ) = {F F † }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Structured output prediction</head><p>In the following, we consider that we are given n examples x i ∈ X (e.g. n images) and an embedding function ϕ θ : X → F whose set of parameters is θ and such that ∀i ∈ {1, · · · , n}, ϕ θ (x i ) = f i (e.g. f i can be the output of a convolutional neural network). All these representations are concatenated into a single matrix</p><formula xml:id="formula_15">F = [f 1 , · · · , f n ] ∈ F n .</formula><p>We are also given a ground truth assignment matrix Y ∈ Y n×k which indicates the desired partition for the n examples. In other words, let C = Y Y † ∈ P be the ground truth clustering matrix of F , we would like to learn the embedding function ϕ θ so that the matrix predicted with the (relaxed) clustering problem f (F ) ⊆ C n,k in Eq. <ref type="formula">(4)</ref> is as close as possible to the ground truth clustering matrix C.</p><p>Different evaluation metrics such as purity, rand index and normalized mutual information exist to evaluate clustering (see <ref type="bibr" target="#b19">(Manning et al., 2008)</ref>  <ref type="bibr">[Chapter 16</ref>.3] for details). As in <ref type="bibr" target="#b11">(Hubert &amp; Arabie, 1985;</ref><ref type="bibr" target="#b2">Bach &amp; Jordan, 2004;</ref><ref type="bibr" target="#b15">Lajugie et al., 2014;</ref><ref type="bibr" target="#b16">Law et al., 2016)</ref>, we choose the Frobenius norm which has many advantages. As explained in <ref type="bibr" target="#b15">(Lajugie et al., 2014)</ref>[Section 3.2], unlike rand index, the Frobenius norm between clustering (orthogonal projection) matrices takes into account the size of the different clusters (i.e. its value is not dominated by the performance on the largest clusters) and thus optimizes intra-class variance that is a rescaled indicator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">PROBLEM FORMULATION</head><p>In the following, we consider that the matrix F is a variable (that depends on ϕ θ ). The problem of minimizing the discrepancy between the ground truth clustering C ∈ P of the matrix F and the (relaxed) clusteringĈ predicted with f (F ) in Eq. <ref type="formula">(4)</ref> can be formulated as the following empirical risk minimization problem:</p><formula xml:id="formula_16">min F ∈F n max C∈f (F ) C −Ĉ 2 (5)</formula><p>As all the matrices in f (F ) have the same rank, we can rewrite Eq. (5): C −Ĉ 2 = C 2 + Ĉ 2 − 2 tr(CĈ) where Ĉ 2 = tr(Ĉ) = rank(Ĉ) = k is a constant. Eq. <ref type="formula">(5)</ref> is then equivalent to the problem:</p><formula xml:id="formula_17">max F ∈F n min C∈f (F ) tr(CĈ)<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Deep Spectral Clustering Learning (DSCL)</head><p>input : Set of training examples (e.g. images) in X , embedding function ϕ θ , number of iterations τ , learning rates η1, · · · , ητ 1: for iteration t = 1 to τ do 2: Randomly sample n training examples x1, · · · , xn ∈ X 3: Create representation matrix <ref type="formula" target="#formula_22">(9)</ref>) where C = Y Y † is the desired partition matrix of the n examples 5: Update the set of parameters θ of ϕ θ by exploiting the rescaled gradient G and perform gradient step with learning rate ηt 6: end for which is naturally lower bounded by (see details in supplementary material):</p><formula xml:id="formula_18">F ← [f1, · · · , fn] ∈ F n s.t. ∀i ∈ {1, · · · , n}, fi = ϕ θ (xi) 4: Create rescaled gradient G ← (I −F F † )C(F † ) (see Eq.</formula><formula xml:id="formula_19">max F ∈F n min C∈f (F ) tr(CĈF F † ) = max F ∈F n tr(CF F † )<label>(7)</label></formula><p>where we use the fact that ∀Ĉ ∈ f (F ),ĈF F † = F F † . We note that Eq. <ref type="formula" target="#formula_19">(7)</ref> equals Eq. (6) if rank(F ) = k.</p><p>The difficulty of optimizing the problem in Eq. <ref type="formula" target="#formula_19">(7)</ref> is that it depends on both F and F † . If we assume that rank(F ) is a constant (with F not necessarily full rank) in Eq. <ref type="formula" target="#formula_19">(7)</ref>, then the problem is differentiable <ref type="bibr" target="#b9">(Golub &amp; Pereyra, 1973)</ref> and the gradient of Eq. <ref type="formula" target="#formula_19">(7)</ref> w.r.t. F is:</p><formula xml:id="formula_20">∇ F = 2(I − F F † )C(F † )<label>(8)</label></formula><p>where I is the identity matrix. Details on the gradient can be found in the supplementary material. Our matrix F is always full rank in our experiments 1 , so the constant rank condition along the iterations is satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">LOW ALGORITHMIC COMPLEXITY</head><p>We now show that in addition to having a closed-form expression, computing our gradient ∇ F is efficient: the complexity to compute it is linear in n and quadratic in d.</p><p>We note C = Y Y † ∈ P the ground truth partition matrix where the matrix Y ∈ Y n×k is given: let y c be the c-th column of Y , then the c-th column of (Y † ) can be written</p><formula xml:id="formula_21">1 max{1,y c 1} y c . The complexity to compute (Y † ) is linear in n due to the sparsity of Y . (Y † )</formula><p>is also sparse (i.e. it contains n nonzero elements). We can then write ∇ F 2 as:</p><formula xml:id="formula_22">G := ∇ F 2 = Y − F [F † Y ] [F † (Y † ) ]<label>(9)</label></formula><p>where [·] indicates d × k matrices which are computed efficiently due to the sparsity of Y . The complexity to compute F † is O(nd min{n, d}) (i.e. O(nd 2 ) as we assume d ≤ n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">DIRECT LOSS MINIMIZATION</head><p>Our method computes in closed-form the gradient of the structured output prediction problem in Eq. (5) when rank(F ) = k, and its algorithmic complexity is low.</p><p>To the best of our knowledge, although we exploit classic spectral relaxation results, our method is the first approach that includes the closed-form solution of the relaxed kmeans problem (see Section 3.1) within a large margin method for structured output. Even Mahalanobis metric learning methods <ref type="bibr" target="#b15">(Lajugie et al., 2014</ref>) cannot use such a simplification due to the nature of their model, and the formulation of their subgradient is then different.</p><p>Including the closed-form solution of the relaxed kmeans algorithm results in a simple problem formulation (see Eq. <ref type="formula" target="#formula_19">(7)</ref>). The resulting large margin optimization problem is easy to optimize as we do not have to perform loss-augmented inference during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning deep models</head><p>Our method can be used to learn neural networks with conventional gradient-based methods by exploiting chain rule. Our approach is illustrated in Algorithm 1.</p><p>In detail, we note a mini-batch matrix</p><formula xml:id="formula_23">F = [f 1 , · · · , f n ] ∈ F</formula><p>n the concatenation into a single matrix of the n different embedding representations ϕ θ (x i ) = f i , where, for instance, ϕ θ : X → F is a neural network embedding function and x i ∈ X is an image. We can rewrite Eq. (7) as a minimization problem by defining our loss function as a function of the mini-batch representation matrix:</p><formula xml:id="formula_24">Loss(F ) := k − tr(CF F † )<label>(10)</label></formula><p>which is nonnegative and where C = Y Y † ∈ P is the ground truth clustering matrix of F . The neural network ϕ θ is then learned via backpropagation as illustrated in Algorithm 1 (i.e. using stochastic gradient descent with rescaled gradient −G where G is defined in Eq. <ref type="formula" target="#formula_22">(9)</ref>).</p><p>We note that the structure of the learned neural network is not limited to the case where F is R d . For instance, if the goal is to partition probability distributions with KL-divergence, the set F can be constrained to be the ddimensional simplex {x ∈ [0, 1]</p><p>d : x 1 = 1} by using a softmax regression at the last layer of the neural network.</p><p>Regression problem: It is worth noting that ∇ F is also the gradient w.r.t. F of the nonlinear least squares problem:</p><formula xml:id="formula_25">max F ∈F n − 1 2 F F † − C 2<label>(11)</label></formula><p>where we assume that the rank of F is constant (otherwise, the problem is not differentiable). Our solver can then be seen as a gradient-based solver for the nonlinear regression problem that iteratively decreases the distance F F † − C . In particular, the spectral relaxation proposed in Section 3.1 allows to write the set of predicted clusterings f (F ) as a function of F F † , and the choice of the Frobenius norm to compare clusterings makes our problem similar to a least squares problem.</p><p>Given the least squares formulation of Eq. (11), one can see that our problem focuses more on the similarity between the matrices C and F F † than between C and the individual matrix F . Our problem can then be seen as a spectral clustering algorithm as explained in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Connection with spectral clustering</head><p>We now explain how our proposed method can be seen as learning spectral clustering in a supervised way.</p><p>As explained in <ref type="bibr" target="#b2">(Bach &amp; Jordan, 2004;</ref><ref type="bibr" target="#b34">Von Luxburg, 2007)</ref>, spectral clustering does not refer to one particular method but to a family of methods (e.g. <ref type="bibr" target="#b27">(Shi &amp; Malik, 2000)</ref>) that partition a dataset by exploiting the leading eigenvectors of a similarity matrix. They rely on the eigenstructure of a similarity matrix to partition examples into disjoint clusters, with examples in the same cluster having high similarity and examples in different clusters having low similarity. In our case, as can be seen in Eq. (7), the (kernel) similarity matrix is K = F F † = U U where U ∈ R n×s is a matrix whose columns are the left-singular vectors of F corresponding to its nonzero singular values and where s = rank(F ). By definition, these left-singular vectors of F are also the s leading eigenvectors of K.</p><p>It is worth noting that our approach is different from classic spectral clustering approaches that perform clustering by exploiting some Laplacian matrix of the similarity matrix. Our approach directly performs clustering by exploiting the leading eigenvectors of the similarity matrix K.</p><p>By increasing the value tr(CF F † ) = tr(CU U ) at each backpropagation iteration, the leading eigenvectors of U U = K become more similar to the leading eigenvectors of C. As C and K = U U are both orthogonal projection matrices, in the ideal case, tr(CU U ) is maximized when the column space of one of the two matrices C or (U U ) is included in the column space of the other matrix. In particular, if rank(C) = rank(U U ), then tr(CU U ) is maximized iff C = U U <ref type="bibr" target="#b8">(Fan, 1949)</ref>. In this ideal case, we have arg maxĈ ∈P Ĉ , U U = {C}, which corresponds to the solution of Eq. (3) when replacing F by U ; in other words, partitioning the rows of U with kmeans will return the desired clustering matrix C.</p><p>It is then clear that comparing the similarity between the rows of U (i.e. using spectral clustering) is at least as relevant as comparing the rows of F to partition the dataset. In our experiments, our method obtains better performance when the left-singular vectors of the test set matrix are used for partitioning rather than the learned features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The goal of metric learning is to learn a metric that can be used to compare new examples, possibly from categories that were not in the training dataset. In this context, the model is learned on a training dataset and tested on a dataset that shares the same "semantical" metric and represents similar concepts. This allows to evaluate the generalization ability of the model. Following the experimental protocol described in <ref type="bibr" target="#b29">(Song et al., 2016;</ref><ref type="bibr" target="#b30">2017)</ref>, we evaluate our method on the following fine-grained datasets and use the exact same train/test splits:</p><p>• The Caltech-UCSD Birds (CUB-200-2001) dataset <ref type="bibr" target="#b35">(Wah et al., 2011</ref>) is composed of 11,788 images of birds from 200 different species/categories. We split the first 100 categories for training (5,864 images) and the rest for test (5,924 images).</p><p>• The CARS196 dataset <ref type="bibr" target="#b14">(Krause et al., 2013</ref>) is composed of 16,185 images of cars from 196 model categories. The first 98 categories are used for training (8,054 images), the rest for test (8,131 images).</p><p>• The Stanford Online Product <ref type="bibr" target="#b29">(Song et al., 2016)</ref> dataset is composed of 120,253 images from 22,634 online product categories. It is partitioned into 59,551 images from 11,318 categories for training and 60,502 images from 11,316 categories for test.</p><p>In these experiments, the categories for training and the categories for testing are disjoint although they belong to the same context (i.e. they all represent birds, cars or products). This makes the problem challenging as deep models may overfit on the training categories 2 . In the same way as the baselines, we then perform early stopping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>We closely follow the experimental setup described in <ref type="bibr" target="#b29">(Song et al., 2016;</ref><ref type="bibr" target="#b30">2017)</ref>. In particular, we implemented our method with the Tensorflow package <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> and used the Inception  network with batch normalization <ref type="bibr" target="#b12">(Ioffe &amp; Szegedy, 2015)</ref> pretrained on ImageNet/ILSVRC 2012-CLS <ref type="bibr">(Russakovsky et al., 2015)</ref>, we fine-tuned the network on the training datasets. We perform two types of fine-tuning: Algorithm 2 DSCL Normalized Spectral Clustering input : Test set Ft ∈ F n t , nt is the number of test examples 1: Create Mt ∈ R n t ×d by mean centering Ft 2: Create r = rank(Mt)</p><formula xml:id="formula_26">3: Create Ut = [u1, · · · , un t ] ∈ R n t ×r s.t. MtM † t = UtU t 4: Create T = [t1, · · · , tn t ] ∈ R n t ×r s.t. ∀i, ti = 1 u i 2</formula><p>ui 5: partition the rows of T into k clusters with kmeans</p><p>• end-to-end: we fine-tune the model and update the parameters of all the layers of the neural network during backpropagation. In this case, we perform 100 iterations of gradient descent.</p><p>• last layer: we freeze all the parameters of the neural network (pretrained on ImageNet) except those in the last layer. Only the parameters in the last layer of the model are updated. We perform 200 iterations of gradient descent.</p><p>In both cases, we remove the softmax function at the end of the last layer.</p><p>The images are first resized to square size (256 × 256) and cropped at 227 × 227. For the dataset augmentation, we use a random horizontal mirroring for training and a single center crop for test. As in <ref type="bibr" target="#b30">(Song et al., 2017)</ref> and unlike <ref type="bibr" target="#b28">(Sohn, 2016)</ref>, we use a single crop per image.</p><p>We ran our experiments on a single Tesla P100 GPU with 16GB RAM, and used a standard Stochastic Gradient optimizer. Our batch size is set to n = o×p = 18×70 = 1260, our method backpropagates the loss in Eq. (10) for all the examples in the batch. As Inception is a large model and to fit into memory, we iteratively compute submatrices F i ∈ F o and concatenate them into a single matrix <ref type="formula" target="#formula_22">(9)</ref>, and minimizes the loss function F, −G = p i=1 F i , −G i with gradient descent where −G is fixed. Different embedding dimensionality values d ∈ {64, 128, 256, 512} are tested in <ref type="bibr" target="#b29">(Song et al., 2016)</ref>, it is reported that d does not play a crucial role. In our case, we then set our dimensionality d to be equal to the number of categories k for the Birds and Cars datasets.</p><formula xml:id="formula_27">F = [F 1 , · · · , F p ] ∈ F n . Our method then computes the gradient matrix G = [G 1 , · · · , G p ] ∈ R n×d defined in Eq.</formula><p>For the Products dataset which contains more than 10k categories, we observed as in <ref type="bibr" target="#b28">(Sohn, 2016)</ref> that the larger the value of d, the better the results. We then set d = 512 in order to be fair with the other models and randomly subsample 512 training categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Partitioning a test dataset</head><p>Partitioning a test dataset</p><formula xml:id="formula_28">X t = [x 1 , · · · , x nt ] ∈ X nt</formula><p>(where x i ∈ X is an image in these experiments, and n t is the number of test examples) is done by first computing the test representation matrix</p><formula xml:id="formula_29">F t = [ϕ θ (x 1 ), · · · , ϕ θ (x nt )] ∈ F nt</formula><p>where ϕ θ is the learned embedding function, and then applying a partition algorithm. The partition algorithm can either be a standard clustering algorithm as described in <ref type="bibr" target="#b3">(Banerjee et al., 2005)</ref>, or can exploit the connection of our method with spectral clustering as explained in Section 3.4.</p><p>The spectral clustering (SC) algorithm that we use, which is inspired by , is illustrated in Algorithm 2: we first mean center F t (the mean of each column of the resulting matrix M t is zero), then we extract the matrix U t that contains the leading left-singular vectors of the resulting matrix M t , the rows of U t are then 2 -normalized and partitioned with the usual kmeans algorithm.</p><p>To test our method without spectral clustering, we 2 -normalize the output representations as done in <ref type="bibr" target="#b30">(Song et al., 2017)</ref> and then apply the usual kmeans algorithm with the squared Euclidean distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative results</head><p>We compare our method to current state-of-the-art metric learning approaches in <ref type="table" target="#tab_0">Tables 1 to 3</ref> by evaluating the Normalized Mutual Information (NMI) and Recall@K performances. In particular, Recall@1 is a useful metric in zeroshot learning contexts, it allows to assign the category of a test image to the category of its nearest neighbor, it then evaluates the generalization performance of a model to new similar concepts. The scores for the following baselines <ref type="bibr" target="#b25">(Schroff et al., 2015;</ref><ref type="bibr" target="#b29">Song et al., 2016;</ref><ref type="bibr" target="#b28">Sohn, 2016;</ref><ref type="bibr" target="#b30">Song et al., 2017)</ref> are reported from <ref type="bibr" target="#b30">(Song et al., 2017)</ref> where the methods are tested in a similar setup. As explained in the related work (Section 5), the NMI-based approach <ref type="bibr" target="#b30">(Song et al., 2017</ref>) is the only baseline that is explicitly learned to optimize a clustering criterion.</p><p>We also report the performance of the vanilla GoogLeNet/Inception features pretrained on ImageNet, and GoogLeNet features fine-tuned for classification with softmax regression. In both cases, we report the results obtained with Spectral Clustering (SC) as illustrated in Algorithm 2, and without SC (as explained in the last paragraph of Section 4.2). We report the scores for Recognition performance: Our spectral clustering approach obtains state-of-the-art Recall@K performance on all the datasets. Only the method in <ref type="bibr" target="#b30">(Song et al., 2017)</ref> obtains better NMI performance on Birds and Products, this can be expected as the model in <ref type="bibr" target="#b30">(Song et al., 2017)</ref> is specifically learned to optimize the NMI evaluation metric. Our choice to optimize the Frobenius norm which allows to compute a gradient in closed-form then seems to be a good trade-off for scalability as it obtains competitive NMI results and state-of-the-art Recall@K performance.   The usual kmeans algorithm applied on the representations of our learned model obtains worse results than our proposed spectral clustering (SC) approach. It obtains better performance than most baselines on Birds and Cars, but also poor results on the Products dataset. This may be explained by the fact that there are only 5.3 images per category in this dataset, which is 10 times less than for the other datasets. Some categories also contain only 2 images, which is hard to generalize on. Our approach is then more appropriate when the categories are large (i.e. more than 50 images) than when there are many (very) small clusters.</p><p>We also note that when our model updates only the last layer during fine-tuning, it obtains state-of-the-art performance on Birds and Cars, but not as good as our fully learned model on the Products dataset. The strong results on the former two is likely due to their small size (fewer than 10k training images); which makes learning all the layers prone to overfitting. Learning in all layers seems beneficial on larger datasets such as Products.</p><p>We observe that most baselines <ref type="bibr" target="#b25">(Schroff et al., 2015;</ref><ref type="bibr" target="#b28">Sohn, 2016;</ref><ref type="bibr" target="#b30">Song et al., 2017)</ref> and our spectral method obtain comparable results on the Products dataset. There is then not a clear way to learn deep models in contexts with small categories. On the other hand, there is a huge gap in Recall@K performance on the other datasets between approaches that optimize clustering and approaches that do not. The largest performance gap is observed on the Cars dataset which contains the largest number of images per category (more than 80).</p><p>It is also worth noting that unlike usual softmax regression for classification that promotes centroids to be one-hot vectors in the ideal case, our approach takes as input the current representations and tries to group similar examples together without fixing the desired centroids. It can then be easily combined with other approaches.</p><p>Training time: Once the matrix representation of the minibatch F ∈ F n has been computed, computing the gradient G takes 1 second (with n = 1280 and d = 100). Since we backpropagate our loss for all the examples in the minibatch, each iteration takes about 50 seconds due to the large architecture of Inception. Nevertheless, multiple GPUs can be used in parallel to speed up training.</p><p>Qualitative results: t-SNE <ref type="bibr" target="#b33">(Van Der Maaten, 2014)</ref> plots are available in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related work</head><p>As explained in Section 1, many approaches have been proposed to learn a similarity metric <ref type="bibr" target="#b38">(Xing et al., 2002;</ref><ref type="bibr">BarHillel et al., 2005;</ref><ref type="bibr" target="#b15">Lajugie et al., 2014;</ref><ref type="bibr" target="#b16">Law et al., 2016;</ref><ref type="bibr" target="#b18">2017b)</ref> or a nonlinear embedding function <ref type="bibr" target="#b25">(Schroff et al., 2015;</ref><ref type="bibr" target="#b30">Song et al., 2017)</ref> optimized to perform clustering. However, most of them belong to the semi-supervised setting <ref type="bibr" target="#b38">(Xing et al., 2002;</ref><ref type="bibr" target="#b4">Bar-Hillel et al., 2005;</ref><ref type="bibr" target="#b25">Schroff et al., 2015)</ref>, i.e. they are designed to learn from small sets of pairwise or triplet-wise relations and do not account for the global clustering performance on the training dataset. In pairwise approaches, the model is given pairs of examples (x i , x j ) which are either similar (e.g. the examples are in the same category) or dissimilar (e.g. the examples are in different categories), the model is then learned so that the distances between representations of similar objects are smaller than the distances between dissimilar objects.</p><p>In the triplet-wise approach <ref type="bibr" target="#b26">(Schultz &amp; Joachims, 2003;</ref><ref type="bibr" target="#b37">Weinberger et al., 2006)</ref>, triplets of examples</p><formula xml:id="formula_30">(x i , x + i , x − i )</formula><p>are provided and the model is learned so that the distance between the representations of the pair (x i , x + i ) is smaller than for the pair (x i , x − i ). We focus on the supervised clustering setting which considers all the possible similar and dissimilar pairs and takes into account the global clustering structure of the training dataset (here a mini-batch).</p><p>In the shallow metric learning literature, <ref type="bibr" target="#b15">(Lajugie et al., 2014;</ref><ref type="bibr" target="#b16">Law et al., 2016</ref>) learn a linear transformation in the supervised clustering setting so that the partition obtained when using kmeans on a dataset is as close as possible to the desired partition. However, they both consider that the data representation is fixed and are limited by the complexity of their linear model. Moreover, their solvers are very different from ours. <ref type="bibr" target="#b15">Lajugie et al. (2014)</ref> propose an extension of the structural SVM <ref type="bibr" target="#b32">(Tsochantaridis et al., 2005)</ref> for Mahalanobis distances, and their projected subgradient method thus has high complexity. <ref type="bibr" target="#b16">Law et al. (2016)</ref> propose a closed-form solver but the method is limited to the case where there is a single training dataset and they do not provide gradient-based strategies to optimize other types of models such as neural networks.</p><p>In the deep learning literature, <ref type="bibr" target="#b29">Song et al. (2016)</ref> proposed to approximate a loss function that considers all the positive and negative pairs. To this end, they iteratively randomly sample a few similar pairs, and then actively add their difficult neighbors to the training mini-batch. This idea is similar to the idea of active set of constraints used in <ref type="bibr" target="#b36">(Weinberger &amp; Saul, 2009;</ref><ref type="bibr" target="#b17">Law et al., 2017a)</ref> to limit the number of active constraints (i.e. the number of constraints that have nonzero subgradients) and be able to optimize over large numbers of triplets. Although their approach considers most of the similarity relations, it is not optimized to group all the similar examples into the same unique cluster. Indeed, each category can be divided in multiple subclusters as explained in <ref type="bibr" target="#b30">(Song et al., 2017)</ref>. <ref type="bibr" target="#b28">Sohn (2016)</ref> proposed to tackle the problem of slow convergence of triplet-wise approaches (caused by hard negative mining) by optimizing losses over (n + 1)-tuplets. In particular, an efficient batch construction method is proposed to require 2n examples instead of the naïve (n + 1)n to build n tuplets of length (n + 1). The loss function recruits multiple distances between dissimilar examples from different categories and approximates the ideal loss that minimizes the distances between similar examples while maximizing the distances between dissimilar examples. This approach considers the global structure of the representation of mini-batches better than triplet-wise approaches. However, although it does take into account most distances in the mini-batch, it does not explicitly optimize the model so that it obtains good clustering performance.</p><p>In the deep metric learning literature, the most similar approach to ours is <ref type="bibr" target="#b30">(Song et al., 2017)</ref>. They select for each category one unique example that will be the medoid (i.e. representative example). The choice of the medoids is not discussed and may be problematic if there is noise in the labels. In contrast, our centroids are learned so that they are the mean vectors of the training examples. Indeed, our set of centroids is written Z =Ŷ † F where F ∈ F n is the representation of our mini-batch andŶ is implicitly included in the formulation of the set C n,k . Our optimization problem then learns a data representation such that training examples are projected close to their respective centroids. Moreover, the gradient in <ref type="bibr" target="#b30">(Song et al., 2017)</ref> requires an iterative greedy algorithm and each iteration of their greedy algorithm has higher complexity than the complexity of computing our gradient (we set the dimensionality d of our learned representations to be k). Each iteration of the greedy algorithm is linear in the size of the mini-batch and cubic in the number of categories in the mini-batch.</p><p>Deep learning was also used in <ref type="bibr" target="#b13">(Ionescu et al., 2015)</ref> to learn a convolutional neural network optimized for clustering. However, it is applied to unsupervised image segmentation with normalized cuts <ref type="bibr" target="#b27">(Shi &amp; Malik, 2000)</ref>. We are interested in this paper in the supervised clustering setting where the ground truth partition is provided. Another deep clustering approach was proposed in <ref type="bibr" target="#b10">(Hershey et al., 2016)</ref>. However, their model is not optimized to be robust to the size of the different clusters as discussed in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented a novel deep learning approach optimized for the supervised clustering task. Our method is simple to implement and scalable thanks to its low algorithmic complexity. It also obtains state-of-the-art recall@K performance on different standard fine-grained datasets. Future work includes improving our proposed solver by exploiting the results in <ref type="bibr" target="#b13">(Ionescu et al., 2015)</ref> which take into account the structure of the neural network instead of using a standard stochastic gradient descent solver.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Our approach learns a nonlinear embedding function in a supervised way so that elements in the same category (here with the same color) are organized into the same cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>al. (2005) demonstrated that, for any value of Y ∈ Y n×k , the minimizer of z c in Eq. (1) is unique and is the mean vector of all the examples in F assigned to cluster c if and only if d φ is a Bregman divergence. In matrix form, this means that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>NMI and Recall@K evaluation on the Birds (CUB-200- 2011) dataset</figDesc><table>Method 
NMI 
R@1 
R@2 
R@4 
R@8 

Triplet s.h. (Schroff et al., 2015) 
55.38 
42.59 
55.03 
66.44 
77.23 
Lifted struct (Song et al., 2016) 
56.50 
43.57 
56.55 
68.59 
79.63 
Npairs (Sohn, 2016) 
57.24 
45.37 
58.41 
69.51 
79.49 
NMI-based (Song et al., 2017) 
59.23 
48.18 
61.44 
71.83 
81.92 

Vanilla GoogLeNet (with SC) 
53.53 
42.56 
55.55 
67.86 
78.39 
Vanilla GoogLeNet (without SC) 
50.28 
40.11 
53.17 
66.02 
76.59 
Softmax regression (with SC) 
55.91 
45.49 
58.64 
70.65 
79.17 
Softmax regression (without SC) 
55.74 
46.00 
58.03 
69.32 
78.28 

Ours (end-to-end/with SC) 
58.12 
49.78 
62.56 
73.55 
82.78 
Ours (end-to-end/without SC) 
56.99 
47.57 
59.66 
71.57 
81.28 
Ours (last layer/with SC) 
59.16 
53.22 
66.09 
76.70 
85.33 
Ours (last layer/without SC) 
56.87 
50.08 
62.24 
73.38 
82.38 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>NMI and Recall@K evaluation on the Cars196 dataset</figDesc><table>Method 
NMI 
R@1 
R@2 
R@4 
R@8 

Triplet s.h. (Schroff et al., 2015) 
53.35 
51.54 
63.78 
73.52 
82.41 
Lifted struct (Song et al., 2016) 
56.88 
52.98 
65.70 
76.01 
84.27 
Npairs (Sohn, 2016) 
57.79 
53.90 
66.76 
77.75 
86.35 
NMI-based (Song et al., 2017) 
59.04 
58.11 
70.64 
80.27 
87.81 

Vanilla GoogLeNet (with SC) 
44.53 
35.37 
47.32 
60.60 
71.69 
Vanilla GoogLeNet (without SC) 
47.99 
35.56 
47.27 
59.37 
72.16 
Softmax regression (with SC) 
53.13 
50.11 
60.49 
71.68 
80.14 
Softmax regression (without SC) 
52.13 
48.75 
58.52 
70.97 
78.37 

Ours (end-to-end/with SC) 
58.04 
59.37 
71.25 
80.62 
88.32 
Ours (end-to-end/without SC) 
56.08 
57.08 
69.23 
79.39 
87.46 
Ours (last layer/with SC) 
64.25 
73.07 
82.19 
89.01 
92.99 
Ours (last layer/without SC) 
61.12 
67.54 
77.77 
85.74 
90.95 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 .</head><label>3</label><figDesc>NMI and Recall@K evaluation on the Products (Stan- ford Online Products) dataset</figDesc><table>Method 
NMI 
R@1 
R@10 
R@100 

Triplet s.h. (Schroff et al., 2015) 
89.46 
66.67 
82.39 
91.85 
Lifted struct (Song et al., 2016) 
88.65 
62.46 
80.81 
91.93 
Npairs (Sohn, 2016) 
89.37 
66.41 
83.24 
93.00 
NMI-based (Song et al., 2017) 
89.48 
67.02 
83.65 
93.23 

Vanilla GoogLeNet (with SC) 
56.01 
44.09 
61.32 
77.82 
Vanilla GoogLeNet (without SC) 
55.32 
43.73 
60.84 
76.54 
Softmax regression (with SC) 
63.53 
51.95 
69.83 
85.36 
Softmax regression (without SC) 
63.10 
51.65 
69.75 
85.32 

Ours (end-to-end/with SC) 
89.40 
67.59 
83.71 
93.25 
Ours (end-to-end/without SC) 
88.70 
64.52 
81.53 
92.35 
Ours (last layer/with SC) 
88.75 
65.30 
81.50 
92.40 
Ours (last layer/without SC) 
86.95 
64.90 
78.05 
91.50 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In our experiments, since the number of rows of F is greater than its number of colums, F has full column rank.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For instance, our model obtains more than 90% performance for all the evaluation metrics on the training categories after 1000 iterations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">-normalized features as they obtain better performance than unnormalized features in our experiments.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: We thank David Duvenaud, Stavros Tsogkas, Dimitris Vlitas and the anonymous reviewers for their helpful comments. This work was supported by Samsung and the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PC00003. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eugene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhifeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Craig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthieu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Np-hardness of euclidean sum-of-squares clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Aloise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preyas</forename><surname>Popat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="245" to="248" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="305" to="312" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Clustering with bregman divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Merugu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srujana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1705" to="1749" />
			<date type="published" when="2005-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a Mahalanobis metric from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aharon</forename><surname>Bar-Hillel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tomer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shental</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphna</forename><surname>Weinshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="937" to="965" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speech coding based upon vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrés</forename><surname>Buzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Markel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="562" to="574" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sumit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A divisive information-theoretic feature clustering algorithm for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subramanyam</forename><surname>Mallela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1265" to="1287" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On a theorem of weyl concerning eigenvalues of linear transformations i</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ky</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences of the United States of America</title>
		<meeting>the National Academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="1949" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">652</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The differentiation of pseudo-inverses and nonlinear least squares problems whose variables separate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gene</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Pereyra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on numerical analysis</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="413" to="432" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Comparing partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phipps</forename><surname>Arabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Matrix backpropagation for deep networks with structured layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orestis</forename><surname>Vantzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2965" to="2973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large margin metric learning for constrained partitioning problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arlot</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Closed-form training of mahalanobis distance for supervised clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yaoliang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poe</surname></persName>
		</author>
		<editor>CVPR. IEEE</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning a distance metric from relative comparisons between quadruplets of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="doi">10.1007/s11263-016-0923-4</idno>
		<idno>1573-1405. doi: 10.1007/ s11263-016-0923-4</idno>
		<ptr target="http://dx.doi.org/10.1007/s11263-016-0923-4" />
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="94" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient multiple instance metric learning using weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yaoliang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Introduction to information retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2008" />
			<publisher>Cambridge university press Cambridge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sided and symmetrized bregman centroids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2882" to="2904" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Approximating k-means-type clustering via semidefinite programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="186" to="205" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanjeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhiheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning a distance metric from relative comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multiclass n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1849" to="1857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4004" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep metric learning via facility location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stefanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yangqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumitru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thorsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1453" to="1484" />
			<date type="published" when="2005-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Accelerating t-sne using treebased algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrike</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Distance metric learning for large margin nearest neighbor classification. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Saul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">1473</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distance metric learning with application to clustering with side-information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spectral relaxation for k-means clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hongyuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaofeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1057" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
