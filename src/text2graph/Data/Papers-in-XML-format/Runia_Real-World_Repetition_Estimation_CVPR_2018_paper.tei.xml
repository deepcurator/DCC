<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:13+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-World Repetition Estimation by Div, Grad and Curl</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">F H</forename><surname>Runia</surname></persName>
							<email>runia@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="laboratory">QUVA Deep Vision Lab</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cees</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
							<email>cgmsnoek@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="laboratory">QUVA Deep Vision Lab</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
							<email>a.w.m.smeulders@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="laboratory">QUVA Deep Vision Lab</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Real-World Repetition Estimation by Div, Grad and Curl</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We consider the problem of estimating repetition in video, such as performing push-ups, cutting   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual repetition is ubiquitous in the world around us. It is present in activities like rowing, music-making and cooking. It arises in natural and urban environments: traffic patterns, blinking lights, and leaves in the wind. Rhythm and repetition are used to approximate velocity, estimate progress and to trigger attention <ref type="bibr" target="#b12">[13]</ref>. In computer vision, understanding repetition in video is important as it can serve action classification <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref>, action localization <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24]</ref>, human motion analysis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref>, 3D reconstruction <ref type="bibr" target="#b2">[3]</ref> and camera calibration <ref type="bibr" target="#b11">[12]</ref>. Estimating repetition remains challenging. First and foremost, repetition appears in many forms due to its variety in motion pattern and motion continuity. The viewpoint is crucial for the perception of recurrence. In practice, camera motion makes repetition estimation inevitably hard.</p><p>Existing work on repetition estimation in video <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref> reports good results under the assumption that the motion is well-localized (static) and strongly periodic (stationary). In short, existing work focuses on video that is static in every aspect of repetition. As real life is more complex, our method relies on motion foreground segmentation to localize the salient motion and handle non-static video. Furthermore, we found fixed-period Fourier analysis <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> to be unsuitable for repetition estimation in real-world video as nonstationarity often appears. To permit non-stationary video dynamics, we adopt the wavelet transform for decomposing video signals into a time-frequency spectrum.</p><p>We reconsider the theory of repetition <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b7">8]</ref> starting from the divergence, gradient and curl operators acting on the 3D flow field. We derive three motion types and three motion continuities. What follows are 3 × 3 fundamental cases of intrinsic periodicity in 3D. For the 2D perception of 3D intrinsic periodicity, the observer's viewpoint can be somewhere in the continuous range between two viewpoint extremes. Ultimately, we distinguish 18 fundamental cases for the 2D perception of 3D intrinsic periodic motion.</p><p>The contributions of our work are the following.</p><p>(1) Starting from the first principles of 3D periodicity and its perception in 2D, we derive 18 fundamentally different cases of repetitive perception. <ref type="bibr" target="#b1">(2)</ref> To estimate repetition in video under realistic circumstances, we compute a diverse flow-based representation over the motion foreground segmentation. Our method uses wavelets to handle non-stationary motion and automatically selects the most discriminative signal based on self-estimated quality assessment. (3) Extending beyond the video dataset of <ref type="bibr" target="#b14">[15]</ref>, we propose the new QUVA Repetition dataset for repetition estimation, that is more realistic and challenging by lifting the static and stationary assumptions. (4) We evaluate on the task of repetition counting and show that our method outperforms the deep learning-based state-of-the-art <ref type="bibr" target="#b14">[15]</ref> on the new dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Existing approaches for repetition estimation in video typically represent video as one-dimensional signals that preserve the repetitive structure of the motion. Then, frequency information is extracted by Fourier analysis <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30]</ref>, peak detection <ref type="bibr" target="#b27">[28]</ref> or singular value decomposition <ref type="bibr" target="#b5">[6]</ref>. Pogalin et al. <ref type="bibr" target="#b18">[19]</ref> estimate the frequency of motion in video by tracking an object, performing principal component analysis over the tracked regions and employing the Fourier-based periodogram. However, methods relying on Fourier-analysis for periodic motion are unable, nor intended, to handle non-stationary motion as is ubiquitous in the real world.</p><p>Briassouli &amp; Ahuja <ref type="bibr" target="#b3">[4]</ref> employ time-frequency analysis using the Short Time Fourier Transform for dealing with multiple periodic motions. In <ref type="bibr" target="#b4">[5]</ref>, the authors propose a spatiotemporal filter bank for estimating repetition in video. Their filters work online and are effective when tuned correctly. However, we question its practical use, as their experiment are limited to stationary motion and the filter bank requires manual tuning. We also use a time-frequency decomposition of signals from video, but concentrate on handling non-stationary repetition. Instead of using the Short-Time Fourier Transform, we adopt the continuous wavelet transform to achieve better resolution <ref type="bibr" target="#b22">[23]</ref>.</p><p>The studies on periodic motion by <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26]</ref> have encouraged us to reconsider visual repetition. Pogalin et al. <ref type="bibr" target="#b18">[19]</ref> identify four visually periodic motion types (translation, rotation, deformation and intensity variation) supplemented with three cases of motion continuity (oscillating, constant and intermittent) in the 2D field of view. In this work, we argue that the 3D flow field is the right starting point to derive the foundations of repetition. From the 3D flow field and the differential operators acting on it, we derive three motion types and three motion continuities that organize into a 3 × 3 Cartesian table. Moreover, the projection of 3D periodicity on 2D perception has to consider the viewpoint. What follows are 18 fundamentally different cases of 2D repetitive perception from 3D periodicity.</p><p>Levy &amp; Wolf <ref type="bibr" target="#b14">[15]</ref> introduce a convolutional neural network for estimating repetition by counting in live video. Their network is trained to predict the motion period on synthetic video sequences in which moving squares exhibit periodic motion of four motion types from <ref type="bibr" target="#b18">[19]</ref>. At test time, the method takes a stack of video frames, computes a region of interest by motion thresholding, and forwards the frame crops through the network to classify the motion period. The system is evaluated on the task of repetition counting and shows near-perfect performance on their YTSegments dataset. The 100 videos are a good initial set of examples but as the majority of videos have static viewpoint and exhibit stationary periodic repetitions, we propose a new dataset. Our dataset better reflects reality by including more non-static and non-stationary examples. Similar to Levy &amp; Wolf, we also evaluate repetition estimation by counting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Theory</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D Intrinsic Periodicity</head><p>In 3D, intrinsic periodicity is defined as the reappearing of the same 3D-flow F (x, t) induced by the motion of an object over time. For a moment in time t, we denote the flow by F t (x). The 3D-flow field tied to the object is periodic as expressed by F t (x) = F t+T (x + S), where we exclude for the moment the trivial case that the flow field is constant. The parameter T is the period over time, where S is the period, if any, over space.</p><p>Let the flow field be given by its directional components:</p><formula xml:id="formula_0">F t = (F x , F y , F z )</formula><p>. From differential geometry, we have the three operators on the flow field:</p><formula xml:id="formula_1">∇F t = ∂F k ∂ x j e j ⊗ e k (1) ∇ · F t = ∂F x ∂ x + ∂F y ∂ y + ∂F z ∂z (2) ∇ × F t = ∂F z ∂ y − ∂F y ∂z , ∂F x ∂z − ∂F z ∂ x , ∂F y ∂ x − ∂F x ∂ y .<label>(3)</label></formula><p>Where in Eq. (1) the product e j ⊗ e k defines a dyadic tensor, and indices are summed over the 9 terms by the Einstein convention <ref type="bibr" target="#b26">[27]</ref>. The equations define the gradient, divergence and curl of the flow field <ref type="bibr" target="#b24">[25]</ref>. Three basic 3D-motion types emerge depending on the values of divergence and curl as follows:</p><formula xml:id="formula_2">translation: ∇ × F t = 0, ∇ · F t = 0 rotation: ∇ × F t 0, ∇ · F t = 0 expansion: ∇ × F t = 0, ∇ · F t 0.</formula><p>In practice there may be a mixture types; as we are aiming to handle realistic video, we select the dominant 3D-periodicity in the object's motion whichever is measurable best. In the rare case of counterbalancing expansion and contraction over different axes, it can be that ∇ · F t = 0 while being periodic.  In addition, the motion continuity in 3D can be a source of periodicity. Depending on the type of motion, the motion field needs fulfill one of the following necessary periodic conditions:</p><formula xml:id="formula_3">∇F t (x) = ∇F t+T (x + ǫ ) ∇ × F t (x) = ∇ × F t+T (x + ǫ ) ∇ · F t (x) = ∇ · F t+T (x + ǫ ),</formula><p>where ǫ denotes a translation as the object's periodicity may be superposed on translation. For robustness to illumination changes, the measurement of ∇F t (x) is preferred over F t . From these equations three different periodic motion continuities can be distinguished: constant, intermittent and oscillating periodicity. Again, in practice the motion continuity may be a mixture between types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">2D Recurrence of 3D Intrinsic Periodicity</head><p>So far we have considered the intrinsic periodicity in 3D. We reserve the term recurrent for the 2D observation of the 3D periodicity. Recurrence in the field of view is defined by:</p><formula xml:id="formula_4">F t (x) = F t+T (σ(x + s)),<label>(4)</label></formula><p>where F t (x) is perceived flow in 2D image coordinates x, s is the observed displacement, T is the recurrence and σ denotes the observational scale (camera zoom). The underlying principle is that the same period length T will be observed in both 3D and 2D for all cases of intrinsic periodicity. As we perform all measurements within one image, from here on F(x) implies F t (x) where subscript t is omitted for clarity.</p><p>In addition, the intrinsic periodicity in 3D does not cover all perceived recurrence in an image sequence. For the trivial cases of constant translation and constant expansion in 3D, perceived recurrence will appear when a repetitive chain of objects (conveyor belt) or a repetitive appearance (checkered balloon) on the object, as given by <ref type="bibr">Equation 4</ref>, is aligned with the motion. In such cases, recurrence will also be observed in the field of view. For constant rotation, the restriction is that the appearance cannot be constant over the surface, as no motion, let alone recurrent motion would be observed. In the rotational case, any rotational symmetry in appearance will induce a higher order recurrence as a multiplication of the symmetry and the rotational speed.</p><p>For the purpose of recurrence, nine cases organize in a 3 × 3 Cartesian table of basic motion type times motion continuity, see <ref type="figure" target="#fig_2">Figure 2a</ref>. The corresponding examples of these nine cases are given in <ref type="figure" target="#fig_2">Figure 2b</ref>. This is the list of fundamental cases, where a mixture of types is permitted. In practice, some cases are ubiquitous, while for others it is hard to find examples at all and a mixture of types is rare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The Viewpoint</head><p>The point of view has a large influence on the perception of the flow field. There are two fundamentally different viewpoints: the frontal view and the side view:</p><p>frontal view: on the main axis of motion side view: perpendicular to the main axis of motion.</p><p>For translation there is one main axis and two perpendicular axes, which are both identical for our purpose. There is no distinction between the two perpendicular views. Similarly, for rotation the two perpendicular cases are also indistinguishable. For expansion there are one, two or three axes of expansion, again leaving us with the frontal case and the perpendicular case as the two fundamental cases. Consequently, for all cases considered, a distinction between frontal view and side view is sufficient. As a result, the perceived recurrence is summarized between the two extreme viewpoints, which results in the Cartesian product of two times nine basic cases as summarized in <ref type="figure" target="#fig_3">Figure 3</ref>. The two views are the end of a continuous range of viewpoints. An actual viewpoint will be somewhere in between the frontal view and the side view, most of the time. This leaves the flow field asymmetrical or skewed, either in gradient, curl or divergence. As long as the signal can be measured this will not affect the recurrent nature of the signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Non-Static Repetition</head><p>So far we have assumed a static camera position. In particular with recurrent motion (1) the camera may move itself because the camera is mounted on the moving object itself, or (2) the camera is following the target of interest, or (3) the camera is in motion independent of the motion of the object. For the first two cases, the camera motion reflects the periodic dynamics of the object's motion. The flow field may be outside the object, but otherwise it displays a complementary pattern in the flow field.</p><p>Only the third case demands removal of the camera motion prior to the repetitive motion analysis. In practice, this situation occurs frequently. Therefore, particular attention needs to be paid to camera motion independent of the target's motion. When due to the camera motion, the viewpoint changes from frontal to side view, the analysis will be inevitably hard. <ref type="figure" target="#fig_3">Figure 3</ref> illustrates the dramatic changes in the flow field when the camera changes from one extreme viewpoint (side) to the other (frontal), or vice versa.</p><p>In addition, even when object motion and camera are both static, for none of the intrinsic motion types (translation, rotation, expansion), a point on the object will be at the same position in the camera field all the time. Under the double static condition, a point will just return to the same point on the camera field. As the intermediate points on the object or background have an arbitrary albedo and radiate an arbitrary luminance, no sinusoidal signal will result in general. This is noteworthy as all previous work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref> implicitly assumed such a signal by considering the Fourier transform or variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Non-Stationary Repetition</head><p>A recurrent signal is said to be stationary when the period length is constant over time. In the initial steps of periodicity analysis, it was assumed the periodic signal was near-stationary. In practice, we have observed that stationary repetitive signals are relatively rare. Decay in frequency or accelerating motion are common in realistic video. Therefore, in contrast to <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref> we do not assume stationarity, making the method more robust to acceleration. We will employ local wavelets in response to the anticipated signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Repetition Estimation</head><p>Our method for repetition estimation follows a threestage approach <ref type="figure">(Figure 4)</ref>. First, we localize the target instance in the scene, then we represent the target by a set of time-varying signals and finally we perform time-frequency r <ref type="figure">Figure 4</ref>. Overview of our method by illustration of an example. First we segment the foreground motion (top row, blue segments) followed by optical flow computation (yellow arrows), then we extract zeroth-and first-order flow signals (4 out of 6 shown) and finally decompose them into a time-frequency spectrum using the continuous wavelet transform (bottom). In the bottom row, the dashed black lines denote the min-cost path whereas the orange lines indicate the maximum power path for counting by integration. Note that for this oscillatory translation frontal view case, ∇ x F x , ∇ y F y and ∇ · F give a good signal, as expected, whereas ∇ × F gives a poor and dispersed signal with heavy cost.</p><note type="other">x F time Amplitude Scale (log scale) r y F Time Time Time Time r · F r×F</note><p>decomposition to estimate repetition and select the most discriminative signal.</p><p>Signals from Video. To deal with camera motion and to handle the wide variety in repetitions, we construct a diverse set of time-varying flow-based signals that we compute over the motion foreground segmentation. Specifically, we measure the average-pooled flow field F = (F x , F y ) and the differentials of the flow. We estimate ∇F by measuring ∇ x F x and ∇ y F y . All the differentials of the flow field are computed using Gaussian derivative filters with a large filter size to obtain a global measurement over the foreground segmentation. The final measurement is the average-pooled value over a small radius around the object's center. The differential operators of the flow field comprise four different measurements (as the curl has only one direction perpendicular to the screen), whereas there are two zeroth-order flow signals. In total these amount to six different signals.</p><p>For the cases of oscillating and intermittent motion observed from the side, ∇F will deliver the strongest repetitive signal. The flow field F will convey a stronger repetitive signal for the cases of constant motion appearance. In practice, it may be hard to select the most discriminative signal, to which we return at the end of this section.</p><p>Time-Frequency Decomposition. Given a discrete signal h n for timesteps n = 1, . . . , N − 1 sampled at equally spaced intervals δt. Let ψ 0 (η) be some admissible wavelet function, depending on the non-dimensional time parameter η. The continuous wavelet transform <ref type="bibr" target="#b9">[10]</ref> is defined as the convolution of h n with a "daughter" wavelet generated by scaling and translating the wavelet function ψ 0 (η):</p><formula xml:id="formula_5">W n (s) = N −1 n ′ =0 h n ′ ψ * (n ′ − n)δt s ,<label>(5)</label></formula><p>where the asterisk represents the complex conjugate. By varying time parameter n and the scale parameter s, the wavelet transform can generate a time-scale representation describing how the amplitude of the signal changes with time and scale. While formally a time-scale representation, it can also be considered a time-frequency representation since the wavelet scale is directly related to the Fourier frequency <ref type="bibr" target="#b28">[29]</ref>. We use the Morlet wavelet, a complex exponential carrier modulated by a Gaussian envelope:</p><formula xml:id="formula_6">ψ 0 (η) = π −1/4 e iω 0 η e η 2 /2 .<label>(6)</label></formula><p>Since the Morlet wavelet is complex, the wavelet transform W n (s) is also complex. Therefore, it is useful to define the wavelet power spectrum or scalogram as |W n (s)| 2 representing the time-frequency localized energy. The 2D representation can reveal the signal's non-stationary repetitive dynamics. Once the wavelet is chosen, what remains is defining the resolution of the time-frequency spectrum |W n (s)| 2 by specifying scales s. In practice, a logarithmic scaling is effective <ref type="bibr" target="#b28">[29]</ref>: s j = s 0 2 jδ j with j = 0, 1, . . . , J.</p><p>The smallest measurable scale s 0 and the number of scales J determine the range of the frequency resolution.</p><p>To estimate non-stationary repetitions in a given video, we decompose the six signals into a time-frequency spectrum using the continuous wavelet transform. What follows are six 2D time-frequency representations that enable further analysis of the repetitive contents of the video. Counting. We assume there is only one dominant repetitive motion observable in the wavelet spectrum; this is reasonable as the foreground motion segmentation encourages temporal consistency. Selecting the modulus maximum from the wavelet spectrum |W n (s)| 2 for every timestep n gives a local frequency measurement of approximately s −1 for a Morlet wavelet. Our method integrates local frequencies over time to estimate the repetition count: c = n δt/s n . For a stationary periodic signal the modulus maximum forms a horizontal ridge through time. We emphasize the ability to count non-stationary signals using our approach since the local frequency may change over time. Therefore, our method is able to deal with accelerations or transient phenomena.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Min-Cost Signal Selection.</head><p>The question that remains is selecting the most discriminative signal out of the six. We propose a selection mechanism that prioritizes signals with local regularity in the time-frequency space. Specifically, we adopt a min-cost algorithm for finding the optimal path through the time-frequency space. We turn the wavelet power into a cost surface for optimization by simply inverting it: 1/|W n (s)| 2 . Traversing over a high-power region translates to low cost. As our goal is to characterize a signal by one cost measure, we run a greedy min-cost pathfinding algorithm to assess the minimum cost required to traverse the spectrum through time. Consequently, the algorithm assigns a lower cost to paths with high local regularity. This is appealing as realistic video signals can be non-stationary but locally smooth. To make a final prediction we select the signal with minimum cost and its corresponding repetition count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Datasets, Evaluation and Implementation</head><p>Motivated by the observation that the YTSegments <ref type="bibr" target="#b14">[15]</ref> dataset for visual repetition estimation is limited in terms of its complexity, we present a new dataset that is more difficult in scene complexity, repetitive appearance and cycle length variation. Our code and data will be made available1.</p><p>1https://tomrunia.github.io/projects/repetition <ref type="table">Table 1</ref>. Dataset statistics of YTSegments <ref type="bibr" target="#b14">[15]</ref> and QUVA Repetition. The cycle length variation is the average value of the absolute difference between minimum and maximum cycle length divided by the average cycle length. For this, we annotated all individual cycle bounds in both datasets. The last two rows are also obtained by manual annotation. Note that our dataset is more realistic and challenging in terms of cycle length variability, camera motion and motion complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YTSegments QUVA Repetition</head><p>Number of Videos 100 100 Duration (s) 14.9 ± 9.8 17.6 ± 13.3 Count Avg.± Std.</p><p>10.8 ± 6.5 12. QUVA Repetition consists of 100 videos displaying a wide variety of repetitive video dynamics, including swimming, stirring, cutting, combing and music-making. The untrimmed videos are collected from YouTube. We asked two human annotators to label the temporal bounds of each interval containing at least four unambiguous repetitions. We found high inter-agreement between the annotators and keep the 100 intervals with the highest overlap to increase clarity. Final intervals are obtained by taking the intersection of the two temporal annotations. Next, we ask the annotators to label the repetition count and the temporal bounds of each cycle. <ref type="figure" target="#fig_4">Figure 5</ref> shows a few video examples along with their annotation. In <ref type="table">Table 1</ref> we compare the characteristics of our dataset to the YTSegments <ref type="bibr" target="#b14">[15]</ref>. Our videos have more variability in cycle length, motion appearance, camera motion and background clutter. By increasing difficulty in both scene complexity and temporal dynamics, our dataset represents a more realistic and challenging benchmark for estimating repetition in video.</p><p>Count Evaluation. Given a set of N videos, we evaluate the performance between ground truth count c i and the count prediction c i for i ∈ {1, . . . , N }. We report the mean absolute error following prior work <ref type="bibr" target="#b14">[15]</ref>:</p><formula xml:id="formula_7">MAE = 1 N N i=1 c i − c i /c i .</formula><p>We also record the off-by-one accuracy (OBOA) or count within-1 accuracy.</p><p>Implementation. We use the motion segmentation of Papazoglou and Ferrari <ref type="bibr" target="#b17">[18]</ref>. To account for incorrect segmentation masks we reuse the segmentation of the previous frame if the fraction of foreground pixels is less than 1% of the entire frame. To compute the dense flow field we rely on EpicFlow <ref type="bibr" target="#b21">[22]</ref>. We compute the divergence and curl by first-order Gaussian derivative filters with a 13 × 13 filter size. We use a Morlet wavelet with logarithmic scales (δ j = 0.125, s 0 = 2δt) based on <ref type="bibr" target="#b28">[29]</ref> in all experiments. We limit the range of J corresponding to a minimum of four repetitions in Baselines. We choose the method of Pogalin et al. <ref type="bibr" target="#b18">[19]</ref> to represent the class of Fourier-based methods for repetition estimation. Our reimplementation uses a more recent object tracker <ref type="bibr" target="#b10">[11]</ref> but is identical otherwise. The tracker is initialized by manually drawing a box on the first frame. Converting the frequency to a count is trivial using the video length and frame rate. Additionally, we compare with the deep-learning method of Levy &amp; Wolf <ref type="bibr" target="#b14">[15]</ref> using their publicly available code and pretrained model without any modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Fourier versus Wavelets</head><p>Setup. We first compare the Fourier-based periodogram with a wavelet-based time-frequency representation for counting the number of repetitions in each signal. To assess this, we generate idealized signals by plotting sinusoidals through the individual cycle bound annotations for every video in our QUVA Repetition dataset. From the periodogram we detect the maximum peak and convert its corresponding frequency to a count using the video's duration. <ref type="figure" target="#fig_5">Figure 6</ref> it is clear that waveletbased counting outperforms the periodogram on idealized signals. We also add a significant amount of Gaussian noise (σ = 0.5) to the signals which has a minor negative effect on both methods (data not shown). We observe that increased cycle length variation negatively affects Fourierbased counting. This is expected as it globally measures frequency and is unable to deal with non-stationarity. As wavelets naturally handle non-stationary repetition they are less sensitive to cycle length variability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. From the results in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Value of Diverse Signals</head><p>Setup. As wavelets prove to be effective for the counting task, we now assess the value of a diverse signal representation. The set of six signals that we verify comprises:</p><formula xml:id="formula_8">F x , F y , ∇ x F x , ∇ y F y , ∇ · F, ∇ × F.</formula><p>These are measured over the foreground segmentation and evaluated for individual performance. Again, we test repetition counting on our QUVA Repetition dataset. To obtain a lower-bound on the error, we select the best signal per video in an oracle fashion. <ref type="table" target="#tab_1">Table 2</ref> reveal that for the wide variability of repetitive appearance there is no one size fits all solution. The individual signals are unable to handle all variety of repetitive appearances by themselves, but their joint diversity results in a good lower-bound. The vertical flow F y is best overall and selected more often than the others by the oracle. We explain this bias towards vertical flow by the observation that our dataset contains many sports videos in which the gravity is often used as opposing force. Repeating this experiment on the YTSegments dataset with oracle signal selection achieves an MAE of 4.2 ± 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. The results in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Video Acceleration Sensitivity</head><p>Setup. In this experiment we examine our method's sensitivity to acceleration by artificially speeding-up videos.</p><p>Starting from the YTSegments dataset, we induce significant non-stationarity by artificially accelerating the videos halfway. Specifically, we modify the videos such that after the midpoint frame, the speed is increased by dropping every second frame. What follows are 100 videos with a 2× acceleration starting halfway. We compare against <ref type="bibr" target="#b14">[15]</ref> which handles non-stationarity by predicting the period of motion in sliding-window fashion over the video. This experiment omits Fourier-based analysis, as by its nature, it will inevitably fail on this task.  <ref type="bibr" target="#b14">[15]</ref> has difficulty dealing with non-stationary acceleration, whereas our method suffers less.</p><p>Results. <ref type="figure" target="#fig_6">Figure 7</ref> presents the MAE in both original and accelerated setting. On their own dataset, the system of Levy &amp; Wolf <ref type="bibr" target="#b14">[15]</ref> excels. Acceleration changes the results as our method suffers less and obtains a lower MAE on the accelerated videos. This reveals their sensitivity to acceleration, whereas our method deteriorates less.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Comparison State-of-the-Art</head><p>Setup. We carry out a full count comparison with the methods of Pogalin et al. <ref type="bibr" target="#b18">[19]</ref> and Levy &amp; Wolf <ref type="bibr" target="#b14">[15]</ref> on both datasets. Our method uses fixed parameters in all cases and utilizes the min-cost signal selection algorithm to pick the most discriminative signal.</p><p>Results. The outcome of the final experiment is presented in <ref type="table" target="#tab_2">Table 3</ref>. For the YTSegments dataset, the method of <ref type="bibr" target="#b14">[15]</ref> performs best with an MAE of 6.5, where our method scores 10.3, better than the Fourier-based approach of <ref type="bibr" target="#b18">[19]</ref>. The results change when considering the more realistic and challenging QUVA Repetition dataset. The method of <ref type="bibr" target="#b14">[15]</ref> performs the worst, with an MAE of 48.2, which we attribute to the fact that their network only considers four motion types during training. The Fourier-based method of <ref type="bibr" target="#b18">[19]</ref> scores an MAE of 38.5, whereas we obtain an error of 23.2. Overall our method is better able to handle the non-static and non-stationary video characteristics in our QUVA Repetition dataset while still performing reasonably well on the videos from YTSegments. We highlight three examples of our method in <ref type="figure" target="#fig_7">Figure 8</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have categorized 3D intrinsic periodic motion as translation, rotation or expansion depending on the divergence and curl of the flow field. Analysis of the time-varying flow gradient distinguishes three motion continuities: constant, intermittent or oscillatory. For the 2D perception of 3D periodicity, two viewpoint extremes are considered. What follows is the categorization of 18 fundamental cases of recurrent perception derived from the differential operators acting on the flow field. The use of the differentials extends beyond theory, as our experiments demonstrate that measuring flow-based signals over the motion foreground segmentation is effective for recurrence estimation in realistic video. We show that our method improves the state-of-the-art and effectively handles complex appearances, camera motion and non-stationarity on a realistic video dataset.</p><p>Scale (log scale) The rower accelerates in the beginning of the video, which appears in the wavelet spectrum of signal F x . Integrating over the max power path results in a repetition count of 31 whereas the true count is 30. Our method effectively handles the acceleration. (b) Stationary periodic motion superposed on translation. The video's repetitive nature is evident from the F y signal. We predict a repetition count of 18.5 whereas the true count is 18. (c) Change of viewpoint from side to front makes this video inevitably hard. Our method is unable to extract a good signal from the video. Note the partial continuity in the spectrum for ∇ × F but distorted by the viewpoint changes. Our method predicts a repetition count of 14.4 whereas the true count is 16.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Four examples of visual repetition under realistic circumstances. The first two rows show oscillatory translation under two different viewpoints. Similarly for constant rotation in the bottom rows. The abstraction on the left symbolizes the perceived flow in 2D, to be detailed in Section 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. 3 × 3 Cartesian table of the motion type times the motion continuity. Following from the differential operators acting on the flow, these are the basic cases of periodicity in 3D. The examples are: escalator, leaping frog, bouncing ball, pirouette, tightening a bolt, laundry machine, inflating a tire, inflating a balloon and a breathing anemone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Observed flow: the 18 fundamental cases for 2D perception of 3D recurrence. The perception follows from the motion pattern (3×), motion continuity (3×) and the viewpoint on the continuous interval between the two extremes: side and front view. ↑ denotes flow direction, denotes a vanishing point, • denotes a rotation point, ⋆ denotes expansion point. Dashed grey lines for constant motion indicate the need for texture to perceive recurrence. Pairs 4-16, 5-17 and 6-18 appear similar at first sight but vary in their signal profile.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Examples from the QUVA Repetition dataset. The timeline with markers illustrate the individual cycle bound annotations, that together determine the final repetition count. Note the diversity in motion appearance and cycle length variability within a video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Fourier-versus wavelet-based repetition counting on idealized signals for the videos from the QUVA Repetition dataset. Our wavelet-based method outperforms a Fourier-based baseline for 83 out of 100 videos. High cycle length variation results in notable error for Fourier measurements, whereas the time-localized wavelets are less sensitive to non-stationary repetition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. The effect of acceleration on the YTSegments dataset. The deep learning method of Levy &amp; Wolf [15] has difficulty dealing with non-stationary acceleration, whereas our method suffers less.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Results of our method for 3 video examples. (a) The rower accelerates in the beginning of the video, which appears in the wavelet spectrum of signal F x . Integrating over the max power path results in a repetition count of 31 whereas the true count is 30. Our method effectively handles the acceleration. (b) Stationary periodic motion superposed on translation. The video's repetitive nature is evident from the F y signal. We predict a repetition count of 18.5 whereas the true count is 18. (c) Change of viewpoint from side to front makes this video inevitably hard. Our method is unable to extract a good signal from the video. Note the partial continuity in the spectrum for ∇ × F but distorted by the viewpoint changes. Our method predicts a repetition count of 14.4 whereas the true count is 16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Value of diversity in six flow-based signals on videos from our QUVA Repetition dataset. The last column denotes how often each signal is selected by the oracle. While the individual signals struggle to obtain good performance by themselves, exploiting their joint diversity is beneficial.</figDesc><table>MAE OBOA # Selected 

∇ · F 
44.9 ± 34.8 
0.35 
8 
∇ × F 
44.9 ± 34.8 
0.42 
14 
∇ x F x 
46.7 ± 30.8 
0.24 
12 
∇ y F y 
42.7 ± 39.8 
0.33 
13 
F x 
38.3 ± 31.4 
0.40 
19 
F y 
32.9 ± 31.4 
0.52 
34 

Oracle Best 10.5 ± 15.7 
0.81 
100 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Comparison with the state-of-the-art on repetition counting for YTSegments and QUVA Repetition. The deep learning-based method of Levy &amp; Wolf achieves good results on their own dataset of relatively clean videos. On the more realistic and challenging QUVA Repetition dataset, our method improves considerably over existing work, be it based on Fourier or deep learning.MAE ↓ OBOA ↑ MAE ↓ OBOA ↑</figDesc><table>YTSegments [15] 
QUVA Repetition 

Pogalin et al. [19] 21.9 ± 30.1 
0.68 38.5 ± 37.6 
0.49 
Levy &amp; Wolf [15] 
6.5 ± 9.2 
0.90 48.2 ± 61.5 
0.45 
This paper 
10.3 ± 19.8 
0.89 23.2 ± 34.4 
0.62 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Generic temporal segmentation of cyclic human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Albu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bergevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Quirion</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>PR</publisher>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="6" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segmentation of periodically moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Azy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structure from periodic motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spatial Coherence for Visual Motion Analysis</title>
		<imprint>
			<biblScope unit="page" from="16" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extraction and analysis of multiple periodic motions in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Briassouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1244" to="1261" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quasi-periodic spatiotemporal filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Burghouts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1572" to="1582" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On motion periodicity of dynamic textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chetverikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fazekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Robust real-time periodic motion detection, analysis, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="781" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Categorical representation and recognition of oscillatory motion patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Richards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Behavior classification by eigendecomposition of periodic motions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goldenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rudzsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>PR</publisher>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1033" to="1043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Decomposition of Hardy functions into square integrable wavelets of constant shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grossmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Mathematical Analysis</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="723" to="736" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploiting the circulant structure of tracking-by-detection with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Camera calibration from periodic motion of a pedestrian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual perception of biological motion and a model for its analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="201" to="211" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Periodic motion detection and segmentation via approximate sequence alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Live Repetition Counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Finding periodicity in space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Repetitive motion analysis: Segmentation and event classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Ferrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="258" to="263" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual quasi-periodicity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pogalin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Thean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detection and recognition of periodic, nonrigid motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Polana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="261" to="282" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pedestrian detection via periodic motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="160" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">EpicFlow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wavelets and signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rioul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="14" to="38" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Separating transparent layers of repetitive dynamic behaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Div, grad, curl, and all that: an informal text on vector calculus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Schey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>WW</publisher>
			<pubPlace>Norton</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">View-invariant analysis of cyclic motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="251" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Comprehensive Introduction to Differential Geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spivak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
			<publisher>Publish or Perish, Inc., University of Tokyo Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Periodic motion detection and estimation via space-time sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thangali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A practical guide to wavelet analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Compo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Meteorological society</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Cyclic motion detection for motion based recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kasparis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>PR</publisher>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1591" to="1603" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
