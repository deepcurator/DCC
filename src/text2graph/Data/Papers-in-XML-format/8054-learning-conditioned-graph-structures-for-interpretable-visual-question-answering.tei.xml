<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Conditioned Graph Structures for Interpretable Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Norcliffe-Brown</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">AimBrain Ltd</orgName>
								<orgName type="institution" key="instit2">AimBrain Ltd</orgName>
								<orgName type="institution" key="instit3">AimBrain Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstathios</forename><surname>Vafeias</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">AimBrain Ltd</orgName>
								<orgName type="institution" key="instit2">AimBrain Ltd</orgName>
								<orgName type="institution" key="instit3">AimBrain Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Parisot</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">AimBrain Ltd</orgName>
								<orgName type="institution" key="instit2">AimBrain Ltd</orgName>
								<orgName type="institution" key="instit3">AimBrain Ltd</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Conditioned Graph Structures for Interpretable Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Visual Question answering is a challenging problem requiring a combination of concepts from Computer Vision and Natural Language Processing. Most existing approaches use a two streams strategy, computing image and question features that are consequently merged using a variety of techniques. Nonetheless, very few rely on higher level image representations, which can capture semantic and spatial relationships. In this paper, we propose a novel graph-based approach for Visual Question Answering. Our method combines a graph learner module, which learns a question specific graph representation of the input image, with the recent concept of graph convolutions, aiming to learn image representations that capture question specific interactions. We test our approach on the VQA v2 dataset using a simple baseline architecture enhanced by the proposed graph learner module. We obtain promising results with 66.18% accuracy and demonstrate the interpretability of the proposed method. Code can be found at github.com/aimbrain/vqa-project.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual Question Answering (VQA) is an emerging topic that has received an increasing amount of attention in recent years <ref type="bibr" target="#b0">[1]</ref>. Its attractiveness lies in the fact that it combines two fields that are typically approached individually (Computer Vision and Natural Language Processing (NLP)). This allows researchers to look at both problems from a new perspective. Given an image and a question, the objective of VQA is to answer the question based on the information provided by the image.</p><p>Understanding both the question and image, as well as modelling their interactions requires us to combine Computer Vision and NLP techniques. The problem is generally framed in terms of classification, such that the network learns to produce answers from a finite set of classes which facilitates training and evaluation. Most VQA methods follow a two-stream strategy, learning separate image and question embeddings from deep Convolutional Neural Networks (CNNs) and well known word embedding strategies respectively. Techniques to combine the two streams range from element-wise product to bilinear pooling <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> as well as attention based approaches <ref type="bibr" target="#b3">[4]</ref>.</p><p>Recent computer vision works have been exploring higher level representation of images, notably using object detectors and graph-based structures for better semantic and spatial image understanding <ref type="bibr" target="#b4">[5]</ref>. Representing images as graphs allows one to explicitly model interactions, so as to seamlessly transfer information between graph items (e.g. objects in the image) through advanced graph processing techniques such as the emerging paradigm of graph CNNs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Such graph based techniques have been the focus of recent VQA works, for abstract image understanding <ref type="bibr" target="#b9">[10]</ref> or object counting <ref type="bibr" target="#b10">[11]</ref>, reaching state of the art performance. Nonetheless, an important drawback of the proposed techniques is the fact that the input graph structures are heavily engineered, image specific rather than question specific, and not easily transferable from abstract scenes to real images. Furthermore, very few approaches provide means to interpret the model's behaviour, an essential aspect that is often lacking in deep learning models. Contributions. In this paper, we propose a novel, interpretable, graph-based approach for visual question answering. Most recent VQA approaches focus on creating new attention architectures of increasing complexity, but fail to model the semantic connections between objects in the scene. Here, we propose to address this issue by introducing a prior in the form of a scene structure, defined as a graph which is learned from observations according to the context of question. Bounding box object detections are defined as graph nodes, while graph edges conditioned on the question are learned via an attention based module. This not only identifies the most relevant objects in the image associated to the question, but the most important interactions (e.g. relative position, similarities) without any handcrafted description of the structure of the graph. Learning a graph structure allows to learn question specific object representations that are influenced by relevant neighbours using graph convolutions. Our intuition is that learning a graph structure not only provides strong predictive power for the VQA task, but also interpretability of the model's behaviour by inspecting the most important graph nodes and edges. Experiments on the VQA v2 dataset confirm our hypothesis. Combined with a relatively simple baseline, our graph learner module achieves 66.18% accuracy on the test set and provides interpretable results via visualisation of the learned graph representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work 2.1 Graph Convolutional Neural Networks</head><p>Graph CNNs (GCNs) are a relatively new concept, aiming to generalise Convolutional Neural Networks (CNNs) to graph structured data. CNNs intrinsically exploit the regular grid-like structure of data defined on the euclidean domain (e.g. images). Extending this concept to non-regularly structured data (e.g. meshes, or social/brain networks) is non trivial. We distinguish graph CNNs defined in the spectral <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6]</ref> and spatial <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref> domains.</p><p>Spectral GCNs exploit concepts from graph signal processing <ref type="bibr" target="#b13">[14]</ref>, using analogies with the Euclidean domain to define a graph Fourier transform, allowing to perform convolutions in the spectral domain as multiplications. Spectral GCNs are the most principled and out of the box approach, however are <ref type="figure">Figure 2</ref>: Overview of the proposed model architecture. We model the VQA problem as a classification problem, where each answer from the training set is a class. The core of our method is the graph learner, which takes as input a question encoding, and a set of object bounding boxes with corresponding image features. The graph learner module learns a graph representation of the image that is conditioned on the question, and models the relevant interactions between objects in the scene. We use this graph representation to learn image features that are influenced by their relevant neighbours using graph convolutions, followed by max-pooling, element-wise product and fully connected layers.</p><p>limited by the requirement that the graph structure is the same for all training samples, as the trained filters are defined on the graph Laplacian's basis.</p><p>Spatial GCNs tend to be more engineered as they require the definition of a node ordering and a patch operator. Several approaches have been defined specifically for regular meshes <ref type="bibr" target="#b12">[13]</ref>. Monti et al. <ref type="bibr" target="#b7">[8]</ref> recently provided a general spatial GCN formulation, learning a patch operator as a mixture of Gaussians. Recently, Graph Attention Networks were proposed in <ref type="bibr" target="#b8">[9]</ref>, modelling the convolution operator as an attention operation on node neighbours, where the attention weights can be interpreted as graph edges. Similar to our work, both of these methods compute a type of attention to learn a graph structure. However, while these methods which learn a fixed graph structure, we aim to learn a dynamic graph that is conditioned on the context of a query. Accordingly, our approach extends the notion of an edge to be adaptive to the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual Question Answering</head><p>Explicit modelling of object interactions through graph representations has recently received growing interest. A graph based approach was notably proposed in <ref type="bibr" target="#b9">[10]</ref>, combining graph representations of questions and abstract images with graph neural networks. This approach beat the state of the art by a large margin, demonstrating the potential of graph based methods for VQA. This approach is however not easily applicable to natural images where the scene graph representation is not known a priori.</p><p>The decision to use object proposals as image features resulted in major improvements in VQA performance. This idea was first introduced in <ref type="bibr" target="#b14">[15]</ref>, outperforming the state of the art with a relatively simple model. Such image representations have since been exploited to model interactions between objects through implicit and explicit graph structures <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>, with a focus on counting. <ref type="bibr" target="#b10">[11]</ref> compute a graph based on the outer product of the attention weights of proposed features. The computed graph is altered with explicitly engineered features solely to improve their baseline model's ability to count. <ref type="bibr" target="#b15">[16]</ref> propose an iterative approach relying on objects similarities to improve the models' counting abilities and interpretability, which was only evaluated on counting questions. The main aim of both approaches is to eliminate duplicate object detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>An overview of the method is shown in <ref type="figure">Fig. 2</ref>. We develop a deep neural network that combines spatial, image and textual features in a novel manner in order to answer a question about an image. Our model first computes a question representation using word embeddings and a Recurrent Neural Network (RNN), and a set of object descriptors comprising bounding box coordinates and image features vectors. Our graph learning module then learns an adjacency matrix of the image objects that is conditioned on a given question. This adjacency matrix enables the next layers -the spatial graph convolutions -to focus not only on the objects but also on the object relationships that are the most relevant to the question. Our convolved graph features are max-pooled, and combined with the question embedding using a simple element-wise product to predict a class pertaining to the answer of the given question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Computing model inputs</head><p>The first stage of our model is to compute embeddings for both the input image and question. We convert a given image into a set of K visual features using an object detector. Object detections are essential for the subsequent step of our model, as each bounding box will constitute a node in the question specific graph representations we are learning. An embedding is produced for each proposed bounding box, which is the mean of the corresponding area of the convolutional feature map. Using such object features has been observed to yield a better performance in VQA tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref>, as this allows the model to focus on object-level features rather than a pure CNN which produces a grid of features. For each question, we use pre-trained word embeddings as suggested in <ref type="bibr" target="#b2">[3]</ref> to convert the question into a variable length sequence of embeddings. Then we use a dynamic RNN with a GRU cell <ref type="bibr" target="#b16">[17]</ref>, to encode the sequence of word embeddings as a single question embedding q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph learner</head><p>In this section, we introduce the key element of our model and our main contribution: the graph learner module. This novel module produces a graphical representation of an image conditioned on a question. It is general, easy to implement and, as we highlight in Section 4.3, learns complex relationships between features that are interpretable and query dependent. The learned graph structure drives the spatial graph convolutions by defining node neighbourhoods, which in contrast to previous models such as <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, allows unary and pairwise attention to be learned naturally as the adjacency matrix contains self loops.</p><p>We seek to construct an undirected graph G = {V, E, A}, where E is the set of graph edges to learn and A ∈ R N ×N the corresponding adjacency matrix. Each vertex v ∈ V with |V| = N corresponds to a detected image object (bounding box coordinates and associated feature vector v n ∈ R d ). We aim to learn the adjacency matrix A so that each edge (i, j, A i,j ) ∈ E is conditioned on the question encoding q. Intuitively, we need to model the similarities between feature vectors as well as their relevance to the given question. This is done by first concatenating the question embedding q onto each of the N visual features v n , which we write as [v n q]. We then compute a joint embedding as:</p><formula xml:id="formula_0">e n = F ([v n q]), n = 1, 2, ..., N<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">F : R dv+dq → R de is a non-linear function and d v , d q , d</formula><p>e are the dimensions of the image feature vectors, question encoding and joint embedding respectively. By concatenating the joint embeddings e n together into a matrix E ∈ R N ×de , it is then possible to define an adjacency matrix for an undirected graph with self loops as A = EE T so that A i,j = e T i e j . Such definition does not impose any constraints on the graph sparsity, and could therefore yield a fully connected adjacency matrix. Not only is this a problem computationally, but the vast majority of VQA questions requires attending to only a small subset of the graph nodes. The learned graph structure will be the backbone of the subsequent graph convolution layers, where the objective is to learn a representation of object features that is conditioned on the most relevant, question-specific neighbours. This requires a sparse graph structure focusing on the most relevant aspects of the image. In order to learn a sparse neighbourhood system for each node, we adopt a ranking strategy as:</p><formula xml:id="formula_2">N (i) = topm(a i )<label>(2)</label></formula><p>where topm returns the indices of the m largest values of an input vector, and a i denotes the i th row of the adjacency matrix. In other words, the neighbourhood system of a given node will correspond to the nodes with which it has the strongest connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Spatial graph convolutions</head><p>Given a question specific graph structure, we then exploit a method of graph convolutions to learn new object representations that are informed by a neighbourhood system tailored to answer the given question. The graph vertices V (i.e. the bounding box and their corresponding features) are notably characterised by their location in the image, making the problem of modelling their interactions inherently spatial. Additionally, a lot of VQA questions require that the model has an awareness of the orientation and relative position of features in an image, an issue that many previous approaches have neglected.</p><p>As a result, we opt to use a graph CNN approach inspired by <ref type="bibr" target="#b7">[8]</ref>, operating directly in the graph domain and heavily relying on spatial relationships. Crucially, their method captures spatial information through the use of a pairwise pseudo-coordinate function u(i, j) which defines, for each vertex i, a coordinate system centred at i, with u(i, j) being the coordinates of vertex j in that system. Our pseudo-coordinate function u(i, j) returns a polar coordinate vector (ρ, θ), describing the relative spatial positions of the centres of the bounding boxes associated with vertices i and j. We considered both Cartesian and polar coordinates as input to the Gaussian kernels and observed that polar coordinates worked significantly better. We posit that this is because polar coordinates separate orientation (θ) and distance (ρ), providing two disentangled factors to represent spatial relationships.</p><p>An essential step and challenge of graph CNNs is the definition of a patch operator describing the influence of each neighbouring node that is robust to irregular neighbourhood structures. Monti et al. <ref type="bibr" target="#b7">[8]</ref> propose to do so using a set of K Gaussian kernels of learnable means and covariances, where the mean is interpretable as a direction and distance in pseudo coordinates. We obtain a kernel weight w k (u) for each k, such that the patch operator is defined at kernel k for node i as:</p><formula xml:id="formula_3">f k (i) = j∈N (i) w k (u(i, j))v j , k = 1, 2, ..., K<label>(3)</label></formula><p>where f n (i) ∈ R dv and N (i) denotes the neighbourhood of vertex i as described in Eq. 2. Considering a given vertex i, we can think of the output of the patch operator as a weighted sum of the neighbouring features, where the set of Gaussian kernels describe the influence of each neighbour on the output of the convolution operation.</p><p>We adjust the patch operator so that it includes an additional weighting factor conditioned on the produced graph edges:</p><formula xml:id="formula_4">f k (i) = j∈N (i) w k (u(i, j))v j α ij<label>(4)</label></formula><p>with α ij = s(a i ) j where s(.) j is the j th element of a scaling function (defined here as a softmax of the selected adjacency matrix elements). This more general form means that the strength of messages passed between vertices can be weighted by information in addition to spatial orientation. For our use case, this can be interpreted as how much attention the network should pay to the relationship between two nodes in terms of answering a question. Thus the network learns to attend on the visual features in a pairwise manner conditioned on the question.</p><p>Finally, we define the output of the convolution operation at vertex i as a concatenation over the K kernels:</p><formula xml:id="formula_5">h i = K k=1 G k f k (i)<label>(5)</label></formula><p>where each</p><formula xml:id="formula_6">G k ∈ R d h</formula><p>K ×dv is a matrix of learnable weights (the convolution filters), with d h as the chosen dimensionality of the outputted convolved features. This results in a convolved graph representation H ∈ R N ×d h .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Prediction layers</head><p>Our convolved graph representation H is computed through L spatial graph convolution layers. We then compute a global vector representation of the graph h max via a max-pooling layer across the node dimension. This operation was chosen so as to get a permutation invariant output, and for its simplicity, so the focus is on the impact of the graph structure. This vector representation of the graph can be considered a highly non-linear compression of the graph, where the representation has been optimised for answering the question at hand. We then merge question q and image h max encodings through a simple element-wise product. Finally, we compute classification logits through a 2-layer MLP with ReLU activations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Loss function</head><p>The VQA task is cast as a multi-class classification problem, where each class corresponds to one of the most common answers in the training set. Following <ref type="bibr" target="#b2">[3]</ref>, we use a sigmoid activation function with soft target scores, which has been shown to yield better predictions. The intuition behind this is that it allows to consider multiple correct answers per question and provides more information regarding the reliability of each answer. Assuming each question is associated with n valid provided answers, we compute the soft target score of each class as t = number of votes n</p><p>. If an answer is not in the top answers (i.e. the considered classes) then it has no corresponding element in the target vector. We then compute the multi-label soft loss which is simply the sum of the binary cross entropy losses for each element in the target vector.:</p><formula xml:id="formula_7">L(t, y) = i t i log(1/(1 + exp(−y i )) + (1 − t i ) log(exp(−y i )/(1 + exp(−y i ))<label>(6)</label></formula><p>where y is the logit vector (i.e. the output of our model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and preprocessing</head><p>We evaluate our model using the VQA 2.0 dataset <ref type="bibr" target="#b19">[20]</ref> which contains a total of 1,105,904 questions and about 204,721 images from the COCO dataset. The dataset is split up roughly into proportions of 40%, 20%, 40% for train, validation and test sets respectively. Each question in the dataset is associated with 10 different answers obtained by crowdsourcing. Accuracy on this dataset is computed so as to be robust to inter-human variability as:</p><p>acc(a) = min{ number of times a is chosen 3 , 1}</p><p>We consider the 3000 most common answers in the train set as possible answers for our network to predict. Each question is tokenized and mapped into a sequence of 300-dimensional pre-trained GloVe word embeddings <ref type="bibr" target="#b20">[21]</ref>. The COCO images are encoded as set of 36 object bounding boxes with corresponding 2048-dimensional feature vectors as described in <ref type="bibr" target="#b14">[15]</ref>. We normalise the bounding box corners by the image height and width so they lie in the interval [0, 1]. The bounding box corners, which provide absolute spatial information, are then concatenated onto the image feature vectors so they become 2052 dimensions (d v = 2052). Pseudo-coordinates are computed as the polar coordinates of the bounding box centres and give the model relative spatial information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation</head><p>Our question encoder is a dynamic Gated Recurrent Unit (GRU) <ref type="bibr" target="#b16">[17]</ref> with a hidden state size of 1024 (d q = 1024). Our function F (see Eq. 1), which learns the adjacency matrix, comprises two dense linear layers of size 512 (d g = 512). We use L=2 spatial graph convolution layers of dimensions 2048 and 1024 so that (d h1 = 2048, d h2 = 1024). All dense layers and convolutional layers are activated using Rectified Linear Unit (ReLU) activation functions. During training we use dropout on the image features and all but the final dense layers' nodes with a 0.5 probability. We train for 35 epochs using batch size of 64 and the Adam optimizer <ref type="bibr" target="#b21">[22]</ref> with a learning rate of 0.0001 which we halve after the 30th epoch. Parameters are chosen based on the performance on the validation set using the training set. The model is then trained on the training and validation sets using the chosen parameters for evaluation on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We investigated the influence of our model's main parameters on the classification accuracy, namely neighbourhood size (m), and the number of Gaussian kernels (K). We trained models for m ∈ 8 − 32, with a step of 4; and K ∈ {2, 4, 8, 16, 32}. Results are reported in <ref type="figure" target="#fig_1">Fig. 3</ref>. Our results suggest that  while an optimal performance is seen for K = 8, in particular for the number questions. In the remaining experiments, we therefore set K = 8 and m = 16. <ref type="table">Table 4</ref>.2 shows our results on the VQA 2.0 test set. We report results on a single version of our model and compare to recent state of the art VQA methods. We compare our model to <ref type="bibr" target="#b18">[19]</ref> (ReasonNet), the approach by <ref type="bibr" target="#b2">[3]</ref> (Bottom-Up) who won the 2017 VQA challenge 1 , and the recent approach proposed in <ref type="bibr" target="#b10">[11]</ref> (Counting module) which focuses on optimising counting questions. To highlight the importance of learning the graph structure, we also report results using a k-nearest neighbour graph (based on distances between bounding box centres) (kNN graph) as input to the graph convolution layers, and train a baseline model which replaces the graph learning and convolutions with a simple question to image attention (Attention). Even though it is simpler than our approach, the Attention model provides a good intuition of the advantage of using a graph structure.</p><p>Despite not being heavily engineered to optimise performance, our model's performance is close to state of the art. It notably improves substantially on numeric questions (with the exception of <ref type="bibr" target="#b10">[11]</ref>, which is specifically designed for this purpose). It should also be noted that both Bottom-Up and Counting module use the object detector with a variable number of objects per image, while we use a fixed number. Our method compares favourably with our baselines <ref type="figure">(Attention, kNN graph)</ref>. This highlights not only the advantage of learning a graph structure, but also of using an attention based approach, as kNN graph is the only method without attention. nodes that are influenced by their closest neighbours in the graph. As a result, the most important nodes can be seen as the locations where the network is "looking", as they will strongly influence most feature representations. Edges represent the most important relationships between objects. As a result, one can identify whether the network focussed on the right objects by looking at the most relevant nodes, while edge weights inform of the relationships that were considered as the most relevant to answer the question. <ref type="figure">Figure 4</ref> reports examples of graph structures learned leading to successful classification. We report results for multiple questions per image, highlighting how the learned graph is tailored to the question at hand. <ref type="figure">Figure 5</ref> shows failure cases and allows to study the interpretability of the proposed model. <ref type="figure">Figures 5-a,d</ref> show cases where the model looked at the wrong object, mistakes which can be attributed to missing detected objects (correct purse for <ref type="figure">Fig. 5</ref>-a, man's face for <ref type="figure">Fig. 5-d)</ref>. <ref type="figure">Figure 5</ref>-b shows that while the focus is on all animals on the bed, the cat is considered to be the most relevant object, hence the answer. Finally, <ref type="figure">Fig. 5-c</ref> shows a case that may not be adapted to bounding box based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In this paper, we propose a novel graph-based approach for Visual Question Answering. Our model learns a graph representation of the input image that is conditioned on the question at hand. It then exploits the learned graph structure to learn better image features that are conditioned on the most relevant neighbours, using the novel and powerful concept of graph convolutions. Experiments on the VQA v2 dataset yield promising results and demonstrate the relevance and interpretability of the learned graph structure.</p><p>Several extensions and improvements could be considered. Our main objective was to show the potential and interpretability of learning a graph structure. We found with a fairly simple architecture the learned graph structure was very effective; further work might want to consider more complex architectures to refine the learned graph further. For example, scalar edge weights may not be able to capture the full complexity of the relationships between graph items and so producing vector edges <ref type="figure">Figure 5</ref>: Visual examples of interpretable failures cases. Boxes and edges thickness/opacity correspond to the strengths of the node degree and edge weight respectively, showing the most graph nodes and edges that were considered to be the most relevant to answer the question.</p><p>could yield improvements. This could be implemented as an adjacency matrix per convolutional kernel. An important limitation of our approach is the use of an object detector as a preprocessing step. The performance of the model is highly dependent on the quality of the detector, which can yield duplicates or miss objects (as highlighted in the results section). Furthermore, our image features comprise a fixed number of detected objects per image, which can further enhance this problem. Finally, while the focus of this paper is the VQA problem, our approach could be adapted to more general problems, such as few-shots learning tasks where one could learn a graph structure from training samples.</p><p>Performance on the VQA v2 dataset is still rather limited, which could be linked to issues within the dataset itself. Indeed, several questions are subjective and cannot be associated with a correct answer (e.g. "Would you want to fly in that plane?") <ref type="bibr" target="#b22">[23]</ref>. In addition, modelling the problem as multi-class classification is the most common approach in recent VQA methods, and can strongly limit performance. Questions often require answers that cannot be found in the predefined answers (e.g. "what time is it?"). This explains the low performance of "Number" questions, as it comprises several questions requiring answers absent from the training set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Epitome of our graph learning module's ability to condition the bounding box connections based on the question. Two graph structures are learned from the same image but tailored to answer different questions. The thickness and opacity of graph nodes (bounding boxes) and edges (white lines) are determined by node degree and edge weights, showing the main objects and relationships of interest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results of parameter exploration. The VQA score for each question type is reported for a variety of settings of the number of kernels (K) and the neighbourhood size (m). The left hand plot shows varying m while keeping K = 8, the right hand plot shows varying K while keeping m = 16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figures 4 Figure 4 :</head><label>44</label><figDesc>Figures 4 and 5 show examples of learned graph structures for multiple questions and images. Figures 4 and 5 show the most important nodes and edges of each learned graph (largest node degree/number of connections and edge weights respectively). GraphCNNs learn feature representations of the graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>VQA 2.0 standard test set results -comparison with baselines and current state of the art 
methods 

Answer type 
All 
Y/N 
Num. Other 

ReasonNet [19] 
64.61 78.86 41.98 57.39 
Bottom-Up [3] 
65.67 82.20 43.90 56.26 
Counting module [11] 68.41 83.56 51.39 59.11 

kNN graph 
61.00 79.35 41.63 49.70 
Attention 
61.90 79.87 42.48 50.95 
Ours 
66.18 82.91 47.13 56.22 

m = 16 and K = 8 are optimal parameters. Performance drops for m &lt; 16 and is stable for m &gt; 16, 
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.visualqa.org/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visual question answering: A survey of methods and datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Vis Image Underst</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="21" to="40" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno>abs/1606.01847</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Xiaodong He, and Anton van den Hengel. Tips and tricks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02711</idno>
	</analytic>
	<monogr>
		<title level="m">Learnings from the 2017 challenge</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical question-image coattention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>abs/1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno>abs/1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5425" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph attention networks. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Graph-structured representations for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno>abs/1609.05600</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning to count objects in natural images for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><forename type="middle">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam Prügel-</forename><surname>Bennett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3189" to="3197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David I Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07998</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Interpretable counting for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Trott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoderdecoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Çaglar</forename><surname>Bart Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">High-order attention models for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3667" to="3677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multimodal learning and reasoning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="551" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual question answering: Datasets, algorithms, and future challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="3" to="20" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
