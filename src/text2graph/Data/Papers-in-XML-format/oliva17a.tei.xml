<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Statistical Recurrent Unit</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junier</forename><forename type="middle">B</forename><surname>Oliva</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
						</author>
						<title level="a" type="main">The Statistical Recurrent Unit</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Sophisticated gated recurrent neural network architectures like LSTMs and GRUs have been shown to be highly effective in a myriad of applications. We develop an un-gated unit, the statistical recurrent unit (SRU), that is able to learn long term dependencies in data by only keeping moving averages of statistics. The SRU's architecture is simple, un-gated, and contains a comparable number of parameters to LSTMs; yet, SRUs perform favorably to more sophisticated LSTM and GRU alternatives, often outperforming one or both in various tasks. We show the efficacy of SRUs as compared to LSTMs and GRUs in an unbiased manner by optimizing respective architectures' hyperparameters for both synthetic and real-world tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The analysis of sequential data has long been a staple in machine learning. Domain areas like natural language <ref type="bibr" target="#b30">(Zaremba et al., 2014;</ref><ref type="bibr" target="#b29">Vinyals et al., 2015)</ref>, speech <ref type="bibr" target="#b13">(Graves et al., 2013;</ref><ref type="bibr" target="#b12">Graves &amp; Jaitly, 2014)</ref>, music <ref type="bibr" target="#b8">(Chung et al., 2014)</ref>, and video <ref type="bibr" target="#b9">(Donahue et al., 2015)</ref> processing have recently garnered much attention. While the study of sequences itself is broad and may be extended to general functional analysis <ref type="bibr" target="#b27">(Ramsay &amp; Silverman, 2002)</ref>, most recent success has been from neural network based models, especially from recurrent architectures.</p><p>Recurrent networks are dynamical systems that represent time recursively. For example, the simple recurrent unit <ref type="bibr" target="#b11">(Elman, 1990</ref>) contains a hidden state that itself depends on the previous hidden state. However, training such networks has been observed to be difficult in practice due to exploding and vanishing gradients when propagating error gradients through time <ref type="bibr" target="#b15">(Hochreiter et al., 2001)</ref>. While explod- ing gradients can be mitigated with techniques like gradient clipping and normalization , vanishing gradients may be harder to deal with. As a result, sophisticated gated architectures like Long-Short Term Memory (LSTM) networks <ref type="bibr" target="#b14">(Hochreiter &amp; Schmidhuber, 1997)</ref> and Gated Recurrent Unit (GRU) networks  have been developed. These gated architectures contain "memory cells" along with gates to control how much they decay through time thereby aiding the networks' ability to learn long term dependencies in sequences.</p><p>Notwithstanding, there are still challenges in capturing long term dependencies in gated architectures <ref type="bibr" target="#b20">(Le et al., 2015)</ref>. In this paper we present a simple un-gated architecture, the Statistical Recurrent Unit, that often outperforms these more complicated alternatives. Although the SRU keeps only simple moving averages of summary statistics, its novel architecture makes it more adept than previous gated units for capturing long term information in sequences and comparing them across different windows of time. For instance, the SRU, unlike traditional recurrent units, can obtain a multitude of viewpoints of the past by simple linear combinations of only a few averages. We shall illustrate the efficacy of the SRU below using both real-world and synthetic sequential data tasks.</p><p>The structure of the paper is as follows: first we detail the architecture of the SRU as well as provide several key intuitions and insights for its design; after, we describe our experiments comparing the SRU to popular gated alternatives, and we perform a "dissective" study of the SRU, gaining further understanding of the unit by exploring how various hyper-parameters affect performance; finally, we discuss conclusions from our study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model</head><p>The SRU maintains long term sequential dependencies in a rather intuitive fashion-through summary statistics. As the name implies, statisticians often employ summary statistics when trying to represent a dataset. Quite naturally then, we look to an algorithm that itself learns to represent data seen previously in much the same vein as a neural statistician <ref type="bibr" target="#b10">(Edwards &amp; Storkey, 2016)</ref>.</p><p>Of course, unlike with unordered i.i.d. samples, simply averaging statistics of sequential points will lose valuable temporal information. The SRU maintains sequential information in two ways: first, we generate recurrent statistics that depend on a context of previously seen data; second, we generate moving averages at several scales, allowing the model to distinguish the type of data seen at different points in the past. We expound on these methods for creating temporally-aware statistics below.</p><p>We shall see that the statistical design of the SRU yields a powerful yet simple model that is able to analyze sequential data and, on the fly, create summary statistics for learning over sequences. Furthermore, through the use of ReLUs and exponential moving averages, the SRU is able to mitigate vanishing gradient issues that are common to many recurrent units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Recurrent Statistics</head><p>We consider an input sequence of real valued points x 1 , x 2 , . . . , x T ∈ R d . As seen in the second row of Table 1, we can compute a vector of statistics φ(x i ) ∈ R D for each point. Here, each vector φ(x i ) is independent of other points x j for j = i. One may then average these vectors as µ = 1 T T i=1 φ(x i ) to produce summary statistics of the sequence. This approach amounts to treating the sequence as a set of i.i.d. points drawn form some distribution and marginalizing out time. Clearly, here one will lose temporal information that will be useful for many sequence related ML tasks. It is interesting to note that global average pooling operations have gained a lot of recent traction in convolutional networks <ref type="bibr" target="#b21">(Lin et al., 2013;</ref><ref type="bibr" target="#b16">Iandola et al., 2016)</ref>. Analogously to the i.i.d. statistic approach, global averaging will lose spatial information, yet the high-level summary statistics provide an effective representation. Still, not marginalizing out time should provide a more robust approach for sequence tasks, thus we consider the following methods for producing statistics.</p><p>First, we provide temporal information whilst still utilizing averages through recurrent statistics that also depend on the values of previous points (see third row of <ref type="table" target="#tab_0">Table 1</ref>). That is, we compute our statistics on the i th point x i not only as a function of x i , but also as a function of the previous statistics of x i−1 , γ i−1 (which itself depends on γ i−2 , etc.):</p><formula xml:id="formula_0">γ 1 = γ(x 1 , γ 0 ), γ 2 = γ(x 2 , γ 1 ), . . .<label>(1)</label></formula><p>where γ(·, ·) is a function for producing statistics given the current point and previous statistics, and γ 0 is a constant initial vector for convention. We note that from a general standpoint if given a flexible model and enough dimensions, then recurrent summary statistics like (1) can perfectly encode ones sequence. Take for instance the following illustrative example where x i ∈ R + and statistics</p><formula xml:id="formula_1">γ i = (0, . . . , 0, T x i , 0, . . .) (2) γ i+1 = (0, . . . , 0, 0, T x i+1 , 0, . . .).<label>(3)</label></formula><p>That is, one records the i th input in the i th index. When averaged the statistics will be</p><formula xml:id="formula_2">1 T T i=1 γ i = (x 1 , x 2 , . . .), i.e.</formula><p>the complete sequence. Such recurrent statistics will undoubtedly suffer from the curse of dimensionality. Hence, we consider a more restrictive model of recurrent statistics which we expound on below (6).</p><p>Second, we provide even more temporal information by considering summary statistics at multiple scales. As a simple hypothetical example, consider taking multiple means across separate time windows (for instance taking means over indices 1-10, then over indices <ref type="bibr">11-20, etc.)</ref>. Such an approach (4) will illustrate how summary statistics evolve through time. </p><p>In practice, we shed light on the dynamics of statistics through time by using several averages of the same summary statistics. The SRU will use exponential moving averages µ i = α γ i +(1−α)µ i−1 to compute means; hence, we consider multiple weights by taking the exponential means at various scales α 1 , . . . , α m as shown in the last row of <ref type="table" target="#tab_0">Table 1</ref>. Later we show that this multi-scaled approach is capable of a combinatorial number of viewpoints of past statistics through simple linear combinations. </p><formula xml:id="formula_4">i.i.d. statistics φ(x1), φ(x2), . . . , φ(xT ) recurrent statistics γ (x1, γ0) , γ(x2, γ1), . . . , γ(xT , γT −1) recurrent multi-scaled statistics α T −1 1 γ(x 1 , γ 0 ), α T −2 1 γ(x 2 , γ 1 ), ... . . . α T −1 m γ(x 1 , γ 0 ), α T −2 m γ(x 2 , γ 1 ), ...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Update Equations</head><p>We have discussed in broad terms how one may create temporally-aware summary statistics through multi-scaled recurrent statistics. Below, we cover specifically how the SRU creates and uses summary statistics for sequences.</p><p>Recall that our input is a sequence of ordered points:</p><formula xml:id="formula_5">x 1 , x 2 , . . . , x t ∈ R d .</formula><p>Throughout, we apply an element-wise non-linearity f (·), which we take to be the ReLU <ref type="bibr" target="#b18">(Jarrett et al., 2009;</ref><ref type="bibr" target="#b25">Nair &amp; Hinton, 2010)</ref>: f (·) = max(·, 0). The SRU operates via exponential moving averages, µ (α) ∈ R s (7), kept at various scales α ∈ A = {α 1 , . . . , α m }, where α i ∈ [0, 1). These moving averages, µ (α) , are of recurrent statistics ϕ (6) that are dependent not only on the current input but also on features of averages, r (5). The moving averages are then concatenated as µ = (µ (α1) , . . . , µ (αm) ) and used to create an output o (8) that is fed upwards in the network. <ref type="figure">Figure 1</ref>. Graphical representation of the SRU. Solid lines indicate a dependence on the current value of a node. Dashed lines indicate a dependence on the previous value of a node. We see that both the current point xt as well as a summary of the previous data rt are used to make statistics ϕt, which in turn are used in moving averages µt, finally an output ot is feed-forward through the rest of the network.</p><p>We detail the update equations for the SRU below (and in <ref type="figure">Figure 1</ref>):</p><formula xml:id="formula_6">r t = f W (r) µ t−1 + b (r)<label>(5)</label></formula><formula xml:id="formula_7">ϕ t = f W (ϕ) r t + W (x) x t + b (ϕ) (6) ∀α ∈ A, µ (α) t = αµ (α) t−1 + (1 − α)ϕ t (7) o t = f W (o) µ t + b (o) .<label>(8)</label></formula><p>In practiced we noted that it suffices to use only a few α's such as A = {0, 0.25, 0.5, 0.9, 0.99}.</p><p>It is worth noting that previous work has considered capturing recurrent information at various timescales in the past. For instance, <ref type="bibr" target="#b19">Koutnik et al. (2014)</ref> considers an RNN scheme that divides the hidden state into different modules for use at different frequencies. Furthermore, exponential averages in recurrent units have been considered previously, e.g. <ref type="bibr" target="#b22">(Mikolov et al., 2015;</ref><ref type="bibr" target="#b3">Bengio et al., 2013;</ref><ref type="bibr" target="#b17">Jaeger et al., 2007)</ref>. However, such works are more akin to un-gated GRUs since they consider only one scale per feature, limiting the views available per statistic to just one. The use of ReLUs in recurrent units has also been recently explored by <ref type="bibr" target="#b20">Le et al. (2015)</ref>, however there no statistics are kept and their use is limited to the simple RNN when initialized in a special manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Intuitions from Mean Map Embeddings</head><p>The design of the SRU is deliberately chosen to allow for long term dependencies to be learned. To better elucidate the design and its intuition, let us take a brief excursion to another use of (summary) statistics in machine learning for the representation of data: mean map embeddings (MMEs) of distributions <ref type="bibr" target="#b28">(Smola et al., 2007)</ref>. At its core, the concept of MMEs is that one may embed, and thereby represent, a distribution through statistics (such as moments).</p><p>The MME for a distribution D given a positive semidefinite kernel k is:</p><formula xml:id="formula_8">µ[D] = E X∼D [φ k (X)] ,<label>(9)</label></formula><p>where φ k are the reproducing kernel Hilbert space (RKHS) features of k, which may be infinite dimensional. To represent a set Y = {y 1 , . . . , y n } iid ∼ D one would use an empirical mean version of the MME:</p><formula xml:id="formula_9">µ[Y ] = 1 n n i=1 φ k (y i ).<label>(10)</label></formula><p>Numerous works have shown success in representing distributions and sets through MMEs <ref type="bibr" target="#b23">(Muandet et al., 2016)</ref>. One interpretation for the design of SRUs is that we are modifying MME's for use on sequences. Of course, one way of applying MMEs directly on sequences is to simply ignore the non-i.i.d. nature of sequences and treat points as comprising a set. This however loses important sequential information, as previously mentioned. Below we discuss the specific modifications we make from traditional MMEs and the benefits they yield.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">DATA-DRIVEN STATISTICS</head><p>First, we note the clear analogue between the mean embedding of a set Y , µ[Y ] (10), and the moving average µ (α) (7). The moving averages µ (α) are clearly serving as summary statistics of previously seen data. However, the statistics we are averaging for µ (α) , ϕ (6), are not comprised of apriori RKHS features as is typical with MMEs, but rather are learned non-linear features. This has the benefit of using data-driven statistics, and may be interpreted as using a linear kernel in the learned features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">RECURSIVE STATISTICS FROM THE PAST</head><p>Second, recall that typical MMEs use statistics that depend only on a single point x, φ k (x). As aforementioned this is fine for i.i.d. data, however it loses sequential information when averaged. Instead, we wish to assign statistics that depend on the data we have seen so far, since it provides context for one's current point in the sequence. For instance, one may want to have a statistic that keeps track of the difference between the current point and the mean of previous data. We provide a context based on previous data by making the statistics considered at time t, ϕ t (6), a function not only of x t but also of {x 1 , . . . , x t−1 } through r t (5). r t may be interpreted as a condensation of the sequence seen so far, and allows us to keep sequential information even through an averaging operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3.">MULTI-SCALED STATISTICS</head><p>Third, the use of multi-scaled moving averages of statistics gives the SRU a simple and powerful rich view of past data that is unique to this recurrent unit. In short, by keeping moving averages at different scales {α 1 , . . . , α m }, we are able to uncover differences in statistics at various times in the past. Note that we may unroll moving averages as:</p><formula xml:id="formula_10">µ (α) t = (1 − α) ϕ t + αϕ t−1 + α 2 ϕ t−2 + . . .<label>(11)</label></formula><p>Thus, a smaller α weighs current statistics more than older statistics; hence, a concatenated vector µ = (µ (α1) , . . . , µ (αm) ) itself provides a multi-scale view of statistics through time (see <ref type="figure" target="#fig_2">Figure 2</ref>). For instance, keeping statistics for short and long terms pasts already yields information on the evolution of the sequence through time. We may unroll the moving average updates as (11). To visualize the different emphasis in the past that varying α has on statistics we plot the values of weights in moving averages (i.e. α i ) for 100 points in the past across rows. We see that alpha values closer to 0 focus only on the recent past, where values close to 1 maintain an emphasis on the distant past as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Viewpoints of the Past</head><p>An interesting and useful property of keeping multiple scales for each statistic is that one can obtain a combinatorial number of viewpoints of the past through simple linear combinations of ones statistics. For instance, for properly chosen w j , w k ∈ R, w j µ (αj ) −w k µ (α k ) provides an aggregate of statistics from the past for α j &gt; α k <ref type="figure" target="#fig_3">(Figure 3</ref>). Of course, more complicated linear combinations may be performed to obtain richer viewpoints that are comprised of multiple windows. Furthermore, by using a linear projection of our statistics µ t , as we do with o t (8), we are able to compute output features of combined viewpoints of several statistics.</p><p>This kind of multi-viewpoint perspective of previously seen data is difficult to produce in traditional gated recurrent  units since they must encode where in the sequence they currently are and then store an activation on separate nodes per each viewpoint for future use. SRUs, on the other hand, only need to take simple linear combinations to capture various viewpoints in the past. For example, as shown above, statistics from just the distant past are available via a simple subtraction of two moving averages <ref type="figure" target="#fig_3">(Figure 3, row 1)</ref>. Such a windowed view would require a gated unit to learn to stop averaging after a certain point in the sequence, and the corresponding statistic would not yield any information outside of this window. In contrast, each statistic kept by the SRU provides a combinatorial number of varying perspectives in the past through linear combinations and their multi-scaled nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Vanishing Gradients</head><p>As previously mentioned, it has been shown that vanishing gradients make learning recurrent units difficult due to an inability to propagate error gradients through time. Notwithstanding its simple un-gated structure, the SRU features several safeguards to alleviate vanishing gradients. First, units and statistics are comprised of ReLUs. ReLUs have been observed to be easier to train for general deep networks <ref type="bibr" target="#b25">(Nair &amp; Hinton, 2010)</ref> and have had success in recurrent units <ref type="bibr" target="#b20">(Le et al., 2015)</ref>. Intuitively, ReLUs allow for the propagation on error on positive inputs without saturation and vanishing gradients as with traditional sigmoid units. The ability of the SRU to use ReLUs (without any special initialization) makes it especially adept at learning long term dependencies through time.</p><p>Furthermore, the explicit moving average of statistics al-lows for longer term learning. Consider the following derivative of the error signal E w.r.t. an element µ (α) t−1 k of the unit's moving averages when [ϕ t ] k = 0:</p><formula xml:id="formula_11">∂E ∂ µ (α) t−1 k = ∂ µ (α) t k ∂ µ (α) t−1 k ∂E ∂ µ (α) t k = α ∂E ∂ µ (α) t k .</formula><p>That is, the factor α directly controls the decay of the error signal through time. Thus, by including an α explicitly near 1 (i.e. 0.999), the decay for that moving average can be made minuscule for the lengths of sequences in ones data. Also, it is interesting to note that, with a large α near 1, SRUs with ReLUs can implement part of the functionality of a gate ("remembering") by carrying through the previous moving average [µ (α) t−1 ] k when the corresponding statistic [ϕ t ] k has be zeroed out (7). The other functionality of a gate (forgetting) can be had by including an α near 0; if the ReLU statistic is not zeroed out, then the moving average for a small α will "forget" the previous value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We compared the performance of the SRU 1 to two popular gated recurrent units, the GRU and LSTM unit. All experiments were performed in Tensorflow <ref type="bibr" target="#b2">(Abadi et al., 2016)</ref> and used the standard implementations of GRUCell and BasicLSTMCell for GRUs and LSTMs respectively. In order to perform a fair, unbiased comparison of the recurrent units and their hyper-parameters, which greatly affect performance <ref type="bibr" target="#b4">(Bergstra &amp; Bengio, 2012)</ref>, we used the Hyperopt <ref type="bibr" target="#b5">(Bergstra et al., 2015)</ref> hyper-parameter optimization package. We believe that such an approach gives each algorithm a fair shot to succeed without injecting biases from experimenters or imposing gross restrictions on architectures considered.</p><p>In all experiments we used SGD for optimization using gradient clipping ) with a norm of 1 on all algorithms. Unless otherwise specified 100 trials were performed to search over the following hyper-parameters on a validation set: one, initial learning rate the initial learning rate used for SGD, in range of [exp(−10), 1]; two, lr decay the multiplier to multiply the learning rate by every 1k iterations, in range of [0.8, 0.999]; three, dropout keep rate, percent of output units that are kept during dropout, in range (0, 1]; four, num units number of units for recurrent unit, in {1, . . . , 256}. In addition, the following two parameters were searched over for the SRU: num stats, the dimensionality of ϕ (6), in {1, . . . , 256}; summary dims, the dimensionality of r (5), in {1, . . . , 64}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Synthetic Recurrent Unit Generated Data</head><p>First we provide evidence that traditional gated units have difficulties capturing the same type of multi-scale recurrent statistic based dependencies that the SRU offers. We show the relative inefficiency of traditional gated units at learning long term dependencies of statistics by considering 1d synthetic data from a ground truth SRU.</p><p>We begin the sequences with x 1 iid ∼ N (0, 100</p><p>2 ), and x t is the results of a projection of o t . We generate a total of 176 points per sequence for 3200 training sequences, 400 validation sequences, and 400 testing sequences.</p><p>The ground truth statistical recurrent unit has three statistics φ t (6): the positive part of inputs (x) + , the negative part of inputs (x) − , and an internal statistic, z. We use α ∈ {α i } 5 i=1 = {0.0, 0.5, 0.9, 0.99, 0.999}. Denote µ</p><formula xml:id="formula_12">(α) + , µ (α) − , µ (α) z</formula><p>as the moving averages using α for each respective statistic. The internal statistic z does not get used (through r t (5)) in updating the statistics for (x) + or (x) − . z is itself updated as:</p><formula xml:id="formula_13">zt = (zt−1) + + µ (α 4 ) + − µ (α 5 ) + − 0.01 + − −µ (α 4 ) − + µ (α 5 ) − − 0.01 + − −µ (α 4 ) + + µ (α 5 ) + − 0.05 + + µ (α 4 ) − − µ (α 5 ) − − 0.05 + ,</formula><p>where each of the summands are r t features. Furthermore we have o t ∈ R 15 (8):</p><formula xml:id="formula_14">o t = (x t ) + , −(x t ) − , v T 1 µ t , . . . , v T 13 µ t ,</formula><p>where v j 's where initialized and fixed as</p><formula xml:id="formula_15">(v j ) k iid ∼ N (0, ( 1 100 )</formula><p>2 ). Finally the next point is generated as:</p><formula xml:id="formula_16">x t+1 = (x t ) + − (x t ) − + w T o t,3: ,</formula><p>where w was initialized and fixed as (w) k iid ∼ N (0, 1), and o t,3: are the last 13 dimensions of o t .</p><p>After the ground truth SRU was constructed we generated the training, validation, and testing sequences. As can be seen in <ref type="figure" target="#fig_4">Figure 4</ref>, the sequences follow a simple pattern: at the start negative values are quickly pushed to zero and positive values follow a parabolic line until hitting zero, at which point they slope downward depending on initial values. While simple, it is clear that trained recurrent units must be able to hold long-term information since all sequences converge at one point and future behaviour depends on initial values.</p><p>We look to minimize the mean of squared errors (MSE); that is, the loss we consider per sequence is 1 175 175 t=1 |x t+1 − p t | 2 , where p t is the output of the network after being fed x t . We conducted 100 trials of hyperparameter optimization as described above and obtained the following results in <ref type="table" target="#tab_2">Table 2</ref>.  Not surprisingly, the SRU performs far better than traditional gated recurrent units. This suggests that the types of long-term statistical relationships captured by the SRU are indeed different than those of traditional recurrent units. As previously mentioned, the SRU is able to obtain a multitude of different views from its statistics, a task that traditional units achieve less efficiently since they must devote one whole memory cell per viewpoint and statistic. As we show below, the SRU is able to outperform traditional gated units in long term problems even for real data that is not generated from its model class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">MNIST Image Classification</head><p>Next we explore the ability of recurrent units to use longterm dependencies in ones data with a synthetic task using a real dataset. It has been observed that LSTMs perform poorly in classifying a long pixel-by-pixel sequence of MNIST digits <ref type="bibr" target="#b20">(Le et al., 2015)</ref>. In this synthetic task, each 28×28 gray-scale MNIST digit image is flattened and observed as a sequence {x 1 , . . . , x 784 }, where x i ∈ [0, 1] (see <ref type="figure" target="#fig_5">Figure 5)</ref>. The task is, based on the output observed after feeding x 784 through the network, to classify the digit of the corresponding image in {0, . . . , 9}. Hence, we project the output after x 784 of each recurrent unit to 10 dimensions and use a softmax activation.</p><p>We report the hyper-parameter optimized results below in <ref type="table" target="#tab_3">Table 3</ref>; due to resource constraints each trial consisted only of 10K training iterations. We see that the SRU is able to out-perform both GRUs and LSTMs. Given the long length and dependencies of pixel sequences in this experiment, it is not surprising that SRUs' abilities to capture long-term dependencies are aiding it to achieve a much lower error.  3.2.1. DISSECTIVE STUDY Next, we study the behavior of the statistical recurrent unit with a dissective study where we vary several parameters of the architecture.</p><p>We consider variants to a base model with: num stats=200; r dims=60; num units=200. We keep the parameters initial learning rate, lr decay fixed at the the optimal values found (0.1, 0.99 respectively) unless we find no learning, in which case we also try learning rates of 0.01 and 0.001.</p><p>The need for multi-scaled recurrent statistics. Recall that we designed the statistics used by the SRU expressly to capture long term time dependencies in sequences. We did so both with recurrent statistics, i.e. statistics that themselves depend on previous points' statistics, and with multiscaled averages. We show below that both of these timedependent design choices are vital to capturing long term dependencies in data. Furthermore, we show that the use of ReLU statistics lends itself to better learning.</p><p>We explored the impact that time-dependent statistics had on learning by first considering naive i.i.d. summary statistics for sequences. This was achieved by using r dims=0 and α ∈ A = {0.99999}. Here no past-dependent context is used for statistics, i.e. we used i.i.d.-type statistics as is typical for unordered sets. Furthermore, the use of a single scale α near 1 means that all of the points' statistics will be weighted nearly identically (11) regardless of index. We optimized the SRU when using no recurrent statistics and a single scale (iid), when using recurrent statistics with a single scale (recur), and when using no recurrent statistics with multiple scales (multi). We report errors below in <ref type="table" target="#tab_4">Table 4</ref>. Next, we explored the effects of the scales at which we keep our statistics by varying from α ∈ A = {0.0, 0.5, 0.9, 0.99, 0.999} considering α ∈ A = {0.0, 0.5, 0.9}, α ∈ A = {0.0, 0.5, 0.9, 0.99}. We see in <ref type="table" target="#tab_5">Table 5</ref> that additional, longer scales aid our learning for this dataset. This is not very surprising given the long term nature of the pixel sequences.  Lastly, we considered the use of non-ReLU statistics by changing the element-wise non-linearity f (·) (5)- <ref type="formula" target="#formula_7">(8)</ref> to be the hyperbolic tangent f (·) = tanh(·). We postulated that the use of ReLUs would help our learning since they have been observed to better handle the problem of vanishing gradients. We find evidence of this when swapping ReLUs for hyperbolic tangent units in SRUs: we get an error rate of 0.18 when using hyperbolic tangent units. Although the previous uses of ReLUs in RNN required careful initialization <ref type="bibr" target="#b20">(Le et al., 2015)</ref>, SRUs are able to use ReLUs for better learning without any special considerations.</p><p>Dimension of recurrent summary. Next we explore the effect of varying the number of dimensions used for the recurrent summary of statistics r t (5). We consider r dims in {5, 20, 240}. As previously discussed r t provides a context based on past data so that the SRU may produce noni.i.d. statistics as it moves along a sequences. As one would expect the dimensionality of r t will limit the information flow from the past and values that are too small will hinder performance. It is also interesting to see that after enough dimensions, there are diminishing returns to adding more. Number of statistics and outputs. Finally, we vary the number of statistics num stats, and outputs units. Interestingly the SRU seems robust to the number of outputs propagated in the network. However, performance is considerably affected by the number of statistics considered. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Polyphonic Music Modeling</head><p>Henceforth we consider real data and sequence learning tasks. First, we used the polyphonic music datasets from Boulanger- <ref type="bibr" target="#b6">Lewandowski et al. (2012)</ref>. Each time-step is a binary vector representing the notes played at the respective time-step. Since we were required to predict binary vectors we used the element-wise sigmoid σ. I.e., the binary vector of notes x t+1 was modeled as σ (p t ), where p t is the output after feeding x t (and previous values x 1 , . . . , x t−1 ) through the recurrent network.</p><p>It is interesting to note in <ref type="table" target="#tab_8">Table 8</ref> that the SRU is able to outperform one of the traditional gated units in every dataset and it outperforms both in two datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Electronica-Genre Music MFCC</head><p>In the following experiment we modeled the Mel frequency cepstrum coefficients (MFCCs) in a dataset of nearly 18 000 scraped 30s sound clips of electronica-genre songs. MFCCs are perceptually based spectral features positioned logarithmically on the mel scale, which approximates the human auditory system's response <ref type="bibr" target="#b24">(Müller, 2007)</ref>. We looked to model the 13 real-valued coefficients using the recurrent units, by modeling x t+1 as a projection of the output of a recurrent unit after being fed x 1 , . . . , x t . As can be seen in <ref type="table" target="#tab_9">Table 9</ref>, SRUs again are outperforming gated architectures and are especially beating GRUs by a wider margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Climate Data</head><p>Next we consider weather data prediction using the North America Regional Reanalysis (NARR) Project (NAR). The dataset provides a long-term set of consistent climate data on a regional scale for the North American domain. The period of the reanalyses is from October 1978 to the present and analyses were made 8 times daily (3 hour intervals).</p><p>We take our input sequences to be year-long sequences of weather variables in a location for the year 2006. I.e. an input sequence will be a 2920 length sequence of weather variables at a given lat/lon coordinate. We considered the following 7 variables: pres10m, 10 m pressure (pa); tcdc, total cloud cover (%); rh2m, relative humidity 2m (%); tmpsfc, surface temperature (k); snod, snow depth surface (m); ugrd10m, u component of wind 10m above ground; vgrd10m, v component of wind 10m above ground. The variables were standardized, see <ref type="figure" target="#fig_7">Figure 6</ref> for example sequences. Below we see results using 51 200 training location sequences and 6 400 validation and testing instances. Again, we look to model the next point in a sequence as a projection of the output of the recurrent unit after feeding the previous points. One may see in <ref type="table" target="#tab_0">Table 10</ref> that SRUs and LSTMs perform nearly identically; perhaps the cyclical nature of climate data was beneficial to the gated units. Finally, we look to predict the positions of National Basketball Association (NBA) players based on previous court positions during a play. Optical tracking data for this project were provided by STATS LLC from their SportVU product and obtained from (NBA). The data are composed of x and y coordinates for each of the ten players and the ball. We again minimize the squared norm of errors for predictions. We observed a large margin of improvement for SRUs over gated architectures in <ref type="table" target="#tab_0">Table 11</ref> that is reminiscent of the synthetic data experiment in §3.1. This suggests that this dataset contains long term dependencies that the SRU is able to exploit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>We believe that the use of summary statistics has been under-explored in modern recurrent units. Although recent studies in convolutional networks have considered global average pooling, which is essentially using high-level summary statistics to represent images, there has been little exploration of summary statistics for modern recurrent networks. To this end we introduce the Statistical Recurrent Unit, a novel architecture that seeks to capture long term dependencies in data using only simple moving averages and rectified-linear units.</p><p>The SRU was motivated by the success of mean-map embeddings for representing unordered datasets, and may be interpreted as an alteration of MMEs for sequential data. The main modifications are as follows: first, the SRU uses data-driven statistics unlike typical MMEs, which will use RKHS features from an a-priori selected class of kernels; second, SRUs will use recurrent statistics that are dependent not only on a current point, but on previous points' statistics through a condensation of kept moving averages; third, the SRU will keep moving averages at various scales. We provide evidence that the combination of these modifications yield much better results than any one of them in isolation.</p><p>The resulting recurrent unit is especially adept for capturing long term dependencies in data and readily has access to a combinatorial number of viewpoints of past windows through simple linear combinations. Moreover, it is interesting to note that even though the SRU is gate-less, it may implement part of both "remembering" and "forgetting" functionalities through ReLUs and moving averages.</p><p>We showed empirically that the SRU is better equipped that traditional gated units for long term dependencies via synthetic and real-world data experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. We may unroll the moving average updates as (11). To visualize the different emphasis in the past that varying α has on statistics we plot the values of weights in moving averages (i.e. α i ) for 100 points in the past across rows. We see that alpha values closer to 0 focus only on the recent past, where values close to 1 maintain an emphasis on the distant past as well.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. We visualize the power of taking linear combinations of µ (α) 's for providing different viewpoints into past data. In row 1 we show the effective weights that would be used for weighing statistics ϕt if one considers .001 −1 µ (.999) − .01 −1 µ (.99) ; we see that this is equivalent to considering only statistics from the distant past. Similarly, we show the effective weights when taking .01 −1 µ (.99) − .1 −1 µ (.9) and .1 −1 µ (.9) − .5 −1 µ (.5) on rows 2 and 3 respectively. We see that these linear combinations amount to considering viewpoints concentrated at various points in the past. Lastly its worth noting that more complicated linear combinations may lead to even richer views on previous statistics; for instance, we show .001 −1 µ (.999) −.01 −1 µ (.99) +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. 25 sequences generated from the ground truth SRU model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Right: example MNIST 28 × 28 image, which is taken as a pixel-by-pixel sequence of length 784 unrolled as shown in yellow. Left: example pixel sequences for 0, 1, and 2 digit images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Two example sequences for weather variables at distinct locations for the year 2006.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Example player/ball x, y positions for two plays.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Methods for keeping statistics of sequences.</figDesc><table>inputs 
x1, x2, . . . , xT 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>In row 1 we show the effective weights that would be used for weighing statistics ϕt if one considers .001We see that these linear combinations amount to considering viewpoints concentrated at various points in the past. Lastly its worth noting that more complicated linear combinations may lead to even richer views on previous statistics; for instance,the statistics of the distant and very recent past, but de-emphasizes statistics of data from less recent past.</figDesc><table>−1 µ 
(.999) − .01 
−1 µ 
(.99) ; we 
see that this is equivalent to considering only statistics from the 
distant past. Similarly, we show the effective weights when taking 
.01 
−1 µ 
(.99) − .1 
−1 µ 
(.9) and .1 
−1 µ 
(.9) − .5 
−1 µ 
(.5) on rows 2 and 
3 respectively. we show .001 
−1 µ 
(.999) −.01 
−1 µ 
(.99) + 

.5 
.09 

µ 
(.9) on row 4, which 
concentrates on </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc>MSEs for synthetically generated dataset.</figDesc><table>SRU GRU LSTM 
Error 0.62 21.72 161.62 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Test error rate for MNIST pixel sequence classification.</figDesc><table>SRU GRU LSTM 
Error Rate 0.11 
0.28 
0.48 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Test error rate for MNIST pixel sequence classification.</figDesc><table>iid recur multi 
Error Rate 0.88 
0.88 
0.63 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 .</head><label>5</label><figDesc>Test error rate for MNIST pixel sequence classification.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 .</head><label>6</label><figDesc>Test error rate varying recurrent summary rt.</figDesc><table>r dims 
5 
20 
240 
Error Rate 0.25 0.20 0.10 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 .</head><label>7</label><figDesc>Test error rate varying number of units.</figDesc><table>num stats 
units 
10 
50 
10 
50 

Error Rate 0.88 0.32 0.15 0.15 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="true"><head>Table 8 .</head><label>8</label><figDesc>Test negative log-likelihood for polyphonic music data.</figDesc><table>Data set 
SRU 
GRU LSTM 
JSB 
8.260 8.548 
8.393 
Muse 
6.336 6.429 
6.293 
Nottingham 3.362 3.386 
3.359 
Piano 
7.737 7.929 
7.931 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 9 .</head><label>9</label><figDesc>Test-set MSEs of MFCC Music data.</figDesc><table>SRU 
GRU LSTM 
Error 1.176 2.080 
1.183 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="true"><head>Table 10 .</head><label>10</label><figDesc>Test MSEs for weather data.</figDesc><table>SRU 
GRU LSTM 
Error 0.465 0.487 
0.466 

3.6. SportVu NBA Tracking data 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 11 .</head><label>11</label><figDesc>Test-set MSEs of NBA data.</figDesc><table>SRU 
GRU 
LSTM 
Error 34.505 329.921 296.908 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA. Correspondence to: Junier B. Oliva &lt;jo-liva@cs.cmu.edu&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">See https://github.com/junieroliva/ recurrent for code.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is supported in part by DOE grant DESC0011114 and NSF grant IIS1563887.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="https://data.noaa.gov/dataset/ncep-north-american-regional-reanalysis-narr-for-1979-to-present.Accessed" />
		<title level="m">Ncep north american regional reanalysis</title>
		<imprint>
			<biblScope unit="page" from="2016" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Data</surname></persName>
		</author>
		<ptr target="https://github.com/sealneaward/nba-movement-data.Accessed" />
		<imprint>
			<biblScope unit="page" from="2016" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jianmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhifeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI)<address><addrLine>Savannah, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Advances in optimizing recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8624" to="8628" />
		</imprint>
	</monogr>
	<note>2013 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hyperopt: a python library for model selection and hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Komer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Science &amp; Discovery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14008</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6392</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<title level="m">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caglar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sergio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Subhashini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02185</idno>
		<title level="m">Towards a neural statistician</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Geoffrey. Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinton</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, speech and signal processing (icassp), 2013 ieee international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoshua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Squeezenet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimization and applications of echo state networks with leaky-integrator neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lukoševičius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mantas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udo</forename><surname>Siewert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="335" to="352" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2009 IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2146" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A clockwork rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1863" to="1871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sumit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marc&amp;apos;aurelio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7753</idno>
		<title level="m">Learning longer memory in recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Kernel mean embedding of distributions: A review and beyonds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kenji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09522</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Information retrieval for music and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meinard</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1310" to="1318" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Applied functional data analysis: methods and case studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Ramsay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">77</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A hilbert space embedding for distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Algorithmic Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="13" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Łukasz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Terry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Slav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
