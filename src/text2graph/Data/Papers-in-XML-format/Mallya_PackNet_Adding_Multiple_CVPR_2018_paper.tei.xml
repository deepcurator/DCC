<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
							<email>amallya2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
							<email>slazebni@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Lifelong or continual learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22</ref>] is a key requirement for general artificially intelligent agents. Under this setting, the agent is required to acquire expertise on new tasks while maintaining its performance on previously learned tasks, ideally without the need to store large specialized models for each individual task. In the case of deep neural networks, the most common way of learning a new task is to fine-tune the network. However, as features relevant to the new task are learned through modification of the network weights, weights important for prior tasks might be altered, leading to deterioration in performance referred to as "catastrophic forgetting" <ref type="bibr" target="#b3">[4]</ref>. Without access to older training data due to the lack of storage space, data rights, or deployed nature of the agent, which are all very realistic constraints, naïve fine-tuning is not a viable option for continual learning.</p><p>Current approaches to overcoming catastrophic forgetting, such as Learning without Forgetting (LwF) <ref type="bibr" target="#b17">[18]</ref> and Elastic Weight Consolidation (EWC) <ref type="bibr" target="#b13">[14]</ref>, try to preserve knowledge important to prior tasks through the use of proxy losses. The former tries to preserve activations of the initial network while training on new data, while the latter penalizes the modification of parameters deemed to be important to prior tasks. Distinct from such prior work, we draw inspiration from approaches in network compression that have shown impressive results for reducing network size and computational footprint by eliminating redundant parameters <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. We propose an approach that uses weight-based pruning techniques <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> to free up redundant parameters across all layers of a deep network after it has been trained for a task, with minimal loss in accuracy. Keeping the surviving parameters fixed, the freed up parameters are modified for learning a new task. This process is performed repeatedly for adding multiple tasks, as illustrated in <ref type="figure">Figure 1</ref>. By using the task-specific parameter masks generated by pruning, our models are able to maintain the same level of accuracy even after the addition of multiple tasks, and incur a very low storage overhead per each new task.</p><p>Our experiments demonstrate the efficacy of our method on several tasks for which high-level feature transfer does not perform very well, indicating the need to modify parameters of the network at all layers. In particular, we take a single ImageNet-trained VGG-16 network <ref type="bibr" target="#b27">[28]</ref> and add to it three fine-grained classification tasks -CUBS birds <ref type="bibr" target="#b28">[29]</ref>, Stanford Cars <ref type="bibr" target="#b14">[15]</ref>, and Oxford Flowers <ref type="bibr" target="#b20">[21]</ref> -while achieving accuracies very close to those of separately trained networks for each individual task. This significantly outperforms prior work in terms of robustness to catastrophic forgetting, as well as the number and complexity of added tasks. We also show that our method is superior to joint training when adding the large-scale Places365 <ref type="bibr" target="#b29">[30]</ref> dataset to an ImageNet-trained network, and obtain competitive performance on a broad range of architectures, including VGG-16 with batch normalization <ref type="bibr" target="#b12">[13]</ref>, ResNets <ref type="bibr" target="#b8">[9]</ref>, and DenseNets <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>A few prior works and their variants, such as Learning without Forgetting (LwF) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref> and Elastic Weight Consolidation (EWC) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>, are aimed at training a net-  <ref type="figure">Figure 1</ref>: Illustration of the evolution of a 5×5 filter with steps of training. Initial training of the network for Task I learns a dense filter as illustrated in (a). After pruning by 60% (15/25) and re-training, we obtain a sparse filter for Task I, as depicted in (b), where white circles denote 0 valued weights. Weights retained for Task I are kept fixed for the remainder of the method, and are not eligible for further pruning. We allow the pruned weights to be updated for Task II, leading to filter (c), which shares weights learned for Task I. Another round of pruning by 33% (5/15) and re-training leads to filter (d), which is the filter used for evaluating on task II (Note that weights for Task I, in gray, are not considered for pruning). Hereafter, weights for Task II, depicted in orange, are kept fixed. This process is completed until desired, or we run out of pruned weights, as shown in filter (e). The final filter (e) for task III shares weights learned for tasks I and II. At test time, appropriate masks are applied depending on the selected task so as to replicate filters learned for the respective tasks.</p><p>work for multiple tasks sequentially. When adding a new task, LwF preserves responses of the network on older tasks by using a distillation loss <ref type="bibr" target="#b9">[10]</ref>, where response targets are computed using data from the current task. As a result, LwF does not require the storage of older training data, however, this very strategy can cause issues if the data for the new task belongs to a distribution different from that of prior tasks. As more dissimilar tasks are added to the network, the performance on the prior tasks degrades rapidly <ref type="bibr" target="#b17">[18]</ref>. EWC tries to minimize the change in weights that are important to previous tasks through the use of a quadratic constraint that tries to ensure that they do not stray too far from their initial values. Similar to LwF and EWC, we do not require the storage of older data. Like EWC, we want to avoid changing weights that are important to the prior tasks. We, however, do not use a soft constraint, but employ network pruning techniques to identify the most important parameters, as explained shortly. In contrast to these prior works, adding even a very unrelated new task using our method does not change performance on older tasks at all.</p><p>As neural networks have become deeper and larger, a number of works have emerged aiming to reduce the size of trained models, as well as the computation required for inference, either by reducing the numerical precision required for storing the network weights <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23]</ref>, or by pruning unimportant network weights <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. Our key idea is to use network pruning methods to free up parameters in the network, and then use these parameters to learn a new task. We adopt the simple weight-magnitude-based pruning method introduced in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> as it is able to prune over 50% of the parameters of the initial network. As we will discuss in Section 5.5, we also experimented with the filter-based pruning of <ref type="bibr" target="#b19">[20]</ref>, obtaining limited success due to the inability to prune aggressively. Our work is related to the very recent method proposed by Han et al. <ref type="bibr" target="#b6">[7]</ref>, which shows that sparsifying and retraining weights of a network serves as a form of regularization and improves performance on the same task. In contrast, we use iterative pruning and re-training to add multiple diverse tasks.</p><p>It is possible to limit performance loss on older tasks if one allows the network to grow as new tasks are added. One approach, called progressive neural networks <ref type="bibr" target="#b25">[26]</ref>, replicates the network architecture for every new dataset, with each new layer augmented with lateral connections to corresponding older layers. The weights of the new layers are optimized, while keeping the weights of the old layers frozen. The initial networks are thus unchanged, while the new layers are able to re-use representations from the older tasks. One unavoidable drawback of this approach is that the size of the full network keeps increasing with the number of added tasks. The overhead per dataset added for our method is lower than in <ref type="bibr" target="#b25">[26]</ref> as we only store one binary parameter selection mask per task, which can further be combined across tasks, as explained in the next section. Another recent idea, called PathNet <ref type="bibr" target="#b2">[3]</ref>, uses evolutionary strategies to select pathways through the network. They too, freeze older pathways while allowing newly introduced tasks to re-use older neurons. At a high hevel, our method aims at achieving similar behavior, but without resorting to computationally intensive search over architectures or pathways.</p><p>To our knowledge, our work presents the most extensive set of experiments on full-scale real image datasets and state-of-the-art architectures to date. Most existing work on transfer and multi-task learning, like <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26]</ref>, performed validation on small-image datasets (MNIST, CIFAR-10) or synthetic reinforcement learning environments (Atari, 3D maze games). Experiments with EWC and LwF have demonstrated the addition of just one task, or subsets of the same dataset <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>. By contrast, we demonstrate the successful combination of up to four tasks in a single network: starting with an ImageNet-trained VGG-16 network, we sequentially add three fine-grained classification tasks on CUBS birds <ref type="bibr" target="#b28">[29]</ref>, Stanford Cars <ref type="bibr" target="#b14">[15]</ref>, and Oxford Flowers <ref type="bibr" target="#b20">[21]</ref> datasets. We also combine ImageNet classification with scene classification on the Places365 <ref type="bibr" target="#b29">[30]</ref> dataset that has 1.8M training examples. In all experiments, our method achieves performance close to the best possible case of using one separate network per task. Further, we show that our pruning-based scheme generalizes to architectures with batch normalization <ref type="bibr" target="#b12">[13]</ref>, residual connections <ref type="bibr" target="#b8">[9]</ref>, and dense connections <ref type="bibr" target="#b10">[11]</ref>.</p><p>Finally, our work is related to incremental learning approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27]</ref>, which focus on the addition of classifiers or detectors for a few classes at a time. Our setting differs from theirs in that we explore the addition of entire image classification tasks or entire datasets at once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>The basic idea of our approach is to use network pruning techniques to create free parameters that can then be employed for learning new tasks, without adding extra network capacity. Training. <ref type="figure">Figure 1</ref> gives an overview of our method. We begin with a standard network learned for an initial task, such as the VGG-16 <ref type="bibr" target="#b27">[28]</ref> trained on ImageNet <ref type="bibr" target="#b24">[25]</ref> classification, referred to as Task I. The initial weights of a filter are depicted in gray in <ref type="figure">Figure 1 (a)</ref>. We then prune away a certain fraction of the weights of the network, i.e. set them to zero. Pruning a network results in a loss in performance due to the sudden change in network connectivity. This is especially pronounced when the pruning ratio is high. In order to regain accuracy after pruning, we need to re-train the network for a smaller number of epochs than those required for training. After a round of pruning and re-training, we obtain a network with sparse filters and minimal reduction in performance on Task I. The surviving parameters of Task I, those in gray in <ref type="figure">Figure 1</ref> (b), are hereafter kept fixed.</p><p>Next, we train the network for a new task, Task II, and let the pruned weights come back from zero, obtaining orange colored weights as shown in <ref type="figure">Figure 1</ref> (c). Note that the filter for Task II makes use of both the gray and orange weights, i.e. weights belonging to the previous task(s) are re-used. We once again prune the network, freeing up some parameters used for Task II only, and re-train for Task II to recover from pruning. This gives us the filter illustrated in <ref type="figure">Figure 1</ref> (d). At this point onwards, the weights for Tasks I and II are kept fixed. The available pruned parameters are then employed for learning yet another new task, resulting in green-colored weights shown in <ref type="figure">Figure 1</ref> (e). This process is repeated until all the required tasks are added or no more free parameters are available. In our experiments, pruning and re-training is about 1.5× longer than simple fine-tuning, as we generally re-train for half the training epochs.</p><p>Pruning Procedure. In each round of pruning, we remove a fixed percentage of eligible weights from every convolutional and fully connected layer. The weights in a layer are sorted by their absolute magnitude, and the lowest 50% or 75% are selected for removal, similar to <ref type="bibr" target="#b6">[7]</ref>. We use a oneshot pruning approach for simplicity, though incremental pruning has been shown to achieve better performance <ref type="bibr" target="#b7">[8]</ref>.</p><p>As previously stated, we only prune weights belonging to the current task, and do not modify weights that belong to a prior task. For example, in going from filter (c) to (d) in <ref type="figure">Figure 1</ref>, we only prune from the orange weights belonging to Task II, while gray weights of Task I remain fixed. This ensures no change in performance on prior tasks while adding a new task.</p><p>We did not find it necessary to learn task-specific biases similar to EWC <ref type="bibr" target="#b13">[14]</ref>, and keep the biases of all the layers fixed after the network is pruned and re-trained for the first time. Similarly, in networks that use batch normalization, we do not update the parameters (gain, bias) or running averages (mean, variance), after the first round of pruning and re-training. This choice helps reduce the additional pertask overhead, and it is justified by our results in the next section and further analysis performed in Section 5.</p><p>The only overhead of adding multiple tasks is the storage of a sparsity mask indicating which parameters are active for a particular task. By following the iterative training procedure, for a particular Task K, we obtain a filter that is the superposition of weights learned for that particular task and weights learned for all previous Tasks 1, · · · , K − 1. If a parameter is first used by Task K, it is used by all tasks K, · · · , N , where N is the total number of tasks. Thus, we need at most log 2 (N ) bits to encode the mask per parameter, instead of 1 bit per task, per parameter. The overhead for adding one and three tasks to the initial ImageNet-trained VGG-16 network (conv1 1 to fc 7) of size 537 MB is only ∼17 MB and ∼34 MB, respectively. A network with four tasks total thus results in a 1/16 increase with respect to the initial size, as a typical parameter is represented using 4 bytes, or 32 bits. <ref type="bibr" target="#b0">1</ref> Inference. When performing inference for a selected task, the network parameters are masked so that the network state matches the one learned during training, i.e. the filter from <ref type="figure">Figure 1</ref> (b) for inference on Task I, <ref type="figure">Figure 1</ref> (d) for inference on Task II, and so on. There is no additional run-time overhead as no extra computation is required; weights only have to be masked in a binary on/off fashion during multipli-cation, which can easily be implemented in the matrix-matrix multiplication kernels.</p><p>It is important to note that our pruning-based method is unable to perform simultaneous inference on all tasks as responses of a filter change depending on its level of sparsity, and are no longer separable after passing through a nonlinearity such as the ReLU. Performing filter-level pruning, in which an entire filter is switched on/off, instead of a single parameter, can allow for simultaneous inference. However, we show in Section 5.5 that such methods are currently limited in their pruning ability and cannot accommodate multiple tasks without significant loss in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>Datasets and Training Settings. We evaluate our method on two large-scale image datasets and three fine-grained classification datasets, as summarized in <ref type="table" target="#tab_1">Table 1</ref>.  In the case of the Stanford Cars and CUBS datasets, we crop object bounding boxes out of the input images and resize them to 224 × 224. For the other datasets, we resize the input image to 256 × 256 and take a random crop of size 224 × 224 as input. For all datasets, we perform left-right flips for data augmentation.</p><p>In all experiments, we begin with an ImageNet-trained network, as it is essential to have a good starting set of parameters. The only change we make to the network is the addition of a new output layer per each new task. After pruning the initial ImageNet-trained network, we fine-tune it on the ImageNet dataset for 10 epochs with a learning rate of 1e-3 decayed by a factor of 10 after 5 epochs. For adding fine-grained datasets, we use the same initial learning rate, decayed after 10 epochs, and train for a total of 20 epochs. For the larger Places365 dataset, we fine-tune for a total of 10 epochs, with learning rate decay after 5 epochs. When a network is pruned after training for a new task, we further fine-tune the network for 10 epochs with a constant learning rate of 1e-4. We use a batch size of 32 and the default dropout rates on all networks. Baselines. The simplest baseline method, referred to as Classifier Only, is to extract the fc7 or pre-classifier features from the initial network and only train a new classifier for each specific task, meaning that the performance on ImageNet remains the same. For training each new classifier layer, we use a constant learning rate of 1e-3 for 20 epochs.</p><p>The second baseline, referred to as Individual Networks, trains separate models for every task, achieving the highest possible accuracies by dedicating all the resources of the network for that single task. To obtain models for individual fine-grained tasks, we start with the ImageNet-trained network and fine-tune on the respective task for 20 epochs total with a learning rate of 1e-3 decayed by factor of 10 after 10 epochs.</p><p>Another baseline used in prior work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22]</ref> is Joint Training of a network for multiple tasks. However, joint fine-tuning is rather tricky when dataset sizes are different (e.g. ImageNet and CUBS), so we do not attempt it for our experiments with fine-grained datasets, especially since individually trained networks provide higher reference accuracies in any case. Joint training works better for similarlysized datasets, thus, when combining ImageNet and Places, we compare with the jointly trained network provided by the authors of <ref type="bibr" target="#b29">[30]</ref>.</p><p>Our final baseline is our own re-implementation of LwF <ref type="bibr" target="#b17">[18]</ref>. We use the same default settings as in <ref type="bibr" target="#b17">[18]</ref>, including a unit tradeoff parameter between the distillation loss and the loss on the training data for the new task. For adding fine-grained datasets with LwF, we use an initial learning rate of 1e-3 decayed after 10 epochs, and train for a total of 20 epochs. In the first 5 epochs, we train only the new classifier layer, as recommended in <ref type="bibr" target="#b17">[18]</ref>.</p><p>Multiple fine-grained classification tasks. <ref type="table" target="#tab_3">Table 2</ref> summarizes the experiments in which we add the three fine-grained tasks of CUBS, Cars, and Flowers classification in varying orders to the VGG-16 network. By comparing the Classifier Only and Individual Networks columns, we can clearly see that the fine-grained tasks benefit a lot by allowing the lower convolutional layers to change, with the top-1 error on cars and birds classification dropping from 56.42% to 13.97%, and from 36.76% to 22.57% respectively.</p><p>There are a total of six different orderings in which the three tasks can be added to the initial network. The Pruning columns of <ref type="table" target="#tab_3">Table 2</ref> report the averages of the top-1 errors obtained with our method across these six orderings, with three independent runs per ordering. Detailed exploration of the effect of ordering will be presented in the next section. By pruning and re-training the ImageNet-trained VGG-16 network by 50% and 75%, the top-1 error slightly increases from the initial 28.42% to 29.33% and 30.87%, respectively, and the top-5 error slightly increases from 9.61% to 9.99% and 10.93%. When three tasks are added to the 75% pruned initial network, we achieve errors CUBS, Stanford Cars, and Flowers that are only 2.38%, 1.78%, and 1.10% worse than the Individual Networks best case. At the same time, the errors are reduced by 11.04%, 30.41%, and 10.41% compared to the Classifier Only baseline. Not surprisingly, starting with a network that is initially pruned by a higher ratio results in better performance on the fine-grained tasks, as it   makes more parameters available for them. This especially helps the challenging Cars classification, reducing top-1 error from 18.08% to 15.75% as the initial pruning ratio is increased from 50% to 75%.</p><p>Our approach also consistently beats LwF on all datasets. As seen in <ref type="figure" target="#fig_1">Figure 2</ref>, while training for a new task, the error on older tasks increases continuously in the case of LwF, whereas it remains fixed for our method. The unpredictable change in older task accuracies for LwF is problematic, especially when we want to guarantee a specific level of performance.</p><p>Finally, as shown in the last row of <ref type="table" target="#tab_3">Table 2</ref>, our pruningbased model is much smaller than training separate networks per task (595 MB v/s 2,173 MB), and is only 33 MB larger than the classifier-only baseline.</p><p>Adding another large-scale dataset task. <ref type="table" target="#tab_4">Table 3</ref> shows the results of adding the large-scale Places365 classification task to a pruned ImageNet network. By adding Places365, which is larger than ImageNet (1.8 M images v/s 1.3 M images), to a 75% pruned ImageNet-trained network, we achieve top-1 error within 0.64% and top-5 error within 0.10% of an individually trained network. By contrast, the jointly trained baseline obtains performance much worse than an individual network for ImageNet (33.49% v/s 28.42% top-1 error). This highlights a common problem associated with joint training, namely, the need to balance mixing ratios between the multiple datasets which may or may not be complementary, and accommodate their possibly different hyperparamter requirements. In comparison, iterative pruning allows for a controlled decrease in prior task performance and for the use of different training hyperparameter settings per task. Further, we trained the pruned network on Places365 for 10 epochs only, while the joint and individual networks were trained for 60-90 epochs <ref type="bibr" target="#b29">[30]</ref>.</p><p>Extension to other networks. The results presented so far were obtained for the vanilla VGG-16 network, a simple and large network, well known to be full of redundancies <ref type="bibr" target="#b1">[2]</ref>. Newer architectures such as ResNets <ref type="bibr" target="#b8">[9]</ref> and DenseNets <ref type="bibr" target="#b10">[11]</ref> are much more compact, deeper, and better-performing. For comparison, the Classifier Only models of VGG-16, ResNet-50, and DenseNet-121 have 140 M, 27 M, and 8.6 M parameters respectively. It is not obvious how well pruning will work on the latter two parameter-efficient networks. Further, one might wonder whether sharing batch normalization parameters across diverse tasks might limit accuracy. <ref type="table">Table 4</ref> shows that our method can indeed be applied to all these architectures, which include residual connections, skip connections, and batch normalization. As described in Section 3, the batch normalization parameters (gain, bias, running means, and variances) are frozen after the network is pruned and retrained for ImageNet. In spite of this constraint, we achieve errors much lower than the baseline that only trains the last classifier layer. In almost all cases, we obtain errors within 1-2% of the best case scenario of one network     per task. While we tried learning separate batchnorm parameters per task and this further improved performance, we chose to freeze batchnorm parameters since it is simpler and avoids the overhead of storing these separate parameters (4 vectors per batchnorm layer). The deeper ResNet and DenseNet networks with 50 and 121 layers, respectively, are very robust to pruning, losing just 0.45% and 0.04% top-1 accuracy on ImageNet, respectively. Top-5 error increases by 0.05% for ResNet, and decreases by 0.13% for DenseNet. In the case of Flowers classification, we perform better than the individual network, probably because training the full network causes it to overfit to the Flowers dataset, which is the smallest. By using the fewer available parameters after pruning, we likely avoid this issue. Apart from obtaining good performance across a range of networks, an additional benefit of our pruning-based approach is that for a given task, the network can be pruned by small amounts iteratively so that the desirable trade-off between loss of current task accuracy and provisioning of free parameters for subsequent tasks can be achieved. Note that the fewer the parameters, the lower the mask storage overhead of our methods, as seen in the Size rows of <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Detailed Analysis</head><p>In this section, we investigate the factors that affect performance while using our method, and justify choices made such as freezing biases of the network. We also compare our weight-pruning approach with a filter-pruning approach, and confirm its benefits over the latter.  <ref type="table" target="#tab_3">Table 2</ref> reports the average over orderings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Effect of training order</head><p>16.75 M parameters are used by Task II, and 50.25 M free parameters available for subsequent tasks. Likewise, Task III uses around 13 M parameters and leaves around 37 M free parameters for Task IV. Accordingly, we observe a reduction of accuracy with order of training, as shown in <ref type="figure" target="#fig_3">Figure 3</ref>. For example, the top-1 error increases from 16.00% to 18.34% to 19.91% for the Stanford Cars dataset as we delay its addition to the network. For the datasets considered, the error increases by 3% on average when the order of addition is changed from first to third. Note that the results reported in <ref type="table" target="#tab_3">Table 2</ref> are averaged over all orderings for a particular dataset. These findings suggest that if it is possible to decide the ordering of tasks beforehand, the most challenging or unrelated task should be added first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Effect of pruning ratios</head><p>In <ref type="figure" target="#fig_2">Figure 4</ref>, we measure the effect of pruning and retraining for a task, when it is first added to a 50% pruned VGG-16 network (except for the initial ImageNet task). We consider this specific case in order to isolate the effect of pruning from the order of training discussed above. We observe that the errors for a task increase immediately upon pruning (⋆ markers) due to sudden change in network connectivity. However, upon re-training, the errors reduce, and might even drop below the original unpruned error, as seen for all datasets other than ImageNet at the 50% pruning ratio, in line with prior work <ref type="bibr" target="#b6">[7]</ref> which has shown that pruning and retraining can function as effective regularization. Multi-step pruning will definitely help reduce errors on ImageNet, as reported in <ref type="bibr" target="#b7">[8]</ref>. This plot shows that re-training is essential, especially when the pruning ratios are large.  The values above correspond to the case when the respective dataset is added as the first task, to an ImageNet-trained VGG-16 that is 50% pruned, except for the values corresponding to the ImageNet dataset which correspond to initial pruning. Note that the 0.75 pruning ratio values correspond to the blue bars in <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>Interestingly, for a newly added task, 50% and 75% pruning without re-training does not increase the error by much. More surprisingly, even a very aggressive single-shot pruning ratio of 90% followed by re-training results in a small error increase compared to the unpruned errors (top-1 error increases from 15.75% to 17.84% for Stanford Cars, 24.13% to 24.72% for CUBS, and 8.96% to 9.48% for Flowers). This indicates effective transfer learning as very few parameter modifications (10% of the available 50% of total parameters after pruning, or 5% of the total VGG-16 parameters) are enough to obtain good accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Effect of training separate biases</head><p>We do not observe any noticeable improvement in performance by learning task-specific biases per layer, as shown in <ref type="table">Table 5</ref>. Sharing biases reduces the storage overhead of our proposed method, as each convolutional, fully-connected, or batch-normalization layer can contain an associated bias term. We thus choose not to learn task-specific biases in our reported results.  <ref type="figure">Figure 5</ref>: This figure shows that having free parameters in the lower layers of the network is essential for good performance. The numbers above are obtained when a task is added to the 50% pruned VGG-16 network and the only the specified layers are finetuned, without any further pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Is training of all layers required?</head><p>Figure 5 measures the effect of modifying freed-up parameters from various layers for learning a new task. For this experiment, we start with the 50% pruned ImageNet-trained vanilla VGG-16 network, and add one new task. For the new task, we train pruned neurons from the specified layers only. Fine-tuning the fully connected layers improves accuracy over the classifier only baseline in all tasks. Further, finetuning the convolutional layers provides the biggest boost in accuracy, and is clearly necessary for obtaining good performance. By using our method, we can control the number of pruned parameters at each layer, allowing one to make use of task-specific requirements, when available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Comparison with filter-based pruning</head><p>For completeness, we report experiments with filter-based pruning <ref type="bibr" target="#b19">[20]</ref>, which eliminates entire filters, instead of sparsifying them. The biggest advantage of this strategy is that it enables simultaneous inference to be performed for all the trained tasks. For filters that survive a round of pruning, incoming weights on all filters that did not survive pruning (and are hence available for subsequent tasks) are set to 0. As a result, when new filters are learned for a new task, their outputs would not be used by filters of prior tasks. Thus, the output of a filter for a prior task would always remain the same, irrespective of filters learned for tasks added later. The method of <ref type="bibr" target="#b19">[20]</ref> ranks all filters in a network based on their importance to the current dataset, as measured by a metric related to the Taylor expansion of the loss function. We prune 400 filters per each epoch of ∼40,000 iterations, for a total of 10 epochs. Altogether, this eliminates 4,000 filters from a total of 12,416 in VGG-16, or ∼30% pruning. We could   <ref type="figure" target="#fig_3">Figure 3</ref>. Values in parentheses are top-5 errors, and the rest are top-1 errors.</p><p>not prune more aggressively without substantially reducing accuracy on ImageNet. A further unfavorable observation is that most of the pruned filters (3,730 out of 4,000) were chosen from the fully connected layers (Liu et al. <ref type="bibr" target="#b18">[19]</ref> proposed a different filter-based pruning method and found similar behavior for VGG-16). This frees up too few parameters in the lower layers of the network to be able to fine-tune effectively for new tasks. As a result, filter-based pruning only allowed us to add one extra task to the ImageNet-trained VGG-16 network, as shown in <ref type="table" target="#tab_11">Table 6</ref>. A final disadvantage of filterbased pruning methods is that they are more complicated and require careful implementation in the case of residual networks and skip connections, as noted by Li et al. <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we have presented a method to "pack" multiple tasks into a single network with minimal loss of performance on prior tasks. The proposed method allows us to modify all layers of a network and influence a large number of filters and features, which is necessary to obtain accuracies comparable to those of individually trained networks for each task. It works not only for the relatively "roomy" VGG-16 architecture, but also for more compact parameterefficient networks such as ResNets and DenseNets.</p><p>In the future, we are interested in exploring a more general framework for multi-task learning in a single network where we jointly train both the network weights and binary sparsity masks associated with individual tasks. In our current approach, the sparsity masks per task are obtained as a result of pruning, but it might be possible to learn such masks using techniques similar to those for learning networks with binary weights <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>a) Initial filter for Task I (b) Final filter for Task I (c) Initial filter for Task II (d) Final filter for Task II (e) Initial filter for Task III 60% pruning + re-training 33% pruning + re-training training training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Change in errors on prior tasks as new tasks are added for LwF (left) and our method (right). For LwF, errors on prior datasets increase with every added dataset. For our pruning-based method, the error remains the same even after a new dataset is added.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4 :</head><label>4</label><figDesc>Results on additional network types. Values in parentheses are top-5 errors, while all others are top-1 errors. The results in the pruning column are averaged over 18 runs with varying order of training of the 3 datasets (6 possible orderings, 3 runs per ordering). Classifier Only and Individual Network values are averaged over 3 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Dependence of errors on individual tasks on the order of task addition (see text for details). Each displayed value and error bar are obtained from 6 different runs. We use an initial pruning ratio of 50% for the ImageNet-trained VGG-16 and a pruning ratio of 75% after each dataset is added. 0.50, 0.75, 0.75 pruning column of Table 2 reports the average over orderings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: This plot measures the change in top-1 error with pruning. The values above correspond to the case when the respective dataset is added as the first task, to an ImageNet-trained VGG-16 that is 50% pruned, except for the values corresponding to the ImageNet dataset which correspond to initial pruning. Note that the 0.75 pruning ratio values correspond to the blue bars in Figure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Summary of datasets used.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Errors on fine-grained tasks. Values in parentheses are top-5 errors, while all others are top-1 errors. The numbers at the top of the Pruning columns indicate the ratios by which the network is pruned after each successive task.</figDesc><table>For example, 0.50, 0.75, 0.75 indicates that 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Results when an ImageNet-trained VGG-16 network is pruned by 50% and 75% and the Places dataset is added to it. Values in parentheses are top-5 errors, while all others are top-1 errors. * indicates models downloaded from https://github.com/ CSAILVision/places365, trained by [30].</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 6 :</head><label>6</label><figDesc>Comparison of filter-based and weight-based pruning for ImageNet-trained VGG-16. This table reports errors after adding only one task to the 30% filter-pruned and 50% weight-pruned network. Values in the Weights column correspond to the blue bars in</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In practice, we store masks inside a PyTorch ByteTensor (1 byte = 8 bits) due to lack of support for arbitrary-precision storage.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">As more tasks are added to a network, a larger fraction of the network becomes unavailable for tasks that are subsequently added. Consider the 0.50, 0.75, 0.75 pruning ratio sequence for the VGG-16 network. The layers from conv1 1 to fc 7 contain around 134 M parameters. After the initial round of 50% pruning for Task I (ImageNet classification), we have ∼67 M free parameters. After the second round of training followed by 75% pruning and re-training,</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Table 5: No noticeable difference in performance is observed by learning task-specific biases. Values are averaged across all 6 task orderings, with 3 runs per ordering. The shared bias column corresponds to the 0.50, 0.75, 0.75 Pruning column of Table 2.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: We would like to thank Maxim Raginsky for a discussion that gave rise to the initial idea of this paper, and Greg Shakhnarovich for suggesting the name PackNet. This material is based upon work supported in part by the National Science Foundation under Grants No. 1563727 and 1718221.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Expert Gate: Lifelong learning with a network of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An analysis of deep neural network models for practical applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Canziani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCAS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Banarse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pathnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08734</idno>
		<title level="m">Evolution channels gradient descent in super neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning with limited numerical precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DSD: Densesparse-dense training for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Binarized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PNAS</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting by incremental moment matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS, 2017. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning efficient convolutional networks through network slimming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pruning convolutional neural networks for resource efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVGIP</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Encoder based lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Xnornet: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">iCaRL: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
		<respStmt>
			<orgName>ImageNet Large Scale Visual Recognition Challenge. IJCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04671</idno>
	</analytic>
	<monogr>
		<title level="j">Progressive neural networks</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Incremental learning of object detectors without catastrophic forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
