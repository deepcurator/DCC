<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hide-and-Seek: Forcing a Network to be Meticulous for Weakly-supervised Object and Action Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hide-and-Seek: Forcing a Network to be Meticulous for Weakly-supervised Object and Action Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We propose 'Hide-and-Seek', a weakly- </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Weakly-supervised approaches have been proposed for various visual classification and localization tasks including object detection <ref type="bibr" target="#b53">[55,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b59">61,</ref><ref type="bibr" target="#b38">40]</ref>, semantic segmentation <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b24">26]</ref> and visual attribute localization <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b37">39]</ref>. The main advantage of weaklysupervised learning is that it requires less detailed annotations compared to the fully-supervised setting, and therefore has the potential to use the vast weakly-annotated visual data available on the Web. For example, weakly-supervised object detectors can be trained using only image-level labels ('dog' or 'no dog') without any object location annotations.</p><p>Existing weakly-supervised methods identify discriminative patterns in the training data that frequently appear in one class and rarely in the remaining classes. This is done either explicitly by mining discriminative image regions or features <ref type="bibr" target="#b53">[55,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b38">40]</ref> or implicitly by analyzing the higher-layer activation maps produced by a deep network trained for image classification <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b59">61]</ref>. However, due to intra-category variations or relying only on a classification objective, these methods often fail to identify the entire extent of the object and instead localize only the  Recent work tries to address this issue of identifying only the most discriminative part. Song et al. <ref type="bibr" target="#b41">[43]</ref> combine multiple co-occurring discriminative regions to cover a larger extent of the object. While multiple selections ensure larger coverage, it does not guarantee selection of less discriminative patches of the object in the presence of many highly discriminative ones. Singh et al. <ref type="bibr" target="#b38">[40]</ref> use motion cues and transfer tracked object boxes from weakly-labeled videos to the images. However, this approach requires additional weakly-labeled videos, which may not always be available. Finally, Zhou et al. <ref type="bibr" target="#b59">[61]</ref> replace max pooling with global average pooling after the final convolution layer of an image classification network. Since average pooling aggregates activations across an entire feature map, it encourages the network to look beyond the most discriminative part (which would suffice for max pooling). However, the network can still avoid finding less discriminative parts if identifying a few highly-discriminative parts can lead to accurate classification performance, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>(top row).</p><p>Main Idea. In this paper, we take a radically different approach to this problem. Instead of making algorithmic changes <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b59">61]</ref> or relying on external data <ref type="bibr" target="#b38">[40]</ref>, we make changes to the input image. The key idea is to hide patches from an image during training so that the model needs to seek the relevant object parts from what remains. We thus name our approach 'Hide-and-Seek'. <ref type="figure" target="#fig_1">Figure 1</ref> (bottom row) demonstrates the intuition: if we randomly remove some patches from the image then there is a possibility that the dog's face, which is the most discriminative, will not be visible to the model. In this case, the model must seek other relevant parts like the tail and legs in order to do well on the classification task. By randomly hiding different patches in each training epoch, the model sees different parts of the image and is forced to focus on multiple relevant parts of the object beyond just the most discriminative one. Importantly, we only apply this random hiding of patches during training and not during testing. Since the full image is observed during testing, the data distribution will be different to that seen during training. We show that setting the hidden pixels' value to be the data mean can allow the two distributions to match, and provide a theoretical justification.</p><p>Since Hide-and-Seek only alters the input image, it can easily be generalized to different neural networks and tasks. In this work, we demonstrate its applicability on AlexNet <ref type="bibr" target="#b26">[28]</ref> and GoogLeNet <ref type="bibr" target="#b44">[46]</ref>, and apply the idea to weakly-supervised object localization in images and weakly-supervised action localization in videos. For the temporal action localization task (in which the start and end times of an action need to be found), random frame sequences are hidden while training a network on action classification, which forces the network to learn the relevant frames corresponding to an action.</p><p>Contributions. Our work has three main contributions: 1) We introduce the idea of Hide-and-Seek for weaklysupervised localization and produce state-of-the-art object localization results on the ILSVRC dataset <ref type="bibr" target="#b34">[36]</ref>; 2) We demonstrate the generalizability of the approach on different networks and layers; 3) We extend the idea to the relatively unexplored task of weakly-supervised temporal action localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Weakly-supervised object localization. Fullysupervised convolutional networks (CNNs) have demonstrated great performance on object detection <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">30]</ref>, segmentation <ref type="bibr" target="#b29">[31]</ref> and attribute localization <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b58">60,</ref><ref type="bibr" target="#b25">27]</ref>, but require expensive human annotations for training (e.g. bounding box for object localization). To alleviate expensive annotation costs, weakly-supervised approaches learn using cheaper labels, for example, image-level labels for predicting an object's location <ref type="bibr" target="#b53">[55,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b59">61]</ref>.</p><p>Most weakly-supervised object localization approaches mine discriminative features or patches in the data that frequently appear in one class and rarely in other classes <ref type="bibr" target="#b53">[55,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b7">8]</ref>. However, these approaches tend to focus only on the most discriminative parts, and thus fail to cover the entire spatial extent of an object. In our approach, we hide image patches (randomly) during training, which forces our model to focus on multiple parts of an object and not just the most discriminative ones. Other methods use additional motion cues from weakly-labeled videos to improve object localization <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b38">40]</ref>. While promising, such videos are not always readily available and can be challenging to obtain especially for static objects. In contrast, our method does not require any additional data or annotations. Recent work modify CNN architectures designed for image classification so that the convolutional layers learn to localize objects while performing image classification <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b59">61]</ref>. Other network architectures have been designed for weakly-supervised object detection <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">24]</ref>. Although these methods have significantly improved the state-of-theart, they still essentially rely on a classification objective and thus can fail to capture the full extent of an object if the less discriminative parts do not help improve classification performance. We also rely on a classification objective. However, rather than modifying the CNN architecture, we instead modify the input image by hiding random patches from it. We demonstrate that this enforces the network to give attention to the less discriminative parts and ultimately localize a larger extent of the object.</p><p>Masking pixels or activations. Masking image patches has been applied for object localization <ref type="bibr" target="#b0">[1]</ref>, self-supervised feature learning <ref type="bibr" target="#b32">[34]</ref>, semantic segmentation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b9">10]</ref>, generating hard occlusion training examples for object detection <ref type="bibr" target="#b52">[54]</ref>, and to visualize and understand what a CNN has learned <ref type="bibr" target="#b57">[59]</ref>. In particular, for object localization, <ref type="bibr" target="#b57">[59,</ref><ref type="bibr" target="#b0">1]</ref> train a CNN for image classification and then localize the regions whose masking leads to a large drop in classification performance. Since these approaches mask out the image regions only during testing and not during training, the localized regions are limited to the highly-discriminative object parts. In our approach, image regions are masked during training, which enables the model to learn to focus on even the less discriminative object parts. Finally, our work is closely related to the adversarial erasing method of <ref type="bibr" target="#b54">[56]</ref>, which iteratively trains a sequence of models for weaklysupervised semantic segmentation. Each model identifies the relevant object parts conditioned on the previous iteration model's output. In contrast, we only train a single model once-and is thus less expensive-and do not rely on saliency detection to refine the localizations as done in <ref type="bibr" target="#b54">[56]</ref>.</p><p>Dropout <ref type="bibr" target="#b42">[44]</ref> and its variants <ref type="bibr" target="#b47">[49,</ref><ref type="bibr" target="#b45">47]</ref> are also related. There are two main differences: (1) these methods are designed to prevent overfitting while our work is designed to improve localization; and (2) in dropout, units in a layer are For each training image, we divide it into a grid of S × S patches. Each patch is then randomly hidden with probability p hide and given as input to a CNN to learn image classification. The hidden patches change randomly across different epochs. Right: During testing, the full image without any hidden patches is given as input to the trained network.</p><p>dropped randomly, while in our work, contiguous image regions or video frames are dropped. We demonstrate in the experiments that our approach produces significantly better localizations compared to dropout.</p><p>Action localization. Action localization is a well studied problem <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">23]</ref>. Recent CNN-based approaches <ref type="bibr" target="#b56">[58,</ref><ref type="bibr" target="#b35">37]</ref> have shown superior performance compared to previous hand-crafted approaches. These fullysupervised methods require the start and end time of an action in the video during the training to be annotated, which can be expensive to obtain. Weakly-supervised approaches learn from movie scripts <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b11">12]</ref> or an ordered list of actions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref>. Sun et al. <ref type="bibr" target="#b43">[45]</ref> combine weakly-labeled videos with web images for action localization. In contrast to these approaches, our approach only uses a single video-level action label for temporal action localization.</p><p>[14] also only use video-level action labels for action localization with the focus on finding the key event frames of an action. We instead focus on localizing the full extent of an action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section, we first describe our Hide-and-Seek algorithm for object localization in images followed by action localization in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Weakly-supervised object localization</head><p>For weakly-supervised object localization, we are given a set of images I set = {I 1 , I 2 , ....., I N } in which each image I is labeled only with its category label. Our goal is to learn an object localizer that can predict both the category label as well as the bounding box for the object-of-interest in a new test image I test . In order to learn the object localizer, we train a CNN which simultaneously learns to localize the object while performing the image classification task. While numerous approaches have been proposed to solve this problem, existing methods (e.g., <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b59">61]</ref>) are prone to localizing only the most discriminative object parts, since those parts are sufficient for optimizing the classification task.</p><p>To enforce the network to learn all of the relevant parts of an object, our key idea is to randomly hide patches of each input image I during training, as we explain next.</p><p>Hiding random image patches. The purpose of hiding patches is to show different parts of an object to the network while training it for the classification task. By hiding patches randomly, we can ensure that the most discriminative parts of an object are not always visible to the network, and thus force it to also focus on other relevant parts of the object. In this way, we can overcome the limitation of existing weakly-supervised methods that focus only on the most discriminative parts of an object.</p><p>Concretely, given a training image I of size W × H × 3, we first divide it into a grid with a fixed patch size of S×S× 3. This results in a total of (W × H)/(S × S) patches. We then hide each patch with p hide probability. For example, in <ref type="figure" target="#fig_2">Fig. 2</ref> left, the image is of size 224 × 224 × 3, and it is divided into 16 patches of size 56 × 56 × 3. Each patch is hidden with p hide = 0.5 probability. We take the new image I ′ with the hidden patches, and feed it as a training input to a CNN for classification.</p><p>Importantly, for each image, we randomly hide a different set of patches. Also, for the same image, we randomly hide a different set of patches in each training epoch. This property allows the network to learn multiple relevant object parts for each image. For example, in <ref type="figure" target="#fig_2">Fig. 2</ref> left, the network sees a different I ′ in each epoch due to the randomness in hiding of the patches. In the first epoch, the dog's face is hidden while its legs and tail are clearly visible. In contrast, in the second epoch, the face is visible while the legs and tail are hidden. Thus, the network is forced to learn all of the relevant parts of the dog rather than only the highly discriminative part (i.e., the face) in order to perform well in classifying the image as a 'dog'.</p><p>We hide patches only during training. During testing, the full image-without any patches hidden-is given as input to the network; <ref type="figure" target="#fig_2">Fig. 2</ref> right. Since the network has learned to focus on multiple relevant parts during training, it is not necessary to hide any patches during testing. This is in direct contrast to <ref type="bibr" target="#b0">[1]</ref>, which hides patches during testing but not during training. For <ref type="bibr" target="#b0">[1]</ref>, since the network has already learned to focus on the most discimirinative parts during training, it is essentially too late, and hiding patches during testing has no significant effect on localization performance.</p><p>Setting the hidden pixel values. There is an important detail that we must be careful about. Due to the discrepancy of hiding patches during training while not hiding patches during testing, the first convolutional layer activations during training versus testing will have different distributions. For a trained network to generalize well to new test data, the activation distributions should be roughly equal. That is, for any unit in a neural network that is connected to x units with w outgoing weights, the distribution of w ⊤ x should be roughly the same during training and testing. However, in our setting, this will not necessarily be the case since some patches in each training image will be hidden while none of the patches in each test image will ever be hidden.</p><p>Specifically, in our setting, suppose that we have a convolution filter F with kernel size K × K and threedimensional weights W = {w 1 , w 2 , ...., w k×k }, which is applied to an RGB patch X = {x 1 , x 2 , ...., x k×k } in image I ′ . Denote v as the vector representing the RGB value of every hidden pixel. There are three types of activations:</p><p>1. F is completely within a visible patch <ref type="figure" target="#fig_3">(Fig. 3, blue</ref> box). The corresponding output will be</p><formula xml:id="formula_0">k×k i=1 w ⊤ i x i .</formula><p>2. F is completely within a hidden patch <ref type="figure" target="#fig_3">(Fig. 3, red</ref> box). The corresponding output will be</p><formula xml:id="formula_1">k×k i=1 w ⊤ i v.</formula><p>3. F is partially within a hidden patch <ref type="figure" target="#fig_3">(Fig. 3</ref>, green box).</p><p>The corresponding output will be</p><formula xml:id="formula_2">m∈visible w ⊤ m x m + n∈hidden w ⊤ n v.</formula><p>During testing, F will always be completely within a visible patch, and thus its output will be k×k i=1 w ⊤ i x i . This matches the expected output during training in only the first case. For the remaining two cases, when F is completely or partially within a hidden patch, the activations will have a distribution that is different to those seen during testing.</p><p>We resolve this issue by setting the RGB value v of a hidden pixel to be equal to the mean RGB vector of the images over the entire dataset:</p><formula xml:id="formula_3">v = µ = 1 N pixels j x j ,</formula><p>where j indexes all pixels in the entire training dataset and N pixels is the total number of pixels in the dataset. Why would this work? Essentially, we are assuming that in expectation, the output of a patch will be equal to that of an average-valued patch:</p><formula xml:id="formula_4">E[ k×k i=1 w ⊤ i x i ] = k×k i=1 w ⊤ i µ.</formula><p>By replacing v with µ, the outputs of both the second and third cases will be k×k i=1 w ⊤ i µ, and thus will match the expected output during testing (i.e., of a fully-visible patch). This process is related to the scaling procedure in dropout <ref type="bibr" target="#b42">[44]</ref>, in which the outputs are scaled proportional to the drop rate during testing to match the expected output during training. In dropout, the outputs are dropped uniformly across the entire feature map, independently of spatial location. If we view our hiding of the patches as equivalent to "dropping" units, then in our case, we cannot have a global scale factor since the output of a patch depends on whether there are any hidden pixels. Thus, we instead set the hidden values to be the expected pixel value of the training data as described above, and do not scale the corresponding output. Empirically, we find that setting the hidden pixel in this way is crucial for the network to behave similarly during training and testing.</p><p>Object localization network architecture. Our approach of hiding patches is independent of the network architecture and can be used with any CNN designed for object localization. For our experiments, we choose to use the network of Zhou et al. <ref type="bibr" target="#b59">[61]</ref>, which performs global average pooling (GAP) over the convolution feature maps to generate a class activation map (CAM) for the input image that represents the discriminative regions for a given class. This approach has shown state-of-the-art performance for the ILSVRC localization challenge <ref type="bibr" target="#b34">[36]</ref> in the weakly-supervised setting, and existing CNN architectures like AlexNet <ref type="bibr" target="#b26">[28]</ref> and GoogLeNet <ref type="bibr" target="#b44">[46]</ref> can easily be modified to generate a CAM. <ref type="bibr" target="#b0">1</ref> For the third case:</p><formula xml:id="formula_5">m∈visible w ⊤ m xm + n∈hidden w ⊤ n µ ≈ m∈visible w ⊤ m µ + n∈hidden w ⊤ n µ = k×k i=1 w ⊤ i µ.</formula><p>To generate a CAM for an image, global average pooling is performed after the last convolutional layer and the result is given to a classification layer to predict the image's class probabilities. The weights associated with a class in the classification layer represent the importance of the last convolutional layer's feature maps for that class. More formally, denote F = {F 1 , F 2 , .., F M } to be the M feature maps of the last convolutional layer and W as the N × M weight matrix of the classification layer, where N is number of classes. Then, the CAM for class c for image I is:</p><formula xml:id="formula_6">CAM (c, I) = M i=1 W (c, i) · F i (I).<label>(1)</label></formula><p>Given the CAM for an image, we generate a bounding box using the method proposed in <ref type="bibr" target="#b59">[61]</ref>. Briefly, we first threshold the CAM to produce a binary foreground/background map, and then find connected components among the foreground pixels. Finally, we fit a tight bounding box to the largest connected component. We refer the reader to <ref type="bibr" target="#b59">[61]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Weakly-supervised action localization</head><p>Given a set of untrimmed videos V set = {V 1 , V 2 , ..., V N } and video class labels, our goal here is to learn an action localizer that can predict the label of an action as well as its start and end time for a test video V test . Again the key issue is that for any video, a network will focus mostly on the highly-discriminative frames in order to optimize classification accuracy instead of identifying all relevant frames. Inspired by our idea of hiding the patches in images, we propose to hide frames in videos to improve action localization.</p><p>Specifically, during training, we uniformly sample video F total frames from each videos. We then divide the F total frames into continuous segments of fixed size F segment ; i.e., we have F total /F segemnt segments. Just like with image patches, we hide each segment with probability p hide before feeding it into a deep action localizer network. We generate class activation maps (CAM) using the procedure described in the previous section. In this case, our CAM is a one-dimensional map representing the discriminative frames for the action class. We apply thresholding on this map to obtain the start and end times for the action class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We perform quantitative and qualitative evaluations of Hide-and-Seek for object localization in images and action localization in videos. We also perform ablative studies to compare the different design choices of our algorithm.</p><p>Datasets and evaluation metrics. We use ILSVRC 2016 <ref type="bibr" target="#b34">[36]</ref> to evaluate object localization accuracy. For training, we use 1.2 million images with their class labels (1000 categories). We compare our approach with the baselines on the validation data. We use three evaluation metrics to measure performance: 1) Top-1 localization accuracy (Top-1 Loc): fraction of images for which the predicted class with the highest probability is the same as the groundtruth class and the predicted bounding box for that class has more than 50% IoU with the ground-truth box. 2) Localization accuracy with known ground-truth class (GT-known Loc): fraction of images for which the predicted bounding box for the ground-truth class has more than 50% IoU with the ground-truth box. As our approach is primarily designed to improve localization accuracy, we use this criterion to measure localization accuracy independent of classification performance. 3) We also use classification accuracy (Top-1 Clas) to measure the impact of Hide-and-Seek on image classification performance.</p><p>For action localization, we use THUMOS 2014 validation data <ref type="bibr">[22]</ref>, which consists of 1010 untrimmed videos belonging to 101 action classes. We train over all untrimmed videos for the classification task and then evaluate localization on the 20 classes that have temporal annotations. Each video can contain multiple instances of a class. For evaluation we compute mean average precision (mAP), and consider a prediction to be correct if it has IoU &gt; θ with ground-truth. We vary θ to be 0.1, 0.2, 0.3, 0.4, and 0.5. As we are focusing on localization ability of the network, we assume we know the ground-truth class label of the video.</p><p>Implementation details. To learn the object localizer, we use the same modified AlexNet and GoogLeNet networks introduced in <ref type="bibr" target="#b59">[61]</ref> (AlexNet-GAP and GoogLeNet-GAP). AlexNet-GAP is identical to AlexNet until pool5 (with stride 1) after which two new conv layers are added. Similarly for GoogLeNet-GAP, layers after inception-4e are removed and a single conv layer is added. For both AlexNet-GAP and GoogLeNet-GAP, the output of the last conv layer goes to a global average pooling (GAP) layer, followed by a softmax layer for classification. Each added conv layer has 512 and 1024 kernels of size 3 × 3, stride 1, and pad 1 for AlexNet-GAP and GoogLeNet-GAP, respectively.</p><p>We train the networks from scratch for 55 and 40 epochs for AlexNet-GAP and GoogLeNet-GAP, respectively, with a batch size of 128 and initial learning rate of 0.01. We gradually decrease the learning rate to 0.0001. We add batch normalization <ref type="bibr" target="#b18">[19]</ref> after every conv layer to help convergence of GoogLeNet-GAP. For simplicity, unlike the original AlexNet architecture <ref type="bibr" target="#b26">[28]</ref>, we do not group the conv filters together (it produces statistically the same Top-1 Loc accuracy as the grouped version for both AlexNet-GAP but has better image classification performance). The network remains exactly the same with (during training) and without (during testing) hidden image patches. To obtain the binary fg/bg map, 20% and 30% of the max value of the CAM is chosen as the threshold for AlexNet-GAP and GoogLeNet-  <ref type="table">Table 1</ref>. Localization accuracy on ILSVRC validation data with different patch sizes for hiding. Our Hide-and-Seek always performs better than AlexNet-GAP <ref type="bibr" target="#b59">[61]</ref>, which sees the full image.</p><p>GAP, respectively; the thresholds were chosen by observing a few qualitative results on training data. During testing, we average 10 crops (4 corners plus center, and same with horizontal flip) to obtain class probabilities and localization maps. We find similar localization/classification performance when fine-tuning pre-trained networks.</p><p>For action localization, we compute C3D <ref type="bibr" target="#b46">[48]</ref> fc7 features using a model pre-trained on Sports 1 million <ref type="bibr" target="#b23">[25]</ref>. We compute 10 feats/sec (each feature is computed over 16 frames) and uniformly sample 2000 features from the video. We then divide the video into 20 equal-length segments each consisting of F segment = 100 features. During training, we hide each segment with p hide = 0.5. For action classification, we feed C3D features as input to a CNN with two conv layers followed by a global max pooling and softmax classification layer. Each conv layer has 500 kernels of size 1 × 1, stride 1. For any hidden frame, we assign it the dataset mean C3D feature. For thresholding, 50% of the max value of the CAM is chosen. All continuous segments after thresholding are considered predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Object localization quantitative results</head><p>We first analyze object localization accuracy on the ILSVRC validation data. <ref type="table">Table 1</ref> shows the results using the Top-1 Loc and GT-known Loc evaluation metrics. AlexNet-GAP <ref type="bibr" target="#b59">[61]</ref> is our baseline in which the network has seen the full image during training without any hidden patches. Alex-HaS-N is our approach, in which patches of size N × N are hidden with 0.5 probability during training.</p><p>Which patch size N should we choose? We explored four different patch sizes N = {16, 32, 44, 56}, and each performs significantly better than AlexNet-GAP for both GT-known Loc and Top-1 Loc. Our GoogLeNet-HaS-N models also outperfors GoogLeNet-GAP for all patch sizes. These results clearly show that hiding patches during training leads to better localization. Although our approach can lose some classification accuracy (Top-1 Clas) since it has  <ref type="table">Table 2</ref>. Localization accuracy on ILSVRC val data compared to state-of-the-art. Our method outperforms all previous methods.</p><p>never seen a complete image and thus may not have learned to relate certain parts, the huge boost in localization performance (which can be seen by comparing the GT-known Loc accuracies) makes up for any potential loss in classification.</p><p>We also train a network (AlexNet-HaS-Mixed) with mixed patch sizes. During training, for each image in every epoch, the patch size N to hide is chosen randomly from 16, 32, 44 and 56 as well as no hiding (full image). Since different sized patches are hidden, the network can learn complementary information about different parts of an object (e.g. small/large patches are more suitable to hide smaller/larger parts). Indeed, we achieve the best results for Top-1 Loc using AlexNet-HaS-Mixed.</p><p>Comparison to state-of-the-art. Next, we choose our best model for AlexNet and GoogLeNet, and compare it with state-of-the-art methods on ILSVRC validation data; see <ref type="table">Table 2</ref>. Our method performs 3.78% and 1.40% points better than AlexNet-GAP <ref type="bibr" target="#b59">[61]</ref> on GT-known Loc and Top-1 Loc, respectively. For GoogLeNet, our model gets a boost of 1.88% and 1.61% points compared to GoogLeNet-GAP for GT-known Loc and Top-1 Loc accuracy, respectively. Importantly, these gains are obtained simply by changing the input image without changing the network architecture.</p><p>Ensemble model. Since each patch size provides complementary information (as seen in the previous section), we also create an ensemble model of different patch sizes (Ours-ensemble). To produce the final localization for an image, we average the CAMs obtained using AlexNet-HaS-16, 32, 44, and 56, while for classification, we average the classification probabilities of all four models as well as the probability obtained using AlexNet-GAP. This ensemble model gives a boost of 5.24 % and 4.15% over AlexNet-GAP for GT-known Loc and Top-1 Loc, respectively. For a more fair comparison, we also combine the results of five independent AlexNet-GAPs to create an ensemble baseline. Ours-ensemble outperforms this strong baseline (AlexNet-GAP-ensemble) by 3.23% and 1.82% for GT-known Loc and Top-1 Loc, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object localization qualitative results</head><p>In <ref type="figure" target="#fig_5">Fig. 4</ref>, we visualize the class activation map (CAM) and bounding box obtained by our AlexNet-HaS approach versus those obtained with AlexNet-GAP. In each image pair, the first image shows the predicted (green) and groundtruth (red) bounding box. The second image shows the CAM, i.e., where the network is focusing for that class. Our approach localizes more relevant parts of an object compared to AlexNet-GAP and is not confined to only the most discriminative parts. For example, in the first, second, and fifth rows AlexNet-GAP only focuses on the face of the animals, whereas our method also localizes parts of the body. Similarly, in the third and last rows AlexNet-GAP misses the tail for the snake and squirrel while ours gets the tail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Further Analysis of Hide-and-Seek</head><p>Comparison with dropout. Dropout <ref type="bibr" target="#b42">[44]</ref> has been extensively used to reduce overfitting in deep network. Although it is not designed to improve localization, the dropping of units is related to our hiding of patches. We therefore conduct an experiment in which 50% dropout is applied at the image layer. We noticed that the due to the large dropout rate at the pixel-level, the learned filters de-  <ref type="table">Table 5</ref>. Applying Hide-and-Seek to the first conv layer. The improvement over <ref type="bibr" target="#b59">[61]</ref> shows the generality of the idea.</p><p>velop a bias toward a dropped-out version of the images and produces significantly inferior classification and localization performance (AlexNet-dropout-trainonly). If we also do dropout during testing (AlexNet-dropout-traintest) then performance improves but is still much lower compared to our approach <ref type="table">Table 3</ref>. Since dropout drops pixels (and RGB channels) randomly, information from the most relevant parts of an object will still be seen by the network with high probability, which makes it likely to focus on only the most discriminative parts.</p><p>Do we need global average pooling? <ref type="bibr" target="#b59">[61]</ref> showed that GAP is better than global max pooling (GMP) for object localization, since average pooling encourages the network to focus on all the discriminative parts. For max pooling, only the most discriminative parts need to contribute. But is global max pooling hopeless for localization? With our Hide-and-Seek, even with max pooling, the network is forced to focus on a different discriminative parts. In <ref type="table">Table 4</ref>, we see that max pooling (AlexNet-GMP) is inferior to average poling (AlexNet-GAP) for the baselines. But with Hide-and-Seek, max pooling (AlexNetMax-HaS) localization accuracy increases by a big margin and even slightly outperforms average pooling (AlexNetAvg-HaS). The slight improvement is likely due to max pooling being more robust to noise.</p><p>Hide-and-Seek in convolutional layers. We next apply our idea to convolutional layers. We divide the convolutional feature maps into a grid and hide each patch (and all of its corresponding channels) with 0.5 probability. We hide patches of size 5 (AlexNet-HaS-conv1-5) and 11 (AlexNetHaS-conv1-11) in the conv1 feature map (which has size 55×55×96).   <ref type="table">Table 7</ref>. Action localization accuracy on THUMOS validation data. Across all 5 IoU thresholds, our Video-HaS outperforms the full video baseline (Video-full).</p><p>shows that our idea of randomly hiding patches can be generalized to the convolutional layers.</p><p>Probability of hiding. In all of the previous experiments, we hid patches with 50% probability. In <ref type="table" target="#tab_4">Table 6</ref>, we measure the GT-known Loc and Top-1 Loc when we use different hiding probabilities. If we increase the probability then GT-known Loc remains almost the same while Top-1 Loc decreases a lot. This happens because the network sees fewer pixels when the hiding probability is high; as a result, classification accuracy reduces and Top-1 Loc drops. If we decrease the probability then GT-known Loc decreases but our Top-1 Loc improves. In this case, the network sees more pixels so its classification improves but since less parts are hidden, it will focus more on only the discriminative parts decreasing its localization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Action localization results</head><p>Finally, we evaluate action localization accuracy. We compare our approach (Video-HaS), which randomly hides frame segments while learning action classification, with a baseline that sees the full video (Video-full). <ref type="table">Table 7</ref> shows the result on THUMOS validation data. Video-HaS consistently outperforms Video-full for action localization task, which shows that hiding frames forces our network to focus on more relevant frames, which ultimately leads to better action localization. We show qualitative results in the supp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented 'Hide-and-Seek', a novel weaklysupervised framework to improve object localization in images and temporal action localization in videos. By randomly hiding patches/frames in a training image/video, we force the network to learn to focus on multiple relevant parts of an object/action. Our extensive experiments showed improved localization accuracy over state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Main idea. (Top row) A network tends to focus on the most discriminative parts of an image (e.g., face of the dog) for classification. (Bottom row) By hiding images patches randomly, we can force the network to focus on other relevant object parts in order to correctly classify the image as 'dog'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Approach overview. Left: For each training image, we divide it into a grid of S × S patches. Each patch is then randomly hidden with probability p hide and given as input to a CNN to learn image classification. The hidden patches change randomly across different epochs. Right: During testing, the full image without any hidden patches is given as input to the trained network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. There are three types of convolutional filter activations after hiding patches: a convolution filter can be completely within a visible region (blue box), completely within a hidden region (red box), or partially within a visible/hidden region (green box).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Qualitative object localization results. We compare our approach with AlexNet-GAP [61] on the ILVRC validation data. For each image, we show the bounding box and CAM obtained by AlexNet-GAP (left) and our method (right). Our Hide-and-Seek approach localizes multiple relevant parts of an object whereas AlexNet-GAP mainly focuses only on the most discriminative parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Table 3. Our approach outperforms Dropout [44] for localization.Table 4. Global average pooling (GAP) vs. global max pooling (GMP). Unlike [61], for Hide-and-Seek GMP still performs well for localization. For this experiment, we use patch size 56.</figDesc><table>Methods 

GT-known Loc Top-1 Loc 
Ours 
58.68 
37.65 
AlexNet-dropout-trainonly 
42.17 
7.65 
AlexNet-dropout-traintest 
53.48 
31.68 

Methods 
GT-known Loc 
Top-1 Loc 
AlexNet-GAP 
54.90 
36.25 
AlexNet-Avg-HaS 
58.43 
37.34 
AlexNet-GMP 
50.40 
32.52 
AlexNet-Max-HaS 
59.27 
37.57 

Methods 
GT-known Loc Top-1 Loc 
AlexNet-GAP 
54.90 
36.25 
AlexNet-HaS-conv1-5 
57.36 
36.91 
AlexNet-HaS-conv1-11 
58.33 
37.38 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table - 5</head><label>-</label><figDesc>shows that this leads to a big boost in performance compared to the baseline AlexNet-GAP. This</figDesc><table>Methods 
GT-known Loc 
Top-1 Loc 
AlexNet-HaS-25% 
57.49 
37.77 
AlexNet-HaS-33% 
58.12 
38.05 
AlexNet-HaS-50% 
58.43 
37.34 
AlexNet-HaS-66% 
58.52 
35.72 
AlexNet-HaS-75% 
58.28 
34.21 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 6 .</head><label>6</label><figDesc>Varying the hiding probability. Higher probabilities lead to decrease in Top-1 Loc whereas lower probability leads to smaller GT-known Loc. For this experiment, we use patch size 56.</figDesc><table>Methods 
IOU thresh = 0.1 
0.2 
0.3 
0.4 
0.5 
Video-full 
34.23 
25.68 
17.72 
11.00 
6.11 
Video-HaS 
36.44 
27.84 
19.49 
12.66 
6.84 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">[61] does not provide GT-known loc, so we compute on our own GAP implementations, which achieve similar Top-1 Loc accuracy.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported in part by Intel Corp, Amazon Web Services Cloud Credits for Research, and GPUs donated by NVIDIA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-taught object localization with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic attribute discovery and characterization from noisy web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly supervised action labeling in videos under ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient activity detection with max-subgraph search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-fold MIL Training for Weakly Supervised Object Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Weakly supervised object localization with multi-fold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00949</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of part-based spatial models for visual object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discovering localized attributes for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic annotation of human actions in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Duchenne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object Class Recognition by Unsupervised Scale-Invariant Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Devnet: A deep event network for multimedia event detection and evidence recounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Connectionist temporal modeling for weakly supervised action labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/,2014.5" />
	</analytic>
	<monogr>
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient feature extraction, encoding and classification for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Contextlocnet: Context-aware deep network models for weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weakly supervised object boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hipster wars: Discovering elements of fashion styles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Is object localization for free? weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Constrained convolutional neural networks for weakly supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning Object Class Detectors from Weakly Annotated Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015. 2, 4</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-end localization and ranking for relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Track and transfer: Watching videos to simulate strong human supervision for weakly-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">In Defence of Negative Mining for Annotating Weakly Labelled Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On Learning to Localize Objects with Minimal Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Weaklysupervised discovery of visual pattern configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Temporal localization of fine-grained actions in videos by domain transfer from web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Regularization of neural network using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Walk and learn: Facial attribute representation learning from egocentric video and contextual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schmidt Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Weakly supervised learning for attribute localization in outdoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A-fast-rcnn: Hard positive generation via adversary for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Models for Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Discovering the spatial extent of relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Endto-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">PANDA: Pose Aligned Networks for Deep Attribute Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
