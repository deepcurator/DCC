<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convolutional Color Constancy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
							<email>barron@google.com</email>
						</author>
						<title level="a" type="main">Convolutional Color Constancy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Color constancy is the problem of inferring the color of the light that illuminated a scene, usually so that the illumination color can be removed. Because this problem is underconstrained, it is often solved by modeling the statistical regularities of the colors of natural objects and illumination. In contrast, in this paper we reformulate the problem of color constancy as a 2D spatial localization task in a logchrominance space, thereby allowing us to apply techniques from object detection and structured prediction to the color constancy problem. By directly learning how to discriminate between correctly white-balanced images and poorly white-balanced images, our model is able to improve performance on standard benchmarks by nearly 40%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Intro</head><p>The color of a pixel in an image can be described as a product of two quantities: reflectance (the color of the paint of the surfaces in the scene) and illumination (the color of the light striking the surfaces in the scene). When a person stands in a room lit by a colorful light they unconsciously "discount the illuminant", in the words of Helmholtz <ref type="bibr" target="#b26">[27]</ref>, and perceive the objects in the room as though they were illuminated by a neutral, white light. Endowing a computer with the same ability is difficult, as this problem is fundamentally underconstrained -given a yellow pixel, how can one discern if it is a white object under a yellow illuminant, or a yellow object under a white illuminant? The most general characterization of this problem is the "intrinsic image" problem <ref type="bibr">[6]</ref>, but the specific problem of inferring and correcting the color of the illumination of an image is commonly referred to as "color constancy" or "white balance". A visualization of this problem can be seen in <ref type="figure">Figure 1</ref>.</p><p>Color constancy is a well studied in both vision science and computer vision, as it relates to the academic study of human perception as well as practical problems such as designing an object recognition algorithm or a camera. Nearly all algorithms for this task work by assuming some regularity in the colors of natural objects viewed under a white light. The simplest such algorihm is "gray world", which assumes that the illuminant color is the average color of all image pixels, thereby implicitly assuming that object reflectances are, on average, gray <ref type="bibr" target="#b11">[12]</ref>. This simple idea can be generalized to modeling gradient information or using generalized norms instead of a simple arithmetic mean <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36]</ref>, modeling the spatial distribution of colors with a filter bank <ref type="bibr" target="#b12">[13]</ref>, modeling the distribution of color histograms <ref type="bibr" target="#b20">[21]</ref>, or implicitly reasoning about the moments of colors using PCA <ref type="bibr" target="#b13">[14]</ref>. Other models assume that the colors of natural objects lie with some gamut <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref>. Most of these models can be thought of as statistical, as they either assume some distribution of colors or they learn some distribution of colors from training data. This connection to learning and statistics is sometimes made more explicit, often in a</p><formula xml:id="formula_0">I = W × L</formula><p>ourŴ ,L, err = 0.13°baselineŴ ,L, err = 5.34°F</p><p>igure 1: Here we demonstrate the color constancy problem: the input image I (taken from <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b32">33]</ref>) looks green, and we want to recover a white-balanced image W and illumination L which reproduces I. Below we have our model's solution and error for this image compared to a state of the art baseline <ref type="bibr" target="#b18">[19]</ref> (recovered illuminations are rendered with respect to ground-truth, so white is correct). More results can be seen in the supplement.</p><p>Bayesian framework <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>One thing that these statistical or learning-based models have in common is that they are all generative models of natural colors -a model is learned from (or assumed of) white-balanced images, and then that model is used to white-balance new images. In this paper, we will operate under the assumption that white-balancing is a discriminative task. That is, instead of training a generative model to assign high likelihoods to white-balanced images under the assumption that such a model will perform well at white-balancing, we will explicitly train a model to distinguish between white-balanced images and non-whitebalanced images. This use of discriminative machine learning appears to be largely unexplored in context of color constancy, though similar tools have been used to augment generative color constancy models with face detection <ref type="bibr" target="#b8">[9]</ref> or scene classification <ref type="bibr" target="#b24">[25]</ref> information. The most related technique to our own is probably that of Finlayson <ref type="bibr" target="#b18">[19]</ref> in which a simple "correction" to a generalized gray-world algorithm is learned using iterative least-squares, producing state-of-the-art results compared to prior art.</p><p>Let us contrast the study of color constancy algorithms with the seemingly disparate problem of object detection. Object detection has seen a tremendous amount of growth and success in the last 20 years owing in large part to standardized challenges <ref type="bibr" target="#b15">[16]</ref> and effective machine learning techniques, with techniques evolving from simple sliding window classifiers <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref> to sophisticated deformable models <ref type="bibr" target="#b16">[17]</ref> or segmentation-based techniques <ref type="bibr" target="#b27">[28]</ref>. The vast majority of this work operates under the assumption that object detection should be phrased as the problem of learning a discriminative classifier which predicts whether an image patch is, in the case of face detection for example, a "face" or a "nonface". It is common knowledge that reasoning about the "nonface" background class is nearly as important as reasoning about the object category of interest, as evidenced by the importance of "mining for hard negatives" <ref type="bibr" target="#b31">[32]</ref> when learning an effective object detection system. In contrast, training a generative model of an object category for detection is widely considered to be ineffective, with some unusual exceptions <ref type="bibr" target="#b28">[29]</ref>. At first glance, it may seem that all of this has little to teach us about color constancy, as most established color constancy algorithms are fundamentally incompatible with the discriminative learning techniques used in object detection. But if the color constancy problem could be reduced to the problem of localizing a template in some n-dimensional space, then presumably the lessons learned from object detection techniques could be used to design an effective color constancy algorithm.</p><p>In this paper we present CCC ("Convolutional Color Constancy"), a novel color constancy algorithm that has been designed under the assumption that color constancy is a discriminative learning task. Our algorithm is based around the observation that scaling the color channels of an image induces a translation in the log-chromaticity histogram of that image. This observation allows us to frame the color constancy problem as a discriminative learning problem, using tools similar to convolutional neural networks <ref type="bibr" target="#b30">[31]</ref> and structured prediction <ref type="bibr" target="#b33">[34]</ref>. Effectively, we are able to reframe the problem of color constancy as the problem of localizing a template in some two-dimensional space, thereby allowing us to borrow techniques from the well-understood problem of object detection. By discriminatively training a color constancy algorithm in this way, we produce state-of-the-art results and reduce error rates on standard benchmarks by nearly 40%.</p><p>Our paper will proceed as follows: In Section 2 we will demonstrate the relationship between image tinting and logchrominance translation. In Section 3 we will describe how to learn a discriminative color constancy algorithm in our newly-defined log-chrominance space. In Section 4 we will explain how to perform efficient filtering in our logchrominance space, which is required for fast training and evaluation. In Section 5 we will show how to generalize our model from individual pixel colors to spatial phenomena like edges and patches. In Section 6 we will evaluate our model on two different color constancy tasks, and in Section 7 we will conclude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Image Formation</head><p>Consider a photometric linear image I taken from a camera, in which black-level correction has been performed and in which no pixel values have saturated. According to a simplified model of image formation, each RGB pixel value in I is the product of the "true" white-balanced RGB value W for that pixel and the RGB illumination L shared by all pixels, as shown in <ref type="figure">Figure 1</ref>.</p><formula xml:id="formula_1">I = W × L (1)</formula><p>This is a severe oversimplification of the larger "intrinsic image" problem, as it ignores shading, reflectance properties, spatially-varying illumination, etc. This model also assumes that color constancy can be achieved by simply modifying the gains of each channel individually (the Von Kries coefficient law <ref type="bibr" target="#b37">[38]</ref>) which, though certainly an approximation <ref type="bibr" target="#b10">[11]</ref>, is an effective and widespread assumption. Our goal is, given I, to estimate L and then produce W = I/L. Let us define two measures of chrominance u and v from the RGB values of I and W :</p><formula xml:id="formula_2">I u = log(I g /I r ) I v = log(I g /I b ) W u = log(W g /W r ) W v = log(W g /W b ) (2)</formula><p>Additionally, it is convenient to define a luminance measure y for I:</p><formula xml:id="formula_3">I y = I 2 r + I 2 g + I 2 b<label>(3)</label></formula><p>Given that we do not care about the absolute scaling of W , the problem of estimating L simplifies further to just estimating the "chrominance" of L, which can just be represented as two numbers:</p><formula xml:id="formula_4">L u = log(L g /L r ) L v = log(L g /L b )<label>(4)</label></formula><p>Notice that by our definitions and by the properties of logarithms, we can rewrite the problem formulation in Equation 1 in this log-chrominance space:</p><formula xml:id="formula_5">W u = I u − L u W v = I v − L v (5)</formula><p>So, our problem reduces to recovering just two quantities:</p><formula xml:id="formula_6">(L u , L v ).</formula><p>Because of the absolute scale ambiguity, the inverse mapping from RGB to UV is undefined. So after recovering (L u , L v ), we make the additional assumption that L is unit-norm which allows us to recover</p><formula xml:id="formula_7">(L r , L g , L b ):</formula><formula xml:id="formula_8">L r = exp(−L u ) z L g = 1 z L b = exp(−L v ) z z = exp(−L u ) 2 + exp(−L v ) 2 + 1<label>(6)</label></formula><p>This log-chrominance formulation has several advantages over the RGB formulation. We have 2 unknowns instead of 3, and we just have a simple linear constraint relating W and I instead of a multiplicative constraint. Though they may seem unimportant, these properties are required to reformulate our problem as a 2D spatial localization task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning</head><p>Let us consider an input image I and its ground-truth illumination L. We will construct a histogram M from I, where M (u, v) is the the number of pixels in I whose chrominance is near (u, v), with histogram counts weighted by each pixel's luminance:</p><formula xml:id="formula_9">M (u, v) = i I (i) y I (i) u − u ≤ ǫ 2 ∧ I (i) v − v ≤ ǫ 2<label>(7)</label></formula><p>Where the square brackets are an indicator function and ǫ is the bin-width of the histogram (in all experiments, ǫ = 0.025 and histograms have 256 bins). To produce our final histogram features N take the square root of the L1-normalized histogram counts, which generally improves the effectiveness of histogram features <ref type="bibr" target="#b1">[2]</ref>.</p><formula xml:id="formula_10">N (u, v) = M (u, v) u ′ ,v ′ M (u ′ , v ′ )<label>(8)</label></formula><p>Any normalization or transformation is allowed at this step as long as the same operation is applied to the entire histogram, though at other stages in the algorithm care must be taken to preserve translational invariance. The images are the same except for "tints" -scaling of red and blue. Tinting an image affects the image's histogram only by a translation in logchrominance space. This observation enables our convolutional approach to color correction, in which our algorithm learns to localize a histogram in this 2D space.</p><p>In <ref type="figure" target="#fig_0">Figure 2</ref> we show three tinted versions of the same image with each image's chrominance histogram. Note that each histogram is a translated version of the other histograms (ignoring sampling artifacts) and that the shape of the histogram does not change. This is a consequence of our definitions of u and v: scaling a pixel's RGB value is equivalent to translating a pixel's log-chrominance, as was noted in <ref type="bibr" target="#b19">[20]</ref>. This equivalence between image-tinting and histogram-shifting enables the rest of our algorithm.</p><p>Our algorithm works by considering all possible tints of an image, scoring each tinted image, and then returning the highest-scoring tint as the estimated illumination of the input image. This may sound like an expensive proposition as it requires a brute-force search over all possible tints, where some scoring function is applied at each tint. However, provided that the scoring function is a linear combination of histogram bins, this brute-force search is actually just the convolution of N with some filter F , and there are many ways that convolution can be made efficient. This gives us a sketch of our algorithm: we will construct a histogram N from the input image I, convolve that histogram with some filter F , and then use the highest-scoring illuminationL to produceŴ = I/L. More formally:</p><formula xml:id="formula_11">(L u ,L v ) = arg max u,v (N * F )<label>(9)</label></formula><p>A visualization of this procedure (actually, a slightly more complicated version which will be explained later) can be seen in <ref type="figure" target="#fig_2">Figure 7</ref>. Now we require a way to learn a filter F from training data such that this convolution produces accurate output.</p><p>To learn F we use a model similar to multinomial logistic regression or structured prediction, in a convolutional framework. Formally, our optimization problem is:</p><formula xml:id="formula_12">min F λ u,v F (u, v) 2 + i,u,v P (u, v) C u, v, L (i) u , L (i) v P (u, v) = exp (N (i) * F )(u, v) u ′ ,v ′ exp (N (i) * F )(u ′ , v ′ )<label>(10)</label></formula><p>Where F is the filter whose weights we learn, {N (i) } and {L (i) } are our training-set chrominance histograms and ground-truth illuminations, respectively, and (N (i) * F )(u, v) is the convolution of N (i) and F indexed at location (u, v). For convenience we define P (u, v) which is a softmax probability for each (u, v) bin in our histogram as a function of N (i) * F . We regularize our filter weights by minimizing the sum of squares of the elements of F , moderated by some hyperparameter λ. At a high level, minimizing our loss finds an F such that</p><formula xml:id="formula_13">N (i) * F is larger at (L (i) u , L (i)</formula><p>v ) than it is elsewhere, where C(u, v, u * , v * ) defines the loss incurred at mis-estimated illuminants:</p><formula xml:id="formula_14">C (u, v, u * , v * ) = arccos ℓ, ℓ * ℓ ℓ * ℓ =[exp(−u), 1, exp(−v)] T ℓ * =[exp(−u * ), 1, exp(−v * )] T<label>(11)</label></formula><p>C measures the angle between the illuminations defined by (u, v) and (u * , v * ), which is the error by which colorconstancy algorithms are commonly evaluated. Visualizations of C can be seen in <ref type="figure" target="#fig_1">Figure 3</ref>. During training we initialize F to all zeros (initialization does not appear to affect accuracy) and we minimize Eq. 10 first using a variant of stochastic gradient descent (detailed in supplement) followed by batch L-BFGS until convergence. Using both optimization techniques produces lower losses and test-set error rates than using only SGD, but more quickly than only using batch L-BFGS. Though our loss function is nonconvex, optimization appears to work well and our learned model performs better than other models trained with various convex approximations to our loss function.</p><p>Our problem resembles multinomial logistic regression, but where every (u, v) has a variable loss C measuring the cost of each possible (u, v) chrominance with respect to some ground-truth chrominance (u * , v * ). The use of a softmax makes our model resemble a classification problem, and the use of a variable cost makes our model resemble structured prediction. We experimented with simply minimizing the cross-entropy of P (u, v) with respect to a delta function at (u * , v * ), and with using maximum-margin structured prediction <ref type="bibr" target="#b33">[34]</ref> with margin rescaling and slack rescaling, but found that our proposed approach produced more accurate results on the test set. We also experimented with learning a "deep" set of filters instead of a single filter F , thereby resulting in a convolutional neural network <ref type="bibr" target="#b30">[31]</ref>, but we found the amount of training data in our datasets insufficient to prevent overfitting.</p><p>A core property of our approach is that our model is trained discriminatively. Our structured-prediction approach means that F is learned directly in accordance with the criteria we care about -how accurately it identifies each illumination color in the training set. This is very different from the majority of color constancy algorithms which either learn or analytically construct generative models of the distributions of colors in natural images viewed under white light. To demonstrate the importance of discriminative training, we will evaluate against a generatively-trained version of our model which learns a model to maximize the likelihood of colors in natural images, while not considering that this generative model will be used for a discriminative task. Our generative model learns our filter F according to the following optimization problem:</p><formula xml:id="formula_15">max F i u,v log (P (u, v)) N (i) (u, v) P (u, v) = exp (δ (i) * F )(u, v) u ′ ,v ′ exp (δ (i) * F )(u ′ , v ′ ) δ (i) = u − L (i) u ≤ ǫ /2 ∧ v − L (i) v ≤ ǫ /2<label>(12)</label></formula><p>Minimizing this loss produces a filter F such that, when F is convolved with a delta function located at the illuminant color's chrominance, the categorical distribution produced by exponentiating that filter output maximizes the likelihood of the training set chroma histograms {N (i) }. We do not regularize F , as it does not improve performance when generative training is used. discriminative F generative F <ref type="figure">Figure 4</ref>: Learned filters on the same training data, with the left filter learned discriminatively and the right filter learned generatively. The generative model just learns a simple "gray-world" like filter, while the discriminative model learns to do things like upweight blues that resemble the sky and downweight pale greens that resemble badly white-balanced images. One can think of the discriminatively learned filter as a histogram of colors in well whitebalanced images minus a histogram of colors in poorly white-balanced images A visualization of filters learned discriminatively and generatively on the same data can be seen in <ref type="figure">Figure 4</ref>. We see that discriminative training learns a much richer and more elaborate model than the generative model. This is because our discriminative training does not just learn what white-balanced images look like -it learns how to distinguish between white-balanced images and improperly white-balanced images. In Section 6 we will show that discriminative training substantially improves model accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Efficient Filtering</head><p>Though our algorithm revolves around a linear filter F with which we will convolve our chroma histograms, the specific parametrization of F affects the accuracy and speed of our model. For example, a filter the size of the input histogram would be likely to overfit and would be expensive to evaluate. We found that accurate filters for our task tend to have a log-polar or "retinotopic" structure, in which the filter contains a large amount of high-frequency variation near the center of the filter but only contains low-frequency variation far from the center. Intuitively, this makes sense: when localizing the illumination color of an image, the model should pay close attention to chroma variation near the predicted white point, while only broadly considering chroma variation far from the predicted white point.</p><p>With the goal of a fast retina-like filter, we chose to use the "pyramid filtering" technique of <ref type="bibr" target="#b4">[5]</ref> for our histogram convolution. Pyramid filtering works by first constructing a Gaussian pyramid of the input signal (in this case, we con- <ref type="figure">Figure 5</ref>: Here we visualize the "pyramid filter" <ref type="bibr" target="#b4">[5]</ref> used to score chroma histograms. Above we show naive convolution of a histogram (top left) with a retina-like filter (top middle), while below we evaluate that same filter more efficiently by constructing a pyramid from the histogram, convolving each scale of the pyramid with a small filter, and then collapsing the filtered histogram. By using the latter filtering approach we simplify regularization during training and improve speed during testing. struct a 7-level pyramid from N (u, v) using bilinear downsampling), then filtering each scale with a small filter (we used 5 × 5 filters), and then collapsing the filtered pyramid down into an image (using bilinear upsampling). When collapsing the pyramid we found it necessary to apply a [1, 2, 1] blur before each upsample operation to reduce sampling artifacts. This filter has several desirable properties: it is efficient to compute, there are few free parameters so optimization and regularization are easy, and the filter can describe fine detail in the center while modeling coarse context far from the center. We regularize this filter by simply minimizing the squared 2-norm of the filter coefficients at each scale, all modulated by a single hyperparameter λ, as in Eq. 10 (this is actually a slight departure from Eq. 10 as the regularization is now in a linearly transformed space). A visualization of pyramid filtering can be seen in <ref type="figure">Figure 5</ref>.</p><p>As described in <ref type="bibr" target="#b4">[5]</ref> pyramid filtering is equivalent to, for every pixel, computing a feature with a log-polar sampling pattern and then classifying that feature with a linear classifier. This sort of feature resembles standard features used in computer vision, like shape context <ref type="bibr" target="#b6">[7]</ref>, geometric blur <ref type="bibr" target="#b7">[8]</ref>, FREAK features <ref type="bibr" target="#b0">[1]</ref>, DAISY <ref type="bibr" target="#b34">[35]</ref>, etc. However, the pyramid approximation requires that the sampling pattern of the feature be rectangular instead of polar, that the scales of the feature be discretized to powers of 2, and that the sampling patterns of the feature at each scale overlap. This difference makes it tractable to compute and classify these features densely at every pixel in the image, which in turn allows us to estimate the illuminant color very precisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Generalization</head><p>The previously described algorithm can estimate the illumination L from an image I by filtering a histogram N constructed from the chroma values of the pixels in I. Effectively, this model is a sophisticated kind of "gray world" algorithm, in that all spatial information is ignored and the image is treated like a "bag" of pixels. However, wellperforming color constancy algorithms generally use additional sources of information, such as the color of edges <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b35">36]</ref> or spatial neighborhoods <ref type="bibr" target="#b12">[13]</ref>. To that end, we present an extension of our algorithm in which instead of constructing and classifying a single histogram N from a single image I, we filter a set of histograms {N j } from a set of "augmented" images {I ′ j }, and sum the filtered responses before computing softmax probabilities. These augmented images will reflect edge and spatial statistics of the image I, thereby enabling our model reason about multiple sources of chroma information beyond individual pixel chroma.</p><p>Naively one might attempt to construct these augmented images {I ′ j } by simply applying common image processing operations to I, such as applying a filter bank, median filters, morphological operations, etc. But remember from Section 3 that the image from which we construct chroma histograms must exactly map scaling to the channels of the input image to shifts in chroma histogram space. This means that our augmented images must also map a perchannel scaling to the same shift in histogram space, limiting the set of possible augmented images that we can use.</p><p>For our color-scaling/histogram-shifting requirement to be met, our augmented-image mappings must preserve scalar multiplication: a scaled-then-filtered version of a channel in the input image I must be equal to a filteredthen-scaled version of that channel. This problem is alluded to in <ref type="bibr" target="#b18">[19]</ref>, in which the authors limit themselves to "color moments which scale with intensity". Additionally, the output of the mappings must be non-negative as we will need to compute the logarithm of the output of each mapping (the input is assumed to be non-negative). Here are three mappings which satisfy our criteria:</p><formula xml:id="formula_16">f (I, filt) = max(0, I * filt) g(I, ρ, w) = blur(I ρ , w) 1/ρ h(I, ρ, w) = (blur(I ρ , w) − blur(I, w) ρ ) 1/ρ<label>(13)</label></formula><p>Where blur(·, w) is a box filter of width w. f (·, filt) convolves each channel of the image with some filter filt and then clamps the filtered value to be at least 0. g(·, p, w) computes a local norm of pixel values in I such that g(·, 1, w) is a blur, g(·, ∞, w) is a "max" filter, and g(·, −∞, w) is a "min" filter. h(·) computes a kind of normalized moment of pixel values, where h(·, 2, w) is the</p><formula xml:id="formula_17">I ′ 1 = I I ′ 2 I ′ 3 I ′ 4</formula><p>Figure 6: Though our model can take the pixel values of the input image I as its sole input, performance can be improved by using a set of "augmented" images {I ′ }. Our extended model uses three augmented images which capture local spatial information (texture, highlights, and edges, respectively) in addition to the input image.</p><p>local standard deviation of pixel values -an unoriented edge/texture detector. These operations all preserve scalar multiplication:</p><formula xml:id="formula_18">f (αI, filt) = αf (I, filt) g(αI, ρ, w) = αg(I, ρ, w) h(αI, ρ, w) = αh(I, ρ, w)<label>(14)</label></formula><p>In our extended model we use four augmented images: the input image I itself, a "sharpened" and rectified I, a "soft" max-filtered I, and a standard-deviation-filtered I.</p><formula xml:id="formula_19">I ′ 1 = I I ′ 2 = max 0, I * 0 −1 0 −1 5 −1 0 −1 0 I ′ 3 = blur(I 4 , 11) 1/4 I ′ 4 = blur(I 2 , 3) − blur(I, 3) 2<label>(15)</label></formula><p>Other similar channels or compositions of these channels could be used as well, though we use a small number of simple channels here for the sake of speed and to prevent overfitting. See <ref type="figure">Figure 6</ref> for visualizations of the information captured by each of these channels. During training we simply learn 4 pyramid filters instead of 1 and sum the individual filter responses before computing the softmax probabilities in Eq. 10. Now that all of our model components have been defined, we can visualize inference in our final model in <ref type="figure" target="#fig_2">Figure 7</ref>. . The set of augmented images is turned into a set of chroma histograms {N j }, for which we have learned a set of weights in the form of pyramid filters {F j }. The histograms are convolved with the filters and then summed, giving us a score for all bins in our chroma histogram. The highest-scoring bin is assumed to be the color of the illuminantL, and the output imageŴ is produced by dividing the input image by that illuminant.</p><formula xml:id="formula_20">I {I ′ j } {N j } {F j } exp( j F j * N j ) output:ŴL</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>We evaluate our algorithm on two datasets: the Color Checker Dataset <ref type="bibr" target="#b23">[24]</ref> reprocessed by Shi and Funt <ref type="bibr" target="#b32">[33]</ref>, and the dataset from Cheng et al. <ref type="bibr" target="#b13">[14]</ref>. The Color Checker dataset is widely used and is reasonably large -568 images from a single camera. The dataset from Cheng et al. is larger, with 1736 images taken from 8 different cameras, but the same scene is imaged multiple times by each of the 8 cameras. As is standard, we evaluate using three-fold cross-validation, computing the angle in degrees between our estimated illuminationL and the true illumination L * for each image. We report several statistics of these errors: the mean, the median, the tri-mean, the means of the errors in the lowest-error 25% of the data and the highesterror 25% of the data, and for the Color Checker Dataset the 95th percentile. Some baseline results on the Color Checker Dataset were taken from past papers, thereby resulting in some missing error metrics for some algorithms. For the Cheng et al. dataset we also report an average error, which is the geometric mean of the other error statistics.</p><p>Cheng et al. ran 8 different experiments with their 8 different cameras, which makes tersely summarizing performance difficult. To that end, we report the geometric mean of each error metric for each algorithm across all cameras. We computed results for our own algorithm identically: we learn a model for each camera independently, compute errors for each camera, and then report the geometric mean across all cameras.</p><p>Our results can be seen in <ref type="table" target="#tab_1">Tables 1 and 2</ref>. On the Color Checker Dataset we see a 30% and 39% reduction in error (mean and median, respectively) from the state-of-the-art ("Corrected-Moment" <ref type="bibr" target="#b18">[19]</ref>), and on the dataset of Cheng et al. we see a 22% reduction in average error from the stateof-the-art <ref type="bibr">(Cheng et al.)</ref>. This improvement is fairly consistent across different choices of error metrics. The increased improvement on the Color Checker Dataset is likely due to the larger size of the Color Checker Dataset (∼ 379 training images as opposed to ∼ 144, for three-fold cross validation), which likely favors our learning-based approach. An example of our performance with respect to the state of the art can be seen in <ref type="figure">Figure 1</ref> and in the supplement.</p><p>In our experiments we evaluated several different versions of our algorithm ("CCC"), indicated by the name of each model. Models labeled "gen" are trained in a generative fashion (Eq. 12), while "disc" models are trained discriminatively (Eq. 10). Models labeled "simp" use our simple feature set (just the input image) while "ext" models use the four augmented images from Section 5. Our results show that discriminative training is superior to generative training by a large margin (30−40% improvement), and that using our extended model produces better results than our simple model (10 − 20% improvement). Our generativelytrained models perform similarly to some past techniques which were also trained in a generative fashion, suggesting that the use of discriminative training is the driving force behind our algorithm's performance.</p><p>Though most of our baseline results were taken from past papers, to ensure a thorough and fair evaluation we obtained the code for the best-performing technique on the Color Checker dataset ("Corrected-Moment" <ref type="bibr" target="#b18">[19]</ref>) and ran it ourselves on the Cheng et al. dataset. We also ran this code on the Color Checker dataset and reported our reproduced results, which differ slightly from those reported in <ref type="bibr" target="#b18">[19]</ref> apparently due to different parameter settings or inconsistencies between the provided code and the paper <ref type="bibr" target="#b17">[18]</ref>. Results for the corrected moment algorithm produced by ourselves are indicated with asterisks in <ref type="table" target="#tab_1">Tables 1 and 2</ref>.</p><p>Evaluating our trained model is reasonably fast. With our unoptimized Matlab implementation running on a 2012 HP Z420 workstation, for each image it takes about 1.2 seconds per megapixel to construct our augmented images and produce normalized log-chrominance histograms from them, and about 20 milliseconds to pyramid-filter those histograms and extract the argmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have presented CCC, a novel learning-based algorithm for color constancy. Our technique builds on the observation that the per-channel scaling in an image caused by the color of the illumination produces a translation in the space of log-chroma histograms. This observation lets us leverage ideas from object detection and structured prediction to discriminatively train a convolutional classifier to perform well at white balancing, as opposed to the majority of prior work which uses generative training. Our algorithm is made more efficient by using a pyramid-based approach to image filtering, and is made more accurate by augmenting the input to our algorithm with variants of the input image that capture different kinds of spatial information. Our technique produces state-of-the-art performance on the two largest color constancy datasets, beating the bestperforming techniques by 20% − 40% on various error metrics. Our experiments suggest that color constancy algorithms may benefit from much larger datasets than are currently used, as has been the case for object detection and recognition. This newly-established connection with object detection suggests that color constancy may be a fruitful domain for researchers to apply new object detection techniques. Furthermore, our results show that many of the lessons learned from discriminative machine learning are more relevant to color constancy than has been previously thought, and suggests that other core low-level vision and imaging tasks may benefit from a similar reevaluation.   <ref type="bibr" target="#b23">[24]</ref>. For each metric the best-performing technique is highlighted in red, and the second-bestperforming (excluding variants of our technique) is in yellow. Some baseline numbers here were taken from past work <ref type="bibr" target="#b13">[14,</ref><ref type="bibr">26]</ref>, which used different error measures, thereby resulting in some missing entries. <ref type="table">Table 2</ref>: Performance on the dataset from Cheng et al. <ref type="bibr" target="#b13">[14]</ref>. For each metric the best-performing technique is highlighted in red, and the second-best-performing (excluding variants of our technique) is in yellow.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Some images and their log-chrominance histograms (with an axis overlayed for easier visualization, horizontal = u, vertical = v). The images are the same except for "tints" -scaling of red and blue. Tinting an image affects the image's histogram only by a translation in logchrominance space. This observation enables our convolutional approach to color correction, in which our algorithm learns to localize a histogram in this 2D space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualizations of the cost function used during training C (u, v, u * , v * ) as a function of the proposed illumination color (u, v), with each plot showing a different choice of the ground-truth illumination color (u * , v * ) (circled). Darker luminance means higher cost. These cost functions are used during training to encourage our learned filter to "fire" strongly at the true illuminant (u * , v * ) when convolved with the input histogram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: An overview of inference in our model for a single image. An input image I is transformed into a set of scalepreserving augmented images {I ′ j } which highlight different aspects of the image (edges, patches, etc). The set of augmented images is turned into a set of chroma histograms {N j }, for which we have learned a set of weights in the form of pyramid filters {F j }. The histograms are convolved with the filters and then summed, giving us a score for all bins in our chroma histogram. The highest-scoring bin is assumed to be the color of the illuminantL, and the output imageŴ is produced by dividing the input image by that illuminant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Algorithm Mean Med. Tri. Best Worst 95% 25% 25% Quant. White-Patch [11] 7.55 5.68 6.35 1.45 16.12 - Edge-based Gamut [3] 6.52 5.04 5.43 1.90 13.58 - Gray-World [12] 6.36 6.28 6.28 2.33 10.58 11.3 1st-order Gray-Edge [36] 5.33 4.52 4.73 1.86 10.03 11.0 2nd-order Gray-Edge [36]Corrected-Moment* (19 Color) [19] 2.96 2.15 2.37 0.64 6.69 8.23 Corrected-Moment* (19 Edge) [19] 3.12 2.38 2.59 0.90 6.46 7.80</figDesc><table>5.13 4.44 4.62 2.11 9.26 
-
Shades-of-Gray [22] 
4.93 4.01 4.23 1.14 10.20 11.9 
Bayesian [24] 
4.82 3.46 3.88 1.26 10.49 
-
General Gray-World [4] 
4.66 3.48 3.81 1.00 10.09 
-
Intersection-based Gamut [3] 
4.20 2.39 2.93 0.51 10.70 
-
Pixel-based Gamut [3] 
4.20 2.33 2.91 0.50 10.72 14.1 
Natural Image Statistics [25] 
4.19 3.13 3.45 1.00 9.22 11.7 
Bright Pixels [30] 
3.98 2.61 -
-
-
-
Spatio-spectral (GenPrior) [13] 
3.59 2.96 3.10 0.95 7.61 
-
Cheng et al. [14] 
3.52 2.14 2.47 0.50 8.74 
-
Corrected-Moment (19 Color) [19] 
3.5 2.6 
-
-
-
8.6 
Corrected-Moment (19 Edge) [19] 
2.8 2.0 
-
-
-
6.9 
CCC (gen+simp) 
3.57 2.62 2.90 1.01 7.74 9.62 
CCC (gen+ext) 
3.24 2.33 2.61 1.02 6.88 8.42 
CCC (disc+simp) 
2.48 1.52 1.70 0.37 6.26 8.30 
CCC (disc+ext) 
1.95 1.22 1.38 0.35 4.76 5.85 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Performance on the reprocessed [33] Color Checker Dataset</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">FREAK: Fast Retina Keypoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Three things everyone should know to improve object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improvements to gamut mapping colour constancy algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A comparison of computational color constancy algorithms -part 2: Experiments with image data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Funt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Volumetric semantic segmentation using pyramid context features. ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V E</forename><surname>Keränen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Biggin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Knowles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>25% White-Patch [11] 10.62 10.58 10.49 1.86 19.45 8.43</idno>
	</analytic>
	<monogr>
		<title level="j">Algorithm Mean Med. Tri. Best Worst Avg</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Recovering Intrinsic Scene Characteristics from Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Barrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Tenenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shape context: A new descriptor for shape matching and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Geometric blur for template matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Color constancy using faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bianco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bayesian color constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Brainard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA A</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Analysis of the retinex theory of color vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Brainard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Wandell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA A</title>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A spatial processor model for object colour perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Buchsbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Franklin Institute</title>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Color constancy with spatio-spectral statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Illuminant estimation for color constancy: why spatial-domain methods work and the role of the color distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA A</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Finlayson</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>personal communication</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Corrected-moment illuminant estimation. ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Finlayson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Color constancy at a pixel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Hordley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA-A</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Color by correlation: A simple, unifying framework for color constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Hordley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Hubel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shades of gray and colour constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trezzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Color Imaging Conference</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A novel algorithm for color constancy. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<title level="m">Bayesian color constancy revisited. CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Color constancy using natural image statistics and scene semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gijsenij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Computational color constancy: Survey and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gijsenij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Seeing Black and White</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Gilchrist</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Discriminative decorrelation for clustering and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R V</forename><surname>Joze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A T</forename><surname>Rey</surname></persName>
		</author>
		<title level="m">The role of bright pixels in illumination estimation. Color Imaging Conference</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
	<note>Neural Computation</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Neural network-based face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Re-processed version of the gehler color constancy dataset of 568 images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Funt</surname></persName>
		</author>
		<ptr target="http://www.cs.sfu.ca/colour/data/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning structured prediction models: A large margin approach. ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chatalbashev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">DAISY: An Efficient Dense Descriptor Applied to Wide Baseline Stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Edge-based color constancy. TIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gijsenij</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Robust real-time object detection. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Die gesichtsempfindungen. Handbuch der Physiologie des Menschen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kries</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
