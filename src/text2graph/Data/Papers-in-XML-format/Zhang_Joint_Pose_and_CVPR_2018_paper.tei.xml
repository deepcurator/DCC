<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Pose and Expression Modeling for Facial Expression Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Communication Engineering</orgName>
								<orgName type="institution">Jiangsu University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CAS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qirong</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Communication Engineering</orgName>
								<orgName type="institution">Jiangsu University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CAS</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
							<email>csxu@nlpr.ia.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CAS</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Pose and Expression Modeling for Facial Expression Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Facial expression recognition (FER) is a challenging</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Facial expression recognition (FER) is one of the most important tasks in computer vision which plays a crucial role in numerous applications in psychology, medicine, security, digital entertainment, and driver monitoring, to name a few <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b2">3]</ref>. The main challenge of the FER is to account for large appearance changes of human faces. Despite of significant progress in recent years, it remains a difficult task for developing robust algorithms to recognize facial expression in scenarios with challenging factors such as pose variations, unconstrained facial expressions, illumination changes, and insufficient training data.</p><p>The facial expression recognition aims to analyze and classify a given facial image into several emotion types, such as, angry, disgust, fear, happy, sad and surprise <ref type="bibr" target="#b8">[9]</ref>. To achieve this goal, numerous algorithms of FER <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref> have been proposed in the literatures during the past several years. Among the existing methods, most of them are based on frontal or nearly frontal view facial images and the non-frontal or the in-the-wild facial expression recognition problem is largely unexplored. In contrast to the frontal FER, expression recognition from non-frontal facial images is challenging because it needs to deal with the issues of face occlusions, accurate non-frontal face alignment, and accurate non-frontal facial points location as shown in Figure 1. As a result, only a small part of algorithms among the proposed various methods address this challenging issue <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b57">58]</ref>. Different from the existing methods, we focus on the pose-invariant FER, which is to perform FER by identifying or authorizing individuals' expressions with facial images captured under arbitrary poses. Therefore, it is more challenging and more applicable in real scenarios.</p><p>However, it is not easy to perform the pose-invariant FER as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The main challenge here is to perform decoupling of the rigid facial changes due to the head-pose and non-rigid facial changes due to the expression, as they are non-linearly coupled in 2D images <ref type="bibr" target="#b65">[66]</ref>. In details, the rigid rotation of the head results in selfocclusion, which means there is loss of information for facial expression recognition. Besides, the shape of facial texture is warped nonlinearly along with the pose change, which causes serious confusion with the inter-personal texture difference. This calls for a joint analysis of head-pose <ref type="table">Table 1</ref>. The details of existing benchmarks for pose-invariant FER including the number of pose, expression, and training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Pose <ref type="table" target="#tab_0">Expression  Training Samples  SFEW  -7  700  Multi-PIE  5  6  7,655  BU-3DFE  35  6  21,000</ref> and facial expressions. Nonetheless, this remains a significant research challenge, mainly due to the large variation in appearance of facial expressions in different poses and difficulty in decoupling these two sources of variation. In order to deal with the above issues, the traditional methods usually have three distinct perspectives: <ref type="table">(</ref> The success of these approaches can be attributed in good part to the quality of the feature representation used as input to the classifier. Most methods are conducted on classical hand-crafted visual features, such as local binary pattern (LBP) <ref type="bibr" target="#b63">[64]</ref>, histograms of oriented gradients (HOG) <ref type="bibr" target="#b10">[11]</ref>, and scaled-invariant feature transform (SIFT) <ref type="bibr" target="#b45">[46]</ref>, which have the limited representation power and may not handle the challenge of nonlinear facial texture warping caused by pose variation well <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Recently, deep networks have been successfully applied on a wide range of visual tasks, such as image classification <ref type="bibr" target="#b23">[24]</ref>, object detection <ref type="bibr" target="#b10">[11]</ref>, segmentation <ref type="bibr" target="#b33">[34]</ref>, and pose estimation <ref type="bibr" target="#b32">[33]</ref>. Inspired by the success of deep networks, an intuitive idea is to learn semantic features for the FER via deep learning. However, deep models need to be trained with enough labeled data <ref type="bibr" target="#b22">[23]</ref>. Thus, the first step in creating any such image classification system is gathering sufficient annotated data where each image is labeled with the correct category. For the pose-invariant FER, the publicly available datasets typically contain a very limited number of labeled samples. As shown in <ref type="table">Table 1</ref>, there are three standard benchmarks. The Static Facial Expressions in the wild (SFEW) dataset <ref type="bibr" target="#b6">[7]</ref> contains only 700 images (including both training and testing) while the Multi-PIE <ref type="bibr" target="#b12">[13]</ref> has 7,655 images (5 poses and 6 expressions).</p><p>In this case, a common solution is to employ deep networks pre-trained on the ImageNet <ref type="bibr" target="#b38">[39]</ref> and do fine tuning to further improve the feature representation power. As a result, the networks are trained separately from the FER, and the extracted features hardly benefit from the end-toend training. End-to-end training of deep architectures is generally preferable to training individual components separately. The reason is that in this manner the free parameters in all components can co-adapt and cooperate to achieve a single objective. The other solution is to generate training data automatically. It is almost impossible to manually label training data because our goal is to perform the FER with arbitrary poses. In recent times, GAN-based approaches have been successfully used to generate impressively realistic faces <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b19">20]</ref>, house-numbers <ref type="bibr" target="#b59">[60]</ref>, bedrooms <ref type="bibr" target="#b34">[35]</ref> and a variety of other image categories <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b64">65]</ref> through a two-player game between a generator G and discriminator D. This inspires us to resort to the GAN to enlarge and enrich the training set. Despite many promising developments <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29]</ref>, image synthesis remains the main objective of GAN, which cannot be straightforwardly applied to facial expression recognition task.</p><p>Inspired by the above discussions, on the one hand, we design a GAN-based structure to generate facial images with different expressions and poses. On the other hand, we embed a classifier into the network to facilitate the image synthesis and conduct facial expression recognition. To disentangle the attributes (expression, pose) from the identity representation, we construct the G with an encoder-decoder structure, which serves as a facial image changer. The input to the encoder G enc is a face image of any expression and pose, the output of the decoder G dec is a synthetic facial image at a target expression and pose, and the learnt identity representation bridges G enc and G dec . Besides, we introduce two discriminators (D att and D i ) into the generative adversarial network. The D att is used to disentangle the pose, expression and identity from a facial image in a latent space to change the attributes (pose and expression) but retain the identity. To smooth the pose and expression transformation, the D i is adopted to control the distribution of identity features. With an additional classifier C exp , it can strive for the generated facial image to have the same expression as the input real facial image, which has two effects on G: (1) The generated facial image looks more like the input subject in terms of expression. (2) The learnt representation is more generative to synthesize an identity-preserving facial image but with different expressions and poses, and the generated facial images can facilitate the FER in turn.</p><p>The major contributions of this work can be summarized as follows. <ref type="formula">(1)</ref> We propose an end-to-end learning model by exploiting different poses and expressions jointly for simultaneous facial image synthesis and pose-invariant facial expression recognition. (2) The identity representation learning is explicitly disentangled from both expression and pose variations through the expression and pose codes in G and D. As a result, the proposed model can automatically generate facial images with an arbitrary expression under an arbitrary pose. <ref type="bibr">(</ref>3) The proposed model achieves state-ofthe-art facial expression recognition performance on Multi-PIE <ref type="bibr" target="#b12">[13]</ref>, BU-3DFE <ref type="bibr" target="#b50">[51]</ref>, and SFEW <ref type="bibr" target="#b6">[7]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we mainly discuss methods that are related to facial expression recognition and generative adversarial network. Facial Expression Recognition. Extensive efforts have been devoted to recognizing facial expressions <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b2">3]</ref>. Most of existing methods on the FER study the expressions of six basic emotions including happiness, sadness, surprise, fear, anger and disgust because of their marked reference representation in our affective lives and the availability of the relevant training and test data <ref type="bibr" target="#b52">[53]</ref>. Generally, the learning system mainly includes two stages, i.e., feature extraction and expression recognition. In the first stage, features are extracted from facial images to characterize facial appearance/geometry changes caused by activation of a target expression. According to whether the features are extracted by manually designed descriptors or by deep learning methods, they can be grouped into engineered features <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b39">40]</ref> and learning-based features <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref>. For the engineered features, it can be further divided into texture-based local features, geometrybased global features, and hybrid features. The texturebased features mainly include SIFT <ref type="bibr" target="#b61">[62]</ref>, HOG <ref type="bibr" target="#b10">[11]</ref>, Histograms of LBP <ref type="bibr" target="#b63">[64]</ref>, Haar features <ref type="bibr" target="#b44">[45]</ref>, and Gabor wavelet coefficients <ref type="bibr" target="#b48">[49]</ref>. The geometry-based global features are mainly based on the landmark points around eyes, mouth, and noses <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>. And the hybrid features usually refer to the features by combining two or more of the engineered features <ref type="bibr" target="#b9">[10]</ref>. The learning-based features are based on deep neutral networks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36]</ref>. Not surprisingly, almost all of them use some form of unsupervised pre-training/learning to initialize their models. It is mainly because the scarcity of labeled data prevent the authors from training a completely supervised model due to the overfitting problem. The most direct and effective solution to this problem is manually labeling more data. However, it may be infeasible for the FER with arbitrary poses. After feature extraction, in the next stage (expression classification), the extracted features are fed into a supervised classifier, e.g., Support Vector Machines (SVMs) <ref type="bibr" target="#b15">[16]</ref>, softmax <ref type="bibr" target="#b17">[18]</ref>, and logistic regression <ref type="bibr" target="#b35">[36]</ref>, to train a facial expression recognizer for a target expression. Different from existing methods, we use a variation of GAN to automatically generate facial images with different expressions and poses. Furthermore, our classifier is trained with the GAN in an end-to-end framework. Generative Adversarial Network. In <ref type="bibr" target="#b11">[12]</ref>, Goodfellow et al. introduce the Generative Adversarial Network (GAN). They train generative models through an objective function that implements a minimax two-player game between a discriminator D -a function aiming to tell apart real from fake input data -and a generator G -a function that is optimized to generate input data (from noise) that 'fools' the discriminator. And through this game, the generator and discriminator can both improve themselves. Concretely, D and G play the game with a value function V (D, G):</p><formula xml:id="formula_0">min G max D V (D, G) =E x∼p d (x) [log D(x)]+ E z∼pz(z) [log(1 − D(G(z)))] (1)</formula><p>The two parts, G and D, are trained alternatively. One of the biggest issues of GAN is that the training process is unstable, and the generated images are often noisy and incomprehensible. The CGAN <ref type="bibr" target="#b30">[31]</ref> is an extension of the GAN <ref type="bibr" target="#b11">[12]</ref>, where G and D receive an additional variable y as input. The objective function of CGAN can be rewritten as:</p><formula xml:id="formula_1">min G max D V (D, G) = E x,y∼p d (x,y) [log D(x, y)]+ E z∼pz(z),y∼py(y) [log(1 − D(G(z, y), y))] (2)</formula><p>This model allows the generator output to be controlled by y. Besides, during the last three years, several approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref> have been proposed to improve the original GAN from different perspectives. For example, the DCGAN <ref type="bibr" target="#b34">[35]</ref> adopts deconvolutional and convolutional neural networks to implement G and D, respectively. It also provides empirical instructions on how to build a stable GAN, e.g., replacing the pooling by strides convolution and using batch normalization. More recent methods focus on incorporating constraints on the input data of generator or leveraging side information for better synthesis. For example, Mirza and Osindero <ref type="bibr" target="#b30">[31]</ref> feed the class label to both G and D to generate images conditioned on the class label. Springenberg <ref type="bibr" target="#b43">[44]</ref> and Luan et al. <ref type="bibr" target="#b49">[50]</ref> generalize GAN to learn a discriminative classifier where D is trained to not only distinguish between real and fake, but also classify the images. Different from the methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b43">44]</ref>, our model can explicitly disentangle the identity representation learning from both expression and pose variations by using their codes. Compared to <ref type="bibr" target="#b49">[50]</ref>, which generates images only restricted by a discriminator, we introduce another discriminator and a content-similarity loss to make the generated facial images look like the inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, we first give a brief overview of the proposed network for simultaneous facial image synthesis and pose-invariant FER. We then describe the learning process and show the difference with existing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Joint Pose and Expression Modeling for FER</head><p>We propose an end-to-end learning model by exploiting different poses and expressions jointly for simultaneous facial image synthesis and pose-invariant facial expression recognition. The architecture of our model is shown in <ref type="figure">Figure</ref> 2, which incorporates a generator, two discriminators, and a classifier. Before passing an image into our model, we first perform face detection using a lib face detection algorithm with 68 landmarks <ref type="bibr" target="#b51">[52]</ref>. After the preprocessing, we feed the facial images into an encoder-decoder structured generator G to learn an identity representation. Specifically, G enc learns a mapping from the input image to the identity feature representation f (x). The representation is then concatenated with the expression and pose codes e and p to feed to G dec for face changing. Through the minimax twoplayer game between the generator G and the discriminator D, we can get the new labeled facial images with different poses and expressions by adding the corresponding labels to the decoder's input. Here, we use a two-discriminator structure including D att and D i . The D att is to learn disentangling representations, and the other D i is to improve the quality of the generated images. After the facial image synthesis, a classifier C exp is then used to perform our FER task. We adopt a deep modeling approach for the classifier, which guarantees that, at each layer, the features become increasingly invariant to nuisance factors while maintaining discriminative information with respect to the task of facial expression recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning</head><p>Given a facial image x with label y = {y e , y p }, where y e represents the label for expression and y p for pose, the objectives of our learning problem are threefold: (1) Synthesize a facial imagex with the corresponding expression and pose labels specified by the expression and pose codes e and p. (2) Train a pose-invariant FER classifier with the generated imagesx and the input x. (3) Retain the identity representation with a content-similarity loss. Next we will introduce them in details. Generator G and Discriminator D att . The discriminator D att is to distinguish between 'fake' imagesx produced by the generator G, and 'real' images from the input images x. We denote the distribution of the training data as P d (x). Conditioned by the expression and pose label y, it can help the generater G learn the disentangling representation from the facial images to change the poses and expressions but retain the identity, which is useful for our FER task, because when we generate new facial images, we just want to modify the facial expression or pose of the input x but without compromising the person's identity. The discriminator on attributes disentangling, D att , and G with condition y (expression and pose) can be trained by:</p><formula xml:id="formula_2">min G max Datt E x,y∼p d (x,y) [log D att (x, y)]+ E x,y∼p d (x,y) [log(1 − D att (G(x, y), y)].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(3)</head><p>Generator G and Discriminator D i . The discriminator D i imposes the uniform distribution on the identity representation f (x), which can help to smooth the pose and expression transformation. Here, the f (x) is the identity representation from G enc . Assuming the P rior(f ) is a prior distribution, and f * ∼ P rior(f ) denotes the random sampling process from P rior(f ). A min-max objective function can be used to train the G and D i :</p><formula xml:id="formula_3">min G max Di E f * ∼ prior(f ) [log D i (f * )]+ E x∼p d (x) [log(1 − D i (G enc (x)))].<label>(4)</label></formula><p>Classifier C exp . The classifier C exp is a task-specific loss.</p><p>In the case of generation, it can be used to penalize the generator loss, which is helpful for improving the performance of the original generator G. And in the case of classification, it tries to classify the expression. We use a typical softmax cross-entropy loss for the classifier:</p><formula xml:id="formula_4">L c (G, C) = E x,y e [ − y e log C(G(x), y e )</formula><p>− y e log C(x, y e )].</p><p>Content-similarity loss. The content-similarity loss attempts to ensure the output face sharing the expression, pose, and identity representation with the input facial image x (during training). Therefore, the input and output faces are expected to be similar as expressed in <ref type="formula" target="#formula_6">(6)</ref>, where L(:, :) denotes the ℓ 1 norm.</p><formula xml:id="formula_6">L con (G) = L(x − G(x, y e , y p )).<label>(6)</label></formula><p>The Objective Function. Finally, the objective function is defined as in <ref type="formula" target="#formula_7">(7)</ref> by considering the above factors.</p><formula xml:id="formula_7">min G,C max Di,Datt αL con (G) + βT V (G(f (x), y)) + L c (G, C) + E x,y∼p d (x,y) [log D att (x, y)] + E x,y∼p d (x,y) [log(1 − D att (G(x, y))]. + E f * ∼ prior(f ) [log D i (f * )] + E x∼p d (x) [log(1 − D i (G enc (x)))],<label>(7)</label></formula><p>where T V ( ) denotes the total variation which is effective in removing the ghosting artifacts. The coefficients α and β balance the smoothness and high resolution. Sequentially updating the network by <ref type="formula">(3)</ref>, <ref type="formula" target="#formula_3">(4)</ref>, <ref type="formula" target="#formula_5">(5)</ref> and <ref type="formula" target="#formula_6">(6)</ref>, we could finally learn the pose-invariant FER model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discussion</head><p>In this section, we show the differences of the proposed model with three most relevant GAN models including Adversarial Autoencoder (AAE) <ref type="bibr" target="#b28">[29]</ref>, disentangled representation learning-GAN (DR-GAN) <ref type="bibr" target="#b49">[50]</ref>, and conditional adversarial autoencoder (CAAE) <ref type="bibr" target="#b58">[59]</ref>. (1) In the AAE <ref type="bibr" target="#b28">[29]</ref>, G is the encoder of an autoencoder. The AAE has two objectives in order to turn an autoencoder into a generative model: the autoencoder reconstructs the input image, and the latent vector generated by the encoder matches an arbitrary prior distribution by training D. Different from AAE, our method can explicitly disentangle the identity representation learning from both expression and pose variations by using their codes. (2) The DR-GAN <ref type="bibr" target="#b49">[50]</ref> generalizes GAN to learn a discriminative classifier where D is trained to not only distinguish between real and fake images, but also classify real images into K classes. It is a variational autoencoderbased method mainly for disentangled representation learning for face recognition task. Different from the DR-GAN, the proposed model is mainly for generating more labeled facial images to train a deep network classifier for FER, because the training samples is the main bottleneck in facial expression recognition. Furthermore, we disentangle both the expression and pose from the facial images, and introduce a separated classifier for expression recognition. <ref type="bibr">(</ref>3) The CAAE <ref type="bibr" target="#b58">[59]</ref> extends adversarial autoencoder (AAE) to generate face images with different ages. Different from this method, our model embeds a classifier in the network and can strive for the generated facial image to have the same expression as the input real facial image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we show experimental results of our model for facial images synthesis and pose-invariant facial expression recognition. For the former task, we show qualitative results of the generated facial images under different poses and expressions. For the latter one, we quantitatively evaluate the expression recognition performance using the generated and original facial images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>To demonstrate the effectiveness of the proposed model, we conduct extensive experiments on three standard datasets including (1) Multi-PIE <ref type="bibr" target="#b12">[13]</ref>: the public multi-pose facial expression dataset, (2) BU-3DFE <ref type="bibr" target="#b50">[51]</ref>: the 3D facial expression dataset, and (3) SFEW <ref type="bibr" target="#b6">[7]</ref>: the static facial expressions in the wild dataset. The details are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-PIE:</head><p>The Multi-PIE is for evaluating facial expression recognition under pose and illumination variations in the controlled setting. Following the setting in <ref type="bibr" target="#b9">[10]</ref>, we use images of 270 subjects depicting acted facial expressions of Neutral (NE), Disgust (DI), Surprise (SU), Smile (SM), Scream(SC), and Squint (SQ), captured at five pan angles −30</p><p>• , −15</p><formula xml:id="formula_8">• , 0 • , 15</formula><p>• and 30</p><p>• , resulted in 1531 images per pose. Consequently, we have 1, 531 × 5 = 7, 655 facial images in total for our experiments. We perform five-fold subject independent cross-validation on the Multi-PIE. As a result, the training dataset comprises 6, 124 facial images whereas the testing one comprises 1, 531 facial images. We train the classifier using both the generated and original images, whose total number is 6124 × 5 × 6+6124=189,844.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BU-3DFE:</head><p>The BU-3DFE is a 3D facial expression dataset having 100 subjects with 3D models and facial images. It contains images depicting seven facial expressions Anger (AN), Disgust (DI), Fear (FE), Happiness (HA), Sadness (SA), Surprise (SU) and Neutral (NE). With the exception of the neutral expression, each of the six prototypic expressions includes four levels of intensity. Following the setting in <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b16">17]</ref>, we render 2D facial images from the 3D models at the fourth level of intensity, six universal facial expressions (AN, DI, FE, HA, SA, SU), and 35 poses including 7 pan angles (0</p><formula xml:id="formula_9">• , ±15 • , ±30 • , ±45</formula><p>• ), and 5 tilt angles (0</p><formula xml:id="formula_10">• , ±15 • , ±30 • )</formula><p>). Consequently, we have 100 × 6 × 35 × 1 = 21, 000 face images in total for our experiments. We randomly divide the 100 subjects into a training set with 80 subjects and a testing one with 20 subjects, such that there are no overlaps between the training subjects and the testing subjects. As a result, the training set comprises 16, 800 facial images whereas the testing one  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SFEW:</head><p>The SFEW is a dataset in the wild with 95 subjects. It consists of 700 images (346 images in Set 1, 354 images in Set 2) extracted from movies covering unconstrained facial expressions, varied head poses, changed illumination, large age range, different face resolutions, occlusions, and varied focus. The images are labeled with Anger (AN), Disgust (DI), Fear (FE), Happiness (HA), Sadness (SA), Surprise (SU) and Neutral (NE). We use this dataset for crossdataset experiments. We train the model on the BU-3DFE, and test it on the SFEW. Specifically, we generate facial images with different poses and expressions on Set 1. Thus, we totally have 346 + 346 × 7 × 35 = 85, 116 training samples. Then we use these images to train a classifier with the same structure used on the Multi-PIE and BU-3DFE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We construct the network according to <ref type="figure" target="#fig_2">Figure 2</ref>. We first use the lib face detection algorithm with 68 landmarks <ref type="bibr" target="#b51">[52]</ref> to crop out the faces, and resize them as 224 × 224. The image intensities are then linearly scaled to the range of <ref type="bibr">[-1,1]</ref>. To stabilize the training process, we design the network architectures of G, D att , and D i based on the techniques in the CAAE <ref type="bibr" target="#b58">[59]</ref>. Specifically, G is a convolutional neural network without batch normalization, and includes G enc and G dec that are bridged by the disentangled identity representation f (x), which is the fully connected layer output in the network. Then f (x) is concatenated with the expression code e and pose code p, which is a one-hot vector with the target expression y e and pose y p being 1. A series of fractionally-strided convolutions (FConv) <ref type="bibr" target="#b34">[35]</ref> transforms the concatenated vector into a synthetic imagê x = G(x, y e , y p ), which is the same size as the x. D img and D f is trained to optimize the object functions (3) and (4). In the discriminators D img and D f , the batch normalization is applied after each convolution layer. We adopt the VGGNet-19 network <ref type="bibr" target="#b42">[43]</ref> as the classifier C exp . And it is trained by using the generated imagesx and the original images x to optimize the objective function <ref type="bibr" target="#b4">(5)</ref>. The model is implemented by using TensorFlow <ref type="bibr" target="#b0">[1]</ref> and is trained with the ADAM optimizer <ref type="bibr" target="#b21">[22]</ref>, which is used with a learning rate of 0.0002 and momentum 0.5. All weights are initialized from  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Experiments on the Multi-PIE Dataset</head><p>The overall performances over each facial expression and each pose are shown in <ref type="figure" target="#fig_4">Figure 3</ref>(a) and <ref type="figure" target="#fig_4">Figure 3</ref>(b). The average FER accuracy is 91.80% showed in the last bar in <ref type="figure" target="#fig_4">Figure3(b)</ref>. A closer look at the figure reveals that, among the six expressions, there are four expressions (SC, SM, SU, and NE) with higher accuracy over 91.5%. The detailed performance of our model is provided in the confusion matrix in <ref type="figure" target="#fig_6">Figure 4</ref>(a), from which we can see that two of the most likely to be confused expressions are disgust and squint. This confusion may be due to these two expressions having similar muscle deformation around eyes. We then evaluate our method by comparing its performance with the current state-of-the-art methods reported in <ref type="bibr" target="#b9">[10]</ref> including kNN, LDA, LPP, D-GPLVM, GPLRF, GMLDA, GMLPP, MvDA, and DS-GPLVM. The detailed results across all views are summarized in <ref type="table" target="#tab_0">Table 2</ref>. The mean FER accuracy is reported in the last column. The results clearly show that our method outperforms all existing methods with a 15.65% to 1.2% improvement in terms of FER accuracy. Note that all other models cannot achieve good performances in the frontal view. However, our model can significantly improve the performance attained by the generated images with arbitrary poses and expressions.</p><p>We also compare our method with the models trained by different number of generated images. Given the original N images, we can obtain 5 × 6 × N generated images. To evaluate the effect of the training data size, we randomly choose 0 × N, 1 × N, 5 × N, 10 × N, 15 × N, 20 × N images from the generated facial images during each training epoch, and then incorporate them with the original images to train the classifier, where 0 × N means that the classifier is trained only using the original images. Specifically, we denote them <ref type="figure" target="#fig_0">as 0N, 1N, 5N, 10N, 15N, 20N</ref> . The overall performance with different training samples is shown in <ref type="figure">Figure 5</ref>. It is clear that our model achieves the best recog-  <ref type="figure">Figure 5</ref>. Effect of the number of training samples.</p><p>nition results. Besides, we can also find out that the average accuracy of the FER can be improved with the increase of the number of training samples, which further indicates the necessity of generating more labeled training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Experiments on the BU-3DFE Dataset</head><p>The results are shown in <ref type="table" target="#tab_1">Table 3</ref>. The rightmost column represents the average recognition error rates for different views (a total of 35 views), the bottom row represents the average recognition error rates for different facial expressions (a total of six universal facial expressions), and the bottom-right corner cell represents the average overall recognition error rate. The results show that our method achieves the average recognition accuracy of 81.20%. Furthermore, among the six expressions, surprise and happiness are easier to be recognized with accuracy over 89%. This is most likely due to the fact that the muscle deformations of both expressions are relatively large compared with others. Moreover, fear is the most difficult expression to be recognized, with the lowest at 67.30%, followed by sadness. In <ref type="figure" target="#fig_6">Figure 4</ref>(b), we show the confusion matrix for facial expression recognition by using our method. One could interpret that a contributing factor to the poor performance of fear is its confusion with happiness. This coincides with the finding of Moore and Bowden in <ref type="bibr" target="#b31">[32]</ref>, where the authors point out that the confusion is due to the expressions of fear and happiness having similar muscle deformation around the mouth. In addition, another two expressions likely to be confused are sadness and anger. These two expressions have the least amount of facial movement and thus are difficult to distinguish. We then compare our method with eight previously published methods in literatures <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b16">17]</ref>. Specifically, the methods <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b57">58]</ref> conduct the FER on a relatively small set of discrete poses containing 5 pan angles. The algorithms <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b16">17]</ref> use the facial images with 35 poses to train their model, which are the same as ours. Expect <ref type="bibr" target="#b57">[58]</ref>, all of other methods train their models with engineered features, such as LBP <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b16">17]</ref>, SIFT <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref>, and geometry features (83 landmark points) <ref type="bibr" target="#b62">[63]</ref>. In <ref type="bibr" target="#b57">[58]</ref>, the SIFT feature is used as the input of DNN to learn features. Here, the model is trained separately for each step. Different from this method, ours is an end-to-end learning model. The accuracy of each model is shown in Table 4. The average FER accuracy is reported in the last column of the table. We can see that our model achieves the average recognition accuracy of 81.20%. A closer look at this table reveals that although the methods in <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b57">58]</ref> are trained/tested on a small set of discrete poses containing only the pan rotation, our method is also competitive to the results achieved by these methods with a 1.1% to 15.2% improvement on the FER accuracy. Moreover, compared with the methods <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b16">17]</ref>, the proposed model also achieves the best accuracy (2.56% to 5.9% higher than others). This may attribute to the feature learning, which can better deal with the nonlinear facial texture warping caused by pose and individual difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Experiments on the SFEW Dataset</head><p>We finally evaluate our method on a more challenging database SFEW, in which the facial expressions are spontaneously displayed in real-world environment. As training samples in this dataset are insufficient, we adopt cross dataset experiments. Specially, we first train the generated model on the BU-3DFE dataset with 35 poses and 7 expressions (AN, DI, FE, HA, SA, SU, NE). Then we generate the corresponding facial images for the images in Set 1 in the SFEW dataset. Finally, we train the classification model on the generated and original images, and test it on Set 2. We compare our method with five previously published methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b9">10]</ref>, which include the baseline obtained by the dataset creators, and four other state-of-the-art methods. The detailed results over each expression obtained from different methods are shown <ref type="table" target="#tab_3">Table 5</ref>. The average FER accuracy is reported in the last column of the table. The difficulty of the task is further evidenced by the results in this table, where we observe a significant drop in accuracy of all methods. Overall, our method outperforms all existing methods with a 1.88% to 7.68% improvement in terms of the FER accuracy. This may attribute to the generated facial images, which can help learn discriminative features to better deal with the nonlinear facial texture warping caused by poses and individual difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Results</head><p>The qualitative results of our model are illustrated in <ref type="figure">Figure 6</ref>. We randomly select a facial image from the test set, which is shown in the pink rectangle. The generated facial images with different expressions (each column) and poses (each row) are shown in the orange rectangle. And the images in the green rectangle are the ground truth. By comparing the generated images with the ground truth, it is clear that the personality has been preserved by the proposed model, and the attributes (expression and pose) have been jointly modeled in the identity representation as shown in the red rectangles. Due to limited space, more qualitative results are reported in the supplementary material.  <ref type="figure">Figure 6</ref>. Example results of the generated facial images with different poses and expressions via the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present an end-to-end learning model for simultaneous facial images synthesis and pose-invariant facial expression recognition. By disentangling the attributes (expression and pose) from the facial image, we can generate facial images with arbitrary expressions and poses to help train the deep neutral classification model. Experiments on three standard datasets demonstrate the effectiveness of our model. In the future, we will take other aspects in images into consideration for facial image synthesis, such as illumination, occlusion <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref>. The proposed model is general and can be applied to other classification tasks, such as face recognition, image classification, and audio event recognition, which we leave as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgment</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Facial expression recognition is a challenging task due to different expressions under arbitrary poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 )</head><label>1</label><figDesc>Extract pose- robust features as facial expression representations and em- ploy conventional classifiers for recognition. (2) Perfor- m pose normalization before conducting the pose-invariant FER. (3) Learn multiple classifiers for each specific poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The overall architecture of the proposed model, which incorporates a generator G, two discriminators Datt and Di, and a classifier Cexp. Conditioned by the expression and pose codes e and p, the proposed model can generate facial images with different expressions under arbitrary poses to enlarge and enrich the training set for the FER task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Overall performance on the Multi-PIE dataset. comprises 4, 200 facial images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>the Multi-PIE dataset. (b) On the BU-3DFE dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The average confusion matrix. The average recognition rate is 91.80% and 81.20%, respectively. a zero-centered normal distribution with a standard deviation of 0.02. The details of our architecture are included in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Comparison of state-of-the-art methods on the Multi-PIE dataset. The highest accuracy for each pose is highlighted in bold.</figDesc><table>Methods 
Poses 
Average 
-30 
-15 
0 
15 
30 
kNN 
80.88 81.74 68.36 75.03 74.78 
76.15 
LDA 
92.52 94.37 77.21 87.07 87.47 
87.72 
LPP 
92.42 94.56 77.33 87.06 87.68 
87.81 
D-GPLVM 
91.65 93.51 78.70 85.96 86.04 
87.17 
GPLRF 
91.65 93.77 77.59 85.66 86.01 
86.93 
GMLDA 
90.47 94.18 76.60 86.64 85.72 
86.72 
GMLPP 
91.86 94.13 78.16 87.22 87.36 
87.74 
MvDA 
92.49 94.22 77.51 87.10 87.89 
87.84 
DS-GPLVM 93.55 96.96 82.42 89.97 90.11 
90.60 
Ours 
90.97 94.72 89.11 93.09 91.30 
91.80 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Results on the BU-3DFE dataset in terms of the recogni-
tion rates (%). The leftmost column indicates different views (pan 
and tilt angles x, y in degrees), and the top row indicates different 
facial expressions. The highest accuracy is highlighted in bold. 

Pose / Exp. 
SU 
SA 
HA 
FE 
DI 
AN 
Ave. 

−45, −30 
90.48 66.67 90.48 61.90 85.71 71.43 77.78 
−45, −15 
100 
80.95 95.24 66.67 85.71 76.19 84.13 
−45, +0 
100 
80.96 90.00 76.19 76.19 76.19 83.25 
−45, +15 
90.48 80.95 90.48 85.71 90.48 76.19 85.71 
−45, +30 
89.48 70.00 87.50 80.95 90.48 71.43 81.64 
−30, −30 
100 
76.19 95.24 61.90 90.00 76.19 83.25 
−30, −15 
85.71 76.19 95.24 71.43 90.48 76.19 82.54 
−30, +0 
85.71 85.71 90.48 85.71 90.48 80.95 86.51 
−30, +15 
100 
80.95 90.48 76.19 90.48 71.43 84.92 
−30, +30 
85.00 66.67 85.00 76.19 90.48 61.90 77.54 
−15, −30 
90.00 76.19 90.48 66.67 80.00 80.95 80.71 
−15, −15 
85.00 80.95 95.24 61.90 80.95 76.19 80.04 
−15, +0 
80.95 80.95 95.24 71.43 80.95 80.95 81.75 
−15, +15 
90.00 76.19 90.48 71.43 90.48 85.71 84.05 
−15, +30 
90.48 71.43 85.71 76.19 90.48 80.95 82.54 
+0, −30 
89.47 76.19 71.43 57.14 80.95 57.14 72.06 
+0, −15 
90.48 75.00 90.48 57.14 85.71 85.71 80.75 
+0, +0 
90.48 76.19 90.48 66.67 76.19 85.71 80.95 
+0, +15 
100 
80.00 85.71 85.71 85.71 90.00 87.86 
+0, +30 
89.47 68.42 82.35 70.00 90.00 85.71 80.99 
+15, −30 
90.00 71.43 85.71 66.67 85.71 80.95 80.08 
+15, −15 
90.00 71.43 90.00 57.14 76.19 85.71 78.41 
+15, +0 
90.48 76.19 95.24 66.67 76.19 85.71 81.75 
+15, +15 
100 
76.19 95.00 61.90 90.48 76.19 83.29 
+15, +30 
95.00 80.95 85.71 66.67 95.00 80.95 84.05 
+30, −30 
85.00 71.43 76.19 52.38 85.00 90.48 76.75 
+30, −15 
90.48 71.43 90.00 52.38 80.95 85.71 78.49 
+30, +0 
90.48 76.19 90.48 57.14 85.71 80.95 80.16 
+30, +15 
100 
80.95 95.24 61.90 85.71 85.71 84.92 
+30, +30 
95.00 80.00 95.00 66.67 85.71 76.19 83.10 
+45, −30 
90.00 66.67 90.48 52.38 90.48 57.14 74.52 
+45, −15 
90.48 71.43 95.24 52.38 90.48 80.95 80.16 
+45, +0 
85.71 76.19 95.24 66.67 85.71 61.90 78.57 
+45, +15 
90.00 61.90 90.48 76.19 90.48 76.19 80.87 
+45, +30 
82.35 65.00 83.33 71.42 80.95 80.00 77.18 
Average 
91.18 75.00 89.82 67.30 85.89 78.04 81.20 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Comparison of the average recognition accuracy with state-of-the-art methods for the FER on the BU-3DFE dataset.</figDesc><table>Methods 
Poses 
Ave. 
tilt 
pan 
total 
Zheng et al. 2009 [63] 
-
(0 
• , +90 
• ) 
5 
78.3 
Moore and Bowden 2011[32] 
-
(0 
• , +90 
• ) 
5 
71.1 
Zheng 2014 [62] 
-
(0 
• , +90 
• ) 
5 
66.0 
Zheng 2014 [62] 
-
(0 
• , +90 
• ) 
5 
78.9 
Zhang et al. 2016 [58] 
-
(0 
• , +90 
• ) 
5 
80.1 

Tang et al. 2010 [46] 
(−30 
• , +30 
• ) (−45 
• , +45 
• ) 
35 
75.3 
Tariq et al. 2013 [47] 
(−30 
• , +30 
• ) (−45 
• , +45 
• ) 
35 
76.34 
Tariq et al. 2014 [48] 
(−30 
• , +30 
• ) (−45 
• , +45 
• ) 
35 
76.60 
Jampour et al. 2015 [17] 
(−30 
• , +30 
• ) (−45 
• , +45 
• ) 
35 
78.64 
Ours 
(−30 
• , +30 
• ) (−45 
• , +45 
• ) 
35 
81.20 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 .</head><label>5</label><figDesc>Comparison of the average recognition accuracy (%) with state-of-the-art methods on the SFEW dataset. The highest accuracy for each expression is highlighted in bold. Method / Emotion Angry Disgust Fear Happy Neutral Sad Surprise Average Baseline 23.00 13.00 13.90 29.00 23.00 17.00 13.50 18.90 MvDA 23.21 17.65 27.27 40.35 27.00 10.10 13.19</figDesc><table>22.70 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno>arXiv preprint:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning and deep learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository (CoRR)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Benitez Quiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5562" to="5570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Selective transfer machine for personalized facial expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D L</forename><surname>Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="529" to="545" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Survey on rgb, 3d, thermal, and multimodal approaches for facial expression recognition: History, trends, and affectrelated applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Corneanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Simón</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Guerrero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1548" to="1568" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2106" to="2112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A comprehensive survey on poseinvariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on intelligent systems and technology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Pictures of facial affect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
			<publisher>Consulting Psychologists Press</publisher>
			<pubPlace>Palo Alto,CA,USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative shared gaussian processes for multiview and view-invariant facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eleftheriadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="189" to="204" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-pie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="807" to="813" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic facial analysis: From bayesian filtering to recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">De</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deligan: Generative adversarial networks for diverse and limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gurumurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Sarvadevabhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename><surname>Radhakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiview facial expression recognition using local appearance features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Ekenel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3533" to="3536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-view facial expressions recognition using local linear regression of sparse codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jampour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mauthner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Winter Workshop Paul Wohlhart</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint fine-tuning in deep neural networks for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2983" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-view discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="808" to="821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative attribute controller with conditional filtered generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hiramatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kashino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6089" to="6098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Do deep neural networks learn facial action units when doing expression recognition? In ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of noisy data for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Perceptual generative adversarial networks for small object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Facial expression recognition via a boosted deep belief network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1805" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Facial expression recognition via deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 International Conference on Smart Computing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="303" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical bayesian theme models for multi-pose facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="861" to="873" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno>arXiv preprint: 1411.1784</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Local binary patterns for multiview facial expression recognition. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="541" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Towards accurate multiperson pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno>arXiv preprint: 1701.01779</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno>arXiv preprint: 1612.00593</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Disentangling factors of variation for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="808" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Coupled gaussian processes for pose-invariant facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPA-MI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1357" to="1369" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Regression-based multi-view facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="4121" to="4124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Imagenet large scale visual recognition challenge. I-JCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">We are not all equal: Personalizing models for facial expression analysis with transductive parameter transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning bases of activity for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sariyanidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1965" to="1978" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generalized multiview analysis: A discriminative latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2160" to="2167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Unsupervised and semi-supervised learning with categorical generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<editor>I-CLR</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Anderson. Generating facial expressions with deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Affective Computing. InTech</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Non-frontal view facial expression recognition based on ergodic hidden markov model supervectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1202" to="1207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Maximum margin gmm learning for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Tariq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Supervised super-vector encoding for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Tariq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PR</publisher>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="89" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Evaluation of gaborwavelet-based facial action unit recognition in image sequences of increasing complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="229" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Disentangled representation learning gan for pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A 3d facial expression database for facial behavior research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rosato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="211" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Lib face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="https://github.com/ShiqiYu/libfacedetection/.2016" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A survey of affect recognition methods: Audio, visual, and spontaneous expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Roisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="58" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on deep evolutional spatial-temporal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4193" to="4203" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep future gaze: Gaze anticipation on egocentric videos using adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4372" to="4381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning multi-task correlation particle filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>PP</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Robust structural sparse tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>PP</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A deep neural network-driven feature learning method for multi-view facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="2528" to="2536" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Age progression/regression by conditional adversarial autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Facial expression intensity estimation using ordinal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3466" to="3474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Multi-view facial expression recognition based on group sparse reduced-rank regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TAC</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="71" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A novel approach to expression recognition from non-frontal face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1901" to="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning active facial patches for expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2562" to="2569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Robust real-time face pose and facial expression recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
