<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient end-to-end learning for quantizable representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeonwoo</forename><surname>Jeong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Oh</forename><surname>Song</surname></persName>
						</author>
						<title level="a" type="main">Efficient end-to-end learning for quantizable representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>/maestrojeong/Deep-Hash-Table-ICML18.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Embedding representation learning via neural networks is at the core foundation of modern similarity based search. While much effort has been put in developing algorithms for learning binary hamming code representations for search efficiency, this still requires a linear scan of the entire dataset per each query and trades off the search accuracy through binarization. To this end, we consider the problem of directly learning a quantizable embedding representation and the sparse binary hash code end-to-end which can be used to construct an efficient hash table not only providing significant search reduction in the number of data but also achieving the state of the art search accuracy outperforming previous state of the art deep metric learning methods. We also show that finding the optimal sparse binary hash code in a mini-batch can be computed exactly in polynomial time by solving a minimum cost flow problem. Our results on Cifar-100 and on ImageNet datasets show the state of the art search accuracy in precision@k and NMI metrics while providing up to 98× and 478× search speedup respectively over exhaustive linear search. The source code is available at https://github.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning the representations that respect the pairwise relationships is one of the most important problems in machine learning and pattern recognition with vast applications. To this end, deep metric learning methods <ref type="bibr" target="#b11">(Hadsell et al., 2006;</ref><ref type="bibr" target="#b30">Weinberger et al., 2006;</ref><ref type="bibr" target="#b22">Schroff et al., 2015;</ref><ref type="bibr" target="#b26">Song et al., 2016;</ref><ref type="bibr" target="#b25">Sohn, 2016;</ref><ref type="bibr" target="#b27">Song et al., 2017;</ref><ref type="bibr" target="#b2">Bell &amp; Bala, 2015;</ref><ref type="bibr" target="#b23">Sener et al., 2016)</ref> aim to learn an embedding representation space such that similar data are close to each other and vice versa for dissimilar data. Some of these methods have shown significant advances in various applications in retrieval <ref type="bibr" target="#b25">(Sohn, 2016;</ref><ref type="bibr" target="#b22">Schroff et al., 2015)</ref>, clustering <ref type="bibr" target="#b27">(Song et al., 2017)</ref>, domain adaptation <ref type="bibr" target="#b23">(Sener et al., 2016)</ref>, video understanding <ref type="bibr" target="#b29">(Wang &amp; Gupta, 2015)</ref>, etc.</p><p>Despite recent advances in deep metric learning methods, deploying the learned embedding representation in large scale applications poses great challenges in terms of the inference efficiency and scalability. To address this, practitioners in large scale retrieval and recommendation systems often resort to a separate post-processing step where the learned embedding representation is run through quantization pipelines such as sketches, hashing, and vector quantization in order to significantly reduce the number of data to compare during the inference while trading off the accuracy.</p><p>In this regard, we propose a novel end-to-end learning method for quantizable representations jointly optimizing for the quality of the network embedding representation and the performance of the corresponding binary hash code. In contrast to some of the recent methods <ref type="bibr" target="#b6">(Cao et al., 2016;</ref><ref type="bibr" target="#b17">Liu &amp; Lu, 2017)</ref>, our proposed method avoids ever having to cluster the entire dataset, offers the modularity to accommodate any existing deep embedding learning techniques <ref type="bibr" target="#b22">(Schroff et al., 2015;</ref><ref type="bibr" target="#b25">Sohn, 2016)</ref>, and is efficiently trained in a mini-batch stochastic gradient descent setting. We show that the discrete optimization problem of finding the optimal binary hash codes given the embedding representations can be computed efficiently and exactly by solving an equivalent minimum cost flow problem. The proposed method alternates between finding the optimal hash codes of the given embedding representations in the mini-batch and adjusting the embedding representations indexed at the activated hash code dimensions via deep metric learning methods.</p><p>Our end-to-end learning method outperforms state-of-theart deep metric learning approaches <ref type="bibr" target="#b22">(Schroff et al., 2015;</ref><ref type="bibr" target="#b25">Sohn, 2016)</ref> in retrieval and clustering tasks on the Cifar-100 <ref type="bibr" target="#b13">(Krizhevsky et al., 2009</ref>) and the ImageNet <ref type="bibr" target="#b21">(Russakovsky et al., 2015)</ref> datasets while providing up to several orders of magnitude speedup during inference. Our method utilizes efficient off-the-shelf implementations from OR-Tools (Google optimization tools for combinatorial optimization problems) <ref type="bibr">(OR-tools, 2018)</ref>, the deep metric learning library implementation in Tensorflow <ref type="bibr" target="#b0">(Abadi et al., 2015)</ref>, and is efficient to train. The state of the art deep metric learning approaches <ref type="bibr" target="#b22">(Schroff et al., 2015;</ref><ref type="bibr" target="#b25">Sohn, 2016)</ref> use the class labels during training (for the hard negative mining procedure) and since our method utilizes the approaches as a component, we focus on the settings where the class labels are available during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Learning the embedding representation via neural networks dates back to over two decades ago. Starting with Siamese networks <ref type="bibr" target="#b4">(Bromley et al., 1994;</ref><ref type="bibr" target="#b11">Hadsell et al., 2006)</ref>, the task is to learn embedding representation of the data using neural networks so that similar examples are close to each other and dissimilar examples are farther apart in the embedding space. Despite the recent successes and near human performance reported in retrieval and verification tasks <ref type="bibr" target="#b22">(Schroff et al., 2015)</ref>, most of the related literature on learning efficient representation via deep learning focus on learning the binary hamming codes for finding nearest neighbors with linear search over entire dataset per each query. Directly optimizing for the embedding representations in deep networks for quantization codes and constructing the hash tables for significant search reduction in the number of data is much less studied. <ref type="bibr" target="#b31">(Xia et al., 2014)</ref> first precompute the hash code based on the labels and trains the embedding to be similar to the hash code. <ref type="bibr" target="#b34">(Zhao et al., 2015)</ref> apply element-wise sigmoid on the embedding and minimizes the triplet loss. <ref type="bibr" target="#b19">(Norouzi et al., 2012)</ref> optimize the upper bound on the triplet loss defined on hamming code vectors. <ref type="bibr" target="#b16">(Liong et al., 2015)</ref> minimize the difference between the original and the signed version of the embedding with orthogonality regularizers in a network. <ref type="bibr" target="#b14">(Li et al., 2017)</ref> employ discrete cyclic coordinate descent <ref type="bibr" target="#b24">(Shen et al., 2015)</ref> on a discrete sub-problem optimizing one hash bit at a time but the algorithm has neither the convergence guarantees nor the bound on the number of iterations. All of these methods focus on learning the binary codes for hamming distance ranking and perform an exhaustive linear search over the entire dataset which is not likely to be suitable for large scale problems. <ref type="bibr" target="#b6">(Cao et al., 2016)</ref> minimize the difference between the similarity label and the cosine distance of network embedding. <ref type="bibr" target="#b17">(Liu &amp; Lu, 2017)</ref> define a distance between a quantized data and continuous embedding, and back-propagates the metric loss error only with respect to the continuous embedding. Both of these methods require repeatedly running k-means clustering on the entire dataset while training the network at the same time. This is unlikely to be practical for large scale problems because of the prohibitive computational complexity and having to store the cluster centroids for all classes in the memory as the number of classes becomes extremely large <ref type="bibr" target="#b20">(Prabhu &amp; Varma, 2014;</ref><ref type="bibr" target="#b7">Choromanska et al., 2013)</ref>.</p><p>In this paper, we propose an efficient end-to-end learning method for quantizable representations jointly optimizing the quality of the embedding representation and the performance of the corresponding hash codes in a scalable mini-batch stochastic gradient descent setting in a deep network and demonstrate state of the art search accuracy and quantitative search efficiency on multiple datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem formulation</head><p>Consider a hash function r(x) that maps an input data x ∈ X onto a d dimensional binary compound hash code h ∈ {0, 1} d with the constraint that k out of d total bits needs to be set. We parameterize the mapping as</p><formula xml:id="formula_0">r(x) = argmin h∈{0,1} d −f (x; θ) h subject to ||h||1 = k,<label>(1)</label></formula><p>where f (·, θ) : X → R d is a transformation (i.e. neural network) differentiable with respect to the parameter θ and takes the input x and emits the d dimensional embedding vector. Given the hash function r(·), we define a hash table H which is composed of d buckets with each bucket indexed by a compound hash code h. Then, given a query x q , union of all the items in the buckets indexed by k active bits in r(x q ) is retrieved as the candidates of the approximate nearest items of x q . Finally, this is followed by a reranking operation where the retrieved items are ranked according to the distances computed using the original embedding representation f (·; θ).</p><p>Note, in quantization based hashing <ref type="bibr" target="#b6">Cao et al., 2016)</ref>, a set of prototypes or cluster centroids are first computed via dictionary learning or other clustering (i.e. k-means) algorithms. Then, the function f (x; θ) is represented by the indices of k-nearest prototypes or centroids. Concretely, if we replace f (x; θ) in Equation (1) with the negative distances of the input item x with respect to all d prototypes or centroids, [−||x − c 1 || 2 , . . . , −||x − c d || 2 ] , then the corresponding hash function r(x) can be used to build the hash table.</p><p>In contrast to most of the recent methods that learn a hamming ranking in a neural network and perform exhaustive linear search over the entire dataset <ref type="bibr" target="#b31">(Xia et al., 2014;</ref><ref type="bibr" target="#b34">Zhao et al., 2015;</ref><ref type="bibr" target="#b19">Norouzi et al., 2012;</ref><ref type="bibr" target="#b14">Li et al., 2017)</ref>, quantization based methods, have guaranteed search inference speed up by only considering a subset of k out d buckets and thus avoid exhaustive linear search. We explicitly maintain the sparsity constraint on the hash code in Equation (1) throughout our optimization without continuous relaxations to inherit the efficiency aspect of quantization based hashing and this is one of the key attributes of the algorithm.</p><p>Although quantization based hashing is known to show high search accuracy and search efficiency , running the quantization procedure on the entire dataset to compute the cluster centroids is computationally very costly and requires storing all of the cluster centroids in the memory. Our desiderata are to formulate an efficient end-to-end learning method for quantizable representations which (1) guarantees the search efficiency by avoiding linear search over the entire data, (2) can be efficiently trained in a minibatch stochastic gradient descent setting and avoid having to quantize the entire dataset or having to store the cluster centroids for all classes in the memory, and (3) offers the modularity to accommodate existing embedding representation learning methods which are known to show the state of the art performance on retrieval and clustering tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods</head><p>We formalize our proposed method in Section 4.1 and discuss the subproblems in Section 4.2 and in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">End-to-end learning for quantizable representations</head><p>Finding the optimal set of embedding representations and the corresponding hash codes is a chicken and egg problem. Embedding representations are required to infer which k activation dimensions to set in the corresponding binary hash code, but the binary hash codes are needed to adjust the embedding representations indexed at the activated bits so that similar items get hashed to the same buckets and vice versa. We formalize this notion in Equation <ref type="formula" target="#formula_1">(2)</ref> below.</p><formula xml:id="formula_1">minimize θ h 1 ,...,hn metric({f (xi; θ)} n i=1 ; h1, . . . , hn) embedding representation quality + γ   n i −f (xi; θ) hi + n i j:y j =y i h i P hj   hash code performance subject to hi ∈ {0, 1} d , ||hi||1 = k, ∀i,<label>(2)</label></formula><p>where the matrix P encodes the pairwise cost for the hash code similarity between each negative pair and γ is the trade-off hyperparameter balancing the loss contribution between the embedding representation quality given the hash codes and the performance of the hash code with respect to the embedding representations. We solve this optimization problem via alternating minimization through iterating over solving for k-sparse binary hash codes h 1 , . . . , h n and updating the parameters of the deep network θ for the continuous embedding representations per each mini-batch. Following subsections discuss these two steps in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Learning the compound hash code</head><p>Given a set of continuous embedding representations</p><formula xml:id="formula_2">{f (x i ; θ)} n i=1</formula><p>, we seek to solve the following subproblem in Equation <ref type="formula" target="#formula_3">(3)</ref> where the task is to (unary) select k as large elements of the each embedding vector as possible, while (pairwise) selecting as orthogonal elements as possible across different classes. The unary term mimics the hash function r(x) in Equation (1) and the pairwise term has the added benefit that it also provides robustness to the optimization especially during the early stages of the training when the embedding representation is not very accurate.</p><formula xml:id="formula_3">minimize h 1 ,...,hn n i −f (xi; θ) hi + n i j:y j =y i h i P hj := g(h 1,...,n ;θ) subject to hi ∈ {0, 1} d , ||hi||1 = k, ∀i,<label>(3)</label></formula><p>However, solving for the optimal solution of the problem in Equation <ref type="formula" target="#formula_3">(3)</ref> is NP-hard in general even for the simple case where k = 1 and d &gt; 2 <ref type="bibr" target="#b3">(Boykov et al., 2001</ref>). Thus, we construct a upper bound functionḡ(h 1,...,n ; θ) to the objective function g(h 1,...,n ; θ) which we argue that it can be exactly optimized by establishing the connection to a network flow algorithm. The upper bound function is a slightly reparameterized discrete objective where we optimize the hash codes over the average embedding vectors per each class instead. We first rewrite 1 the objective function by indexing over each class and then over each data per class and derive the upper bound function as shown below.</p><formula xml:id="formula_4">g(h1,...,n; θ) = nc i k:y k =i −f (x k ; θ) h k + nc i k:y k =i, l:y l =i h k P h l ≤ nc i k:y k =i −c i h k + nc i k:y k =i, l:y l =i h k P h l + maximize h 1 ,...,hn h i ∈{0,1} d ,||h i || 1 =k nc i=1 k:y k =i (ci − f (x k ; θ)) h k :=M (θ) =ḡ(h1,...,n; θ)<label>(4)</label></formula><p>where n c denotes the number of classes in the mini-batch, m = |{k : y k = i}|, and c i = 1 m k:y k =i f (x k ; θ). Here, w.l.o.g we assume each class has m number of data in the mini-batch (i.e. Npairs <ref type="bibr" target="#b25">(Sohn, 2016)</ref> mini-batch construction). The last term in upper bound, denoted as M (θ), is constant with respect to the hash codes and is non-negative. Note, from the bound in Equation <ref type="formula" target="#formula_4">(4)</ref>, the gap between the minimum value of g and the minimum value ofḡ is bounded above by M (θ). Furthermore, since this value corresponds to the maximum deviation of an embedding vector from its class mean of the embedding, the bound gap decreases over iterations as we update the network parameter θ to attract similar pairs of data and vice versa for dissimilar pairs in the other embedding subproblem (more details in Section 4.4).</p><p>Moreover, minimizing the upper bound over each hash codes {h i } n i=1 is equivalent to minimizing a reparameter-izationĝ(z 1,...,nc ; θ) over {z i } nc i=1 defined below because for a given class label i, each h k shares the same c i vector.</p><formula xml:id="formula_5">minimize h 1 ,...,hn h i ∈{0,1} d ,||h i || 1 =k nc i k:y k =i −c i h k + nc i k:y k =i, l:y l =i h k P h l = minimize z 1 ,...,zn c z i ∈{0,1} d ,||z i || 1 =k m   nc i −c i zi + nc i j =i z i P zj   :=ĝ(z 1,...,nc ;θ)</formula><p>, where P = mP . Therefore, we formulate the following optimization problem below whose objective upper bounds the original objective in Equation <ref type="formula" target="#formula_3">(3)</ref> over all feasible hash</p><formula xml:id="formula_6">codes {h i } n i=1 . minimize z 1 ,...,zn c nc i −c i zi + i,j =i z i P zj subject to zi ∈ {0, 1} d , ||zi||1 = k, ∀i<label>(5)</label></formula><p>In the upper bound problem above, we consider the case where the pairwise cost matrix P is a diagonal matrix of non-negative values 2 . Theorem 1 in the next subsection proves that finding the optimal solution of Equation <ref type="formula" target="#formula_6">(5)</ref>  Proof. Suppose we construct a complete bipartite graph G = (A ∪ B, E) and create a directed graph G = (A ∪ B ∪ {s, t}, E ) from G by adding source s and sink t and directing all edges E in G from A to B. We also add edges from s to each vertex a p ∈ A. For each vertex b q ∈ B, we add n c number of edges to t. Edges incident to s have capacity u(s, a p ) = k and cost v(s, a p ) = 0. The edges between a p ∈ A and b q ∈ B have capacity u(a p , b q ) = 1 and cost v(a p , b q ) = −c p <ref type="bibr">[q]</ref>. Each edge r ∈ {0, . . . , n c − 1} from b q ∈ B to t has capacity u((b q , t) r ) = 1 and cost v((b q , t) r ) = 2λ q r. <ref type="figure" target="#fig_6">Figure 2</ref> illustrates the flow network G . The amount of flow to be sent from s to t is n c k.</p><p>Then we define the flow {f z (e)} e∈E , indexed both by (1) a given configuration of z 1 , . . . , z nc where each z i ∈ {0, 1} d , ||z i || 1 = k, ∀i, and by (2) the edges of G , below:</p><formula xml:id="formula_7">(i) f z (s, a p ) = k, (ii) f z (a p , b q ) = z p [q], (iii) f z ((b q , t) r ) = 1 for r &lt; nc p=1 z p [q] 0 otherwise (6)</formula><p>We first show the flow f z defined above is feasible for G . The capacity constraints are satisfied by construction in Equation (6), so we only need to check the flow conservation conditions. First, the amount of input flow at s is n c k and the output flow from s is ap∈A f z (s, a p ) = ap∈A k = n c k which is equal. The amount of input flow to each vertex a p ∈ A is given as k and the output flow is</p><formula xml:id="formula_8">bq∈B f z (a p , b q ) = d q z p [q] = ||z p || 1 = k.</formula><p>Let us denote the amount of input flow at a ver-</p><formula xml:id="formula_9">tex b q ∈ B as y q = nc p z p [q].</formula><p>The output flow from the vertex b q is</p><formula xml:id="formula_10">nc−1 r=0 f z ((b q , t) r ) = yq−1 r=0 f z ((b q , t) r ) + nc−1</formula><p>r=yq f z ((b q , t) r ) = y q from Equation (6) (iii). The last condition to check is that the amount of input flow at t is equal to the output flow at s.</p><formula xml:id="formula_11">bq∈B nc−1 r=0 f z ((b q , t) r ) = d q=1 y q = q,p z p [q] = n c k.</formula><p>This shows the construction of the flow {f z (e)} e∈E in Equation <ref type="formula">(6)</ref> is valid in G . Now denote {f o (e)} e∈E as the minimum cost flow solution of the flow network G which minimizes the total cost e∈E v(e)f o (e). Denote the optimal flow from a vertex a p ∈ A to a vertex b q ∈ B as z p [q] := f o (a p , b q ). By the optimality of the flow {f o (e)} e∈E , we have that e∈E v(e)f o (e) ≤ e∈E v(e)f z (e). By Lemma 1, the lhs of the inequality is equal to p −c p z p + p1 =p2 z p1 P z p2 . Also, by Lemma 2, the rhs is equal to p −c p z p + p1 =p2 z p1 P z p2 .</p><p>Finally, we have that p −c p z p + p1 =p2 z p1 P z p2 ≤ p −c p z p + p1 =p2 z p1 P z p2 , ∀{z p }. Thus, we have proved that finding the minimum cost flow solution on the flow network G and translating the flows between each vertices between A and B as {z p }, we can find the optimal solution to the optimization problem in Equation (5).</p><p>Lemma 1. For the minimum cost flow {f o (e)} e∈E of the network G , we have that the total cost is</p><formula xml:id="formula_12">e∈E v(e)f o (e) = p −c p z p + p1 =p2 z p1 P z p2 .</formula><p>Proof. The total minimum cost e∈E v(e)f o (e) is broken down as </p><formula xml:id="formula_13">= p f o (a p , b q ) = nc p z p [q]</formula><p>. From the cost definition at the edges between b q and t, v((b q , t) r ) = 2λ q r, and by the optimality of the minimum cost flow, we have that f o ((b q , t) r ) = 1 ∀r &lt; y q and f o ((b q , t) r ) = 0 ∀r ≥ y q . Thus, the total cost is</p><formula xml:id="formula_14">e∈E v(e)fo(e) = 0 + nc p d q −cp[q]z p [q] + bq ∈B y q −1 r=0 2λqr = p −c p z p + q λqy q (y q − 1) = p −c p z p + q λqy q 2 − p q λqz p [q] = p −c p z p + p z p P p z p − p z p P z p = p −c p z p + p 1 =p 2 z p 1 P z p 2<label>(7)</label></formula><p>Lemma 2. For the {f z (e)} e∈E defined as Equation (6) of the network G , we have that the total cost is e∈E v(e)f z (e) = p −c p z p + p1 =p2 z p1 P z p2 .</p><p>Proof. The proof is similar to Lemma 1 except that we use the definition of the flow {f z (e)} e∈E in Equation <ref type="formula">(6) (iii)</ref> to reduce the cost of the flow from B to t to</p><formula xml:id="formula_15">y q −1 r=0 2λ q r.</formula><p>Time complexity For λ q n c , note that the worst case time complexity of finding the minimum cost flow (MCF) solution in the network G is O (n c + d) 2 n c d log (n c + d) <ref type="bibr" target="#b10">(Goldberg &amp; Tarjan, 1990</ref>). In practice, however, it has been shown that implementation heuristics such as price updates, price refinement, push-look-ahead, <ref type="bibr" target="#b9">(Goldberg, 1997)</ref> and set-relabel <ref type="bibr" target="#b5">(Bünnagel et al., 1998)</ref> methods drastically improve the reallife performance. Also, we emphasize again that we solve the minimum cost flow problem only within the mini-batch not on the entire dataset. We benchmarked the wall clock running time of the method at varying sizes of n c and d and observed approximately linear time complexity in n c and d.     </p><formula xml:id="formula_16">1 , − c p [ 1 ] 1,−c p [q] 1 , − c p [ d ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Learning the embedding</head><p>As the hash codes become more and more sparse, it becomes increasingly likely for hamming distances defined on binary codes <ref type="bibr" target="#b19">(Norouzi et al., 2012;</ref><ref type="bibr" target="#b34">Zhao et al., 2015)</ref> to become zero regardless of whether the input pair of data is similar or dissimilar. This phenomenon can be problematic when trained in a deep network because the back-propagation gradient would become zero and thus the embedding representations would not be updated at all. In this regard, we propose a distance function based on gated residual as shown in Equation <ref type="formula" target="#formula_17">(8)</ref>. This parameterization outputs zero distance only if the embedding representations of the two input data are identical at all the hash code activations. Concretely, given a pair of embedding vectors f (x i ; θ), f (x j ; θ) and the corresponding binary k-sparse hash codes h i , h j , we define the following distance function d hash ij between the embedding vectors</p><formula xml:id="formula_17">d hash ij = || (hi ∨ hj) (f (xi; θ) − f (xj; θ)) ||1,<label>(8)</label></formula><p>where ∨ denotes the logical or operation of the two binary hash codes and denotes the element-wise multiplication. Then, using the distance function above, we can define the following subproblems using any existing deep metric learning methods <ref type="bibr" target="#b30">(Weinberger et al., 2006;</ref><ref type="bibr" target="#b22">Schroff et al., 2015;</ref><ref type="bibr" target="#b26">Song et al., 2016;</ref><ref type="bibr" target="#b25">Sohn, 2016;</ref><ref type="bibr" target="#b27">Song et al., 2017)</ref>. Given a set of binary hash codes {h i } n i=1 , we seek to solve the following subproblems where the task is to optimize the embedding representations so that similar pairs of data get hashed to the same buckets and dissimilar pairs of data get hashed to different buckets. In other words, we need similar pairs of data to have similar embedding representations indexed at the activated hash code dimensions and vice versa. In terms of the hash code optimization in Equation <ref type="formula" target="#formula_4">(4)</ref>, updating the network weight has the effect of tightening the bound gap M (θ).</p><p>Equation <ref type="formula" target="#formula_19">(9)</ref> and Equation <ref type="formula" target="#formula_0">(10)</ref> show the subproblems defined on the distance function above using Triplet <ref type="bibr" target="#b22">(Schroff et al., 2015)</ref> and Npairs (Sohn, 2016) method respectively. We optimize these embedding subproblems by updating the network parameter θ via stochastic gradient descent using the subgradients</p><formula xml:id="formula_18">∂ metric(θ;h1,...,n ) ∂θ</formula><p>given the hash codes per each mini-batch.</p><formula xml:id="formula_19">minimize θ 1 |T | (i,j,k)∈T [d hash ij + α − d hash ik ]+ triplet (θ; h 1,...,n ) subject to ||f (x; θ)||2 = 1,<label>(9)</label></formula><p>where</p><formula xml:id="formula_20">T = {(x i , x + i , x − i )} i</formula><p>is the set of triplets <ref type="bibr" target="#b22">(Schroff et al., 2015)</ref>, and the embedding vectors are normalized onto unit hypersphere ||f (x; θ)|| 2 = 1, ∀x ∈ X . We also apply the semi-hard negative mining procedure <ref type="bibr" target="#b22">(Schroff et al., 2015)</ref> where hard negatives farther than the distance between the anchor and positives are mined within the mini-batch. In practice, since our method can be applied to any deep metric learning methods, we use existing deep metric learning implementations available in tf.contrib.losses.metric learning. Similarly, we could also employ npairs <ref type="bibr" target="#b25">(Sohn, 2016)</ref> method,</p><formula xml:id="formula_21">minimize θ −1 |P| (i,j)∈P log exp −d hash ij exp −d hash ij + k:y k =y i exp −d hash ik npairs (θ; h 1,...,n ) + λ m i ||f (xi; θ)|| 2 2 ,<label>(10)</label></formula><p>where the npairs mini-batch B is constructed with positive pairs (x, x + ) which are negative with respect to all other pairs. B = {(x 1 , x + 1 ), . . . , (x n , x + n ))} and P denotes the set of all positive pairs within the mini-batch. We use the existing implementation of npairs loss in Tensorflow as well. Note that even though the distance d hash ij is defined after masking the embeddings with the union binary vector (h i ∨ h j ), it's important to normalize or regularize the embedding representation before the masking operations for the optimization stability due to the sparse nature of the hash codes.</p><formula xml:id="formula_22">Algorithm 1 Learning algorithm input θ emb b (pretrained metric learning base model); θ d ∈ R d initialize θ f = [θ b , θ d ]</formula><p>1: for t = 1, . . . , MAXITER do 2: Sample a minibatch {xj} 3: Update the flow network G by recomputing the cost vectors for all classes in the minibatch</p><formula xml:id="formula_23">ci = 1 m k:y k =i f (x k ; θ f ) 4:</formula><p>Compute the hash codes {hi} minimizing Equation <ref type="formula" target="#formula_6">(5) via</ref> finding the minimum cost flow on G 5: Update the network parameter given the hash codes</p><formula xml:id="formula_24">θ f ← θ f − η (t) ∂ metric(θf ; h1,...,n c )/∂θ f 6: Update stepsize η (t) ← ADAM rule (Kingma &amp; Ba, 2014) 7: end for output θ f (final estimate);</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Query efficiency analysis</head><p>In this subsection, we examine the expectation and the variance of the query time speed up over linear search. Recall the properties of the compound hash code defined in Section 3, h ∈ {0, 1} d and ||h|| 1 = k. Given n such hash codes, we have that</p><formula xml:id="formula_25">Pr(h i h j = 0) = d−k k / d k</formula><p>assuming the hash code uniformly distributes the items throughout different buckets. For a given hash code h q , the number of retrieved data is N q = i =q 1(h i h q = 0). Then, the expected number of retrieved data is</p><formula xml:id="formula_26">E[N q ] = (n − 1) 1 − d−k k / d k</formula><p>. Thus, in contrast to linear search, the expected speedup factor (SUF) under perfectly uniform distribution of the hash code is</p><formula xml:id="formula_27">E[SUF] = 1 − d−k k d k −1<label>(11)</label></formula><p>In the case where d k, the speedup factor approaches d k 2 . Similarly, we have that the variance is</p><formula xml:id="formula_28">V [N q ] = (n − 1) 1 − d−k k / d k d−k k / d k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation details</head><p>Network architecture In our experiments, we used the NIN <ref type="bibr" target="#b15">(Lin et al., 2013)</ref> architecture (denote the parameters as θ b ) with leaky relu <ref type="bibr" target="#b32">(Xu et al., 2015)</ref> with α = 5.5 as activation function and trained Triplet embedding network with semi-hard negative mining <ref type="bibr" target="#b22">(Schroff et al., 2015)</ref> and Npairs network <ref type="bibr" target="#b25">(Sohn, 2016)</ref>  Hash table construction and query We use the learned hash network θ f and apply Equation <ref type="formula" target="#formula_0">(1)</ref> to convert a hash data x i into the hash code h(x i ; θ f ) and use the base embedding network θ emb b to convert the data into the embedding representation f (x i ; θ emb b ). Then, the embedding representation is hashed to buckets corresponding to the k set bits in the hash code. We use the similar procedure and convert a query data x q into the hash code h(x q ; θ f ) and into the embedding representation f (x q ; θ emb b ). Once we retrieve the union of all bucket items indexed at the k set bits in the hash code, we apply a reranking procedure  based on the euclidean distance in the embedding representation space.</p><p>Evaluation metrics We report our accuracy results using precision@k (Pr@k) and normalized mutual information (NMI) <ref type="bibr" target="#b18">(Manning et al., 2008)</ref> metrics. Precision@k is computed based on the reranked ordering (described above) of the retrieved items from the hash table. We evaluate NMI, when the code sparsity is set to k = 1, treating each bucket as individual clusters. In this setting, NMI becomes perfect, if each bucket has perfect class purity (pathologically putting one item per each bucket is prevented by construction since d n). We report the speedup results by comparing the number of retrieved items versus the total number of data (exhaustive linear search) and denote this metric as SUF. As the hash code becomes uniformly distributed, SUF metric approaches the theoretical expected speedup in Equation (11). <ref type="figure" target="#fig_9">Figure 3</ref> shows that the measured SUF of our method closely follows the theoretical upper bound in contrast to other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We report our results on Cifar-100 <ref type="bibr" target="#b13">(Krizhevsky et al., 2009)</ref> and ImageNet <ref type="bibr" target="#b21">(Russakovsky et al., 2015)</ref> datasets and compare the accuracy against several baseline methods. First baseline methods are the state of the art deep metric learning models <ref type="bibr" target="#b22">(Schroff et al., 2015;</ref><ref type="bibr" target="#b25">Sohn, 2016)</ref> performing an exhaustive linear search over the whole dataset given a query data. Another baselines are the Binarization transform <ref type="bibr" target="#b1">(Agrawal et al., 2014;</ref><ref type="bibr" target="#b33">Zhai et al., 2017)</ref> methods where the dimensions of the hash code corresponding to the top k dimensions of the embedding representation are set. We also perform vector quantization  on the learned embedding representation from the deep metric learning methods above on the entire dataset and compute the hash code based on the indices of the k nearest centroids.</p><p>'Triplet' and 'Npairs' denotes the deep metric learning base models performing an exhaust linear search per each query. 'Th' denotes the binarization transform baseline, 'VQ' denotes the vector quantization baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Cifar-100</head><p>Cifar-100 <ref type="bibr" target="#b13">(Krizhevsky et al., 2009</ref>) dataset has 100 classes. Each class has 500 images for train and 100 images for test. Given a query image from test, we experiment the search performance both when the hash table is constructed from train and from test. We subtract the per-pixel mean of training images across all the images and augmented the dataset by zero-padding 4 pixels on each side, randomly cropping 32 × 32, and applying random horizontal flipping. The batch size is set to 128. The metric learning base model is trained for 175k iterations, and learning rate decays to 0.1 of previous learning rate after 100k iterations. We finetune the base model for 70k iterations and decayed the learning rate to 0.1 of previous learning rate after 40k iterations. <ref type="table">Table 1</ref> show results using the triplet network with d = 256 and <ref type="table" target="#tab_1">Table 2</ref> show results using the npairs network with d = 64. The results show that our method not only outperforms search accuracies of the state of the art deep metric learning base models but also provides up to 98× speed up over exhaustive search.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">ImageNet</head><p>ImageNet ILSVRC-2012 <ref type="bibr" target="#b21">(Russakovsky et al., 2015)</ref> dataset has 1, 000 classes and comes with train (1, 281, 167 images) and val set (50, 000 images). We use the first nine splits of train set to train our model, the last split of train set for validation, and use validation dataset to test the query performance. We use the images downsampled to 32 × 32 from <ref type="bibr" target="#b8">(Chrabaszcz et al., 2017)</ref>. Preprocessing step is identical with cifar-100 and we used the pixel mean provided in the dataset. The batch size for the metric learning base model is set to 512 and is trained for 450k iterations, and learning rate decays to 0.3 of previous learning rate after each 200k iterations. When we finetune npairs base model for d = 512, we set the batch size to 1024 and total iterations to 35k with decaying the learning rate to 0.3 of previous learning rate after each 15k iterations. When we finetune the triplet base model for d = 256, we set the batch size to 512 and total iterations to 70k with decaying the learning rate to 0.3 of previous learning rate after each 30k iterations. Our results in <ref type="table">Table 3</ref> and <ref type="table">Table 4</ref> show that our method outperforms the state of the art deep metric learning base models in search accuracy while providing up to 478× speed up over exhaustive linear search. <ref type="table">Table 5</ref> compares the NMI metric and shows that the hash table constructed from our representation yields buckets with significantly better class purity on both datasets and on both methods.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have presented a novel end-to-end optimization algorithm for jointly learning a quantizable embedding representation and the sparse binary hash code which then can be used to construct a hash table for efficient inference. We also show an interesting connection between finding the optimal sparse binary hash code and solving a minimum cost flow problem. Our experiments show that the proposed algorithm not only achieves the state of the art search accuracy outperforming the previous state of the art deep metric learning approaches <ref type="bibr" target="#b22">(Schroff et al., 2015;</ref><ref type="bibr" target="#b25">Sohn, 2016)</ref> but also provides up to 98× and 478× search speedup on Cifar-100 and ImageNet datasets respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>is equivalent to finding the minimum cost flow solution of the flow network G illustrated in Figure 2 which can be solved efficiently and exactly in polynomial time. In prac- tice, we use the efficient implementations from OR-Tools (Google Optimization Tools for combinatorial optimization problems) (OR-tools, 2018) to solve the minimum cost flow problem per each mini-batch. 4.3. Equivalence of problem 5 to minimum cost flow Theorem 1. The optimization problem in Equation (5) can be solved by finding the minimum cost flow solution on the flow network G'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>bq, t)r)fo((bq, t)r) flow from B to t Denote the amount of input flow at a vertex b q given the minimum cost flow {f o (e)} e∈E as y q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 Figure 1 .</head><label>11</label><figDesc>Figure 1. Average wall clock run time of computing minimum cost flow on G per mini-batch using (OR-tools, 2018). In practice, the run time is approximately linear in nc and d. Each data point is averaged over 20 runs on machines with Intel Xeon E5-2650 CPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Equivalent flow network diagram G for the optimization problem in Equation (5). Labeled edges show the capacity and the cost respectively. The amount of total flow to be sent is nck.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>d dimensional fully connected projection layer (θ d ) and finetune the hash network (denote the pa- rameters as θ f = [θ b , θ d ]). Algorithm 1 summarizes the training procedure in detail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. SUF metric for on Cifar-100 and ImageNet respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Table 1. Results with Triplet network with hard negative mining. Querying Cifar-100 test data against hash tables built on train set and on test set.</figDesc><table>train 
test 

Method 
SUF 
Pr@1 
Pr@4 
Pr@16 
SUF 
Pr@1 
Pr@4 
Pr@16 

Triplet 
1.00 
62.64 
61.91 
61.22 
1.00 
56.78 
55.99 
53.95 

k = 1 

Triplet-Th 
43.19 
61.56 
60.24 
58.23 
41.21 
54.82 
52.88 
48.03 
Triplet-VQ 
40.35 
62.54 
61.78 
60.98 
22.78 
56.74 
55.94 
53.77 
Triplet-Ours 
97.77 
63.85 
63.40 
63.39 
97.67 
57.63 
57.16 
55.76 

k = 2 

Triplet-Th 
15.34 
62.41 
61.68 
60.89 
14.82 
56.55 
55.62 
52.90 
Triplet-VQ 
6.94 
62.66 
61.92 
61.26 
5.63 
56.78 
56.00 
53.99 
Triplet-Ours 
78.28 
63.60 
63.19 
63.09 
76.12 
57.30 
56.70 
55.19 

k = 3 

Triplet-Th 
8.04 
62.66 
61.88 
61.16 
7.84 
56.78 
55.91 
53.64 
Triplet-VQ 
2.96 
62.62 
61.92 
61.22 
2.83 
56.78 
55.99 
53.95 
Triplet-Ours 
44.36 
62.87 
62.22 
61.84 
42.12 
56.97 
56.25 
54.40 

k = 4 

Triplet-Th 
5.00 
62.66 
61.94 
61.24 
4.90 
56.84 
56.01 
53.86 
Triplet-VQ 
1.97 
62.62 
61.91 
61.22 
1.91 
56.77 
55.99 
53.94 
Triplet-Ours 
16.52 
62.81 
62.14 
61.58 
16.19 
57.11 
56.21 
54.20 

train 
test 

Method 
SUF 
Pr@1 
Pr@4 
Pr@16 
SUF 
Pr@1 
Pr@4 
Pr@16 

Npairs 
1.00 
61.78 
60.63 
59.73 
1.00 
57.05 
55.70 
53.91 

k = 1 

Npairs-Th 
13.65 
60.80 
59.49 
57.27 
12.72 
54.95 
52.60 
47.16 
Npairs-VQ 
31.35 
61.22 
60.24 
59.34 
34.86 
56.76 
55.35 
53.75 
Npairs-Ours 
54.90 
63.11 
62.29 
61.94 
54.85 
58.19 
57.22 
55.87 

k = 2 

Npairs-Th 
5.36 
61.65 
60.50 
59.50 
5.09 
56.52 
55.28 
53.04 
Npairs-VQ 
5.44 
61.82 
60.56 
59.70 
6.08 
57.13 
55.74 
53.90 
Npairs-Ours 
16.51 
61.98 
60.93 
60.15 
16.20 
57.27 
55.98 
54.42 

k = 3 

Npairs-Th 
3.21 
61.75 
60.66 
59.73 
3.10 
56.97 
55.56 
53.76 
Npairs-VQ 
2.36 
61.78 
60.62 
59.73 
2.66 
57.01 
55.69 
53.90 
Npairs-Ours 
7.32 
61.90 
60.80 
59.96 
7.25 
57.15 
55.81 
54.10 

k = 4 

Npairs-Th 
2.30 
61.78 
60.66 
59.75 
2.25 
57.02 
55.64 
53.88 
Npairs-VQ 
1.55 
61.78 
60.62 
59.73 
1.66 
57.03 
55.70 
53.91 
Npairs-Ours 
4.52 
61.81 
60.69 
59.77 
4.51 
57.15 
55.77 
54.01 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Results with Npairs (Sohn, 2016) network. Querying Cifar-100 test data against hash tables built on train set and on test set.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Npairs-Ours 116.61 16.40 14.49 12.00Table 3. Results with Npairs (Sohn, 2016) network. Querying ImageNet val data against hash table built on val set.</figDesc><table>Method 
SUF 
Pr@1 Pr@4 Pr@16 

Npairs 
1.00 
15.73 13.75 
11.08 

k = 1 

Npairs-Th 
1.74 
15.06 12.92 
9.92 
Npairs-VQ 
451.42 15.20 13.27 
10.96 
Npairs-Ours 478.46 16.95 15.27 
13.06 

k = 2 

Npairs-Th 
1.18 
15.70 13.69 
10.96 
Npairs-VQ 
116.26 15.62 13.68 
11.15 
k = 3 

Npairs-Th 
1.07 
15.73 13.74 
11.07 
Npairs-VQ 
55.80 
15.74 13.74 
11.12 
Npairs-Ours 
53.98 
16.24 14.32 
11.73 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Results with Triplet network with hard negative mining. Querying ImageNet val data against hash table built on val set.Npairs-Ours 84.90 68.56 55.09Hash table NMI for Cifar-100 and Imagenet.</figDesc><table>Method 
SUF 
Pr@1 Pr@4 Pr@16 

Triplet 
1.00 
10.90 
9.39 
7.45 

k = 1 

Triplet-Th 
18.81 
10.20 
8.58 
6.50 
Triplet-VQ 
146.26 10.37 
8.84 
6.90 
Triplet-Ours 221.49 11.00 
9.59 
7.83 

k = 2 

Triplet-Th 
6.33 
10.82 
9.30 
7.32 
Triplet-VQ 
32.83 
10.88 
9.33 
7.39 
Triplet-Ours 
60.25 
11.10 
9.64 
7.73 

k = 3 

Triplet-Th 
3.64 
10.87 
9.38 
7.42 
Triplet-VQ 
13.85 
10.90 
9.38 
7.44 
Triplet-Ours 
27.16 
11.20 
9.55 
7.60 

Cifar-100 
ImageNet 

train 
test 
val 

Triplet-Th 
68.20 54.95 
31.62 
Triplet-VQ 
76.85 62.68 
45.47 
Triplet-Ours 89.11 68.95 
48.52 

Npairs-Th 
51.46 44.32 
15.20 
Npairs-VQ 
80.25 66.69 
53.74 
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We also omit the dependence of the index i for each h k and h l to avoid the notation clutter.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that we absorb the scaler factor m from the definition of P and redefine P = diag(λ1, . . . , λ d ).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Zhen Li at Google Research for helpful discussions and anonymous reviewers for their constructive comments. This work was partially supported by Kakao, Kakao Brain and Basic Science Research Program through the National Research Foundation of Korea (NRF) (2017R1E1A1A01077431). Hyun Oh Song is the corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="http://tensorflow.org/.Softwareavailablefromtensorflow.org" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analyzing the performance of multilayer neural networks for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malik</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning visual similarity for product design with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIG-GRAPH</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sackinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient implementation of the goldberg-tarjan minimum-cost flow algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bünnagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vygen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optimization Methods and Software</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="174" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep quantization network for efficient image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Extreme multi class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A downsampled variant of imagenet as an alternative to the cifar datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chrabaszcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08819</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An efficient implementation of a scaling minimum-cost flow algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of algorithms</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Finding minimum-cost circulations by successive approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cifar-100 (canadian institute for advanced research)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/˜kriz/cifar.html" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep supervised discrete hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<idno>abs/1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep hashing for compact binary codes learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep representations with diode loss for quantization-based similarity search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schutze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hamming distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning transferrable representations for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Supervised discrete hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep metric learning via facility location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murphy</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00185</idno>
		<title level="m">A survey on learning to hash</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Supervised hashing for image retrieval via image representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visual discovery at pinterest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kislyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web Companion</title>
		<meeting>the 26th International Conference on World Wide Web Companion</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep semantic ranking based hashing for multi-label image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
