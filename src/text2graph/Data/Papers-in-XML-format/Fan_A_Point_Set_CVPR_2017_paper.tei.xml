<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Point Set Generation Network for 3D Object Reconstruction from a Single Image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="department" key="dep2">Leonidas Guibas Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Tsinghua University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
							<email>haosu@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="department" key="dep2">Leonidas Guibas Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Tsinghua University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Point Set Generation Network for 3D Object Reconstruction from a Single Image</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generation of 3D data by deep neural networks has been attracting increasing attention in the research</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As we try to duplicate the successes of current deep convolutional architectures in the 3D domain, we face a fundamental representational issue. Extant deep net architectures for both discriminative and generative learning in the signal domain are well-suited to data that is regularly sampled, such as images, audio, or video. However, most common 3D geometry representations, such as 2D meshes or point clouds are not regular structures and do not easily fit into architectures that exploit such regularity * equal contribution</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Reconstructed 3D point cloud for weight sharing, etc. That is why the majority of extant works on using deep nets for 3D data resort to either volumetric grids or collections of images (2D views of the geometry). Such representations, however, lead to difficult trade-offs between sampling resolution and net efficiency. Furthermore, they enshrine quantization artifacts that obscure natural invariances of the data under rigid motions, etc. In this paper we address the problem of generating the 3D geometry of an object based on a single image of that object. We explore generative networks for 3D geometry based on a point cloud representation. A point cloud representation may not be as efficient in representing the underlying continuous 3D geometry as compared to a CAD model using geometric primitives or even a simple mesh, but for our purposes it has many advantages. A point cloud is a simple, uniform structure that is easier to learn, as it does not have to encode multiple primitives or combinatorial connectivity patterns. In addition, a point cloud allows simple manipulation when it comes to geometric transformations and deformations, as connectivity does not have to be updated. Our pipeline infers the point positions in a 3D frame determined by the input image and the inferred viewpoint position.</p><p>Given this unorthodox network output, one of our challenges is how to measure loss during training, as the same geometry may admit different point cloud representations at the same degree of approximation. Unlike the usual L 2 type losses, we use the solution of a transportation problem based on the Earth Mover's distance (EMD), effectively solving an assignment problem. We exploit an approximation to the EMD to provide speed as well as ensure differentiability for end-to-end training.</p><p>Our approach effectively attempts to solve the ill-posed problem of 3D structure recovery from a single projection using certain learned priors. The network has to estimate depth for the visible parts of the image and hallucinate the rest of the object geometry, assessing the plausibility of several different completions. From a statistical perspective, it would be ideal if we can fully characterize the landscape of the ground truth space, or be able to sample plausible candidates accordingly. If we view this as a regression problem, then it has a rather unique and interesting feature arising from inherent object ambiguities in certain views. These are situations where there are multiple, equally good 3D reconstructions of a 2D image, making our problem very different from classical regression/classification settings, where each training sample has a unique ground truth annotation. In such settings the proper loss definition can be crucial in getting the most meaningful result.</p><p>Our final algorithm is a conditional sampler, which samples plausible 3D point clouds from the estimated ground truth space given an input image. Experiments on both synthetic and real world data verify the effectiveness of our method. Our contributions can be summarized as follows:</p><p>• We use deep learning techniques to study the point set generation problem;</p><p>• On the task of 3D reconstruction from a single image, we apply our point set generation network and significantly outperform state of the art;</p><p>• We systematically explore issues in the architecture and loss function design for point generation network;</p><p>• We discuss and address the ground-truth ambiguity issue for the 3D reconstruction from single image task.</p><p>Source code demonstrating our system can be obtained from https://github.com/fanhqme/PointSetGeneration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D reconstruction from single images While most researches focus on multi-view geometry such as SFM and SLAM <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9]</ref>, ideally, one expect that 3D can be reconstructed from the abundant single-view images.</p><p>Under this setting, however, the problem is ill-posed and priors must be incorporated. Early work such as ShapeFromX <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b0">1]</ref> made strong assumptions over the shape or the environment lighting conditions. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref> pioneered the use of learning-based approach for simple geometric structures. Coarse correspondences in an image collection can also be used for rough 3D shape estimation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3]</ref>. As commodity 3D sensors become popular, RGBD database has been built and used to train learningbased systems <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>. Though great progress has been made, these methods still cannot robustly reconstruct complete and quality shapes from single images. Stronger shape priors are missing.</p><p>Recently, large-scale repositories of 3D CAD models, such as ShapeNet <ref type="bibr" target="#b3">[4]</ref>, have been introduced. They have great potential for 3D reconstruction tasks. For example, <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13]</ref> proposed to deform and reassemble existing shapes into a new model to fit the observed image. These systems rely on high-quality image-shape correspondence, which is a challenging and ill-posed problem itself.</p><p>More relevant to our work is <ref type="bibr" target="#b4">[5]</ref>. Given a single image, they use a neural network to predict the underlying 3D object as a 3D volume. There are two key differences between our work and <ref type="bibr" target="#b4">[5]</ref>: First, the predicted object in <ref type="bibr" target="#b4">[5]</ref> is a 3D volume; whilst ours is a point cloud. As demonstrated and analyzed in Sec 5.2, point set forms a nicer shape space for neural networks, thus the predicted shapes tend to be more complete and natural. Second, we allow multiple reconstruction candidates for a single input image. This design reflects the fact that a single image cannot fully determine the reconstruction of a 3D shape.</p><p>Deep learning for geometric object synthesis In general, the field of how to predict geometries in an end-to-end fashion is quite a virgin land. In particular, our output, 3D point set, is still not a typical object in the deep learning community. A point set contains orderless samples from a metric-measure space. Therefore, equivalent classes are defined up to a permutation; in addition, the ground distance must be taken into consideration. To our knowledge, we are not aware of prior deep learning systems with the abilities to predict such objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem and Notations</head><p>Our goal is to reconstruct the complete 3D shape of an object from a single 2D image (RGB or RGB-D). We represent the 3D shapes in the form of unordered point set</p><formula xml:id="formula_0">S = {(x i , y i , z i )} N i=1</formula><p>where N is a predefined constant. We observed that for most objects using N = 1024 is sufficient to preserve the major structures. One advantage of point set comes from its unorderedness. Unlike 2D based representations like the depth map no topological constraint is put on the represented object. Compared to 3D grids, the point set enjoys higher efficiency by encoding only the points on the surface. Also, the coordinate values (x i , y i , z i ) go over simple linear transformations when the object is rotated or scaled, which is in contrast to the case in volumetric representations.</p><p>To model the problem's uncertainty, we define the groundtruth as a probability distribution P(·|I) over the shapes conditioned on the input I. In training we have access to one sample from P(·|I) for each image I.</p><p>We train a neural network G as a conditional sampler from P(·|I):</p><formula xml:id="formula_1">S = G(I, r; Θ)<label>(1)</label></formula><p>where Θ denotes network parameter, r ∼ N(0, I) is a random variable to perturb the input <ref type="bibr" target="#b0">1</ref> . During test time multiple samples of r could be used to generate different predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Overview</head><p>Our task of building a conditional generative network for point sets is challenging, due to the unordered form of representation and the inherent ambiguity of groundtruth. These challenges have pushed us to invent new architecture, loss function, and learning paradigm. Specifically, we have to address three subproblems: Point set generator architecture: Network to predict point set is barely studied in literature, leaving a huge open space for us to explore the design choices. Ideally, a network should make the best use of its data statistics and possess enough representation power. We propose a network with two prediction branches, one enjoys high flexibility in capturing complicated structures and the other exploits geometric continuity. See Sec 4.2. Loss function for point set comparison: For our novel type of prediction, point set, it is unclear how to measure <ref type="bibr" target="#b0">1</ref> Similar to the Conditional Generative Adversarial Network <ref type="bibr" target="#b14">[15]</ref>.</p><p>the distance between the prediction and groundtruth. We propose two distance metrics for point sets -the Chamfer distance and the Earth Mover's distance. We show that both metrics are differentiable almost everywhere and can be used as the loss function, but has different properties in capturing shape space. See Sec 4.3. Modeling the uncertainty of groundtruth: Our problem of 3D structural recovery from a single image is illposed, thus the ambiguity of groundtruth arises during the train and test time. It is fundamentally important to characterize the ambiguity of groundtruth for a given input, and practically desirable to be able to generate multiple predictions. Surprisingly, this goal can be achieved tactfully by simply using the min function as a wrapper to the above proposed loss, or by a conditional variational autoencoder. See Sec 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Point Set Prediction Network</head><p>The task of building a network for point set prediction is new. We design a network with the goal of possessing strong representation power for complicated structures, and make the best use of the statistics of geometric data. To introduce our network progressively, we start from a simple version and gradually add components.</p><p>As in <ref type="figure" target="#fig_1">Fig 2 (top)</ref>, our network has an encoder stage and a predictor stage. The encoder maps the input pair of an image I and a random vector r into an embedding space. The predictor outputs a shape as an N × 3 matrix M, each row containing the coordinates of one point.</p><p>The encoder is a composition of convolution and ReLU layers; in addition, a random vector r is subsumed so that it perturbs the prediction from the image I. We postpone the explanation of how r is used to Sec 4.4. The predictor generates the coordinates of N points through a fully connected network. Though simple, this version works reasonably well in practice.</p><p>We further improve the design of the predictor branch to better accommodate large and smooth surfaces which are common in natural objects. The fully connected predictor as above cannot make full use of such natural geometric statistics, since each point is predicted independently. The This version has two parallel predictor branches -a fully-connected (fc) branch and a deconvolution (deconv) branch. The fc branch predicts N 1 points as before. The deconv branch predicts a 3 channel image of size H × W , of which the three values at each pixel are the coordinates of a point, giving another H × W points. Their predictions are later merged together to form the whole set of points in M. Multiple skip links are added to boost information flow across encoder and predictor.</p><p>With the fc branch, our model enjoys high flexibility, showing good performance at describing intricate structures. With the deconvolution branch, our model becomes not only more parameter parsimonious by weight sharing; but also more friendly to large smooth surfaces, due to the spatial continuity induced by deconv and conv. Refer to Sec 5.5 for experimental evidences.</p><p>Above introduces the design of our network G in Eq 1. To train this network, however, we still need to design a proper loss function for point set prediction, and enable the role r for multiple candidates prediction. We explain in the next two sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Distance Metric between Point Sets</head><p>A critical challenge is to design a good loss function for comparing the predicted point cloud and the groundtruth. To plug in a neural network, a suitable distance must satisfy at least three conditions: 1) differentiable with respect to point locations; 2) efficient to compute, as data will be forwarded and back-propagated for many times; 3) robust against small number of outlier points in the sets (e.g. Hausdorff distance would fail).</p><p>We seek for a distance d between subsets in R 3 , so that the loss function L({S</p><formula xml:id="formula_2">pred i }, {S gt i }) takes the form L({S pred i }, {S gt i }) = i d(S pred i , S gt i ),<label>(2)</label></formula><p>where i indexes training samples, S pred i</p><p>and S gt i are the prediction and groundtruth of each sample, respectively.</p><p>We propose two candidates: Chamfer distance (CD) and Earth Mover's distance (EMD) <ref type="bibr" target="#b16">[17]</ref>.</p><p>Chamfer distance We define the Chamfer distance between S 1 , S 2 ⊆ R 3 as:</p><formula xml:id="formula_3">d CD (S 1 , S 2 ) = x∈S1 min y∈S2 x − y 2 2 + y∈S2 min x∈S1 x − y 2 2<label>(3)</label></formula><p>In the strict sense, d CD is not a distance function because triangle inequality does not hold. We nevertheless use the term "distance" to refer to any non-negative function defined on point set pairs. For each point, the algorithm of CD finds the nearest neighbor in the other set and sums the squared distances up. Viewed as a function of point locations in S 1 and S 2 , CD is continuous and piecewise smooth. The range search for each point is independent, thus trivially parallelizable. Also, spatial data structures like KD-tree can be used to accelerate nearest neighbor search. Though simple, CD produces reasonable high quality results in practice.</p><p>Earth Mover's distance Consider S 1 , S 2 ⊆ R 3 of equal size s = |S 1 | = |S 2 |. The EMD between A and B is defined as:</p><formula xml:id="formula_4">d EM D (S 1 , S 2 ) = min φ:S1→S2 x∈S1 x − φ(x) 2<label>(4)</label></formula><p>where φ : S 1 → S 2 is a bijection. The EMD distance solves an optimization problem, namely, the assignment problem. For all but a zeromeasure subset of point set pairs, the optimal bijection φ is unique and invariant under infinitesimal movement of the points. Thus EMD is differentiable almost everywhere. In practice, exact computation of EMD is too expensive for deep learning, even on graphics hardware. We therefore implement a (1 + ǫ) approximation scheme given by <ref type="bibr" target="#b1">[2]</ref>. We allocate fix amount of time for each instance and incrementally adjust allowable error ratio to ensure termination. For typical inputs, the algorithm gives highly accurate results (approximation error on the magnitude of 1%). The algorithm is easily parallelizable on GPU.</p><p>Shape space Despite remarkable expressive power embedded in the deep layers, neural networks inevitably encounter uncertainty in predicting the precise geometry of an object. Such uncertainty could arise from limited network capacity, insufficient use of input resolution, or the ambiguity of groundtruth due to information loss in 3D-2D projection. Facing the inherent inability to resolve the shape precisely, neural networks tend to predict a "mean" shape averaging out the space of uncertainty. The mean shape carries the characteristics of the distance itself.</p><p>In <ref type="figure">Figure 3</ref>, we illustrate the distinct mean-shape behavior of EMD and CD on synthetic shape distributions, by minimizing E s∼S [L(x, s)] through stochastic gradient descent, where S is a given shape distribution, L is one of the distance functions.</p><p>In the first and the second case, there is a single continuously changing hidden variable, namely the radius of the circle in (a) and the location of the arc in (b). EMD roughly captures the shape corresponding to the mean value of the hidden variable. In contrast CD induces a splashy shape that blurs the shape's geometric structure. In the latter two cases, there are categorical hidden variables: which corner the square is located at (c) and whether there is a circle besides the bar (d). To address the uncertain presence of the varying part, the minimizer of CD distributes some points outside the main body at the correct locations; while the minimizer of EMD is considerably distorted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Generation of Multiple Plausible Shapes</head><p>To better model the uncertainty or inherent ambiguity (e.g. unseen parts in the single view), we need to enable the system to generate distributional output. We expect that the random variable r passed to G (see Eq (1)) would help it explore the groundtruth distribution. However, naively plugging G from Eq (1) into Loss (2) to predict S pred i won't work, as the loss minimization will nullify the randomness.</p><p>We find practically a simple and effective method for uncertainty modeling: the MoN (min of N) loss:</p><formula xml:id="formula_5">minimize Θ k min rj ∼N(0,I) 1≤j≤n {d(G(I k , r j ; Θ), S gt k )}<label>(5)</label></formula><p>By giving n chances to minimizes the distance, the network learns to spread its prediction upon receiving different random vectors. In practice, we find that setting n = 2 already enables our method to explore the groundtruth space.</p><p>In principle, to model uncertainty we should use generative frameworks like the conditional GAN (CGAN) <ref type="bibr" target="#b14">[15]</ref> or the variational auto-encoder (VAE). One key element in  these methods is a complementary network (the discriminator in GAN, or encoder in VAE) that consumes input in the target modality (the 3D point set in our case) to generate prediction or distribution parameters. However, how to feed the 3D point set to deep neural network is still an open problem at the production of this paper. Our point set representation will greatly benefit from future advances in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Training Data Generation by Synthesis</head><p>To start, we introduce our training data preparation. We take the approach of rendering 2D views from CAD object models. Our models are from the ShapeNet dataset <ref type="bibr" target="#b3">[4]</ref>, containing large volume of manually cleaned 3D object models with textures. Concretely we used a subset of 220K models covering 2,000 object categories. The use of synthesized data has been adopted in a number of existing works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref>. For each model, we normalized the radius of its bounding hemi-sphere to unit 1 and aligned their ground plane. Then each model was rendered into 2D images according to the Blinn-Phong shading formula with randomly chosen environmental maps. In our experiments we used a simple local lightening model for the sake of computation time. However, it is straight-forward to extend our method to incorporate global illumination algorithms and more complex backgrounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">3D Shape Reconstruction from RGB Images</head><p>Comparison to state-of-the-art We compare our work to 3D-R2N2 <ref type="bibr" target="#b4">[5]</ref> which is the state-of-the-art in deep learning based 3D object generation. 3D-R2N2 reconstructs 3D from single or multi-view images into a volumetric representation. To enable the comparison we re-trained our networks on the dataset used by 3D-R2N2's authors. The results are compared under three different metrics CD, EMD and IoU (intersection over union). In 3D-R2N2 only IoU values are reported, so we used the trained network provided by the authors to compute their predictions. To compute CD and EMD, their predicted and ground truth volumes are sampled by iterative farthest point sampling <ref type="bibr" target="#b6">[7]</ref> to a discrete set of points with the same cardinality as ours. We post-processed our point-set into a volumetric one with the same resolution as in 3D-R2N2 when computing IoU. Refer to supplementary for details.</p><p>In <ref type="figure" target="#fig_5">Fig 6</ref> we report the result of our network compared with the single-view 3D-R2N2. To determine the absolute scale of CD and EMD we define unit 1 as 1/10 of the length of the 3D grid used to encode the ground truth shape in 3D-R2N2's dataset. Though not directly trained by IoU, our network gives significantly better performance under all three measures.</p><p>We report the IoU value for each category as in <ref type="bibr" target="#b4">[5]</ref>. From <ref type="table">Table 1</ref>, we can see that for single view reconstruction the proposed method consistently achieves higher IoU in all categories. 3R-R2N2 is also able to predict 3D shapes from more than one views. On many categories our method even outperforms the 3D-R2N2's prediction given 5 views.</p><p>Notice that both methods learn much more than predicting the object's class. In 3D-R2N2's dataset, for example, the average CD value from a shape to its categorical mean is 1.1, much larger than the result of any method.</p><p>We visually compare reconstruction examples in <ref type="figure" target="#fig_4">Fig 5</ref>. As stated in <ref type="bibr" target="#b4">[5]</ref>, their method often misses thin features of objects (e.g. legs of furnitures). We surmise that this is due to their volumetric representation and voxel-wise loss function which unduly punishes mispositioned thin structures. In contrast, our point-cloud based objective function encourages the preservation of fine structures and makes our predictions more structurally plausible.</p><p>In our current implementation, processing one input image consumes 0.13 seconds on a laptop CPU. One interesting feature of our approach is that we can easily inject additional input information into the system. When the neural network is given RGBD input our system can be viewed as a 3D shape completion method. <ref type="figure" target="#fig_6">Fig 7  visualizes</ref> examples of the predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Injecting Additional Information</head><p>The neural network successfully guesses the missing parts of the model. By using the shape priors embedded in the object repository, the system can leverage cues of both symmetry (e.g. airplanes should have symetric sides) and functionality (tractors should have wheels). The flexible representation of point set facilitates the resolution of the object's general shape and topology. More fine-grained methods that directly exploit local geometric cues could be cascaded after our predictions to enrich higher frequency details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Predicting Multiple Plausible Shapes</head><p>The randomness in our network enables prediction of different shapes given the same input image. To show this, we take the RGB image as the input. During training we handle randomness by using either the Mo2 or the VAE method. At test time when the ground truth is unknown, the random numbers are sampled from the predefined distribution. <ref type="figure" target="#fig_7">Fig 8 plots</ref> examples of the set of predictions of our method. The network is able to reveal its uncertainty about the shape or the ambiguity in the input. Points that the neural network is certain about its position moves little between different predictions. Along the direction of ambiguity (e.g. the thickness of the penguin's body) the variation is significantly larger. In this figure we trained our network with Mo2 and Chamfer Distance. Other combinations of settings and methods give qualitatively similar results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Network Design Analysis</head><p>Effect of combining deconv and fc branches for reconstruction We compared different designs of the neural network architectures. The performance values are reported based on our own rendered training set.As shown in <ref type="figure" target="#fig_0">Fig 12,   Figure 10</ref>. Visualization of points predicted by the deconvolution branch (blue) versus the fully connected branch (red).</p><p>the introduction of deconvolution significantly improves performance.</p><p>We further visualize the output of the deconv branch and fully connected branch separately to gain a better understanding of their functions. <ref type="figure" target="#fig_8">In Fig 9</ref> the values in the x, y and z channels are plotted as 2D images for one of the models. In the deconv branch the network learns to use the convolution structure to constructs a 2D surface that warps around the object. In the fully connected branch the output is less organized as the channels are not ordered.</p><p>In <ref type="figure" target="#fig_0">Fig 10</ref> we render the two set of predictions in 3D space. The deconv branch is in general good at capturing the "main body" of the object, while the fully connected branch complements the shape with more detailed components (e.g. tip of gun, tail of plane, arms of a sofa). This reveals the complementarity of the two branches. The predefined weights sharing and node connectivity endow the deconv branch with higher efficiency when they are congruent with the desired output's structure. The fully connected branch is more flexible but the independent control of each point consumes more network capacity.</p><p>Analysis of distance metrics Different choices of the loss functions have distinct effect on the network's prediction pattern. <ref type="figure" target="#fig_0">Fig 13 exemplifies</ref> the difference between two networks trained by CD and EMD correspondingly. The network trained by CD tends to scatter a few points in its uncertain area (e.g. behind the door) but is able to better preserve the detailed shape of the grip. In contrast, the network trained by EMD produces more compact results but sometimes overly shrinks local structures. This is in line with experiment on synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">More results and application to real world data</head><p>Fig 11 lists more example predictions on both synthetic data and real world photos. These real world photos are acquired from a viewpoint and distance that resemble the setting we used for our synthetic data. A segmentation mask is also needed to indicate the scope of the object.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>The major difficulties we faced in generating 3D point cloud, namely how to represent unordered data and how to handle ambiguity are universal in machine learning. We hope our demonstration of single image based 3D reconstruction can help motivate further advances in these two realms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. A 3D point cloud of the complete object can be reconstructed from a single image. Each point is visualized as a small sphere. The reconstruction is viewed at two viewpoints (0 • and 90 • along azimuth). A segmentation mask is used to indicate the scope of the object in the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. PointOutNet structure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Figure 3. Mean-shape behavior of EMD and CD. The shape distributions are (a) a circle with varying radius; (b) a spiky arc moving along the diagonal; (c) a rectangle bar, with a squareshaped attachment allocated randomly on one of the four corners; (d) a bar, with a circular disk appearing next to it with probability 0.5. The red dots plot the mean shape calculated according to EMD and CD accordingly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Visual comparison to 3D-R2N2. Our method better preserves thin structures of the objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Quantitative comparison to 3D-R2N2. (a) Point-set based metrics CD and EMD. (b) Volumetric representation based metric 1 -IoU. Lower bars indicate smaller errors. Our method gives better results on all three metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Shape completion from a single RGBD image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Multiple predictions for a single input image. The point sets are visualized from different view points for each object to better reveal the differences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Visualization of the channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Visualization of predictions on synthetic and real world data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Comparison of different networks by Chamfer Distance (CD) and Earth Mover Distance (EMD). More complex network gives better results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Comparison of predictions of networks trained by CD (blue, on the left) and EMD (green, on the right).</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The project receives support from NSF grant IIS-1528025, the Stanford AI Lab-Toyota Center for Artificial Intelligence Research, a Samsung GRO grant and a Google Focused Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shape from texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="345" to="360" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A distributed asynchronous relaxation algorithm for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1985" />
			<biblScope unit="page" from="1703" to="1704" />
		</imprint>
	</monogr>
	<note>Decision and Control</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lifting object detection datasets into 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1342" to="1355" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">ShapeNet: An Information-Rich 3D Model Repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
	<note>cs.GR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00449</idno>
		<title level="m">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The farthest point strategy for progressive image sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Porat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Zeevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1305" to="1315" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Data-driven 3D primitives for single image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rendón-Mancha. Visual simultaneous localization and mapping: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fuentes-Pacheco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruiz-Ascencio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="81" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The structure-from-motion reconstruction pipeline-a survey with focus on short image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Häming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Kybernetika</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="926" to="937" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic photo pop-up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="577" to="584" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Obtaining shape from shading information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shape from shading</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="1989" />
			<biblScope unit="page" from="123" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Single-view reconstruction via joint analysis of image and shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">87</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Categoryspecific object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.00662</idno>
		<title level="m">Unsupervised learning of 3d structure from images</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The earth mover&apos;s distance as a metric for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="121" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="824" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimating image depth using shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
