<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader>
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lip Reading Sentences in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
							<email>andrewsenior@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
							<email>vinyals@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Lip Reading Sentences in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract xml:lang="en">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text>
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Lip reading, the ability to recognize what is being said from visual information alone, is an impressive skill, and very challenging for a novice. It is inherently ambiguous at the word level due to homophemes -different characters that produce exactly the same lip sequence (e.g. 'p' and 'b'). However, such ambiguities can be resolved to an extent using the context of neighboring words in a sentence, and/or a language model.</p><p>A machine that can lip read opens up a host of applications: 'dictating' instructions or messages to a phone in a noisy environment; transcribing and re-dubbing archival silent films; resolving multi-talker simultaneous speech; and, improving the performance of automated speech recogition in general.</p><p>That such automation is now possible is due to two developments that are well known across computer vision tasks: the use of deep neural network models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>; and, the availability of a large scale dataset for training <ref type="bibr" target="#b30">[31]</ref>. In this case the model is based on the recent sequence-tosequence (encoder-decoder with attention) translater architectures that have been developed for speech recognition and machine translation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">34]</ref>. The dataset developed in this paper is based on thousands of hours of BBC television broadcasts that have talking faces together with subtitles of what is being said.</p><p>We also investigate how lip reading can contribute to audio based speech recognition. There is a large literature on this contribution, particularly in noisy environments, as well as the converse where some derived measure of audio can contribute to lip reading for the deaf or hard of hearing. To investigate this aspect we train a model to recognize characters from both audio and visual input, and then systematically disturb the audio channel or remove the visual channel.</p><p>Our model (Section 2) outputs at the character level, is able to learn a language model, and has a novel dual attention mechanism that can operate over visual input only, audio input only, or both. We show (Section 3) that training can be accelerated by a form of curriculum learning. We also describe (Section 4) the generation and statistics of a new large scale Lip Reading Sentences (LRS) dataset, based on BBC broadcasts containing talking faces together with subtitles of what is said. The broadcasts contain faces 'in the wild' with a significant variety of pose, expressions, lighting, backgrounds, and ethnic origin.</p><p>The performance of the model is assessed on a test set of the LRS dataset, as well as on public benchmarks datasets for lip reading including LRW <ref type="bibr" target="#b8">[9]</ref> and GRID <ref type="bibr" target="#b10">[11]</ref>. We demonstrate open world (unconstrained sentences) lip reading on the LRS dataset, and in all cases on public benchmarks the performance exceeds that of prior work.</p><p>works (CNNs) to predict phonemes <ref type="bibr" target="#b26">[27]</ref> or visemes <ref type="bibr" target="#b20">[21]</ref> from still images, as opposed recognising to full words or sentences. A phoneme is the smallest distinguishable unit of sound that collectively make up a spoken word; a viseme is its visual equivalent.</p><p>For recognising full words, Petridis et al. <ref type="bibr" target="#b29">[30]</ref> trains an LSTM classifier on a discrete cosine transform (DCT) and deep bottleneck features (DBF). Similarly, Wand et al. <ref type="bibr" target="#b37">[38]</ref> uses an LSTM with HOG input features to recognise short phrases. The shortage of training data in lip reading presumably contributes to the continued use of shallow features. Existing datasets consist of videos with only a small number of subjects, and also a very limited vocabulary (&lt;60 words), which is also an obstacle to progress. The recent paper of Chung and Zisserman <ref type="bibr" target="#b8">[9]</ref> tackles the small-lexicon problem by using faces in television broadcasts to assemble a dataset for 500 words. However, as with any word-level classification task, the setting is still distant from the realworld, given that the word boundaries must be known beforehand. A very recent work <ref type="bibr" target="#b1">[2]</ref> uses a CNN and LSTMbased network and Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b14">[15]</ref> to compute the labelling. This reports strong speaker-independent performance on the constrained grammar and 51 word vocabulary of the GRID dataset <ref type="bibr" target="#b10">[11]</ref>. However, the method, suitably modified, should be applicable to longer, more general sentences. Audio-visual speech recognition. The problems of audiovisual speech recognition (AVSR) and lip reading are closely linked. Mroueh et al. <ref type="bibr" target="#b25">[26]</ref> employs feed-forward Deep Neural Networks (DNNs) to perform phoneme classification using a large non-public audio-visual dataset. The use of HMMs together with hand-crafted or pre-trained visual features have proved popular - <ref type="bibr" target="#b35">[36]</ref> encodes input images using DBF; <ref type="bibr" target="#b13">[14]</ref> used DCT; and <ref type="bibr" target="#b27">[28]</ref> uses a CNN pre-trained to classify phonemes; all three combine these features with HMMs to classify spoken digits or isolated words. As with lip reading, there has been little attempt to develop AVSR systems that generalise to real-world settings. Speech recognition. There is a wealth of literature on speech recognition systems that utilise separate components for acoustic and language-modelling functions (e.g. hybrid DNN-HMM systems), that we will not review here. We restrict this review to methods that can be trained end-to-end.</p><p>For the most part, prior work can be divided into two types. The first type uses CTC <ref type="bibr" target="#b14">[15]</ref>, where the model typically predicts framewise labels and then looks for the optimal alignment between the framewise predictions and the output sequence. The weakness is that the output labels are not conditioned on each other.</p><p>The second type is sequence-to-sequence models <ref type="bibr" target="#b33">[34]</ref> that first read all of the input sequence before starting to predict the output sentence. A number of papers have adopted this approach for speech recognition <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, and the most related work to ours is that of Chan et al. <ref type="bibr" target="#b4">[5]</ref> which proposes an elegant sequence-to-sequence method to transcribe audio signal to characters. They utilise a number of the latest sequence learning tricks such as scheduled sampling <ref type="bibr" target="#b3">[4]</ref> and attention <ref type="bibr" target="#b7">[8]</ref>; we take many inspirations from this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Architecture</head><p>In this section, we describe the Watch, Listen, Attend and Spell network that learns to predict characters in sentences being spoken from a video of a talking face, with or without audio.</p><p>We model each character y i in the output character sequence y = (y 1 , y 2 , ..., y l ) as a conditional distribution of the previous characters y &lt;i , the input image sequence . Hence, we model the output probability distribution as:</p><formula xml:id="formula_0">x v = (x v 1 , x v 2 , ..., x</formula><formula xml:id="formula_1">P (y|x v , x a ) = i P (y i |x v , x a , y &lt;i )<label>(1)</label></formula><p>Our model, which is summarised in <ref type="figure">Figure 1</ref>, consists of three key components: the image encoder Watch (Section 2.1), the audio encoder Listen (Section 2.2), and the character decoder Spell (Section 2.3). Each encoder transforms the respective input sequence into a fixeddimensional state vector s, and sequences of encoder outputs o = (o 1 , ..., o p ), p ∈ (n, m); the decoder ingests the state and the attention vectors from both encoders and produces a probability distribution over the output character sequence.</p><formula xml:id="formula_2">s v , o v = Watch(x v ) (2) s a , o a = Listen(x a ) (3) P (y|x v , x a ) = Spell(s v , s a , o v , o a )<label>(4)</label></formula><p>The three modules in the model are trained jointly. We describe the modules next, with implementation details given in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Watch: Image encoder</head><p>The image encoder consists of the convolutional module that generates image features f </p><formula xml:id="formula_3">f v i = CNN(x v i ) (5) h v i , o v i = LSTM(f v i , h v i+1 ) (6) s v = h v 1<label>(7)</label></formula><p>The convolutional network is based on the VGG-M model <ref type="bibr" target="#b5">[6]</ref>, as it is memory-efficient, fast to train and has a decent classification performance on ImageNet <ref type="bibr" target="#b30">[31]</ref>. The</p><formula xml:id="formula_4">LSTM LSTM LSTM LSTM LSTM LSTM fc6</formula><p>. . .        ConvNet layer configuration is shown in <ref type="figure">Figure 2</ref>, and is abbreviated as conv1 · · · fc6 in the main network diagram.</p><p>The encoder LSTM network consumes the output features f v i produced by the ConvNet at every input timestep, and generates a fixed-dimensional state vector s v . In addition, it produces an output vector o v i at every timestep i. Note that the network ingests the inputs in reverse time order (as in <ref type="bibr">Equation 6</ref>), which has shown to improve results in <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Listen: Audio encoder</head><p>The Listen module is an LSTM encoder similar to the Watch module, without the convolutional part. The LSTM directly ingests 13-dimensional MFCC features in reverse time order, and produces the state vector s a and the output vectors o a .</p><formula xml:id="formula_5">h a j , o a j = LSTM(x a j , h a j+1 )<label>(8)</label></formula><formula xml:id="formula_6">s a = h a 1<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Spell: Character decoder</head><p>The Spell module is based on a LSTM transducer <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8]</ref>, here we add a dual attention mechanism. At every output step k, the decoder LSTM produces the decoder states h The inner working of the attention mechanisms is described in <ref type="bibr" target="#b2">[3]</ref>, and repeated in the supplementary material. We use two independent attention mechanisms for the lip and the audio input streams to refer to the asynchronous inputs with different sampling rates. The attention vectors are fused with the output states (Equations 11 and 12) to produce the context vectors c v k and c a k that encapsulate the information required to produce the next step output. The probability distribution of the output character is generated by an MLP with softmax over the output.</p><formula xml:id="formula_7">h d k , o d k = LSTM(h d k−1 , y k−1 , c v k−1 , c a k−1 ) (10) c v k = o v · Attention v (h d k , o v ) (11) c a k = o a · Attention a (h d k , o a )<label>(12)</label></formula><formula xml:id="formula_8">P (y i |x v , x a , y &lt;i ) = softmax(MLP(o d k , c v k , c a k ))<label>(13)</label></formula><p>At k = 1, the final encoder states s l and s a are used as the input instead of the previous decoder state -i.e. h In our experiments, we have observed that the attention mechanism is absolutely critical for the audiovisual speech recognition system to work. Without attention, the model appears to 'forget' the input signal, and produces an output sequence that correlates very little to the input, beyond the first word or two (which the model gets correct, as these are the last words to be seen by the encoder). The attention-less model yields Word Error Rates over 100%, so we do not report these results.</p><p>The dual-attention mechanism allows the model to extract information from both audio and video inputs, even when one stream is absent, or the two streams are not timealigned. The benefits are clear in the experiments with noisy or no audio (Section 5).</p><p>Bidirectional LSTMs have been used in many sequence learning tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17]</ref> for their ability to produce outputs conditioned on future context as well as past context. We have tried replacing the unidirectional encoders in the Watch and Listen modules with bidirectional encoders, however these networks took significantly longer to train, whilst providing no obvious performance improvement. This is presumably because the Decoder module is anyway conditioned on the full input sequence, so bidirectional encoders are not necessary for providing context, and the attention mechanism suffices to provide the additional local focus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Training strategy</head><p>In this section, we describe the strategy used to effectively train the Watch, Listen, Attend and Spell network, making best use of the limited amount of data available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Curriculum learning</head><p>Our baseline strategy is to train the model from scratch, using the full sentences from the 'Lip Reading Sentences' dataset -previous works in speech recognition have taken this approach. However, as <ref type="bibr" target="#b4">[5]</ref> reports, the LSTM network converges very slowly when the number of timesteps is large, because the decoder initially has a hard time extracting the relevant information from all the input steps.</p><p>We introduce a new strategy where we start training only on single word examples, and then let the sequence length grow as the network trains. These short sequences are parts of the longer sentences in the dataset. We observe that the rate of convergence on the training set is several times faster, and it also significantly reduces overfitting, presumably because it works as a natural way of augmenting the data. The test performance improves by a large margin, reported in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Scheduled sampling</head><p>When training a recurrent neural network, one typically uses the previous time step ground truth as the next time step input, which helps the model learn a kind of language model over target tokens. However during inference, the previous step ground-truth is unavailable, resulting in poorer performance because the model was not trained to be tolerant to feeding in bad predictions at some time steps. We use the scheduled sampling method of Bengio et al. <ref type="bibr" target="#b3">[4]</ref> to bridge this discrepancy between how the model is used at training and inference. At train time, we randomly sample from the previous output, instead of always using the ground-truth. When training on shorter sub-sequences, ground-truth previous characters are used. When training on full sentences, the sampling probability from the previous output was increased in steps from 0 to 0.25 over time. We were not able to achieve stable learning at sampling probabilities of greater than 0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-modal training</head><p>Networks with multi-modal inputs can often be dominated by one of the modes <ref type="bibr" target="#b12">[13]</ref>. In our case we observe that the audio signal dominates, because speech recognition is a significantly easier problem than lip reading. To help prevent this from happening, one of the following input types is uniformly selected at train time for each example: (1) audio only; (2) lips only; (3) audio and lips.</p><p>If mode <ref type="formula" target="#formula_1">(1)</ref> is selected, the audio-only data described in Section 4.1 is used. Otherwise, the standard audio-visual data is used.</p><p>We have over 300,000 sentences in the recorded data, but only around 100,000 have corresponding facetracks. In machine translation, it has been shown that monolingual dummy data can be used to help improve the performance of a translation model <ref type="bibr" target="#b31">[32]</ref>. By similar rationale, we use the sentences without facetracks as supplementary training data to boost audio recognition performance and to build a richer language model to help improve generalisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training with noisy audio</head><p>The WLAS model is initially trained with clean input audio for faster convergence. To improve the model's tolerance to audio noise, we apply additive white Gaussian noise with SNR of 10dB (10:1 ratio of the signal power to the noise power) and 0dB (1:1 ratio) later in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation details</head><p>The input images are 120×120 in dimension, and are sampled at 25Hz. The image only covers the lip region of the face, as shown in <ref type="figure">Figure 3</ref>. The ConvNet ingests 5-frame sliding windows using the Early Fusion method of <ref type="bibr" target="#b8">[9]</ref>, moving 1-frame at a time. The MFCC features are calculated over 25ms windows and at 100Hz, with a timestride of 1. For Watch and Listen modules, we use a three layer LSTM with cell size of 256. For the Spell module, we use a three layer LSTM with cell size of 512. The output size of the network is 45, for every character in the alphabet, numbers, common punctuations, and tokens for Our implementation is based on the TensorFlow library <ref type="bibr" target="#b0">[1]</ref> and trained on a GeForce Titan X GPU with 12GB memory. The network is trained using stochastic gradient descent with a batch size of 64 and with dropout and label smoothing. The layer weights of the convolutional layers are initialised from the visual stream of <ref type="bibr" target="#b9">[10]</ref>. All other weights are randomly initialised.</p><p>An initial learning rate of 0.1 was used, and decreased by 10% every time the training error did not improve for 2,000 iterations. Training on the full sentence data was stopped when the validation error did not improve for 5,000 iterations. The model was trained for around 500,000 iterations, which took approximately 10 days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dataset</head><p>In this section, we describe the multi-stage pipeline for automatically generating a large-scale dataset for audiovisual speech recognition. Using this pipeline, we have been able to collect thousands of hours of spoken sentences and phrases along with the corresponding facetrack. We use a variety of BBC programs recorded between 2010 and 2016, listed in <ref type="table">Table 1</ref>, and shown in <ref type="figure">Figure 3</ref>.</p><p>The selection of programs are deliberately similar to those used by <ref type="bibr" target="#b8">[9]</ref> for two reasons: (1) a wide range of speakers appear in the news and the debate programs, unlike dramas with a fixed cast; (2) shot changes are less frequent, therefore there are more full sentences with continuous facetracks.</p><p>The processing pipeline is summarised in <ref type="figure">Figure 4</ref>. Most of the steps are based on the methods described in <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b9">[10]</ref>, but we give a brief sketch of the method here. Video preparation.</p><p>First, shot boundaries are de- tected by comparing colour histograms across consecutive frames <ref type="bibr" target="#b23">[24]</ref>. The HOG-based face detection <ref type="bibr" target="#b19">[20]</ref> is then performed on every frame of the video. The face detections of the same person are grouped across frames using a KLT tracker <ref type="bibr" target="#b36">[37]</ref>. Facial landmarks are extracted from a sparse subset of pixel intensities using an ensemble of regression trees <ref type="bibr" target="#b18">[19]</ref>.</p><p>Audio and text preparation. The subtitles in BBC videos are not broadcast in sync with the audio. The Penn Phonetics Lab Forced Aligner <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">39]</ref> is used to force-align the subtitle to the audio signal. Errors exist in the alignment as the transcript is not verbatim -therefore the aligned labels are filtered by checking against the commercial IBM Watson Speech to Text service. AV sync and speaker detection. In BBC videos, the audio and the video streams can be out of sync by up to around one second, which can cause problems when the facetrack corresponding to a sentence is being extracted. The twostream network described in <ref type="bibr" target="#b9">[10]</ref> is used to synchronise the two streams. The same network is also used to determine who is speaking in the video, and reject the clip if it is a voice-over. Sentence extraction. The videos are divided into invididual sentences/ phrases using the punctuations in the transcript. The sentences are separated by full stops, commas and question marks; and are clipped to 100 characters or 10 seconds, due to GPU memory constraints. We do not impose any restrictions on the vocabulary size.</p><p>The training, validation and test sets are divided according to broadcast date, and the dates of videos corresponding to each set are shown in <ref type="table" target="#tab_3">Table 2</ref>. The dataset contains thousands of different speakers which enables the model to be speaker agnostic. <ref type="table" target="#tab_4">Table 3</ref> compares the 'Lip Reading Sentences' (LRS) dataset to the largest existing public datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Audio-only data</head><p>In addition to the audio-visual dataset, we prepare an auxiliary audio-only training dataset. These are the sentences in the BBC programs for which facetracks are not available. The use of this data is described in Section 3.3. It is only used for training, not for testing.     <ref type="table">Table 4</ref>. Statistics of the Audio-only training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section we evaluate and compare the proposed architecture and training strategies. We also compare our method to the state of the art on public benchmark datasets.</p><p>To clarify which of the modalities are being used, we call the models in lips-only and audio-only experiments Watch, Attend and Spell (WAS), Listen, Attend and Spell (LAS) respectively. These are the same Watch, Listen, Attend and Spell model with either of the inputs disconnected and replaced with all-zeros.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation.</head><p>The models are trained on the LRS dataset (the train/val partition) and the Audio-only training dataset (Section 4). The inference and evaluation procedures are described below. Beam search. Decoding is performed with beam search of width 4, in a similar manner to <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34]</ref>. At each timestep, the hypotheses in the beam are expanded with every possible character, and only the 4 most probable hypotheses are stored. <ref type="figure" target="#fig_13">Figure 5</ref> shows the effect of increasing the beam width -there is no observed benefit for increasing the width beyond 4. Evaluation protocol. The models are evaluated on an independent test set (Section 4). For all experiments, we report the Character Error Rate (CER), the Word Error Rate (WER) and the BLEU metric. CER and WER are defined as ErrorRate = (S + D + I)/N , where S is the number of substitutions, D is the number of deletions, I is the number of insertions to get from the reference to the hypothesis, and N is the number of words in the reference. BLEU <ref type="bibr" target="#b28">[29]</ref> is a modified form of n-gram precision to compare a candidate sentence to one or more reference sentences. Here, we use the unigram BLEU.  Results. All of the training methods discussed in Section 3 contribute to improving the performance. A breakdown of this is given in <ref type="table" target="#tab_6">Table 5</ref> for the lips-only experiment. For all other experiments, we only report results obtained using the best strategy. Lips-only examples. The model learns to correctly predict extremely complex unseen sentences from a wide range of content -examples are shown in  <ref type="table" target="#tab_7">Table 6</ref>. Examples of unseen sentences that WAS correctly predicts (lips only).</p><p>Audio-visual examples. As we hypothesised, the results in <ref type="table" target="#tab_6">Table 5</ref> demonstrate that the mouth movements provide important cues in speech recognition when the audio signal is noisy; and also give an improvement in performance even when the audio signal is clean -the character error rate is reduced from 16.2% for audio only to 13.3% for audio together lip reading. <ref type="table" target="#tab_9">Table 7</ref> shows some of the many examples where the WLAS model fails to predict the correct sentence from the lips or the audio alone, but successfully deciphers the words when both streams are present.  Attention visualisation. The attention mechanism generates explicit alignment between the input video frames (or the audio signal) and the hypothesised character output. <ref type="figure" target="#fig_14">Figure 6</ref> visualises the alignment of the characters "Good afternoon and welcome to the BBC News at One" and the corresponding video frames. This result is better shown as a video; please see supplementary materials. Decoding speed. The decoding happens significantly faster than real-time. The model takes approximately 0.5 seconds to read and decode a 5-second sentence when using a beam width of 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Human experiment</head><p>In order to compare the performance of our model to what a human can achieve, we instructed a professional lip reading company to decipher a random sample of 200 videos from our test set. The lip reader has around 10 years of professional experience and deciphered videos in a range of settings, e.g. forensic lip reading for use in court, the royal wedding, etc.</p><p>The lip reader was allowed to see the full face (the whole picture in the bottom two rows of <ref type="figure">Figure 3</ref>), but not the background, in order to prevent them from reading subtitles or guessing the words from the video content. However, they were informed which program the video comes from, and were allowed to look at some videos from the training set with ground truth.</p><p>The lip reader was given 10 times the video duration to predict the words being spoken, and within that time, they were allowed to watch the video as many times as they wished. Each of the test sentences was up to 100 characters in length.</p><p>We observed that the professional lip reader is able to correctly decipher less than one-quarter of the spoken words <ref type="table" target="#tab_6">(Table 5</ref>). This is consistent with previous studies on the accuracy of human lip reading <ref type="bibr" target="#b24">[25]</ref>. In contrast, the WAS model (lips only) is able to decipher half of the spoken words. Thus, this is significantly better than professional lip readers can achieve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">LRW dataset</head><p>The 'Lip Reading in the Wild' (LRW) dataset consists of up to 1000 utterances of 500 isolated words from BBC television, spoken by over a thousand different speakers. Evaluation protocol. The train, validation and test splits are provided with the dataset. We give word error rates. Results. The network is fine-tuned for one epoch to classify only the 500 word classes of this dataset's lexicon. As shown in <ref type="table">Table 8</ref>, our result exceeds the current state-ofthe-art on this dataset by a large margin.  <ref type="table">Table 8</ref>. Word error rates on external lip reading datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">GRID dataset</head><p>The GRID dataset <ref type="bibr" target="#b10">[11]</ref> consists of 34 subjects, each uttering 1000 phrases. The utterances are single-syntax multi-word sequences of verb (4) + color (4) + preposition (4) + alphabet (25) + digit (10) + adverb (4) ; e.g. 'put blue at A 1 now'. The total vocabulary size is 51, but the number of possibilities at any given point in the output is effectively constrained to the numbers in the brackets above. The videos are recorded in a controlled lab environment, shown in <ref type="figure" target="#fig_15">Figure 7</ref>. Evaluation protocol. The evaluation follows the standard protocol of <ref type="bibr" target="#b37">[38]</ref> and <ref type="bibr" target="#b1">[2]</ref> -the data is randomly divided into train, validation and test sets, where the latter contains 255 utterances for each speaker. We report the word error rates. Some of the previous works report word accuracies, which is defined as (WAcc = 1 − WER). Results. The network is fine-tuned for one epoch on the GRID dataset training set. As can be seen in <ref type="table">Table 8</ref>, our method achieves a strong performance of 3.0% (WER), that substantially exceeds the current state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Summary and extensions</head><p>In this paper, we have introduced the 'Watch, Listen, Attend and Spell' network model that can transcribe speech into characters. The model utilises a novel dual attention mechanism that can operate over visual input only, audio input only, or both. Using this architecture, we demonstrate lip reading performance that beats a professional lip reader on videos from BBC television. The model also surpasses the performance of all previous work on standard lip reading benchmark datasets, and we also demonstrate that visual information helps to improve speech recognition performance even when the audio is used.</p><p>There are several interesting extensions to consider: first, the attention mechanism that provides the alignment is unconstrained, but in practice should always move monotonically from left to right. This monotonicity could be incorporated as a soft or hard constraint; second, the sequence-to-sequence model is used in batch modedecoding a sentence given the entire corresponding lip sequence. Instead, a more on-line architecture could be used, where the decoder does not have access to the part of the lip sequence in the future; finally, it is possible that research of this type could discern important discriminative cues that are beneficial for teaching lip reading to the hard of hearing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and the recurrent module that produces the fixed- dimensional state vector s v and a set of output vectors o v .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 1 .Figure 2 .</head><label>12</label><figDesc>Figure 1. Watch, Listen, Attend and Spell architecture. At each time step, the decoder outputs a character yi, as well as two attention vectors. The attention vectors are used to select the appropriate period of the input visual and audio sequences. input (120x120) conv1 conv2 conv3 conv4 conv5 fc6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>vectors are generated from the attention mecha- nisms Attention v and Attention a .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>the absence of the previous state or context. Discussion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>[sos], [eos], [pad]. The full list is given in the sup- plementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Figure 3. Top: Original still images from the BBC lip reading dataset -News, Question Time, Breakfast, Newsnight (from left to right). Bottom: The mouth motions for 'afternoon' from two different speakers. The network sees the areas inside the red squares.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The effect of beam width on Word Error Rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Alignment between the video frames and the character output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Still images from the GRID dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 .</head><label>2</label><figDesc>The</figDesc><table>Lip Reading Sentences (LRS) audio-visual 
dataset. Division of training, validation and test data; and the 
number of utterances and vocabulary size of each partition. Of 
the 6,882 words in the test set, 6,253 are in the training or the 
validation sets; 6,641 are in the audio-only training data. Utter: 
Utterances 

Name 
Type 
Vocab 
# Utter. # Words 
GRID [11] 
Sent. 
51 
33,000 
165,000 
MODALITY [12] Sent. 
182 
5,880 
8,085 
LRW [9] 
Words 
500 450,000 
450,000 
LRS 
Sent. 
17,428 
118,116 
807,375 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Comparison to existing large-scale lip reading datasets.</figDesc><table>Set 
Dates 
# Utter. 
Vocab 

Train 01/2010 -12/2015 
342,644 
25,684 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>Performance on the LRS test set. WAS: Watch, Attend 
and Spell; LAS: Listen, Attend and Spell; WLAS: Watch, Listen, 
Attend and Spell; CL: Curriculum Learning; SS: Scheduled Sam-
pling; BS: Beam Search.  †Unigram BLEU with brevity penalty. 
 ‡Excluding samples that the lip reader declined to annotate. In-
cluding these, the CER rises to 78.9% and the WER to 87.6%. 
⋆ The Kaldi SGMM+MMI model used here achieves a WER of 
3.6% on the WSJ (eval92) test set, which is within 0.2% of the 
current state-of-the-art. The acoustic and language models have 
been re-trained on our dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table>MANY MORE PEOPLE WHO WERE INVOLVED IN THE 
ATTACKS 
CLOSE TO THE EUROPEAN COMMISSION'S MAIN 
BUILDING 
WEST WALES AND THE SOUTH WEST AS WELL AS 
WESTERN SCOTLAND 
WE KNOW THERE WILL BE HUNDREDS OF JOURNAL-
ISTS HERE AS WELL 
ACCORDING TO PROVISIONAL FIGURES FROM THE 
ELECTORAL COMMISSION 
THAT'S THE LOWEST FIGURE FOR EIGHT YEARS 
MANCHESTER FOOTBALL CORRESPONDENT FOR 
THE DAILY MIRROR 
LAYING THE GROUNDS FOR A POSSIBLE SECOND 
REFERENDUM 
ACCORDING TO THE LATEST FIGURES FROM THE OF-
FICE FOR NATIONAL STATISTICS 
IT COMES AFTER A DAMNING REPORT BY THE 
HEALTH WATCHDOG 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 7 .</head><label>7</label><figDesc>Examples of AVSR results. GT: Ground Truth; A: Audio only (10dB SNR); L: Lips only; AV: Audio-visual.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. Funding for this research is provided by the EPSRC Programme Grant Seebibyte EP/M013774/1. We are very grateful to Rob Cooper and Matt Haynes at BBC Research for help in obtaining the dataset at Oxford.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01599</idno>
		<title level="m">Lipnet: Sentence-level lipreading</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01211</idno>
		<title level="m">Listen, attend and spell</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Endto-end continuous speech recognition using attention-based recurrent nn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1602</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">first results. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Out of time: automated lip sync in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Multi-view Lip-reading, ACCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An audio-visual corpus for speech perception and automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2421" to="2424" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An audio-visual corpus for multimodal automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Czyzewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kostek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bratoszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kotus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szykulski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Audio-visual speech recognition incorporating facial depth information captured by the kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Galatas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Makedon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing Conference (EUSIPCO), 2012 Proceedings of the 20th European</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2714" to="2717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Perceptual linear predictive (PLP) analysis of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">the Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1738" to="1752" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit. The Journal of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning of mouth shapes for sign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="85" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bowden. Comparing visual features for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-J</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Auditory-Visual Speech Processing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="102" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reliable transition detection in videos: A survey and practitioner&apos;s guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Image and Graphics</title>
		<imprint>
			<date type="published" when="2001-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The Oxford handbook of deaf studies, language, and education</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marschark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Spencer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Oxford University Press</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep multimodal learning for audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2130" to="2134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lipreading using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Noda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakadai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Okuno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1149" to="1153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Audio-visual speech recognition using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Noda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakadai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Okuno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="722" to="737" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep complementary bottleneck features for visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2304" to="2308" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06709</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Audio-visual speech recognition using deep bottleneck features and high-performance lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ninomiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kitaoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osuga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Iribe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hayamizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (AP-SIPA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="575" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Selecting and tracking features for image sequence analysis. Robotics and Automation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Lipreading with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="6115" to="6119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Speaker identification on the scotus corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3878</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A review of recent advances in visual speech decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="590" to="605" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
