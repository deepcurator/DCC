<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader>
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:24+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Improved Deep Learning Architecture for Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ejaz</forename><surname>Ahmed</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Mitsubishi Electric Research Labs</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<addrLine>3364 A.V. Williams</addrLine>
									<postCode>20740, 201, 02139</postCode>
									<settlement>College Park, Broadway, Cambridge</settlement>
									<region>MD, MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
							<email>mjones@merl.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Mitsubishi Electric Research Labs</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<addrLine>3364 A.V. Williams</addrLine>
									<postCode>20740, 201, 02139</postCode>
									<settlement>College Park, Broadway, Cambridge</settlement>
									<region>MD, MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
							<email>tmarks@merl.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Mitsubishi Electric Research Labs</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<addrLine>3364 A.V. Williams</addrLine>
									<postCode>20740, 201, 02139</postCode>
									<settlement>College Park, Broadway, Cambridge</settlement>
									<region>MD, MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Improved Deep Learning Architecture for Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract xml:lang="en">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In this work, we propose a method for simultaneously learning features and a corresponding similarity metric for person re-identification. We present a deep convolutional architecture with layers specially designed to address the problem of re-identification. Given a pair of images as input, our network outputs a similarity value indicating whether the two input images depict the same person. Novel elements of our architecture include a layer that computes cross-input neighborhood differences, which capture local relationships between the two input images based on midlevel features from each input image. A high-level summary of the outputs of this layer is computed by a layer of patch summary features, which are then spatially integrated in subsequent layers. Our method significantly outperforms the state of the art on both a large data set (CUHK03) and a medium-sized data set (CUHK01), and is resistant to overfitting. We also demonstrate that by initially training on an unrelated large data set before fine-tuning on a small target data set, our network can achieve results comparable to the state of the art even on a small data set (VIPeR).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text>
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification is the problem of identifying people across images that have been taken using different cameras, or across time using a single camera. Reidentification is an important capability for surveillance systems as well as human-computer interaction systems. It is an especially difficult problem, because large variations in viewpoint and lighting across different views can cause two images of the same person to look quite different and can cause images of different people to look very similar. See <ref type="figure" target="#fig_0">Figure 1</ref> for some difficult examples. The problem of reidentification is usually formulated in a similar way to face recognition. A typical re-identification system takes as input two images, each of which usually contains a person's full body, and outputs either a similarity score between the two images or a classification of the pair of images as same (if the two images depict the same person) or different (if the images are of different people). In this paper, we follow this approach and use a novel deep learning network to assign similarity scores to pairs of images of human bodies. Our network architecture includes two novel layers: a neighborhood difference layer that compares convolutional image features in each patch of one input image to the same features computed on nearby patches in the other input image, and a subsequent layer whose features summarize each patch's neighborhood differences. These novel aspects of our network lead to large improvements over the previous state of the art on the CUHK03 and CUHK01 data sets. We also show that our method is less prone to overfitting on small training sets. Results on CUHK01 and VIPeR demonstrate its effectiveness on smaller data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Overview of Previous Re-Identification Work</head><p>Typically, methods for re-identification include two components: a method for extracting features from input images, and a metric for comparing those features across images. Research on re-identification usually focuses either on finding an improved set of features ( <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>), finding an improved similarity metric for comparing features ( <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29]</ref>), or a combination of both ( <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>). The basic idea behind the search for better features is to find features that are at least partially invariant to lighting, pose, and viewpoint changes. Features that have been used include variations on color histograms <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29]</ref>, local binary patterns <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b13">14]</ref>, Gabor features <ref type="bibr" target="#b13">[14]</ref>, color names <ref type="bibr" target="#b25">[26]</ref>, and local patches <ref type="bibr" target="#b27">[28]</ref>. The basic idea behind metric learning approaches is to find a mapping from feature space to a new space in which feature vectors from same image pairs are closer than feature vectors from different image pairs. Metric learning approaches that have been applied to re-identification include Mahalanobis metric learning <ref type="bibr" target="#b11">[12]</ref>, Locally Adaptive Decision Functions <ref type="bibr" target="#b16">[17]</ref>, saliency weighted distances <ref type="bibr" target="#b19">[20]</ref>, Local Fisher Discriminant Analysis <ref type="bibr" target="#b24">[25]</ref>, Marginal Fisher Analysis <ref type="bibr" target="#b24">[25]</ref>, and attribute consistent matching <ref type="bibr" target="#b10">[11]</ref>. Our approach is to learn a deep network that simultaneously finds an effective set of features and a corresponding similarity function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep Learning for Re-Identification</head><p>To our knowledge, there have been two previous papers that also used a deep learning approach for re-identification: Yi et al. <ref type="bibr" target="#b26">[27]</ref> and Li et al. <ref type="bibr" target="#b15">[16]</ref>. In <ref type="bibr" target="#b26">[27]</ref>, a "siamese" convolutional network is presented for metric learning. Their network architecture consists of three independent convolutional networks that act on three overlapping parts of the two input images. Each part-specific network consists of two convolutional layers with max pooling, followed by a fully connected layer. The fully connected layer produces an output vector for each input image, and the two output vectors are compared using a cosine function. The cosine outputs for each of the three parts are then fused to get a final similarity score.</p><p>Li et al. <ref type="bibr" target="#b15">[16]</ref> use a different network architecture that begins with a single convolutional layer with max pooling, followed by a patch-matching layer that multiplies convolutional feature responses from the two inputs at a variety of horizontal offsets. (The response to each patch in one input image is multiplied separately by the response to every other patch sampled from the same horizontal strip in the other input image.) This is followed by a max-out grouping layer that keeps the largest response from each horizontal strip of patch-match responses, followed by another convolutional layer with max pooling, and finally a fully connected layer and softmax output.</p><p>Our architecture differs substantially from these previous approaches. Our network begins with two layers of convolution and max pooling to learn a set of features for comparing the two input images. We then use a novel layer that computes cross-input neighborhood difference features, which compare the features from one input image with the features computed in neighboring locations of the other image. This is followed by a subsequent novel layer that distills these local differences into a smaller patch summary feature. Next, we use another convolutional layer with max pooling, followed by two fully connected layers with softmax output. Along with our new layers which have learnable parameters in them, our network has three convolutional layers as compared to just two in <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b26">[27]</ref>, making our network deeper than previously presented networks for re-identification in the literature. In addition, our network introduces a more powerful way to compare the features learned in the early layers.</p><p>Our deep network's re-identification performance exceeds that of all previous approaches on both the large CUHK03 <ref type="bibr" target="#b15">[16]</ref> data set and the smaller CUHK01 <ref type="bibr" target="#b14">[15]</ref> data set. In addition, even though small data sets can make effective training of large networks difficult or impossible <ref type="bibr" target="#b15">[16]</ref>, our network performs comparably with the state of the art on the much smaller VIPeR data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Architecture</head><p>In this paper, we propose a deep neural network architecture that formulates the problem of person re-identification as binary classification. Given an input pair of images, the task is to determine whether or not the two images represent the same person. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates our network's architecture. As briefly described in the previous section, our network consists of the following distinct layers: two layers of tied convolution with max pooling, cross-input neighborhood differences, patch summary features, across-patch features, higher-order relationships, and finally a softmax function to yield the final estimate of whether the input images are of the same person or not. Each of these layers is explained in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Tied Convolution</head><p>To determine whether two input images are of the same person, we need to find relationships between the two views. In the deep learning literature, convolutional features have proven to provide representations that are useful for a variety of classification tasks. The first two layers of our network are convolution layers, which we use to compute higher-order features on each input image separately. In order for the features to be comparable across the two images in later layers, our first two layers perform tied convolution, in which weights are shared across the two views, to ensure that both views use the same filters to compute features. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, in the first convolution layer we pass input pairs of RGB images of size 60 ⇥ 160 ⇥ 3 through 20 learned filters of size 5 ⇥ 5 ⇥ 3. The resulting feature maps are passed through a max-pooling kernel that halves the width and height of features. These features are passed through another tied convolution layer that uses 25 learned filters of size 5 ⇥ 5 ⇥ 20, followed by a max-pooling layer that again decreases the width and height of the feature map by a factor of 2. At the end of these two feature computation layers, each input image is represented by 25 feature maps of size 12 ⇥ 37.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cross-Input Neighborhood Differences</head><p>The two tied convolution layers provide a set of 25 feature maps for each input image, from which we can learn relationships between the two views. Let f i and g i , respectively, represent the ith feature map (1  i  25) from the first and second views. A cross-input neighborhood differences layer computes differences in feature values across the two views around a neighborhood of each feature location, producing a set of 25 neighborhood difference maps</p><formula xml:id="formula_0">K i . Since f i , g i 2 R 12⇥37 , K i 2 R 12⇥37⇥5⇥5</formula><p>, where 5 ⇥ 5 is the size of the square neighborhood. Each K i is a 12 ⇥ 37 grid of 5 ⇥ 5 blocks, in which the block indexed by (x, y) is denoted K i (x, y) 2 R 5⇥5 , where x, y are integers (1  x  12 and 1  y  37). More precisely,</p><formula xml:id="formula_1">K i (x, y) = f i (x, y) (5, 5) N [g i (x, y)]<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">(5, 5) 2 R 5⇥5 is a 5 ⇥ 5 matrix of 1s, N [g i (x, y)] 2 R 5⇥5 is the 5 ⇥ 5 neighborhood of g i centered at (x, y).</formula><p>In words, the 5 ⇥ 5 matrix K i (x, y) is the difference of two 5 ⇥ 5 matrices, in the first of which every element is a copy of the scalar f i (x, y), and the second of which is the 5 ⇥ 5 neighborhood of g i centered at (x, y). The motivation behind taking differences in a neighborhood is to add robustness to positional differences in corresponding features of the two input images. Since the operation in <ref type="formula" target="#formula_1">(1)</ref> is asymmetric, we also consider the neighborhood difference map K 0 i , which is defined just like K i in (1) except that the roles of f i and g i are reversed. This yields 50 neighborhood difference maps,</p><formula xml:id="formula_3">{K i } 25 i=1 and {K 0 i } 25 i=1</formula><p>, each of which has size 12 ⇥ 37 ⇥ 5 ⇥ 5. We pass these neighborhood difference maps through a rectified linear unit (ReLu).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Patch Summary Features</head><p>In the previous layer, we have computed a rough relationship among features from the two input images in the form of neighborhood difference maps. A patch summary layer summarizes these neighborhood difference maps by producing a holistic representation of the differences in each 5 ⇥ 5 block. This layer performs the mapping from K 2 R 12⇥37⇥5⇥5⇥25 ! L 2 R 12⇥37⇥25 . This is accomplished by convolving K with 25 filters of size 5 ⇥ 5 ⇥ 25, with a stride of 5. By exactly matching the stride to the width of the square blocks, we ensure that the 25-dimensional feature vector at location (x, y) of L is computed only from the 25 blocks K i (x, y), i.e., from the 5 ⇥ 5 grid square (x, y) of each neighborhood difference map K i (where 1  i  25). Since these are in turn computed only from the local neighborhood of (x, y) in the feature maps f i and g i , the 25-dimensional patch summary feature vector at location (x, y) of L provides a high-level summary of the cross-input differences in the neighborhood of location (x, y). We also compute patch summary features L 0 from K 0 in the same way that we computed L from K. Note that filters for the mapping K ! L and K 0 ! L 0 are different, not tied as in the first two layers of the network. Both L and L 0 are then passed through a rectified linear unit (ReLu).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Across-Patch Features</head><p>So far we have obtained a high-level representation of differences within a local neighborhood, by computing neighborhood difference maps and then obtaining a highlevel local representation of these neighborhood difference maps. In the next layer, we learn spatial relationships across neighborhood differences. This is done by convolving L with 25 filters of size 3 ⇥ 3 ⇥ 25 with a stride of 1. The resultant features are passed through a max pooling kernel to reduce the height and width by a factor of 2. This yields 25 feature maps of size 5 ⇥ 18, which we denote M 2 R 5⇥18⇥25 . We similarly obtain across-patch features</p><formula xml:id="formula_4">M 0 from L 0 . Filters for the mapping L ! M and L 0 ! M 0</formula><p>are not tied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Higher-Order Relationships</head><p>We apply a fully connected layer after M and M 0 . This captures higher-order relationships by a) combining information from patches that are far from each other and b) combining information from M with information from M 0 . The resultant feature vector of size 500 is passed through a ReLu nonlinearity. These 500 outputs are then passed to another fully connected layer containing 2 softmax units, which represent the probability that the two images in the pair are of the same person or different people. The images labeled L3 are responses of a feature from the cross-input neighborhood differences layer (see Section 3.2). Recall from (1) that this layer computes the differences of feature maps across the two views in a neighborhood. The resultant feature difference map is then passed through a ReLu, which clips all negative responses to zero. For a positive pair, ideally the neighborhood difference map should be close to zero. Nonzero values should be small and relatively uniform across the map, mainly because the two feature maps compared are very similar. This is illustrated in the L3 map on the left in the positive pair (one of the K i maps), which has small but non-zero values distributed throughout the map. The image just to its right, which is its complement K 0 i , has values that are all zero or close to zero. For the negative pair, different regions are highlighted by f i than by g i , so K i gives a strong response to legs but zeros elsewhere, whereas K 0 i responds only to the person's torso. A similar pattern is observed in the patch summary feature for the negative pair (see Section 3.3), labeled L4. Higher-order relations across summarized neighborhood difference maps are captured in L5 (see Section 3.4). Finally, L6 shows features after the first fully connected layer (section 3.5). Notice that this feature representation of a positive pair is quite different than that of a negative pair. This top-layer feature is discriminative and can be used as input to an off-the-shelf classifier. <ref type="figure" target="#fig_4">Figure 4</ref> shows a visualization of the weights learned by the first tied convolution layer. The weights shown were learned on the CUHK03 data set. In addition to capturing some low-level texture information, several of these learned filters exhibit a strong color specialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Visualization of Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Comparison with Other Deep Architectures</head><p>In <ref type="figure" target="#fig_6">Figure 5c</ref>, we compare our presented network with other variations to gain insights into how much each of our network's novel features contributes to its overall performance. We describe some of these variations here. More details about these architectures can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Element-wise difference:</head><p>This architecture illustrates the benefit of comparing with the neighborhood in cross-image comparisons. In this architecture, we perform two layers of tied convolution followed by max pooling. We then compute a cross-input element-wise difference (rather than cross-input neighborhood differences) of the corresponding feature maps. This difference is passed through another layer of convolution followed by a fully connected layer and then softmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disparity-wise convolution:</head><p>This architecture illustrates the benefit of computing patch summary features. As in our presented network, this architecture performs two tied convolutions followed by max pooling, after which cross-input neighborhood differences are computed. But in this network, the 50 neighborhood difference maps of size R 12⇥37⇥5⇥5 , are rearranged to give 25 groups of 50 feature maps, where each feature map has size R 12⇥37 . A convolution is then applied to each of these groups. This is then passed through a fully connected layer and then softmax. Rather than explicitly summarizing neighborhood differences, this architecture instead directly learns across-patch relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Four-layer convnet:</head><p>This architecture illustrates the benefit of having a total of four convolutional layers, rather than two as in previous deep approaches to re-identification. We implemented a siamese type network similar to <ref type="bibr" target="#b26">[27]</ref>, but built the network with 4 layers of convolution rather than 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FPNN:</head><p>We also created our own implementation of FPNN <ref type="bibr" target="#b15">[16]</ref> to facilitate comparisons with their results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Training the Network</head><p>We pose the re-identification problem as binary classification. Training data consist of image pairs labeled as positive (same) and negative (different). The optimization objective is average loss over all pairs in the data set. As the data set can be quite large, in practice we use a stochastic approximation of this objective. Training data are randomly divided into mini-batches. The model performs forward propagation on the current mini-batch and computes the output and loss. Backpropagation is then used to compute the gradients on this batch, and network weights are updated. We perform stochastic gradient descent <ref type="bibr" target="#b2">[3]</ref> to perform weight updates. We start with a base learning rate of ⌘ (0) = 0.01 and gradually decrease it as the training progresses using an inverse policy:</p><formula xml:id="formula_5">⌘ (i) = ⌘ (0) (1 + · i) p</formula><p>where = 10 4 , p = 0.75, and i is the current mini-batch iteration. We use a momentum of µ = 0.9 and weight decay = 5 ⇥ 10 4 . With more passes over the training data, the model improves until it converges. We use a validation set to evaluate intermediate models and select the one that has maximum performance. See the supplementary material for performance on the validation set as a function of mini-batch iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Data Augmentation</head><p>There are not nearly as many positive pairs as negative pairs, which can lead to data imbalance and overfitting. To reduce overfitting, we artificially enlarge the data set using label-preserving transformations <ref type="bibr" target="#b12">[13]</ref>. We augment the data by performing random 2D translation, as also done in <ref type="bibr" target="#b15">[16]</ref>. For an original image of size W ⇥ H, we sample 5 images around the image center, with translation drawn from a uniform distribution in the range [ 0.05H, 0.05H]⇥ [ 0.05W, 0.05W ]. For the smallest data set (see Section 7.3), we also horizontally reflect each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Hard Negative Mining</head><p>Data augmentation increases the number of positive pairs, but the training data set is still imbalanced with many more negatives than positives. If we trained the network with this imbalanced data set, it would learn to predict every pair as negative. Therefore, we randomly downsample the negative set to get just twice as many negatives as positives (after augmentation), then train the network. The converged model thus obtained is not optimal since it has not seen all possible negatives. We use the current model to classify all of the negative pairs, and identify negatives on which the network performs worst. We retrain the fully connected (top) layer of the network using a set containing as many of these difficult negative pairs as positive pairs 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Fine-tuning</head><p>For small data sets that contain too few positives for effective training, we initialize the model by training on a large data set. After hard negative mining on the large set, the parameters of the converged model are then adapted on the new, small data set. For this new network learning, we begin stochastic gradient descent with learning rate ⌘ (0) = 0.001 (which is 1/10th the initial pre-training rate).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head><p>We implemented our architecture using the Caffe [10] deep learning framework, adapting various layers from the framework and writing our own layers that are specific to our architecture. Network training converges in roughly 12-14 hours on NVIDIA GTX780 and NVIDIA K40 GPUs.</p><p>We present a comprehensive evaluation of our approach by comparing it to the state-of-the-art methods on various data sets. The experiments are conducted with five random splits, and all of the Cumulative Matching Characteristics (CMC) curves are single-shot results. We first report results on the largest re-identification data set in the literature, CUHK03 <ref type="bibr" target="#b15">[16]</ref>. We then report results on the CUHK01 data set <ref type="bibr" target="#b14">[15]</ref>, using two distinct settings: a) 100 identities in the test set, as reported in <ref type="bibr" target="#b15">[16]</ref>, and b) 486 identities in the test set, as reported in most previous work on the CUHK01 data set. We also report results on the VIPeR data set <ref type="bibr" target="#b7">[8]</ref>. VIPeR and the 486-identities setting of CUHK01 are small data sets, making it difficult for deep networks to learn their parameters without overfitting. Because of this, <ref type="bibr" target="#b15">[16]</ref> does not report results on these two data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Experiments on CUHK03</head><p>The CUHK03 data set contains 13,164 images of 1,360 pedestrians, captured by six surveillance cameras. Each identity is observed by two disjoint camera views. On average, there are 4.8 images per identity in each view. This data set provides both manually labeled pedestrian bounding boxes and bounding boxes automatically obtained by running a pedestrian detector <ref type="bibr" target="#b5">[6]</ref>. We report results on both of these versions of the data (labeled and detected).</p><p>Following the protocol used in <ref type="bibr" target="#b15">[16]</ref>, we randomly divide 1360 identities into non-overlapping train (1160), test (100), and validation (100) sets. This yields about 26,000 positive pairs before data augmentation. We use a mini-batch size of 150 samples and train the network for 210,000 iterations. We use the validation set to design the network architecture.</p><p>We compare our method against KISSME <ref type="bibr" target="#b11">[12]</ref>, eSDC <ref type="bibr" target="#b29">[30]</ref>, SDALF <ref type="bibr" target="#b4">[5]</ref>, ITML <ref type="bibr" target="#b3">[4]</ref>, logistic distance metric learning (LDM) <ref type="bibr" target="#b8">[9]</ref>, largest margin nearest neighbor (LMNN) <ref type="bibr" target="#b23">[24]</ref>, metric learning to rank (RANK) <ref type="bibr" target="#b20">[21]</ref>, and directly using Euclidean distance to compare features. When using metric learning methods and Euclidean distance, dense color histograms and dense SIFT are used <ref type="bibr" target="#b29">[30]</ref>. We also compare against the deep network FPNN <ref type="bibr" target="#b15">[16]</ref>, which is the current state of the art on this data set. <ref type="figure" target="#fig_6">Figure 5a</ref> plots the CMC curves of all these methods on the CUHK03 labeled image data set. We outperform the previous deep learning method, FPNN, by a large margin. Our rank-1 accuracy is more than double that of the previous state of the art (54.74% vs. 20.65%). <ref type="figure" target="#fig_6">Figure 5b</ref> plots performance on the CUHK03 detected image data set. Although the performance of our method on CUHK03-detected is less than on CUHK03-labeled, mainly due to misalignment caused by the detector, our method still greatly outperforms the state of the art (44.96% vs. 19.89%). <ref type="figure" target="#fig_0">Figure 1</ref> shows some true positive, false positive, and true negative example results of our system. More qualitative results can be found in the supplementary material.</p><p>We also implemented a variety of other deep network architectures, explained in Section 5, to illustrate the benefits of various features of our architecture. We compare with these methods in <ref type="figure" target="#fig_6">Figure 5c</ref>. The top two performing     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Experiments on CUHK01</head><p>The CUHK01 data set has 971 identities, with 2 images per person in each view. We report results for two different settings of this data set: 100 test IDs, and 486 test IDs. a) 100 test IDs: In this setting, 100 identities are used for testing, with the remaining 871 identities used for training and validation. This protocol is better suited for deep learning because it uses 90% of the data for training. FPNN <ref type="bibr" target="#b15">[16]</ref> uses this setting on this data set. <ref type="figure" target="#fig_7">Figure 6a</ref> compares the performance of our network with previous methods. Our method outperforms the state of the art in this setting by a wide margin, with a rank-1 recognition rate of 65% (vs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>29.40%</head><p>by the next best method). Notice that the secondbest method on this data set is KISSME, and not the deep network FPNN. This can be attributed to a decrease in training data as compared to CUHK03, causing FPNN to overfit. In contrast, our method is able to generalize even with this smaller data set. b) 486 test IDs: Most previous papers report results on the CUHK01 data set by considering 486 identities for testing. We compare our approach against mid-level filters (mFilter) <ref type="bibr" target="#b30">[31]</ref>, saliency matching (SalMatch) <ref type="bibr" target="#b28">[29]</ref>, patch matching (PatMatch) <ref type="bibr" target="#b28">[29]</ref>, generic metric <ref type="bibr" target="#b14">[15]</ref>, ITML <ref type="bibr" target="#b3">[4]</ref>, LMNN <ref type="bibr" target="#b23">[24]</ref>, eSDC <ref type="bibr" target="#b29">[30]</ref>, SDALF <ref type="bibr" target="#b4">[5]</ref>, l2-norm, l1-norm <ref type="bibr" target="#b30">[31]</ref>, and co-occurrence model using visual word (visWord) <ref type="bibr" target="#b27">[28]</ref>. With 486 identities in the test set, only 485 identities are left for training. This leaves only 1940 positive samples for training, which makes it practically impossible for a deep architecture of reasonable size not to overfit if trained from scratch on this data. One way to solve this problem is to use a model trained on CUHK03, then test on the 486 identities of CUHK01. This is unlikely to work well since the network does not know the statistics of the test data set, and in fact, our model trained on CUHK03 and tested on CUHK01 gave rank-1 accuracy of around 6%, which is far below the state of the art. Instead, we pre-train a network on CUHK03 and adapt it for CUHK01 by finetuning (see Section 6.3) it on CUHK01 with 485 training identities (non-overlapping with the test set). The performance of the network after fine-tuning for 210K iterations increases dramatically, to a rank-1 accuracy of 40.5%. Using this model, we search for hard negatives and use them to retrain the top layer of the network (see Section 6.2). After 210K iterations, we achieve a rank-1 accuracy of 47.5%, beating the state of the art. See <ref type="figure" target="#fig_7">Figure 6b</ref> for a comparison with other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Experiments on VIPeR</head><p>The VIPeR data set contains 632 pedestrian pairs in two views, with only one image per person in each view. The testing protocol is to split the data set into half, 316 for training and 316 for testing. In addition to the methods listed in section 7.2 and 7.1, we compare our method against local Fisher discriminant analysis (LF) <ref type="bibr" target="#b22">[23]</ref>, PCCA <ref type="bibr" target="#b21">[22]</ref>, aPRDC <ref type="bibr" target="#b17">[18]</ref>, PRDC <ref type="bibr" target="#b31">[32]</ref>, eBiCov <ref type="bibr" target="#b18">[19]</ref>, LMNNR <ref type="bibr" target="#b0">[1]</ref>, PRSVM <ref type="bibr" target="#b1">[2]</ref>, and ELF <ref type="bibr" target="#b6">[7]</ref>. This data set is extremely challenging for deep network architectures for two reasons: a) there are only 316 identities for training with 1 image per person in each view, giving a total of just 316 positives, and b) the resolution of the images is lower (48 ⇥ 128 as compared to 60 ⇥ 160 for CUHK01). We train a model using the CUHK03 and CUHK01 data sets, then adapt the trained model to the VIPeR data set by fine-tuning on 316 training identities. Since the number of negatives is small for this data set (90K), hard negative mining does not improve results after fine-tuning because most of the negatives were already used during fine-tuning. <ref type="figure" target="#fig_7">Figure 6c</ref> compares performance of our approach with other methods. Our method obtains 34.81% rank-1 accuracy, beating all other methods individually, although a combination of two approaches (mFilter <ref type="bibr" target="#b30">[31]</ref> + LADF <ref type="bibr" target="#b16">[17]</ref>) performs better than ours with a rank-1 accuracy of 43.4% as reported in <ref type="bibr" target="#b30">[31]</ref>. The deepmetric-learning-based method <ref type="bibr" target="#b26">[27]</ref> also reports results on the VIPeR data set, with a lower rank-1 accuracy of 28.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Analysis of different body parts</head><p>To understand the contribution of different body regions to identification, we trained 5 different networks on different body parts, as shown in <ref type="figure" target="#fig_8">Figure 7a</ref>. The experiment was performed on the CUHK03 labeled data set, and the performance of each part is shown in <ref type="figure" target="#fig_8">Figure 7b</ref>. The part that performs best is part 1: the upper region of the body includ- ing the face. As we move down the body, the performance decreases, with legs capturing the least discriminative information. This experiment suggests a direction for future work in which different models can be trained for different parts of the body, and the scores from different part pairs can then be accumulated to reach a final decision. Such a system may be helpful in handling severe occlusions and to identify people in images that have been taken across time (e.g., sitting in one view and standing in the other).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We have presented a novel deep architecture for person re-identification. We have designed two novel layers for capturing relationships between two views: a cross-input neighborhood differences layer, and a subsequent layer that summarizes these differences. We demonstrate the effectiveness of our method by performing a comprehensive evaluation of our approach on various data sets. On the large CUHK03 data set, our method outperforms the state of the art by a huge margin. On the smaller CUHK01 data set (100 test IDs setting), whereas other deep methods overfit <ref type="bibr" target="#b15">[16]</ref>, our method is able to generalize and produce stateof-the-art results. We also show that models learned by our method on a large data set can be adapted to new, smaller data sets. We demonstrate this by evaluating our method on two small data sets. On CUHK01 (486 test IDs setting), we outperform all previous methods, and on VIPeR, our results are comparable to the state of the art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of true positives (first row), false positives (second row), and true negatives (bottom row) for our trained network on CUHK03. More results can be found in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Proposed Architecture. Paired images are passed through the network. While initial layers extract features in the two views individually, higher layers compute relationships between them. The number and size of convolutional filters that must be learned are shown. For example, in the first tied convolution layer, 5 ⇥ 5 ⇥ 3 ! 20 indicates that there are 20 convolutional features in the layer, each with a kernel size of 5 ⇥ 5 ⇥ 3. There are 2,308,147 learnable parameters in the whole network. Refer to Section 3 for more details. [Note that all of the figures in this paper are best viewed in color.]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>Figure 3 gives a visualization of feature responses at each layer (L1-L6) of the network. The left and right sides of the figure display responses to a positive (same) and negative (different) input pair, respectively. The response maps labeled L1 for the positive pair show the response of one of the 20 features after the first tied convolution layer (see Section 3.1). This feature responds strongly to bright white regions of the image, highlighting shirt regions of the person in both views. The maps labeled L1 for the negative pair show the response of a different one of the 20 first-layer features. This feature responds strongly to black regions, highlighting the shirt of the person in view 1 and the pants of the person in view 2. The label L2 indicates feature responses after the second tied convolution layer, which show a pair of feature maps f i and g i . The L2 feature shown for the positive pair captures tan and skin-color regions, giving higher responses to the legs, hands, and face of the person. Since this is a positive pair, similar parts of the image are highlighted in the two views. In contrast, the L2 feature for the negative pair activates for different portions of the image across the two views: the legs (pink shorts and pinkish skin) of the person in view 1, versus the torso (pink shirt and pinkish arms) of the person in view 2. The images labeled L3 are responses of a feature from the cross-input neighborhood differences layer (see Section 3.2). Recall from (1) that this layer computes the differences of feature maps across the two views in a neighborhood. The resultant feature difference map is then passed through a ReLu, which clips all negative responses to zero. For a positive pair, ideally the neighborhood difference map should be close to zero. Nonzero values should be small and relatively uniform across the map, mainly because the two feature maps compared are very similar. This is illustrated in the L3 map on the left in the positive pair (one of the K i maps), which has small but non-zero values distributed throughout the map. The image just to its right, which is its complement K</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of features learned by our architecture. Initial layers learn image features that are important to distinguish between a positive and a negative pair. Deeper layers learn relationships across the two views so that classification performance is maximized. For details, see Section 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of the weights learned in the first tied convolution layer. Each filter has size 5 ⇥ 5 ⇥ 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: CMC curves on CUHK03 data set: (a) and (b) compare our method with previous methods on CUHK03 labeled and detected, respectively. Rank-1 identification rates are shown in the legend next to the method name. Our method beats the state of the art by a large margin. c) Comparison of our method with our own variations of deep architectures on CUHK03 labeled. Out of the shown methods, only FPNN is previously mentioned in the literature. See section 7.1 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: CMC curves on CUHK01 and VIPeR data sets: a) CUHK01 data set with 100 test IDs: Our method outperforms the state of the art by more than a factor of 2. b) CUHK01 data set with 486 test IDs: Our method outperforms all previous methods on this data set with this protocol, as well. c) VIPeR: Our method beats all previous methods individually, although a combination of mFilter + LADF performs better than us. Note that (b) and (c) are especially challenging for deep learning methods since there are very few positive pairs. See Sections 7.2 and 7.3 for more details</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Analysis of different body parts: a) Left column shows parts 1 to 4 (from top to bottom). Right column shows full pedestrian image and part 5. b) Shows performance of different parts on the CUHK03 data set. Refer to Section 7.4 for more details.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We also tried retraining the entire network, but retraining just the top layer was more effective.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multipleshot human re-identification by mean riemannian covariance grid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Corvee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thonnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 8th IEEE International Conference on Advanced Video and Signal Based Surveillance, AVSS &apos;11</title>
		<meeting>the 2011 8th IEEE International Conference on Advanced Video and Signal Based Surveillance, AVSS &apos;11<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="179" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multipleshot person re-identification by chromatic and epitomic analyses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="898" to="903" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stochastic gradient tricks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks, Tricks of the Trade, Reloaded, Lecture Notes in Computer Science</title>
		<editor>G. Montavon, G. B. Orr, and K.-R. Müller</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">7700</biblScope>
			<biblScope unit="page" from="430" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Information-theoretic metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning, ICML &apos;07</title>
		<meeting>the 24th International Conference on Machine Learning, ICML &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2010-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Person reidentification using spatiotemporal appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gheissari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1528" to="1535" />
		</imprint>
	</monogr>
	<note>CVPR &apos;06</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Performance Evaluation for Tracking and Surveillance</title>
		<meeting><address><addrLine>Rio de Janeiro</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Is that you? metric learning approaches for face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<meeting><address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint learning for attribute-consistent person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop on Visual Surveillance and Reidentification</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Locally aligned feature transforms across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human re-identification with transferred metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning locally-adaptive decision functions for person verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Person reidentification: What features are important</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<editor>A. Fusiello, V. Murino, and R. Cucchiara</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">7583</biblScope>
			<biblScope unit="page" from="391" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bicov: a novel image representation for person re-identification and face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC&apos;12</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Saliency weighted features for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Feresti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop on Visual Surveillance and Re-identification</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Metric learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual International Conference on Machine Learning (ICML)</title>
		<meeting>the 27th annual International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pcca: A new approach for distance learning from sparse pairwise constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mignon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2666" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Local fisher discriminant analysis for pedestrian re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pedagadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Orwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Velastin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boghossian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page" from="3318" to="3325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Person reidentification using kernel-based metric learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Salient color names for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep metric learning for practical person re-identification. ICPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A novel visual word co-occurrence model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop on Visual Surveillance and Re-identification</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Person re-identification by salience matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning mid-level filters for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Person re-identification by probabilistic relative distance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;11</title>
		<meeting>the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;11<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="649" to="656" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
