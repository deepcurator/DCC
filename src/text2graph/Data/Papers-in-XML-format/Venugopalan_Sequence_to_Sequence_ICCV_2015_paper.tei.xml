<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader>
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequence to Sequence -Video to Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">International Computer Science Institute</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Lowell</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sequence to Sequence -Video to Text</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract xml:lang="en">
<div xmlns="http://www.tei-c.org/ns/1.0"> (M-VAD and  MPII-MD)<p>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text>
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Describing visual content with natural language text has recently received increased interest, especially describing images with a single sentence <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40]</ref>. Video description has so far seen less attention despite its important applications in human-robot interaction, video indexing, and describing movies for the blind. While image description handles a variable length output sequence of words, video description also has to handle a variable length input sequence of frames. Related approaches to video description have resolved variable length input by holistic video representations <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b10">11]</ref>, pooling over frames <ref type="bibr" target="#b38">[39]</ref>, or sub-sampling on a fixed number of input frames <ref type="bibr" target="#b42">[43]</ref>. In contrast, in this work we propose a sequence to sequence model which is trained end-to-end and is able to learn arbitrary temporal structure in the input sequence. Our model is sequence to sequence in a sense that it reads in frames  sequentially and outputs words sequentially.</p><p>The problem of generating descriptions in open domain videos is difficult not just due to the diverse set of objects, scenes, actions, and their attributes, but also because it is hard to determine the salient content and describe the event appropriately in context. To learn what is worth describing, our model learns from video clips and paired sentences that describe the depicted events in natural language. We use Long Short Term Memory (LSTM) networks <ref type="bibr" target="#b11">[12]</ref>, a type of recurrent neural network (RNN) that has achieved great success on similar sequence-to-sequence tasks such as speech recognition <ref type="bibr" target="#b9">[10]</ref> and machine translation <ref type="bibr" target="#b33">[34]</ref>. Due to the inherent sequential nature of videos and language, LSTMs are well-suited for generating descriptions of events in videos.</p><p>The main contribution of this work is to propose a novel model, S2VT, which learns to directly map a sequence of frames to a sequence of words. <ref type="figure" target="#fig_1">Figure 1</ref> depicts our model. A stacked LSTM first encodes the frames one by one, taking as input the output of a Convolutional Neural Network (CNN) applied to each input frame's intensity values. Once all frames are read, the model generates a sentence word by word. The encoding and decoding of the frame and word representations are learned jointly from a parallel corpus. To model the temporal aspects of activities typically shown in videos, we also compute the optical flow <ref type="bibr" target="#b1">[2]</ref> between pairs of consecutive frames. The flow images are also passed through a CNN and provided as input to the LSTM. Flow CNN models have been shown to be beneficial for activity recognition <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>To our knowledge, this is the first approach to video description that uses a general sequence to sequence model. This allows our model to (a) handle a variable number of input frames, (b) learn and use the temporal structure of the video and (c) learn a language model to generate natural, grammatical sentences. Our model is learned jointly and end-to-end, incorporating both intensity and optical flow inputs, and does not require an explicit attention model. We demonstrate that S2VT achieves state-of-the-art performance on three diverse datasets, a standard YouTube corpus (MSVD) <ref type="bibr" target="#b2">[3]</ref> and the M-VAD <ref type="bibr" target="#b36">[37]</ref> and MPII Movie Description <ref type="bibr" target="#b27">[28]</ref> datasets. Our implementation (based on the Caffe <ref type="bibr" target="#b14">[15]</ref> deep learning framework) is available on github. https://github.com/vsubhashini/caffe/ tree/recurrent/examples/s2vt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Early work on video captioning considered tagging videos with metadata <ref type="bibr" target="#b0">[1]</ref> and clustering captions and videos <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b41">42]</ref> for retrieval tasks. Several previous methods for generating sentence descriptions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b35">36]</ref> used a two stage pipeline that first identifies the semantic content (subject, verb, object) and then generates a sentence based on a template. This typically involved training individual classifiers to identify candidate objects, actions and scenes. They then use a probabilistic graphical model to combine the visual confidences with a language model in order to estimate the most likely content (subject, verb, object, scene) in the video, which is then used to generate a sentence. While this simplified the problem by detaching content generation and surface realization, it requires selecting a set of relevant objects and actions to recognize. Moreover, a template-based approach to sentence generation is insufficient to model the richness of language used in human descriptions -e.g., which attributes to use and how to combine them effectively to generate a good description. In contrast, our approach avoids the separation of content identification and sentence generation by learning to directly map videos to full human-provided sentences, learning a language model simultaneously conditioned on visual features.</p><p>Our models take inspiration from the image caption generation models in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40]</ref>. Their first step is to generate a fixed length vector representation of an image by extracting features from a CNN. The next step learns to decode this vector into a sequence of words composing the description of the image. While any RNN can be used in principle to decode the sequence, the resulting long-term dependencies can lead to inferior performance. To mitigate this issue, LSTM models have been exploited as sequence decoders, as they are more suited to learning long-range dependencies. In addition, since we are using variable-length video as input, we use LSTMs as sequence to sequence transducers, following the language translation models of <ref type="bibr" target="#b33">[34]</ref>.</p><p>In <ref type="bibr" target="#b38">[39]</ref>, LSTMs are used to generate video descriptions by pooling the representations of individual frames. Their technique extracts CNN features for frames in the video and then mean-pools the results to get a single feature vector representing the entire video. They then use an LSTM as a sequence decoder to generate a description based on this vector. A major shortcoming of this approach is that this representation completely ignores the ordering of the video frames and fails to exploit any temporal information. The approach in <ref type="bibr" target="#b7">[8]</ref> also generates video descriptions using an LSTM; however, they employ a version of the two-step approach that uses CRFs to obtain semantic tuples of activity, object, tool, and locatation and then use an LSTM to translate this tuple into a sentence. Moreover, the model in <ref type="bibr" target="#b7">[8]</ref> is applied to the limited domain of cooking videos while ours is aimed at generating descriptions for videos "in the wild".</p><p>Contemporaneous with our work, the approach in <ref type="bibr" target="#b42">[43]</ref> also addresses the limitations of <ref type="bibr" target="#b38">[39]</ref> in two ways. First, they employ a 3-D convnet model that incorporates spatiotemporal motion features. To obtain the features, they assume videos are of fixed volume (width, height, time). They extract dense trajectory features (HoG, HoF, MBH) <ref type="bibr" target="#b40">[41]</ref> over non-overlapping cuboids and concatenate these to form the input. The 3-D convnet is pre-trained on video datasets for action recognition. Second, they include an attention mechanism that learns to weight the frame features nonuniformly conditioned on the previous word input(s) rather than uniformly weighting features from all frames as in <ref type="bibr" target="#b38">[39]</ref>. The 3-D convnet alone provides limited performance improvement, but in conjunction with the attention model it notably improves performance. We propose a simpler approach to using temporal information by using an LSTM to encode the sequence of video frames into a distributed vector representation that is sufficient to generate a sentential description. Therefore, our direct sequence to sequence model does not require an explicit attention mechanism.</p><p>Another recent project <ref type="bibr" target="#b32">[33]</ref> uses LSTMs to predict the future frame sequence from an encoding of the previous frames. Their model is more similar to the language translation model in <ref type="bibr" target="#b33">[34]</ref>, which uses one LSTM to encode the input text into a fixed representation, and another LSTM to decode it into a different language. In contrast, we employ a single LSTM that learns both encoding and decoding based on the inputs it is provided. This allows the LSTM to share weights between encoding and decoding.</p><p>Other related work includes <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b7">8]</ref>, which uses LSTMs for activity classification, predicting an activity class for the representation of each image/flow frame. In contrast, our model generates captions after encoding the complete sequence of optical flow images. Specifically, our final model is an ensemble of the sequence to sequence models trained on raw images and optical flow images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>We propose a sequence to sequence model for video description, where the input is the sequence of video frames (x 1 , . . . , x n ), and the output is the sequence of words (y 1 , . . . , y m ). Naturally, both the input and output are of variable, potentially different, lengths. In our case, there are typically many more frames than words.</p><p>In our model, we estimate the conditional probability of an output sequence (y 1 , . . . , y m ) given an input sequence (x 1 , . . . , x n ) i.e. p(y 1 , . . . , y m |x 1 , . . . , x n ) (1) This problem is analogous to machine translation between natural languages, where a sequence of words in the input language is translated to a sequence of words in the output language. Recently, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">34]</ref> have shown how to effectively attack this sequence to sequence problem with an LSTM Recurrent Neural Network (RNN). We extend this paradigm to inputs comprised of sequences of video frames, significantly simplifying prior RNN-based methods for video description. In the following, we describe our model and architecture in detail, as well as our input and output representation for video and sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">LSTMs for sequence modeling</head><p>The main idea to handle variable-length input and output is to first encode the input sequence of frames, one at a time, representing the video using a latent vector representation, and then decode from that representation to a sentence, one word at a time.</p><p>Let us first recall the Long Short Term Memory RNN (LSTM), originally proposed in <ref type="bibr" target="#b11">[12]</ref>. Relying on the LSTM unit proposed in <ref type="bibr" target="#b43">[44]</ref>, for an input x t at time step t, the LSTM computes a hidden/control state h t and a memory cell state c t which is an encoding of everything the cell has observed until time t:</p><formula xml:id="formula_0">i t = σ(W xi x t + W hi h t−1 + b i ) f t = σ(W xf x t + W hf h t−1 + b f ) o t = σ(W xo x t + W ho h t−1 + b o ) g t = φ(W xg x t + W hg h t−1 + b g ) c t = f t ⊙ c t−1 + i t ⊙ g t h t = o t ⊙ φ(c t )<label>(2)</label></formula><p>where σ is the sigmoidal non-linearity, φ is the hyperbolic tangent non-linearity, ⊙ represents the element-wise product with the gate value, and the weight matrices denoted by W ij and biases b j are the trained parameters. Thus, in the encoding phase, given an input sequence X (x 1 , . . . , x n ), the LSTM computes a sequence of hidden states (h 1 , . . . , h n ). During decoding it defines a distribution over the output sequence Y (y 1 , . . . , y m ) given the input sequence X as p(Y |X) is</p><formula xml:id="formula_1">p(y 1 , . . . , y m |x 1 , . . . , x n ) = m t=1 p(y t |h n+t−1 , y t−1 ) (3)</formula><p>where the distribution of p(y t |h n+t ) is given by a softmax over all of the words in the vocabulary (see <ref type="table">Equation 5</ref>). Note that h n+t is obtained from h n+t−1 , y t−1 based on the recursion in Equation 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sequence to sequence video to text</head><p>Our approach, S2VT, is depicted in <ref type="figure">Figure 2</ref>. While <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">34]</ref> first encode the input sequence to a fixed length vector using one LSTM and then use another LSTM to map the vector to a sequence of outputs, we rely on a single LSTM for both the encoding and decoding stage. This allows parameter sharing between the encoding and decoding stage.</p><p>Our model uses a stack of two LSTMs with 1000 hidden units each. <ref type="figure">Figure 2</ref> shows the LSTM stack unrolled over time. When two LSTMs are stacked together, as in our case, the hidden representation (h t ) from the first LSTM layer (colored red) is provided as the input (x t ) to the second LSTM (colored green). The top LSTM layer in our architecture is used to model the visual frame sequence, and the next layer is used to model the output word sequence.</p><p>Training and Inference In the first several time steps, the top LSTM layer (colored red in <ref type="figure">Figure 2</ref>) receives a sequence of frames and encodes them while the second LSTM layer receives the hidden representation (h t ) and concatenates it with null padded input words (zeros), which it then encodes. There is no loss during this stage when the LSTMs are encoding. After all the frames in the video clip are exhausted, the second LSTM layer is fed the beginning-ofsentence (&lt;BOS&gt;) tag, which prompts it to start decoding its current hidden representation into a sequence of words. While training in the decoding stage, the model maximizes for the log-likelihood of the predicted output sentence given the hidden representation of the visual frame sequence, and the previous words it has seen. From Equation 3 for a model with parameters θ and output sequence Y = (y 1 , . . . , y m ), this is formulated as:</p><formula xml:id="formula_2">θ * = argmax θ m t=1 log p(y t |h n+t−1 , y t−1 ; θ)<label>(4)</label></formula><p>This log-likelihood is optimized over the entire training dataset using stochastic gradient descent. The loss is computed only when the LSTM is learning to decode. Since this  <ref type="figure">Figure 2</ref>. We propose a stack of two LSTMs that learn a representation of a sequence of frames in order to decode it into a sentence that describes the event in the video. The top LSTM layer (colored red) models visual feature inputs. The second LSTM layer (colored green) models language given the text input and the hidden representation of the video sequence. We use &lt;BOS&gt; to indicate begin-of-sentence and &lt;EOS&gt; for the end-of-sentence tag. Zeros are used as a &lt;pad&gt; when there is no input at the time step.</p><p>loss is propagated back in time, the LSTM learns to generate an appropriate hidden state representation (h n ) of the input sequence. The output (z t ) of the second LSTM layer is used to obtain the emitted word (y). We apply a softmax function to get the probability distribution over the words y ′ in the vocabulary V :</p><formula xml:id="formula_3">p(y|z t ) = exp(W y z t ) y ′ ∈V exp(W y ′ z t )<label>(5)</label></formula><p>We note that, during the decoding phase, the visual frame representation for the first LSTM layer is simply a vector of zeros that acts as padding input. We require an explicit end-of-sentence tag (&lt;EOS&gt;) to terminate each sentence since this enables the model to define a distribution over sequences of varying lengths. At test time, during each decoding step we choose the word y t with the maximum probability after the softmax (from Equation 5) until it emits the &lt;EOS&gt; token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Video and text representation</head><p>RGB frames. Similar to previous LSTM-based image captioning efforts <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40]</ref> and video-to-text approaches <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b42">43]</ref>, we apply a convolutional neural network (CNN) to input images and provide the output of the top layer as input to the LSTM unit. In this work, we report results using the output of the fc7 layer (after applying the ReLU non-linearity) on the Caffe Reference Net (a variant of AlexNet) and also the 16-layer VGG model <ref type="bibr" target="#b31">[32]</ref>. We use CNNs that are pretrained on the 1.2M image ILSVRC-2012 object classification subset of the ImageNet dataset <ref type="bibr" target="#b29">[30]</ref> and made available publicly via the Caffe ModelZoo. <ref type="bibr" target="#b0">1</ref> Each input video frame is scaled to 256x256, and is cropped to a random 227x227 1 https://github.com/BVLC/caffe/wiki/Model-Zoo region. It is then processed by the CNN. We remove the original last fully-connected classification layer and learn a new linear embedding of the features to a 500 dimensional space. The lower dimension features form the input (x t ) to the first LSTM layer. The weights of the embedding are learned jointly with the LSTM layers during training. Optical Flow. In addition to CNN outputs from raw image (RGB) frames, we also incorporate optical flow measures as input sequences to our architecture. Others <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b7">8]</ref> have shown that incorporating optical flow information to LSTMs improves activity classification. As many of our descriptions are activity centered, we explore this option for video description as well. We follow the approach in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> and first extract classical variational optical flow features <ref type="bibr" target="#b1">[2]</ref>. We then create flow images (as seen in <ref type="figure" target="#fig_1">Figure 1</ref>) in a manner similar to <ref type="bibr" target="#b8">[9]</ref>, by centering x and y flow values around 128 and multiplying by a scalar such that flow values fall between 0 and 255. We also calculate the flow magnitude and add it as a third channel to the flow image. We then use a CNN <ref type="bibr" target="#b8">[9]</ref> initialized with weights trained on the UCF101 video dataset to classify optical flow images into 101 activity classes. The fc6 layer activations of the CNN are embedded in a lower 500 dimensional space which is then given as input to the LSTM. The rest of the LSTM architecture remains unchanged for flow inputs.</p><p>In our combined model, we use a shallow fusion technique to integrate flow and RGB features. At each time step of the decoding phase, the model proposes a set of candidate words. We then rescore these hypotheses with the weighted sum of the scores by the flow and RGB networks, where we only need to recompute the score of each new word p(y t = y ′ ) as: Text input. The target output sequence of words are represented using one-hot vector encoding (1-of-N coding, where N is the size of the vocabulary). Similar to the treatment of frame features, we embed words to a lower 500 dimensional space by applying a linear transformation to the input data and learning its parameters via back propagation. The embedded word vector concatenated with the output (h t ) of the first LSTM layer forms the input to the second LSTM layer (marked green in <ref type="figure">Figure 2</ref>). When considering the output of the LSTM we apply a softmax over the complete vocabulary as in Equation 5.</p><formula xml:id="formula_4">α · p rgb (y t = y ′ ) + (1 − α) · p f low (y t = y ′ ) the</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>This secction describes the evaluation of our approach. We first describe the datasets used, then the evaluation protocol, and then the details of our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Video description datasets</head><p>We report results on three video description corpora, namely the Microsoft Video Description corpus (MSVD) <ref type="bibr" target="#b2">[3]</ref>, the MPII Movie Description Corpus (MPII-MD) <ref type="bibr" target="#b27">[28]</ref>, and the Montreal Video Annotation Dataset (M-VAD) <ref type="bibr" target="#b36">[37]</ref>. Together they form the largest parallel corpora with open domain video and natural language descriptions. While MSVD is based on web clips with short humanannotated sentences, MPII-MD and M-VAD contain Hollywood movie snippets with descriptions sourced from script data and audio description. Statistics of each corpus are presented in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Microsoft Video Description Corpus (MSVD)</head><p>The Microsoft Video description corpus <ref type="bibr" target="#b2">[3]</ref>, is a collection of Youtube clips collected on Mechanical Turk by requesting workers to pick short clips depicting a single activity. The videos were then used to elicit single sentence descriptions from annotators. The original corpus has multi-lingual descriptions, in this work we use only the English descriptions. We do minimal pre-processing on the text by converting all text to lower case, tokenizing the sentences and removing punctuation. We use the data splits provided by <ref type="bibr" target="#b38">[39]</ref>. Additionally, in each video, we sample every tenth frame as done by <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">MPII Movie Description Dataset (MPII-MD)</head><p>MPII-MD <ref type="bibr" target="#b27">[28]</ref> contains around 68,000 video clips extracted from 94 Hollywood movies. Each clip is accompanied with a single sentence description which is sourced from movie scripts and audio description (AD) data. AD or Descriptive Video Service (DVS) is an additional audio track that is added to the movies to describe explicit visual elements in a movie for the visually impaired. Although the movie snippets are manually aligned to the descriptions, the data is very challenging due to the high diversity of visual and textual content, and the fact that most snippets have only a single reference sentence. We use the training/validation/test split provided by the authors and extract every fifth frame (videos are shorter than MSVD, averaging 94 frames).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Montreal Video Annotation Dataset (M-VAD)</head><p>The M-VAD movie description corpus <ref type="bibr" target="#b36">[37]</ref> is another recent collection of about 49,000 short video clips from 92 movies. It is similar to MPII-MD, but contains only AD data with automatic alignment. We use the same setup as for MPII-MD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>Quantitative evaluation of the models are performed using the METEOR <ref type="bibr" target="#b6">[7]</ref> metric which was originally proposed to evaluate machine translation results. The ME-TEOR score is computed based on the alignment between a given hypothesis sentence and a set of candidate reference sentences. METEOR compares exact token matches, stemmed tokens, paraphrase matches, as well as semantically similar matches using WordNet synonyms. This semantic aspect of METEOR distinguishes it from others such as BLEU <ref type="bibr" target="#b25">[26]</ref>, ROUGE-L <ref type="bibr" target="#b20">[21]</ref>, or CIDEr <ref type="bibr" target="#b37">[38]</ref>. The authors of CIDEr <ref type="bibr" target="#b37">[38]</ref> evaluated these four measures for image description. They showed that METEOR is always better than BLEU and ROUGE and outperforms CIDEr when the number of references are small (CIDEr is comparable to METEOR when the number of references are large). Since MPII-MD and M-VAD have only a single reference, we decided to use METEOR in all our evaluations. We employ METEOR version 1.5 2 using the code 3 released with the Microsoft COCO Evaluation Server <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental details of our models</head><p>All our models take as input either the raw RGB frames directly feeding into the CNN, or pre-processed optical flow images (described in Section 3.3). In all of our models, we unroll the LSTM to a fixed 80 time steps during training. We found this to be a good trade-off between memory consumption and the ability to provide many frames (videos) to the LSTM. This setting allows us to fit multiple videos in a single mini-batch (up to 8 for AlexNet and up to 3 for flow models). We note that 94% of the YouTube training videos satisfied this limit (with frames sampled at the rate of 1 in 10). For videos with fewer than 80 time steps (of words and frames), we pad the remaining inputs with zeros. For longer videos, we truncate the number of frames to ensure that the sum of the number of frames and words is within this limit. At test time, we do not constrain the length of the video and our model views all sampled frames. We use the pre-trained AlexNet and VGG CNNs. For VGG, we fix all layers below fc7 to reduce memory consumption and allow faster training.</p><p>We compare our sequence to sequence LSTM architecture with RGB image features extracted from both AlexNet, and the 16-layer VGG network. In order to compare features from the VGG network with previous models, we include the performance of the mean-pooled model proposed in <ref type="bibr" target="#b38">[39]</ref> using the output of the fc7 layer from the 16 layer VGG as a baseline (line 3, <ref type="table" target="#tab_3">Table 2</ref>). All our sequence to sequence models are referenced in <ref type="table" target="#tab_3">Table 2</ref> under S2VT. Our first variant, RGB (AlexNet) is the end-to-end model that uses AlexNet on RGB frames. Flow (AlexNet) refers to the model that is obtained by training on optical flow images. RGB (VGG) refers to the model with the 16-layer VGG model on RGB image frames. We also experiment with randomly re-ordered input frames (line 10) to verify that S2VT learns temporal-sequence information. Our final model is an ensemble of the RGB (VGG) and Flow (AlexNet) where the prediction at each time step is a weighted average of the prediction from the individual models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Related approaches</head><p>We compare our sequence to sequence models against the factor graph model (FGM) in <ref type="bibr" target="#b35">[36]</ref>, the mean-pooled models in <ref type="bibr" target="#b38">[39]</ref> and the Soft-Attention models of <ref type="bibr" target="#b42">[43]</ref>. FGM proposed in <ref type="bibr" target="#b35">[36]</ref> uses a two step approach to first obtain confidences on subject, verb, object and scene (S,V,O,P) elements and combines these with confidences from a language model using a factor graph to infer the most likely (S,V,O,P) tuple in the video. It then generates a sentence based on a template. The Mean Pool model proposed in <ref type="bibr" target="#b38">[39]</ref> pools AlexNet fc7 activations across all frames to create a fixed-length vector representation of the video. It then uses an LSTM to then decode the vector into a sequence of words. Further, the model ia pre-trained on the Flickr30k <ref type="bibr" target="#b12">[13]</ref> and MSCOCO <ref type="bibr" target="#b21">[22]</ref> image-caption datasets and fine-tuned on MSVD for a significant improvement in performance. We compare  our models against their basic mean-pooled model and their best model obtained from fine-tuning on Flickr30k and COCO datasets. We also compare against the GoogleNet <ref type="bibr" target="#b34">[35]</ref> variant of the mean-pooled model reported in <ref type="bibr" target="#b42">[43]</ref>.</p><p>The Temporal-Attention model in <ref type="bibr" target="#b42">[43]</ref> is a combination of weighted attention over a fixed set of video frames with input features from GoogleNet and a 3D-convnet trained on HoG, HoF and MBH features from an activity classification model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Discussion</head><p>This section discussses the result of our evaluation shown in <ref type="table" target="#tab_3">Tables 2, 4</ref>, and 5. <ref type="table" target="#tab_3">Table 2</ref> shows the results on the MSVD dataset. Rows 1 through 7 present related approaches and the rest are variants of our S2VT approach. Our basic S2VT AlexNet model on RGB video frames (line 9 in <ref type="table" target="#tab_3">Table 2</ref>) achieves 27.9% METEOR and improves over the basic mean-pooled model in <ref type="bibr" target="#b38">[39]</ref> (line 2, 26.9%) as well as the VGG meanpooled model (line 3, 27.7%);suggesting that S2VT is a more powerful approach. When the model is trained with randomly-ordered frames (line 10 in <ref type="table" target="#tab_3">Table 2</ref>), the score is considerably lower, clearly demonstrating that the model benefits from exploiting temporal structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">MSVD dataset</head><p>Our S2VT model which uses flow images (line 8) achieves only 24.3% METEOR but improves the performance of our VGG model from 29.2% (line 11) to 29.8% (line 12), when combined. A reason for the low performance of the flow model could be that optical flow features even for the same activity can vary significantly with context e.g. 'panda eating' vs 'person eating'. Also, the Edit-Distance k = 0 k &lt;= 1 k &lt;= 2 k &lt;= 3 model only receives very weak signals with regard to the kind of activities depicted in YouTube videos. Some commonly used verbs such as "play" are polysemous and can refer to playing a musical instrument ("playing a guitar") or playing a sport ("playing golf"). However, integrating RGB with Flow improves the quality of descriptions. Our ensemble using both RGB and Flow performs slightly better than the best model proposed in <ref type="bibr" target="#b42">[43]</ref>, temporal attention with GoogleNet + 3D-CNN (line 7). The modest size of the improvement is likely due to the much stronger 3D-CNN features (as the difference to GoogleNet alone (line 6) suggests). Thus, the closest comparison between the Temporal Attention Model <ref type="bibr" target="#b42">[43]</ref> and S2VT is arguably S2VT with VGG (line 12) vs. their GoogleNet-only model (line 6). <ref type="figure" target="#fig_2">Figure 3</ref> shows descriptions generated by our model on sample Youtube clips from MSVD. To compare the originality in generation, we compute the Levenshtein distance of the predicted sentences with those in the training set. From <ref type="table">Table 3</ref>, for the MSVD corpus, 42.9% of the predictions are identical to some training sentence, and another 38.3% can be obtained by inserting, deleting or substituting one word from some sentence in the training corpus. We note that many of the descriptions generated are relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Movie description datasets</head><p>For the more challenging MPII-MD and M-VAD datasets we use our single best model, namely S2VT trained on RGB frames and VGG. To avoid over-fitting on the movie corpora we employ drop-out which has proved to be beneficial on these datasets <ref type="bibr" target="#b26">[27]</ref>. We found it was best to use dropout at the inputs and outputs of both LSTM layers. Further, we used ADAM <ref type="bibr" target="#b16">[17]</ref> for optimization with a first momentum coefficient of 0.9 and a second momentum coefficient of 0.999. For MPII-MD, reported in <ref type="table" target="#tab_5">Table 4</ref>, we improve over the SMT approach from <ref type="bibr" target="#b27">[28]</ref> from 5.6% to 7.1% METEOR and over Mean pooling <ref type="bibr" target="#b38">[39]</ref> by 0.4%. Our performance is similar to Visual-Labels <ref type="bibr" target="#b26">[27]</ref>, a contemporaneous LSTM-based approach which uses no temporal encoding, but more diverse visual features, namely object detectors, as well as activity and scene classifiers.</p><p>On M-VAD we achieve 6.7% METEOR which significantly outperforms the temporal attention model <ref type="bibr" target="#b42">[43]</ref> Approach METEOR SMT (best variant) <ref type="bibr" target="#b27">[28]</ref> 5.6 Visual-Labels <ref type="bibr" target="#b26">[27]</ref> 7.0 Mean pool (VGG) 6.7 S2VT: RGB (VGG), ours 7.1  <ref type="table">Table 5</ref>. M-VAD dataset (METEOR in %, higher is better).</p><p>(4.3%) <ref type="bibr" target="#b3">4</ref> and Mean pooling (6.1%). On this dataset we also outperform Visual-Labels [27] (6.3%).</p><p>We report results on the LSMDC challenge <ref type="bibr" target="#b4">5</ref> , which combines M-VAD and MPII-MD. S2VT achieves 7.0% METEOR on the public test set using the evaluation server.</p><p>In <ref type="figure">Figure 4</ref> we present descriptions generated by our model on some sample clips from the M-VAD dataset. More example video clips, generated sentences, and data are available on the authors' webpages 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper proposed a novel approach to video description. In contrast to related work, we construct descriptions using a sequence to sequence model, where frames are first read sequentially and then words are generated sequentially. This allows us to handle variable-length input and output while simultaneously modeling temporal structure. Our model achieves state-of-the-art performance on the MSVD dataset, and outperforms related work on two large and challenging movie-description datasets. Despite its conceptual simplicity, our model significantly benefits from additional data, suggesting that it has a high model capacity, and is able to learn complex temporal structure in the input and output sequences for challenging moviedescription datasets.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>network is connected to a CNN for RGB frames or a CNN for optical flow images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Our S2VT approach performs video description using a sequence to sequence model. It incorporates a stacked LSTM which first reads the sequence of frames and then generates a sequence of words. The input visual sequence to the model is comprised of RGB and/or optical flow CNN outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Qualitative results on MSVD YouTube dataset from our S2VT model (RGB on VGG net). (a) Correct descriptions involving different objects and actions for several videos. (b) Relevant but incorrect descriptions. (c) Descriptions that are irrelevant to the event in the video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Ours): (1) Now, the van pulls out a window and a tall brick facade of tall trees . a figure stands at a curb. (2) Someone drives off the passenger car and drives off. (3) They drive off the street. (4) They drive off a suburban road and parks in a dirt neighborhood. (5) They drive off a suburban road and parks on a street. (6) Someone sits in the doorway and stares at her with a furrowed brow.Temporal Attention (GNet+3D-conv att ): (1) At night , SOMEONE and SOMEONE step into the parking lot. (2) Now the van drives away. (3) They drive away. (4) They drive off. (5) They drive off. (6) At the end of the street , SOMEONE sits with his eyes closed. DVS: (1) Now , at night , our view glides over a highway , its lanes glittering from the lights of traffic below. (2) Someone's suv cruises down a quiet road. (3) Then turn into a parking lot . (4) A neon palm tree glows on a sign that reads oasis motel. (5) Someone parks his suv in front of some rooms. (6) He climbs out with his briefcase , sweeping his cautious gaze around the area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>hyper-parameter α is tuned on the validation set.</figDesc><table>MSVD MPII-MD MVAD 

#-sentences 
80,827 
68,375 
56,634 
#-tokens 
567,874 
679,157 
568,408 
vocab 
12,594 
21,700 
18,092 
#-videos 
1,970 
68,337 
46,009 
avg. length 
10.2s 
3.9s 
6.2s 
#-sents per video 
≈41 
1 
1-2 

Table 1. Corpus Statistics. The the number of tokens in all datasets 
are comparable, however MSVD has multiple descriptions for 
each video while the movie corpora (MPII-MD, MVAD) have a 
large number of clips with a single description each. Thus, the 
number of video-sentence pairs in all 3 datasets are comparable. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 .</head><label>2</label><figDesc>MSVD dataset (METEOR in %, higher is better).</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>Table 3. Percentage of generated sentences which match a sen- tence of the training set with an edit (Levenshtein) distance of less than 4. All values reported in percentage (%).</figDesc><table>MSVD 
42.9 
81.2 
93.6 
96.6 
MPII-MD 
28.8 
43.5 
56.4 
83.0 
MVAD 
15.6 
28.7 
37.8 
45.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 .</head><label>4</label><figDesc>MPII-MD dataset (METEOR in %, higher is better).</figDesc><table>Approach 
METEOR 

Visual-Labels [27] 
6.3 
Temporal att. (GoogleNet+3D-CNN) [43] 

4 

4.3 
Mean pool (VGG) 
6.1 
S2VT: RGB (VGG), ours 
6.7 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.cs.cmu.edu/˜alavie/METEOR 3 https://github.com/tylin/coco-caption</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We report results using the predictions provided by [43] but using the orginal COCO Evaluation scripts. [43] report 5.7% METEOR for their temporal attention + 3D-CNN model using a different tokenization. 5 LSMDC: sites.google.com/site/describingmovies 6 http://vsubhashini.github.io/s2vt.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Lisa Anne Hendricks, Matthew Hausknecht, Damian Mrowca for helpful discussions; and Anna Rohrbach for help with both movie corpora; and the anonymous reviewers for insightful comments and suggestions. We acknowledge support from ONR ATL Grant N00014-11-1-010, DARPA, AFRL, DoD MURI award N000141110688, DEFT program (AFRL grant FA8750-13-2-0026), NSF awards IIS-1427425, IIS-1451244, and IIS-1212798, and BVLC. Raymond and Kate acknowledge support from Google. Marcus was supported by the FITweltweit-Program of the German Academic Exchange Service (DAAD).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video2text: Learning to annotate video content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yagnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDMW</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Microsoft COCO captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<title level="m">On the properties of neural machine translation: Encoderdecoder approaches</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shoot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A multi-modal clustering method for web videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCTCS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<title level="m">Convolutional architecture for fast feature embedding. ACMMM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generating natural-language video descriptions using text-mined knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2013-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Treetalk: Composition and compression of trees for image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">C</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL-04 Workshop</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6632</idno>
		<title level="m">Deep captioning with multimodal recurrent neural networks (mrnn)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">TRECVID 2012 -an overview of the goals, tasks, data, evaluation mechanisms and metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Quéenot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TRECVID 2012</title>
		<meeting>TRECVID 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The long-short story of movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>GCPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A dataset for movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Translating video content to natural language descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ILSVRC</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<title level="m">Going deeper with convolutions. CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Integrating language and vision to generate natural language descriptions of videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Using descriptive video services to create a large data source for video annotation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01070v1</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">CIDEr: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Translating videos to natural language using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Show and tell: A neural image caption generator. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multimodal fusion for video search reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.08029v4</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning to execute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.4615</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
