<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Breaking the Activation Function Bottleneck through Adaptive Parameterization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Flennerhag</surname></persName>
							<email>sflennerhag@turing.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Manchester</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Alan Turing Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Yin</surname></persName>
							<email>hujun.yin@manchester.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Manchester</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Alan Turing Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Keane</surname></persName>
							<email>john.keane@manchester.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Manchester</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Elliot</surname></persName>
							<email>mark.elliot@manchester.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Manchester</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Breaking the Activation Function Bottleneck through Adaptive Parameterization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Standard neural network architectures are non-linear only by virtue of a simple element-wise activation function, making them both brittle and excessively large. In this paper, we consider methods for making the feed-forward layer more flexible while preserving its basic structure. We develop simple drop-in replacements that learn to adapt their parameterization conditional on the input, thereby increasing statistical efficiency significantly. We present an adaptive LSTM that advances the state of the art for the Penn Treebank and WikiText-2 word-modeling tasks while using fewer parameters and converging in less than half the number of iterations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While a two-layer feed-forward neural network is sufficient to approximate any function <ref type="bibr" target="#b11">(Cybenko, 1989;</ref><ref type="bibr" target="#b26">Hornik, 1991)</ref>, in practice much deeper networks are necessary to learn a good approximation to a complex function. In fact, a network tends to generalize better the larger it is, often to the point of having more parameters than there are data points in the training set <ref type="bibr" target="#b6">(Canziani et al., 2016;</ref><ref type="bibr" target="#b42">Novak et al., 2018;</ref><ref type="bibr" target="#b16">Frankle &amp; Carbin, 2018)</ref>.</p><p>One reason why neural networks are so large is that they bias towards linear behavior: if the activation function is largely linear, so will the hidden layer be. Common activation functions, such as the Sigmoid, Tanh, and ReLU all behave close to linear over large ranges of their domain. Consequently, for a randomly sampled input to break linearity, layers must be wide and the network deep to ensure some elements lie in non-linear regions of the activation function. To overcome the bias towards linear behavior, more sophisticated activation functions have been designed <ref type="bibr" target="#b9">(Clevert et al., 2015;</ref><ref type="bibr" target="#b24">He et al., 2015;</ref><ref type="bibr" target="#b30">Klambauer et al., 2017;</ref><ref type="bibr" target="#b12">Dauphin et al., 2017)</ref>. However, these still limit all non-linearity to sit in the activation function.</p><p>We instead propose adaptive parameterization, a method for learning to adapt the parameters of the affine transformation to a given input. In particular, we present a generic adaptive feed-forward layer that retains the basic structure of the standard feed-forward layer while significantly increasing the capacity to model non-linear patterns. We develop specific instances of adaptive parameterization that can be trained end-to-end jointly with the network using standard backpropagation, are simple to implement, and run at minimal additional cost.</p><p>Empirically, we find that adaptive parameterization can learn non-linear patterns where a non-adaptive baseline fails, or outperform the baseline using 30-50% fewer parameters. In particular, we develop an adaptive version of the Long Short-Term Memory model (LSTM; <ref type="bibr" target="#b25">Hochreiter &amp; Schmidhuber, 1997;</ref><ref type="bibr" target="#b18">Gers et al., 2000)</ref> that enjoys both faster convergence and greater statistical efficiency.</p><p>The adaptive LSTM advances the state of the art for the Penn Treebank and WikiText-2 word modeling tasks using~20-30% fewer parameters and converging in less than half as many iterations. proceed as follows: section 2 presents the adaptive feed-forward layer, section 3 develops the adaptive LSTM, section 4 discusses related work and section 5 presents empirical analysis and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Adaptive Parameterization</head><p>To motivate adaptive parameterization, we show that deep neural networks learn a family of compositions of linear maps and because the activation function is static, the inherent flexibility in this family is weak. Adaptive parameterization is a means of increasing this flexibility and thereby increasing the model's capacity to learn non-linear patterns. We focus on the feed-forward layer, f (x) := φ(W x + b), for some activation function φ : R → R. Define the pre-activation layer as a = A(x) := W x + b and denote by g(a) := φ(a)/ a the activation effect of φ given a, where division is element-wise. Let G = diag(g(a)). We then have f (x) = g(a) a = G a; we use " " to denote the Hadamard product. For any pair</p><formula xml:id="formula_0">(x, y) ∈ R n × R k , a deep feed-forward network with N ∈ N layers, f (N ) • · · · • f (1)</formula><p>, approximates the relationship x → y by a composition of linear maps. To see this, note that x is sufficient to determine all activation effects G = {G (l) } N l=1 . Together with fixed transformations</p><formula xml:id="formula_1">A = {A (l) } N l=1</formula><p>, the network can be expressed aŝ</p><formula xml:id="formula_2">y = (f (N ) • · · · • f (1) )(x) = (G (N ) • A (N ) • · · · • G (1) • A (1) )(x).<label>(1)</label></formula><p>A neural network can therefore be understood as learning a "prior" A in parameter space around which it constructs a family of compositions of linear maps (as G varies across inputs). The neural network adapts to inputs through the set of activation effects G. This adaptation mechanism is weak: if φ is close to linear over the distribution of a, as is often the case, little adaptation can occur. Moreover, because G does not have any learnable parameters itself, the fixed prior A must learn to encode both global input-invariant information as well as local contextual information. We refer to this as the activation function bottleneck. Adaptive parameterization breaks this bottleneck by parameterizing the adaptation mechanism in G, thereby circumventing these issues.</p><p>To see how the activation function bottleneck arises, note that φ is redundant whenever it is closely approximated by a linear function over some non-trivial segment of the input distribution. For these inputs, φ has no non-linear effect and such lost opportunities imply that the neural network must be made larger than necessary to fully capture non-linear patterns. For instance, both the Sigmoid and the Tanh are closely approximated around 0 by a linear function, rendering them redundant for inputs close to 0. Consequently, the network must be made deeper and its layers wider to mitigate the activation function bottleneck. In contrast, adaptive parameterization places the layer's non-linearity within the parameter matrix itself, thereby circumventing the activation function bottleneck. Further, by relaxing the element-wise non-linearity constraint imposed on the standard feed-forward layer, it can learn behaviors that would otherwise be very hard or impossible to model, such as contextual rotations and shears, and adaptive feature activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Adaptive Feed-Forward Layer</head><p>Our goal is to break the activation function bottleneck by generalizing G into a parameterized adaptation policy, thereby enabling the network to specialize parameters in A to encode global, input invariant information while parameters in G encode local, contextual information.</p><p>Consider the standard feed-forward layer, defined by one adaptation block f (x) = (G • A)(x). As described above, we increase the capacity of the adaptation mechanism G by replacing it with a parameterized adaptation mechanism D (j) := diag(π (j) (x)), where π (j) is a learnable adaptation policy. Note that π (j) can be made arbitrarily complex. In particular, even if π (j) is linear, the adaptive mechanism D (j) a is quadratic in x, and as such escapes the bottleneck. To ensure that the adaptive feed-forward layer has sufficient capacity, we generalize it to q ∈ N adaptation blocks,</p><formula xml:id="formula_3">3 f (x) := φ D (q) W (q−1) · · · W (1) D (1) x + D (0) b .<label>(2)</label></formula><p>We refer to the number of adaptation blocks q as the order of the layer. Strictly speaking, the adaptive feed-forward layer does not need an activation function, but it can provide desirable properties depending on the application. It is worth noting that the adaptive feed-forward layer places no restrictions on the form of the adaptation policy π = (π (0) , . . . , π (q) ) or its training procedure. In this paper, we parameterize π as a neural network trained jointly with the main model. Next, we show how different adaptive feed-forward layers are generated by the choice of adaptation policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adaptation Policies</head><p>Higher-order adaptation (i.e. q large) enables expressive adaptation policies, but because the adaptation policy depends on x, high-order layers are less efficient than a stack of low-order layers. We find that low-order layers are surprisingly powerful, and present a policy of order 2 that can express any other adaptation policy.</p><p>Partial Adaptation The simplest adaptation policy (q = 1) is given by f (x) = W D</p><p>(1) x +D (0) b. This policy is equivalent to a mean shift and a re-scaling of the columns of W , or alternatively re-scaling the input. It can be thought of as a learned contextualized standardization mechanism that conditions the effect on the specific input. As such, we refer to this policy as input adaptation. Its mirror image, output adaptation, is given by</p><formula xml:id="formula_4">f (x) = D (1) W x +D (0) b.</formula><p>This is a special case of second-order adaptation policies, where D</p><p>(1) = I, where I denotes the identity matrix. Both these policies are restrictive in that they only operate on either the rows or the columns of W ( <ref type="figure">fig. 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IO-adaptation</head><p>The general form of second-order adaptation policies integrates input-and outputadaptation into a jointly learned adaptation policy. As such we refer to this as IO-adaptation,</p><formula xml:id="formula_5">f (x) = D (2) W D (1) x +D (0) b .<label>(3)</label></formula><p>IO-adaptation is much more powerful than either input-or output-adaptation alone, which can be seen by the fact that it essentially learns to identify and adapt sub-matrices in W by sharing adaptation vectors across rows and columns ( <ref type="figure">fig. 1</ref>). In fact, assuming π is sufficiently powerful, IO-adaptation can express any mapping from input to parameter space.</p><p>Property 1. Let W be given and fix x. For any G of same dimensionality as W , there are arbitrarily</p><formula xml:id="formula_6">many (D (1) , D (2) ) such that G x = D (2) W D (1) x.</formula><p>Proof: see supplementary material.</p><p>Singular Value Adaptation (SVA) Another policy of interest arises as a special case of third-order adaptation policies, where D (1) = I as before. The resulting policy,</p><formula xml:id="formula_7">f (x) = W (2) DW (1) x +D (0) b,<label>(4)</label></formula><p>is reminiscent of Singular Value Decomposition. However, rather than being a decomposition, it composes a projection by adapting singular values to the input. In particular, letting W (1) = V T A and W (2) = BU , with U and V appropriately orthogonal, eq. 4 can be written as B(U DV T )A x, with U DV T adapted to x through its singular values. In our experiments, we initialize weight matrices as semi-orthogonal <ref type="bibr" target="#b46">(Saxe et al., 2013</ref>), but we do not enforce orthogonality after initialization.</p><p>The drawback of SVA is that it requires learning two separate matrices of relatively high rank. For problems where the dimensionality of x is large, the dimensionality of the adaptation space has to be made small to control parameter count. This limits the model's capacity by enforcing a low-rank factorization, which also tends to impact training negatively <ref type="bibr" target="#b13">(Denil et al., 2013)</ref>.</p><p>SVA and IO-adaptation are simple but flexible policies that can be used as drop-in replacements for any feed-forward layer. Because they are differentiable, they can be trained using standard backpropagation. Next, we demonstrate adaptive parameterization in the context of Recurrent Neural Networks (RNNs), where feed-forward layers are predominant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Adaptive Parameterization in RNNs</head><p>RNNs are common in sequence learning, where the input is a sequence {x 1 , . . . , x t } and the target variable either itself a sequence or a single point or vector. In either case, the computational graph of an RNN, when unrolled over time, will be of the form in eq. 1, making it a prime candidate for adaptive parameterization. Moreover, in sequence-to-sequence learning, the model estimates a conditional distribution p(y t | x 1 , . . . , x t ) that changes significantly from one time step to the next. Because of this variance, an RNN must be very flexible to model the conditional distribution. By embedding adaptive parameterization, we can increase flexibility for a given model size. Consider the LSTM model <ref type="bibr" target="#b25">(Hochreiter &amp; Schmidhuber, 1997;</ref><ref type="bibr" target="#b18">Gers et al., 2000)</ref>, defined by the gating mechanism</p><formula xml:id="formula_8">c t = σ(u f t ) c t−1 + σ(u i t ) τ (u z t ) h t = σ(u o t ) τ (c t ),<label>(5)</label></formula><p>where σ and τ represent Sigmoid and Tanh activation functions respectively and each u s∈{i,f,o,z} t is a linear transformation of the form u</p><formula xml:id="formula_9">s t = W (s) x t +V (s) h t−1 + b (s)</formula><p>. Adaptation in the LSTM can be derived directly from the adaptive feed-forward layer (eq. 2). We focus on IO-adaptation as this adaptation policy performed better in our experiments. For π, we use a small neural network to output a latent variable z t that we map into each sub-policy with a projection</p><formula xml:id="formula_10">U (j) : π (j) (z t ) = τ U (j) z t .</formula><p>We test a static and a recurrent network as models for the latent variable,</p><formula xml:id="formula_11">z t = ReLU (W v t + b) ,<label>(6)</label></formula><formula xml:id="formula_12">z t = m(v t , z t−1 )<label>(7)</label></formula><p>where m is a standard LSTM and v t a summary variable of the state of the system, normally v t = [x t ; h t−1 ] (we use [· ; ·] to denote concatenation). The potential benefit of using a recurrent model is that it is able to retain a separate memory that facilitates learning of local, sub-sequence specific patterns <ref type="bibr" target="#b23">(Ha et al., 2017)</ref>. Generally, we find that the recurrent model converges faster and generalizes marginally better. To extend the adaptive feed-forward layer to the LSTM, index sub-policies with a tuple (s, j) ∈ {i, f, o, z} × {0, 1, 2, 3, 4} such that D (s,j) t = diag(π (s,j) (z t )). At each time step t we adapt the LSTM's linear transformations through IO-adaptation,</p><formula xml:id="formula_13">u s t = D (s,4) t W (s) D (s,3) t x t +D (s,2) t V (s) D (s,1) t h t−1 +D (s,0) t b (s) .<label>(8)</label></formula><p>An undesirable side-effect of the formulation in eq. 8 is that each linear transformation requires its own modified input, preventing a vectorized implementation of the LSTM. We avoid this by tying all input adaptations across s: that is,</p><formula xml:id="formula_14">D (s ,j) = D (s,j) for all (s , j) ∈ {i, f, o, z} × {1, 3}.</formula><p>Doing so approximately halves the computation time and speeds up convergence considerably. When stacking multiple aLSTM layers, the computational graph of the model becomes complex in that it extends both in the temporal dimension and along the depth of the stack. For the recurrent adaptation policy (eq. 7) to be consistent, it should be conditioned not only by the latent variable in its own layer, but also on that of the preceding layer, or it will not have a full memory of the computational graph. To achieve this, for a layer l ∈ {1, . . . , L}, we define the input summary variable as</p><formula xml:id="formula_15">v (l) t = h (l−1) t ; h (l) t−1 ; z (l−1) t ,<label>(9)</label></formula><p>where h</p><formula xml:id="formula_16">(0) t = x t and z (0) t = z (L)</formula><p>t−1 . In doing so, the credit assignment path of adaption policy visits all nodes in the computational graph. The resulting adaptation model becomes a blend of a standard LSTM and a Recurrent Highway Network (RHN; <ref type="bibr" target="#b54">Zilly et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Adaptive parameterization is a special case of having a relatively inexpensive learning algorithm search a vast parameter space in order to parameterize the larger main model <ref type="bibr" target="#b48">(Stanley et al., 2009;</ref><ref type="bibr" target="#b14">Fernando et al., 2016)</ref>. The notion of using one model to generate context-dependent parameters for another was suggested by <ref type="bibr" target="#b47">Schmidhuber (1992)</ref>; <ref type="bibr" target="#b19">Gomez &amp; Schmidhuber (2005)</ref>. Building on this idea, <ref type="bibr" target="#b23">Ha et al. (2017)</ref> proposed to jointly train a small network to generate the parameters of a larger network; such HyperNetworks have achieve impressive results in several domains <ref type="bibr" target="#b49">(Suarez, 2017;</ref><ref type="bibr" target="#b22">Ha &amp; Eck, 2018;</ref><ref type="bibr" target="#b5">Brock et al., 2018)</ref>. The general concept of learning to parameterize a model has been explored in a variety of contexts, for example <ref type="bibr" target="#b47">Schmidhuber (1992)</ref> Parameter adaptation has also been explored in meta-learning, usually in the context of few-shot learning, where a meta-learner is trained across a set of tasks to select task-specific parameters of a downstream model <ref type="bibr" target="#b3">(Bengio et al., 1991</ref><ref type="bibr" target="#b2">(Bengio et al., , 1995</ref><ref type="bibr" target="#b47">Schmidhuber, 1992)</ref>. Similar to adaptive parameterization, <ref type="bibr" target="#b4">Bertinetto et al. (2016)</ref> directly tasks a meta learner with predicting the weights of the task-specific learner. <ref type="bibr" target="#b45">Ravi &amp; Larochelle (2017)</ref> defines the adaptation policy as a gradient-descent rule, where the meta learner is an LSTM tasked with learning the update rule to use. An alternative method pre-defines the adaptation policy as gradient descent and meta-learns an initialization such that performing gradient descent on a given input from some new task yields good task-specific parameters <ref type="bibr" target="#b15">(Finn et al., 2017;</ref><ref type="bibr" target="#b34">Lee &amp; Choi, 2017;</ref><ref type="bibr" target="#b0">Al-Shedivat et al., 2018)</ref>.</p><p>Using gradient information to adjust parameters has also been explored in sequence-to-sequence learning, where it is referred to as dynamic evaluation <ref type="bibr" target="#b39">(Mikolov, 2012;</ref><ref type="bibr" target="#b21">Graves, 2013;</ref><ref type="bibr" target="#b32">Krause et al., 2017)</ref>. This form of adaptation relies on the auto-regressive property of RNNs to adapt parameters at each time step by taking a gradient step with respect to one or several previous time steps.</p><p>Many extensions have been proposed to the basic RNN and the LSTM model <ref type="bibr" target="#b25">(Hochreiter &amp; Schmidhuber, 1997;</ref><ref type="bibr" target="#b18">Gers et al., 2000)</ref>, some of which can be seen as implementing a form of constrained adaptation policy. The multiplicative RNN (mRNN; <ref type="bibr" target="#b50">Sutskever et al., 2011)</ref> and the multiplicative LSTM (mLSTM; <ref type="bibr" target="#b31">Krause et al., 2016)</ref> can be seen as implementing an SVA policy for the hidden-tohidden projections. mRNN improves upon RNNs in language modeling tasks <ref type="bibr" target="#b50">(Sutskever et al., 2011;</ref><ref type="bibr" target="#b41">Mikolov et al., 2012)</ref>, but tends to perform worse than the standard LSTM <ref type="bibr" target="#b10">(Cooijmans et al., 2016)</ref>. mLSTM has been shown to improve upon RNNs and LSTMs on language modeling tasks <ref type="bibr" target="#b32">(Krause et al., 2017;</ref><ref type="bibr" target="#b44">Radford et al., 2017)</ref>. The multiplicative-integration RNN and its LSTM version <ref type="bibr" target="#b51">(Wu et al., 2016)</ref> essentially implement a constrained output-adaptation policy.</p><p>The implicit policies in the above models conditions only on the input, ignoring the state of the system. In contrast, the GRU <ref type="bibr" target="#b8">Chung et al., 2014)</ref> can be interpreted as implementing an input-adaptation policy on the input-to-hidden matrix that conditions on both the input and the state of the system. Most closely related to the aLSTM are HyperNetworks <ref type="bibr" target="#b23">(Ha et al., 2017;</ref><ref type="bibr" target="#b49">Suarez, 2017)</ref>; these implement output adaptation conditioned on both the input and the state of the system using a recurrent adaptation policy. HyperNetworks have attained impressive results on character level modeling tasks and sequence generation tasks, including hand-writing and drawing sketches <ref type="bibr" target="#b23">(Ha et al., 2017;</ref><ref type="bibr" target="#b22">Ha &amp; Eck, 2018)</ref>. They have also been used in neural architecture search by generating weights conditional on the architecture <ref type="bibr" target="#b5">(Brock et al., 2018)</ref>, demonstrating that adaptive parameterization can be conditioned on some arbitrary context, in this case the architecture itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We compare the behavior of a model with adaptive feed-forward layers to standard feed-forward baselines in a controlled regression problem and on MNIST <ref type="bibr" target="#b33">(LeCun et al., 1998</ref>). The aLSTM is tested on the Penn Treebank and WikiText-2 word modeling tasks. We use the ADAM optimizer <ref type="bibr" target="#b29">(Kingma &amp; Ba, 2015)</ref> unless otherwise stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Extreme Tail Regression</head><p>To study the flexibility of the adaptive feed-forward layer, we sample x = (x 1 , x 2 ) from N (0, I) and construct the target variable as y = (2x 1 ) 2 − (3x 2 ) 4 + with ∼ N (0, 1). Most of the data lies on a hyperplane, but the target variable grows or shrinks exponentially as x 1 or x 2 moves away from 0. We compare a 3-layer feed-forward network with 10 hidden units to a 2-layer model with 2 hidden units, where the first layer is adaptive and the final layer is static. We use an SVA policy where π is a gated linear unit <ref type="bibr" target="#b12">(Dauphin et al., 2017)</ref>. Both models are trained for 10 000 steps with a batch size of 50 and a learning rate of 0.003. The baseline model fails to represent the tail of the distribution despite being three times larger. In contrast, the adaptive model does a remarkably good job given how small the model is and the extremity of the distribution. It is worth noting how the adaptation policy encodes local information through the distribution of its singular values ( <ref type="figure" target="#fig_2">fig. 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">MNIST</head><p>We compare performance of a 3-layer feed-forward model against (a) a single-layer SVA model and (b) a 3-layer SVA model. We train all models with Stochastic Gradient Descent with a learning rate of 0.001, a batch size of 128, and train for 50 000 steps. The single-layer adaptive model reduces to a logistic regression conditional on the input. By comparing it to a logistic regression, we measure the marginal benefit of the SVA policy to approximately 1 percentage point gain in accuracy. In fact, if the one-layer SVA model has a sufficiently expressive adaptation model it matches and even outperforms the deep feed-forward baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Penn Treebank</head><p>The Penn Treebank corpus (PTB; <ref type="bibr" target="#b35">Marcus et al., 1993;</ref><ref type="bibr" target="#b40">Mikolov et al., 2010</ref>) is a widely used benchmark for language modeling. It consists of heavily processed news articles and contains no capital letters, numbers, or punctuation. As such, the vocabulary is relatively small at 10 000 unique words. We evaluate the aLSTM on word-level modeling following standard practice in training setup (e.g. <ref type="bibr" target="#b53">Zaremba et al., 2015)</ref>. As we are interested in statistical efficiency, we fix the number of layers to 2, though more layers tend to perform better, and use a policy latent variable size of 100. For details on hyper-parameters, see supplementary material. As we are evaluating underlying architectures, we do not compare against bolt-on methods <ref type="bibr" target="#b20">(Grave et al., 2017;</ref><ref type="bibr" target="#b52">Yang et al., 2018;</ref><ref type="bibr" target="#b39">Mikolov, 2012;</ref><ref type="bibr" target="#b21">Graves, 2013;</ref><ref type="bibr" target="#b32">Krause et al., 2017)</ref>. These are equally applicable to the aLSTM.  <ref type="bibr" target="#b38">Merity et al., 2018)</ref>. Drops correspond to learning rate cuts.</p><p>The aLSTM improves upon previously published results using roughly 30% fewer parameters, a smaller hidden state size, and fewer layers while converging in fewer iterations <ref type="table" target="#tab_2">(table 2)</ref>. Notably, for the standard LSTM to converge at all, gradient clipping is required and dropout rates must be reduced by~25%. In our experimental setup, a percentage point change to these rates cause either severe overfitting or failure to converge. Taken together, this indicates that adaptive parameterization enjoys both superior stability properties and substantially increases model capacity, even when the baseline model is complex; we explore both further in sections sections 5.5 and 5.6. <ref type="bibr" target="#b36">Melis et al. (2018)</ref> applies a large-scale hyper-parameter search to an LSTM version with tied input and forget gates and inter-layer skip-connections (TG-SC LSTM), making it a challenging baseline that the aLSTM improves upon by a considerable margin.</p><p>Previous state-of-art performance was achieved by the ASGD Weight-Dropped LSTM (AWD-LSTM; <ref type="bibr" target="#b38">Merity et al., 2018)</ref>, which uses regularization, optimization, and fine-tuning techniques designed specifically for language modeling 4 . The AWD-LSTM requires approximately 500 epochs to converge to optimal performance; the aLSTM outperforms the AWD-LSTM after 144 epochs and converges to optimal performance in 180 epochs. Consequently, even if the AWD-LSTM runs on top of the CuDNN implementation of the LSTM, the aLSTM converges approximately~25% faster in wall-clock time. In summary, any form of adaptation is beneficial, and a recurrent adaptation model (eq. 7) enjoys both fastest convergence rate and best final performance in this experiment.  <ref type="bibr" target="#b53">Zaremba et al. (2015)</ref> use tied input and output embeddings <ref type="bibr" target="#b43">(Press &amp; Wolf, 2017</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation Study</head><p>We isolate the effect of each component in the aLSTM through an ablation study on PTB. We adjust the hidden state to ensure every model has approximately 17M learnable parameters. We use the same hyper-parameters for all models except for (a) the standard LSTM (see above) and (b) the aLSTM under an output-adaptation policy and a feed-forward adaptation model, as this configuration needed slightly lower dropout rates to converge to good performance.</p><p>As table 4 shows, any form of adaptation yields a significant performance gain. Going from a feed-forward adaptation model (eq. 6) to a recurrent adaptation model (eq. 7) yields a significant improvement irrespective of policy, and our hybrid RHN-LSTM (eq. 9) provides a further boost. Similarly, moving from a partial adaptation policy to IO-adaptation leads to significant performance improvement under any adaptation model. These results indicate that the LSTM is constrained by the activation function bottleneck and increasing its adaptive capacity breaks the bottleneck. 58.5 56.5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Robustness</head><p>We further study the robustness of the aLSTM with respect to hyper-parameters. We limit ourselves to dropout rates and train for 10 epochs on PTB. All other hyper-parameters are held fixed. For each model, we draw 100 random samples uniformly from intervals of the form [r − 0.1, r + 0.1], with r being the optimal rate found through previous hyper-parameter tuning. The two models exhibit very different distributions <ref type="figure" target="#fig_4">(fig. 4</ref>). The distribution of the aLSTM is tight, reflecting robustness with respect to hyper-parameters. In fact, no sampled model fails to converge. In contrast, approximately 25% of the population of LSTM configurations fail to converge. In fact, fully 45% of the LSTM population fail to outperform the worst aLSTM configuration; the 90 th percentile of the aLSTM distribution is on the same level as the 10 th percentile of the LSTM distribution. On WT-2 these results are amplified, with half of the LSTM population failing to converge and 80% of the LSTM population failing to outperform the worst-case aLSTM configuration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>By viewing deep neural networks as adaptive compositions of linear maps, we have showed that standard activation functions induce an activation function bottleneck because they fail to have significant non-linear effect on a non-trivial subset of inputs. We break this bottleneck through adaptive parameterization, which allows the model to adapt the affine transformation to the input.</p><p>We have developed an adaptive feed-forward layer and showed empirically that it can learn patterns where a deep feed-forward network fails whilst also using fewer parameters. Extending the adaptive feed-forward layer to RNNs, we presented an adaptive LSTM that significantly increases model capacity and statistical efficiency while being more robust to hyper-parameters. In particular, we obtain new state of the art results on the Penn Treebank and the WikiText-2 word-modeling tasks, using~20-30% fewer parameters and converging in less than half as many iterations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>; Gomez &amp; Schmidhuber (2005); Denil et al. (2013); Jaderberg et al. (2017); Andrychowicz et al. (2016); Yang et al. (2018).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Extreme tail regression. Left: Predictions of the adaptive model (blue) and the baseline model (green) against ground truth (black). Center &amp; Right: distribution of adaptive singular values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Validation loss on PTB for our LSTM (green), aLSTM (blue), aLSTM with static policy (dashed), and the AWD-LSTM (orange; Merity et al., 2018). Drops correspond to learning rate cuts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Distribution of validation scores on WikiText-2 (top) and Penn Treebank (bottom) for randomly sampled hyper-parameters. The aLSTM (blue) is more robust than the LSTM (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Figure 1: Adaptation policies. Left: output adaptation shifts the mean of each row in W ; center left: input adaptation shifts the mean of each column; center right: IO-adaptation shifts mean and variance across sub-matrices; Right: SVA scales singular values.</figDesc><table>1 We 

1 Code available at https://github.com/flennerhag/alstm. 

32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada. 

arXiv:1805.08574v4 [cs.LG] 22 Nov 2018 

DW 

W D 
D 
(2) W D 

(1) 

W 
(2) DW 

(1) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Train and test set accuracy on MNIST</figDesc><table>Model 
Size 
Train 
Test 

Logistic Regression 
8K 92.00% 92.14% 
3-layer feed-forward 100K 97.57% 97.01% 

1-layer SVA 
8K 94.05% 93.86% 
1-layer SVA 
100K 98.62% 97.14% 
3-layer SVA 
100K 99.99% 97.65% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Validation and test set perplexities on Penn Treebank. All results except those from</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>).It is about twice as large with three times as many unique tokens. We evaluate the aLSTM using the same settings as on PTB, and additionally test a version with larger hidden state size to match the parameter count of current state of the art models. Without tuning for WT2, both outperform previously published results in 150 epochs (table 3) and converge to new state of the art performance in 190 epochs. In contrast, the AWD-LSTM requires 700 epochs to reach optimal performance. As such, the aLSTM trains~40% faster in wall-clock time. The TG-SC LSTM in Melis et al. (2018) uses fewer parameters, but its hyper-parameters are tuned for WT2, in contrast to both the AWD-LSTM and aLSTM. We expect that tuning hyper-parameters specifically for WT2 would yield further gains.</figDesc><table>Model 
Size Depth Valid Test 

LSTM, Zaremba et al. (2015) 
24M 
2 
82.2 78.4 
RHN, Zilly et al. (2016) 
24M 
10 
67.9 65.4 
NAS, Zoph &amp; Le (2017) 
54M 
-
-62.4 
TG-SC LSTM, Melis et al. (2018) 10M 
4 
62.4 60.1 
TG-SC LSTM, Melis et al. (2018) 24M 
4 
60.9 58.3 
AWD-LSTM, Merity et al. (2018) 24M 
3 
60.0 57.3 

LSTM 

20M 
2 
71.7 68.9 
aLSTM, static policy (eq. 6) 
17M 
2 
60.2 58.0 
aLSTM, recurrent policy (eq. 7) 
14M 
2 
59.6 57.2 
aLSTM, recurrent policy (eq. 7) 
17M 
2 
58.7 56.5 
aLSTM, recurrent policy (eq. 7) 
24M 
2 
57.6 55.3 

5.4 WikiText-2 

WikiText-2 (WT2; Merity et al., 2017) is a corpus curated from Wikipedia articles with lighter 
processing than PTB. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Validation and test set perplexities on WikiText-2.</figDesc><table>Model 
Size Depth Valid Test 

LSTM, Grave et al. (2017) 
-
-
-99.3 
LSTM, Inan et al. (2017) 
22M 
3 
91.5 87.7 
AWD-LSTM, Merity et al. (2018) 33M 
3 
68.6 65.8 
TG-SC LSTM, Melis et al. (2018) 24M 
2 
69.1 65.9 

aLSTM, recurrent policy (eq. 7) 
27M 
2 
68.1 65.5 
aLSTM, recurrent policy (eq. 7) 
32M 
2 
67.5 64.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 :</head><label>4</label><figDesc>Ablation study: perplexities on Penn Treebank. † Equivalent to the HyperNetwork, except the aLSTM uses one projection from z to π instead of nesting two (Ha et al., 2017).</figDesc><table>Model 
Adaptation model Adaptation policy Valid Test 

LSTM 

-
-
71.7 68.9 

aLSTM 
feed-forward output-adaptation 
66.0 63.1 
aLSTM 

 † 

LSTM 

output-adaptation 
59.9 58.2 
aLSTM 

LSTM-RHN 

output-adaptation 
59.7 57.3 

aLSTM 
feed-forward 

IO-adaptation 

61.6 59.1 
aLSTM 

LSTM 
IO-adaptation 

59.0 56.9 
aLSTM 

LSTM-RHN 
IO-adaptation 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This holds almost everywhere, but not for {a | ai = 0, ai ∈ a}. Being measure 0, we ignore this exception.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The ordering of W and D matrices can be reversed by setting the first and / or last adaptation matrix to be the identity matrix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Public release of their code at https://github.com/salesforce/awd-lstm-lm</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank anonymous reviewers for their comments. This work was supported by ESRC via the North West Doctoral Training Centre, grant number ES/J500094/1.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of property 1</head><p>Property 1 asserts that for given W and x, for any G of same dimensionality as W , there are arbitrarily</p><p>We now prove this property:</p><p>ii and write</p><p>Consequently, choose some k for which x k = 0 and set d</p><p>(1)</p><p>Moreover, since this holds for any x k = 0, every linear combination of such solutions are also solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyper-parameters for PTB and WT2</head><p>We use tied embedding weights <ref type="bibr" target="#b43">(Press &amp; Wolf, 2017;</ref><ref type="bibr" target="#b27">Inan et al., 2017</ref>) and a weight decay rate of 10 −6 . The initial learning rate is set to 0.003 with decay rates β 1 = 0 and β 2 = 0.999. The learning rate is cut after epochs 100 and 160 by a factor of 10. We use a batch size of 20 and variable truncated backprogagation through time centered at 70, as in <ref type="bibr" target="#b38">Merity et al. (2018)</ref>. Dropout rates were tuned with a coarse random search. We apply variational dropout <ref type="bibr" target="#b17">(Gal &amp; Ghahramani, 2016)</ref> to word and word embedding vectors, the policy latent variable, the hidden state of the aLSTM and the final aLSTM output with drop probabilities 0.16, 0.6, 0.1, 0.25, and 0.6 respectively.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Continuous adaptation via meta-learning in nonstationary and competitive environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Al-Shedivat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trapit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Misha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sergio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3981" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the optimization of a synaptic learning rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoshua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Cloutier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gecsei</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimality in Biological and Artificial Networks</title>
		<imprint>
			<date type="published" when="1995-01" />
			<biblScope unit="page" from="6" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning a synaptic learning rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Cloutier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>Université de Montréal, Département d&apos;informatique et de recherche opérationnelle</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning feedforward one-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="523" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SMASH: One-shot model architecture search through hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Theo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An analysis of deep neural network models for practical applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfredo</forename><surname>Canziani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenio</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07678</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Çaglar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fethi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Emperical Methods in Natural Language Processing</title>
		<meeting>Emperical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Çaglar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. arXiv preprint</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Djork-Arné</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sepp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent Batch Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">César</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MCSS</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Language Modeling with Gated Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predicting parameters in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shakibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2148" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Banarse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malcolm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Besse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frederic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Max</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<title level="m">Convolution by Evolution -Differentiable Pattern Producing Networks. GECCO</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03635</idno>
		<title level="m">The lottery ticket hypothesis: Training pruned neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to Forget: Continual Prediction with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evolving modular fast-weight networks for control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="383" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving Neural Language Models with a Continuous Cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<title level="m">Generating Sequences With Recurrent Neural Networks. arXiv preprint</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A neural representation of sketch drawings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
		<title level="m">HyperNetworks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Delving Deep into RectifiersSurpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="80" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Inan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Decoupled neural interfaces using synthetic gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Günter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sepp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="972" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multiplicative lstm for sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<idno>arXiv:1609:07959</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kahembwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<idno>arXiv:1709:07432</idno>
		<title level="m">Dynamic Evaluation of Neural Sequence Models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Léon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="9" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Meta-Learning with Adaptive Layerwise Metric and Subspace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: the Penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the State of the Art of Evaluation in Neural Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Regularizing and optimizing LSTM language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shirish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Statistical language models based on neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanjeev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interspeech</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Subword language modeling with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anoop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hai-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cernocky</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012-01" />
		</imprint>
	</monogr>
<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sensitivity and generalization in neural networks: an empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yasaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Abolafia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Chapter</title>
		<meeting>the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning to Generate Reviews and Discovering Sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01444</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Surya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6120</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning to control fast-weight memories: An alternative to dynamic recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="139" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A hypercube-based encoding for evolving large-scale neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">B</forename><surname>Gauci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Life</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="212" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Character-level language modeling with recurrent highway hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Suarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3269" to="3278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">On Multiplicative Integration with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saizheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2864" to="2872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Breaking the Softmax Bottleneck: A High-Rank RNN Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zihang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Recurrent Neural Network Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Zilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Georg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.03474</idno>
		<title level="m">Recurrent Highway Networks. arXiv preprint</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Neural Architecture Search with Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
