<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supervision-by-Registration: An Unsupervised Approach to Improve the Precision of Facial Landmark Detectors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
							<email>xuanyi.dong@student.uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CAI</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoou-I</forename><surname>Yu</surname></persName>
							<email>shoou-i.yu@fb.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Facebook Reality Labs</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
							<email>xinshuo.weng@fb.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Facebook Reality Labs</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
							<email>shih-en.wei@fb.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Facebook Reality Labs</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yi.yang@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CAI</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
							<email>yaser.sheikh@fb.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Facebook Reality Labs</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Supervision-by-Registration: An Unsupervised Approach to Improve the Precision of Facial Landmark Detectors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In this paper, we present supervision-by-registration, an unsupervised approach to improve the precision of facial landmark detectors on both images and video. Our key observation is that the detections of the same landmark in adjacent frames should be coherent with registration, i.e., optical flow. Interestingly, coherency of optical flow is a source of supervision that does not require manual labeling, and can be leveraged during detector training. For example, we can enforce in the training loss function that a detected landmark at frame t−1 followed by optical flow tracking from frame t−1 to frame t should coincide with the location of the detection at frame t . Essentially, supervisionby-registration augments the training loss function with a registration loss, thus training the detector to have output that is not only close to the annotations in labeled images, but also consistent with registration on large amounts of unlabeled videos. End-to-end training with the registration loss is made possible by a differentiable Lucas-Kanade operation, which computes optical flow registration in the forward pass, and back-propagates gradients that encourage temporal coherency in the detector. The output of our method is a more precise image-based facial landmark detector, which can be applied to single images or video. With supervision-by-registration, we demonstrate (1) improvements in facial landmark detection on both images (300W, ALFW) and video (300VW, Youtube-Celebrities), and (2) significant reduction of jittering in video detections.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Precise facial landmark detection lays the foundation for high quality performance of many computer vision and computer graphics tasks, such as face recognition <ref type="bibr" target="#b14">[15]</ref>, face animation <ref type="bibr" target="#b1">[2]</ref> and face reenactment <ref type="bibr" target="#b31">[32]</ref>. Many face recognition methods rely on locations of detected facial land- * Work done during an internship at Facebook Oculus <ref type="figure">Figure 1</ref>. Annotations are imprecise. We show annotations of nine annotators on two images of the mouth. Each color indicates a different landmark. Note the inconsistencies of annotations even on the more discriminative landmarks such as the corner of the mouth. This could be harmful to both the training and evaluation of detectors, thus motivating the use of supervisory signals which does not rely on human annotations. marks to spatially align faces, and imprecise landmarks could lead to bad alignment and degrade face recognition performance. In face animation and reenactment methods, 2D landmarks are used as anchors to deform 3D face meshes toward realistic facial performances, so temporal jittering of 2D facial landmark detections in video will be propagated to the 3D face mesh and could generate perceptually jarring results.</p><p>Precise facial landmark detection is still an unsolved problem. While significant work has been done on imagebased facial landmark detection <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref>, these detectors tend to be accurate but not precise, i.e., the detector's bias is small but variance is large. The main causes could be: <ref type="bibr" target="#b0">(1)</ref> insufficient training samples and (2) imprecise annotations, as human annotations inherently have limits on precision and consistency as shown in <ref type="figure">Figure 1</ref>. As a result, jittering is observed when we apply the detector independently to each video frame, and the detected landmark does not adhere well to an anatomically defined point (e.g., mouth corner) on the face across time. Other methods that focus on video facial landmark detection <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> utilize both detections and tracking to combat jittering and increase precision, but these methods require per-frame annotations in video, which are (1) tedious to annotate due to the sheer volume of video frames and (2) difficult to annotate consistently across frames, even for temporally adjacent frames. Therefore, precise facial landmark detection might not be simply solved with large amounts of human annotations.</p><p>Instead of completely relying on human annotations, we present Supervision-by-Registration (SBR), which augments the training loss function with supervision automatically extracted from unlabeled videos. The key observation is that the coherency of (1) the detections of the same landmark in adjacent frames and (2) registration, i.e., optical flow <ref type="bibr" target="#b17">[18]</ref>, is a source of supervision. This supervision can complement the existing human annotations during the training of the detector. For example, a detected landmark at frame t−1 followed by optical flow tracking between frame t−1 and frame t should coincide with the location of the detection at frame t . So, if the detections are incoherent with the optical flow, the amount of mismatch is a supervisory signal enforcing the detector to be temporally consistent across frames, thus enabling a SBR-trained detector to better locate the correct location of a landmark that is hard to annotate precisely. The key advantage of SBR is that no annotations are required, thus the training data is no longer constrained by the quantity and quality of human annotations.</p><p>The overview of our method is shown in <ref type="figure">Figure 2</ref>. Our end-to-end trainable model consists of two components: a generic detector built on convolutional networks <ref type="bibr" target="#b15">[16]</ref>, and a differentiable Lucas-Kanade (LK, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18]</ref>) operation. During the forward pass, the LK operation takes the landmark detections from the past frame and estimates their locations in the current frame. The tracked landmarks are then compared with the direct detections on the current frame. The registration loss is defined as the offset between them. In the backward pass, the gradient from the registration loss is back-propagated through the LK operation to encourage temporal coherency in the detector. To ensure that the supervision from registration is reasonable, supervision is only enforced for landmarks whose optical flow pass the forward-backward check <ref type="bibr" target="#b11">[12]</ref>. The final output of our method is an enhanced image-based facial landmark detector which has leveraged large amounts of unlabeled video to achieve higher precision in both images and videos, and more stable predictions in videos.</p><p>Note that our approach is fundamentally different from post-processing such as temporal filtering, which often sacrifices precision for stability. Our method directly incorporates the supervision of temporal coherency during model training, thus producing detectors that are inherently more stable. Therefore, neither post-processing, optical flow tracking, nor recurrent units are required upon per-frame detection in test time. Also note that SBR is not regularization, which limits the freedom of model parameters to prevent overfitting. Instead, SBR brings more supervisory signals from registration to enhance the precision of the detector. In sum, SBR has the following benefits:   <ref type="figure">Figure 2</ref>. The supervision-by-registration (SBR) framework takes labeled images and unlabeled video as input to train an image-based facial landmark detector which is more precise on images/video and also more stable on video.</p><p>1. SBR can enhance the precision of a generic facial landmark detector on both images and video in an unsupervised fashion. 2. Since the supervisory signal of SBR does not come from annotations, SBR can utilize a very large amount of unlabeled video to enhance the detector. 3. SBR can be trained end-to-end with the widely used gradient back-propagation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Facial landmark detection is mainly performed on two modalities: images and video. In images, the detector can only rely on the static image to detect landmarks, whereas in video the detector has additional temporal information to utilize. Though image-based facial landmark detectors <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20]</ref> can achieve very good performance on images, sequentially running these detectors on each frame of a video in a tracking-by-detection fashion usually leads to jittering and unstable detections.</p><p>There are various directions for improving facial landmark detection in videos apart from tracking-by-detection. Pure temporal tracking <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref> is a common method but often suffer from tracker drift. Once the tracker has failed in the current frame, it is difficult to make the correct prediction in the following frames. Therefore, hybrid methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22]</ref> jointly utilize tracking-by-detection and temporal information in a single framework to predict more stable facial landmarks. Peng et al. <ref type="bibr" target="#b21">[22]</ref> and Liu et al. <ref type="bibr" target="#b16">[17]</ref> utilize recurrent neural networks to encode the temporal information across consecutive frames. Khan et al. <ref type="bibr" target="#b12">[13]</ref> uti-lize global variable consensus optimization to jointly optimize detection and tracking in consecutive frames. Unfortunately, these methods require per-frame annotations, which are resource-intensive to acquire. Our approach SBR shares the high-level idea of these algorithms by leveraging temporal coherency, but SBR does not require any video-level annotation, and is therefore capable of enhancing detectors from large numbers of unlabeled videos.</p><p>Other approaches utilize temporal information in video to construct person-specific models <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. Most of these methods usually leverage offline-trained static appearance models. The detector, which is used to generate initial landmark prediction, is not updated based on the tracking result in their algorithms, whereas SBR dynamically refines the detector based on LK tracking results. Self-training <ref type="bibr" target="#b40">[41]</ref> can also be utilized for creating person-specific models, and was shown to be effective in pose estimation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref>. However, unlike our method which can be trained end-to-end, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref> did alternating bootstrapping to progressively improve the detectors. This leads to longer training times, and also inaccurate gradient updates as detailed in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>SBR consists of two complementary parts, the general facial landmark detector and the LK tracking operation, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. The key idea of this framework is that we can directly perform back-propagation through the LK operation, thus enabling the detector before the LK operation to receive gradients which encourage temporal coherency across adjacent frames. LK was chosen because it is fully differentiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">LK Operation</head><p>Motivated by <ref type="bibr" target="#b3">[4]</ref>, we design an LK operation through which we can perform back-propagation. Given the feature F t−1 1 from frame t−1 and feature F t from the frame t , we estimate the parametric motion for a small patch near</p><formula xml:id="formula_0">x t−1 = [x, y]</formula><p>T from frame t−1 . The motion model is represented by the displacement warp function W (x; p). A displacement warp contains two parameters</p><formula xml:id="formula_1">p = [p 1 , p 2 ]</formula><p>T , and can be formulated as</p><formula xml:id="formula_2">W (x; p) = [x + p 1 , y + p 2 ]</formula><p>T . We leverage the inverse compositional algorithm <ref type="bibr" target="#b0">[1]</ref> for our LK operation. It finds the motion parameter p by minimizing</p><formula xml:id="formula_3">x∈Ω α x F t−1 (W (x; ∆p)) − F t (W (x; p)) 2 ,<label>(1)</label></formula><p>with respect to ∆p. Here, Ω is a set of locations in a patch centered at x t−1 , and α x = exp(−</p><formula xml:id="formula_4">||x−xt−1|| 2 2 2σ 2</formula><p>) is the weight value for x determined by the distance from x t−1 to downweight pixels further away from the center of the patch. Af- <ref type="bibr" target="#b0">1</ref> The features can be RGB images or the output of convolution layers. ter obtaining the motion parameter, the LK operation updates the warp parameter as follows:</p><formula xml:id="formula_5">W (x; p) ← W (W (x; ∆p) −1 ; p) = x + p 1 − ∆p 1 y + p 2 − ∆p 2 .<label>(2)</label></formula><p>p is an initial motion parameter (p = [0, 0] T in our case), which will be iteratively updated by Eq. (2) until convergence.</p><p>The first order Taylor expansion on Eq. (1) gives:</p><formula xml:id="formula_6">x∈Ω αx Ft−1(W (x; 0)) + ∇Ft−1 ∂W ∂p ∆p − Ft(W (x; p)) 2<label>(3)</label></formula><p>We then have the solution to Eq. (3) according to <ref type="bibr" target="#b0">[1]</ref>:</p><formula xml:id="formula_7">∆p = H −1 x∈Ω J(x) T αx(Ft(W (x; p)) − Ft−1(W (x; 0))),<label>(4)</label></formula><p>where</p><formula xml:id="formula_8">H = J T AJ ∈ R 2×2 is the Hessian matrix. J ∈ R</formula><p>C|Ω|×2 is the vertical concatenation of J(x) ∈ R C×2 , x ∈ Ω, which is the Jacobian matrix of F t−1 (W (x; 0))). C is the number of channels of F. A is a diagonal matrix, where elements in the main diagonal are the α x 's corresponding to the x's used to create J. H and J are constant over iterations and can thus be pre-computed.</p><p>We illustrate the detailed steps of the LK operation in <ref type="figure">Figure 4</ref>, and describe it in Algorithm 1. We define the LK operation asL t = G(  <ref type="formula" target="#formula_5">(2)</ref> xt-1 + p xt <ref type="figure">Figure 4</ref>. Overview of the LK operation. This operation takes the features of two adjacent frames, i.e., Ft−1 and Ft, and a location xt−1 at framet−1 as inputs. The inverse compositional LK algorithm iteratively updates the motion parameter p and outputs the corresponding coordinates xt at framet. The iterative portion of the algorithm is indicated by the red arrows. Every step of this process is differentiable, thus gradients can back-propagate from xt to Ft−1, Ft and xt−1.</p><formula xml:id="formula_9">F t−1 , F t , L t−1 ). This function takes a matrix L t−1 = [x 1 t−1 , x 2 t−1 , ..., x K t−1 ] ∈ R 2×K ,</formula><p>frame. Since, all steps in the LK operation are differentiable, the gradient can back-propagate to the facial landmark locations and the feature maps through LK.</p><p>We apply a very small value to the diagonal elements of H. This ensures that H is invertible. Also, in order to crop a patch at a sub-pixel location x, we use the spatial transformer network <ref type="bibr" target="#b10">[11]</ref> to calculate the bilinear interpolated values of the feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Supervision-by-Registration</head><p>We describe the details of the two complementary losses: the detection loss based on human annotations and the registration loss used to enforce temporal coherency.</p><p>Detection loss. Many facial landmark detectors take an image I as input and regresses to the coordinates of the facial landmarks, i.e., D(I) = L. They usually apply an L 2 loss on these coordinates L with the ground-truth labels L * , i.e., ℓ det = ||L − L * || 2 2 . Other methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b19">20]</ref> predict a heat-map rather than the coordinates for each landmark, and the L 2 loss is usually applied on the heatmap during the training procedure. During testing, instead of directly regressing the location of the landmarks, the argmax operation is used on the heatmap to obtain the location of the landmark. Unfortunately, the argmax operation is not differentiable, so these methods cannot be directly used with our LK operation. To enable the information to be back-propagated through the predicted coordinates in heatmap-based methods, we replace the argmax operation with a heatmap peak finding operation which is based on a weighted sum of heatmap confidence scores. Let M be the predicted heatmap. For each landmark, we first compute a coarse location using </p><formula xml:id="formula_10">arg max M = [x ′ , y ′ ] T .</formula><note type="other">for iter = 1; iter ≤ max; iter++ do 4. Extract target feature from Ft centered at xt−1 + p 5. Compute the error of the template and target features 6. Compute ∆p using Eq. (4) 7. Update the motion model p using Eq. (2) end for Output: xt</note><formula xml:id="formula_11">= xt−1 + p M ′ = {(i, j) | i ∈ [x ′ − r, x ′ + r], j ∈ [y ′ − r, y ′ + r]}.</formula><p>Lastly, we use the soft-argmax operation on this square region to obtain the final coordinates:</p><formula xml:id="formula_12">x = (i,j)∈M ′ M i,j × [i, j] T (i,j)∈M ′ M i,j .<label>(5)</label></formula><p>Since Eq. <ref type="formula" target="#formula_12">(5)</ref> is differentiable, we can utilize this peak finding operation to incorporate heatmap-based methods into our framework. Note that when we train with different kinds of networks, we can still use the original loss functions and settings described in their papers <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">40]</ref>, but for simplicity, we still denote in this paper the detection loss as the L 2 distance between the predicted and groundtruth coordinates. Registration loss. Registration loss can be computed in an unsupervised manner to enhance the detector. It is realized with a forward-backward communication scheme between the detection output and the LK operation, as shown in <ref type="figure" target="#fig_2">Figure 5</ref>. The forward communication computes the registration loss while the backward communication evaluates the reliability of the LK operation.</p><p>In the forward communication, the detector passes the detected landmarks of past frames (e.g., L t−1 for frame t−1 ) to the LK operation. The LK operation then generates new landmarks of future frames (e.g.,L t = G(F t−1 , F t , L t−1 ) for frame t ) by tracking. LK-generated landmarks for future frames should be spatially near the detections in the future frames. Therefore, the registration loss directly computes the distance between the LK operation's predictions (green dots in <ref type="figure" target="#fig_2">Figure 5</ref>) and the detector's predictions (blue dots in <ref type="figure" target="#fig_2">Figure 5</ref>), thus encouraging the detector to be more temporally consistent. The loss is as follows: L t,i andL t,i denote the i-th row of L t andL t , which correspond to the i-th landmark location. β t,i ∈ {0, 1} indicates the reliability of the i-th tracked landmark at time t, which is determined by the backward communication scheme. The LK tracking may not always succeed, and supervision should not be applied when LK tracking fails. Therefore, the backward communication stream utilizes the forward-backward check <ref type="bibr" target="#b11">[12]</ref> to evaluate the reliability of LK tracking. Specifically, the LK operation takesL t as input and generates the landmarks of frame t−1 by tracking in reverse order, formulated as:</p><formula xml:id="formula_13">ℓ t regi = K i=1 β t,i ||L t,i −L t,i || 2 (6) = K i=1 β t,i ||L t,i − G(F t−1 , F t , L t−1,i )|| 2 .</formula><formula xml:id="formula_14">L t−1 = G(F t , F t−1 ,L t ).</formula><p>Our premise is that if the LK operation output is reliable, a landmark should return to the same location after forwardbackward tracking. Therefore, if the backward tracks are reliable, then β t,i = 1 else β t,i = 0, i.e., this point is not included in the registration loss. Since only reliable tracks will be used, the forward-backward communication scheme ensures that the registration loss yields improvement in performance when unlabeled data are exploited. Note that the registration loss is not limited to adjacent frames and can be applied to a sequence of frames as shown in <ref type="figure" target="#fig_2">Figure 5</ref>.</p><p>Complete loss function. Let N be the number of training samples with ground truth. For notation brevity, we assume there is only one unlabeled video with T frames. Then, the complete loss function of SBR is as follows:</p><formula xml:id="formula_15">ℓ final = N n=1 ℓ n det + γ T −1 t=1 ℓ t regi ,<label>(7)</label></formula><p>which is a weighted combination of the detection and registration loss controlled by the weight parameter γ. Computation Complexity. The computational cost of the LK module consists of two parts, the pre-computed operations, and iterative updating. The cost of the first part is O(C|Ω|), where C is the channel size of the input (C = 3 for RGB images), and |Ω| is the patch size used in LK which is usually less than 10 × 10. The second part is O(T C|Ω|), where T is the number of iterations and usually less than 20. Therefore, for all K landmarks, the LK cost is O(KC|Ω|)+O(KCT |Ω|), which is negligible when compared to the complexity of evaluating a CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Personalized Adaptation Modeling</head><p>SBR can also be used to generate personalized facial landmark detectors, which is useful in (1) unsupervised adaptation to a testing video which may be in a slightly different domain than the training set and (2) generating the best possible detector for a specific person, e.g., a star actor in a movie. SBR can achieve this by treating testing videos as unlabeled videos and including them in training. During the training process, the detector can remember certain personalized details in an unsupervised fashion to achieve more precise and stable facial landmark detection. <ref type="bibr" target="#b27">[28]</ref> provides annotations for 3837 face images with 68 landmarks. We follow <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">40]</ref> to split the dataset into four sets, training, common testing, challenging testing, and full testing, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets 300-W</head><p>AFLW <ref type="bibr" target="#b14">[15]</ref> consists in total of 25993 faces in 21997 real-world images, where each face is annotated with up to 21 landmarks. Following <ref type="bibr" target="#b18">[19]</ref>, we ignore two landmarks of ears and only use the remaining 19 landmarks.</p><p>YouTube-Face <ref type="bibr" target="#b35">[36]</ref> contains 3425 short videos of 1595 different people. This dataset does not have facial landmark labels, but the large variety of people makes it very suitable to provide to SBR as unlabeled video. We filter videos with low resolution <ref type="bibr" target="#b1">2</ref> , and use the remaining videos to train SBR in an unsupervised way. 300-VW <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b5">6]</ref>. This video dataset contains 50 training videos with 95192 frames. The test set consists of three categories with different levels. These three subsets (A, B and C) have 62135, 32805 and 26338 frames, respectively. C is the most challenging one. Following <ref type="bibr" target="#b12">[13]</ref>, we report the results for the 49 inner points on subset A and C.</p><p>YouTube Celebrities <ref type="bibr" target="#b13">[14]</ref>. This dataset contains videos of 35 celebrities under varying poses, illumination and occlusion. Following the same setting as in <ref type="bibr" target="#b22">[23]</ref>, we perform PAM on the same six video clips as <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment Settings</head><p>Baselines. We exploit two facial landmark detectors as baselines on which we further perform SBR and PAM.</p><p>The first detector is CPM <ref type="bibr" target="#b34">[35]</ref>, which utilizes the ImageNet pre-trained models <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10]</ref> as the feature extraction part. In our experiment, we use the first four convolutional layers of VGG-16 <ref type="bibr" target="#b30">[31]</ref>  are cropped and resized into 256×256 for pre-processing. We train the CPM with a batch size of 8 for 40 epochs in total. The learning rate starts at 0.00005 and is reduced by 0.5 at 20th and 30th epochs. The second detector is a simple regression network, denoted as Reg. We use VGG-16 as our base model and change the output neurons of the last fully-connected layer to K ×2, where K is the number of landmarks. Since VGG-16 requires the input size to be 224×224, we thus resize the cropped face to 224×224 for this regression network. Following <ref type="bibr" target="#b16">[17]</ref>, we normalize the L 2 loss by the size of faces as the detection loss.</p><p>Training with LK. We perform LK tracking over three consecutive frames. For Ω in Eq. (1), we crop a 10 × 10 patch centered at the landmark. To cope with faces with different resolutions, we resize the images accordingly such that a 10 × 10 crop is a reasonable patch size. Too large or small patch size can lead to poor LK tracking and hurt performance. The maximum iterations of LK is 20 and the convergence threshold for ∆p = 10 −6 . For the input feature of the LK operation, we use the RGB image by default and also perform ablation studies when using the conv-1 feature layer (see <ref type="bibr">Section 5)</ref>. The weight of the registration loss is γ = 0.5. When training a model from scratch, we first make sure the detection loss has converged before activating the registration loss. When training with SBR, the ratio of labeled images and unlabeled video for each batch should be balanced. In the case when there are more unlabeled video than labeled images, we duplicate the labeled images such that the ratio is still balanced. Also, when applying SBR, one should confirm that the distribution of faces in unlabeled video is similar to the distribution of labeled images. Otherwise, the initial detector may perform poorly on the unlabeled videos, which leads to very few reliable LK tracks and a less effective PAM. All of our experiments are implemented in PyTorch <ref type="bibr" target="#b20">[21]</ref>.</p><p>Evaluation Metrics. Normalized Mean Error (NME) is used to evaluate the performance on images. Following <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25]</ref>, the interocular distance and face size is employed to   <ref type="table">Table 3</ref>. Comparisons of NME on YouTube Celebrities dataset.</p><p>normalize mean error on 300-W and AFLW respectively. We also use Cumulative Error Distribution (CED) <ref type="bibr" target="#b25">[26]</ref> and Area Under the Curve (AUC) <ref type="bibr" target="#b12">[13]</ref> for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on Image Datasets</head><p>In order to show that the proposed SBR can enhance generic landmark detectors, we show the results of SBR performed on both the Reg (regression-based) and CPM (heatmap-based) on AFLW and 300-W. We also compare against nine facial landmark detection algorithms.</p><p>Results on 300-W. As shown in <ref type="table">Table 1</ref>, the baseline CPM obtains competitive performance (4.36 NME) on the full testing set of 300-W. We then run SBR with unlabeled videos from YouTube-Face for both the CPM and Reg, which further improves the CPM by a relative 7% and the Reg by a relative 6% without using any additional annotation. The compared results are provided by the official 300-W organizer <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Results on AFLW. The distribution of face size on AFLW dataset is different from that of 300-W. Thus we resize the face to 176×176. <ref type="table">Table 1</ref> shows that SBR improves the CPM by a relative 9% and Reg by a relative 5%.</p><p>Overall, the SBR improves both CPMs and regression networks on 300-W and AFLW. We also achieve the stateof-the-art performance with CPMs. This demonstrates the flexibility and effectiveness of SBR. YouTube-Face is used as unlabeled videos in our experiments, but in hindsight, it may not be the best choice to enhance the detector on 300-W, because the size of faces in YouTube-Face is smaller than 300-W, and compression artifacts further affect LK tracking. By using a video dataset with higher resolution, our approach can potentially obtain higher performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on Video Datasets</head><p>To show that SBR/PAM can enhance a detector to produce coherent predictions across frames, we evaluate on 300VW. We follow <ref type="bibr" target="#b12">[13]</ref> and use the full training set. Since we lack images from the XM2VTS and FRGC datasets, we use the same number labeled images from an internal dataset instead of these two datasets. The images in 300-VW have a lower resolution than 300-W, thus we resize the images to 172×172 during training according to the face size statistics.</p><p>Results on 300VW. <ref type="table" target="#tab_5">Table 2</ref> shows that SBR improves the CPM by 1%, and PAM further improves it by 1.2%. t-Test shows a p-value of 0.0316 and 0.0001 when using SBR to enhance CPM and using PAM to enhance CPM+SBR. These two statistical significance tests demonstrate the improvement of SBR and PAM. We show that CPM+SBR+PAM achieves the state-of-the-art performance against all other methods. Importantly, SBR+PAM does not utilize any more annotations than what the baselines use.</p><p>Results on YouTube Celebrities. We also compare different personalized methods in <ref type="table">Table 3</ref>. The baselines Reg and CPM are pre-trained on 300-W. The proposed PAM reduces the error of CPM from 5.26 to 4.74, achieving stateof-the-art performance.</p><p>Qualitative comparison. <ref type="figure" target="#fig_3">Figure 6</ref> shows the qualitative results. CPM predictions are often incoherent across frames. For example, in the third row, the predictions on the eyebrow drifts, but SBR/PAM can produce more stable predictions as the coherency is satisfied during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Image Resolution. The resolution of the face can affect the chance of the success of the LK operation as well as detector performance. <ref type="figure" target="#fig_4">Figure 7</ref> shows that videos with higher face resolution usually result in a higher possibility to pass the forward-backward check. However, the perfor- mance improvement is not related to the face size. There could be other factors which have more influence, such as occlusion and head pose.</p><p>Temporal Length for Tracking. The duration of LK tracking could be more than three consecutive frames. However, a longer period will result in a stricter forwardbackward check, which reduces the number of landmarks to be included in the registration loss. We tested CPM + PAM with 5 frames LK tracking on YouTube Celebrities and achieved 5.01 NME, which is worse than the result of three frames, 4.74 NME.</p><p>Image Features for Tracking. We also tested using the conv-1 feature instead of the RGB image to do LK tracking, which resulted in an increase of error on YouTube Celebrities from 4.74 to 5.13 NME. This could be caused by the convolutional feature losing certain information that is useful for LK tracking, and more attention is required to learn features suitable for LK tracking.</p><p>Effect of imprecise annotation. SBR and PAM only show a small improvement based on the NME and AUC evaluation metric, but we observe significant reduction of jittering in videos (see demo video). There could be two reasons: (1) NME and AUC treat the annotations of each frame independently and does not take into account the smoothness of the detections, and (2) imprecise annotations in the testing set may adversely affect the evaluation results. We further analyze reason (2) by generating a synthetic face dataset named "SyntheticFace" from a 3D face avatar. The key advantage of a synthetic data set is that there is zero annotation error because we know exactly where each 3D vertex is projected into a 2D image. This enables us to analyze the effect of annotation errors by synthetically adding noise to the perfect "annotations". We generated 2537 training and 2526 testing face images under different expressions and identified 20 landmarks to detect. The image size is 5120×3840. We add varying levels of Gaussian noise to the training and testing set, which are then used to train and evaluate our detector. If we train on different levels of noise and evaluate the models on clean annotations, the testing performance is surprisingly close across models, as shown in <ref type="figure">Figure 8a</ref>. This means that our detector is able to "average out" the errors in annotation. However, the same models evaluated against testing annotations with varying error <ref type="figure">(Figure 8b</ref>) look significantly worse than <ref type="figure">Figure 8a</ref>. This means that a well-performing model may have poor results simply due to the annotation error in the testing data. In sum, annotation errors could greatly affect quantitative results, and a lower score does not necessarily mean no improvement.</p><p>Connection with Self-Training. Our method is interestingly a generalization of self-training, which was utilized by <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref> to take advantage of unlabeled videos in the pose estimation task. The procedure of self-training is (1) train a classifier with the current training set, (2) predict on unlabeled data, (3) treat high confidence predictions as pseudo-labels and add them to the training set, and (4) repeat step 1. The main drawback of this method is that high-confidence pseudo-labels are assumed "correct" and no feedback is provided to fix errors in the pseudo-labels.</p><p>In our problem setting, the pseudo-labels areL t , which are detections tracked from frame t−1 with LK. If we simply perform self-training,L t are directly used as labels to learn L t . No feedback toL t is provided even if it is erro-(b) evaluation results on noisy testing data (a) evaluation results on clean testing data <ref type="figure">Figure 8</ref>. Effect of annotation error. We add Gaussian noise to the annotations of SyntheticFace, and train the model on these noisy data. Different levels of Gaussian noise is indicated by different colors. Left: The models are evaluated on clean testing data of SyntheticFace. Right: The models are evaluated on noisy testing data of SyntheticFace, which has the same noise distribution as training.</p><p>neous. However, our registration loss provides feedback for both L t andL t , thus if the pseudo-labels are inaccurate,L t will also be adjusted. This is the key difference between our method and self-training. More formally, the gradient of our registration loss Eq. (6) with respect to the detector parameter θ is as follows:</p><formula xml:id="formula_16">∇ θ ℓ t regi = K i=1 η t,i (∇ θ L t,i − ∇ θ LK(F t−1 , F t , L t−1,i )),</formula><p>where η t,i = βt,i 2||Lt,i−LK(Ft−1,Ft,Lt−1,i)||2 . For selftraining, the gradients fromL t : ∇ θ LK(F t−1 , F t , L t−1,i ) are missing. This compromises the correctness of the gradient for θ, which is used to generate both L t andL t . Empirically we observed that the detector tends to drift in a certain incorrect direction when the gradients ofL t are ignored, which led to an increase of error from 4.74 to 5.45 NME on YouTube Celebrities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We present supervision-by-registration (SBR), which is advantageous because: (1) it does not rely on human annotations which tend to be imprecise, (2) the detector is no longer limited to the quantity and quality of human annotations, and (3) back-propagating through the LK layer enables more accurate gradient updates than self-training. Also, experiments on synthetic data show that annotation errors in the evaluation set may make a well-performing model seem like it is performing poorly, so one should be careful of annotation imprecision when interpreting quantitative results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The training procedure of supervision-by-registration with two complementary losses. The detection loss utilizes appearance from a single image and label information to learn a better landmark detector. The registration loss uncovers temporal consistency by incorporating a Lucas-Kanade operation into the network. Gradients from the registration loss are back-propagated through the LK operation to the detector network, thus enforcing the predictions in neighboring frames to be consistent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Forward-backward communication scheme between the detector and the LK operation during the training procedure. The green and pink lines indicate the forward and backward LK tracking routes. The blue/green/pink dots indicate the landmark predictions from the detector/forward-LK/backward-LK. The forward direction of this communication adjusts the detection results of future frames based on the past frame. The backward direction assesses the reliability of the LK operation output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Qualitative results of CPM (green) and CPM+SBR/PAM (blue) on 300VW. We sample predictions every 10 frames from videos. Yellow circles indicate the clear failures from the CPM. CPM+SBR/PAM can produce more stable predictions across adjacent frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Analysis on category A of 300-VW. The x-axis indicates the face size of 31 videos in ascending order. The left yaxis indicates the AUC@0.08, and the right shows the number of landmarks that are considered as reliable by the forward-backward communication scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>which represents the coordinates of K landmarks from frame t−1 , as input to generate the landmarksL t for the next (future)</figDesc><table>compute p 
via Eq. (4) 

warp 

warp 

-

compute 
image 
gradients 

initialization 

Ft-1 

Ft 

subregion 
of Ft-1 

step 1 
step 2 
step 3 

subregion 
of Ft 

xt-1 

step 4 
step 5 

error 
feature 

compute 
J, H 

step 6 
step 7 
step 8 

update p 
via Eq. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>for feature extraction and use only three CPM stages for heatmap prediction. The faces</figDesc><table>Method 

300-W 
AFLW 
Common Challenging Full Set 
SDM [37] 
5.57 
15.40 
7.52 
5.43 
LBF [25] 
4.95 
11.98 
6.32 
4.25 
MDM [33] 
4.83 
10.14 
5.88 
-
TCDCN [38] 
4.80 
8.60 
5.54 
-
CFSS [39] 
4.73 
9.98 
5.76 
3.92 
Two-Stage [19] 
4.36 
7.56 
4.99 
2.17 
Reg 
8.14 
16.90 
9.85 
5.01 
Reg + SBR 
7.93 
15.98 
9.46 
4.77 
CPM 
3.39 
8.14 
4.36 
2.33 
CPM + SBR 
3.28 
7.58 
4.10 
2.14 

Table 1. Comparison of NME on 300-W and AFLW datasets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>AUC @ 0.08 error on 300-VW category C. Note that SBR 
and PAM do not utilize any additional annotations, but can still 
improve the baseline CPM and achieve the state-of-the-art results. 

Method SDM [37] ESR [3] RLB [25] PIEFA [23] 
NME 
5.85 
5.61 
5.37 
4.92 
Ours 
Reg 
Reg+PAM CPM CPM+PAM 
NME 
10.21 
9.31 
5.26 
4.74 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Videos with mean face size &lt; 100 2 are considered as low-resolution.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<title level="m">Lucas-Kanade 20 years on: A unifying framework. IJCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3D shape regression for real-time facial animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CLKN: Cascaded lucas-kanade networks for image alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-N</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Personalizing human video pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Magee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Offline deformable face tracking in arbitrary videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV-W</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">More is less: A more complicated network with less inference complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Style aggregated network for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Struck: Structured output tracking with kernels. T-PAMI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Forward-backward error: Automatic detection of tracking failures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Synergy between face alignment and tracking via discriminative global consensus optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Face tracking and recognition with visual constraints in real-world videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, realworld database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV-W</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Two-stream transformer networks for video-based face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A deep regression architecture with two-stage reinitialization for high performance facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A recurrent encoder-decoder network for sequential face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PIEFA: Personalized incremental and ensemble face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Toward personalized modeling: Incremental and ensemble alignment for sequential faces in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: Database and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">RAPS: Robust and efficient automatic construction of person-specific deformable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV-W</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised video adaptation for parsing human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The first facial landmark tracking in-the-wild challenge: Benchmark and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV-W</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Face2Face: Real-time face capture and reenactment of RGB videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Project-out cascaded regression with an application to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unconstrained face alignment via cascaded compositional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semi-supervised learning tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
