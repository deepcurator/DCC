<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Non-Local Deep Features for Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
							<email>zhiming.luo@usherbrooke.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Cognitive Science</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Fujian Key Laboratory of Brain-Inspired Computing Technique and Applications</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sherbrooke</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshaya</forename><surname>Mishra</surname></persName>
							<email>amishra@miovision.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Miovision Technologies Inc</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Achkar</surname></persName>
							<email>aachkar@miovision.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Miovision Technologies Inc</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Eichel</surname></persName>
							<email>jeichel@miovision.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Miovision Technologies Inc</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Cognitive Science</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Fujian Key Laboratory of Brain-Inspired Computing Technique and Applications</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Marc</forename><surname>Jodoin</surname></persName>
							<email>pierre-marc.jodoin@usherbrooke.ca</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sherbrooke</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Non-Local Deep Features for Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Saliency detection aims to mimic the human visual system which naturally separates predominant objects of a scene from the rest of the image. Several applications benefit from saliency detection including image and video compression <ref type="bibr" target="#b13">[14]</ref>, context aware image re-targeting <ref type="bibr" target="#b24">[25]</ref>, scene parsing <ref type="bibr" target="#b49">[50]</ref>, image resizing <ref type="bibr" target="#b2">[3]</ref>, object detection <ref type="bibr" target="#b43">[44]</ref> and segmentation <ref type="bibr" target="#b32">[33]</ref>.</p><p>A salient object is often defined as a region whose visual features differ from the rest of the image and whose shape follows some a priori criteria <ref type="bibr" target="#b4">[5]</ref>. Traditional methods typically extract local pixel-wise or region-wise features and compare it with global features. The result of that comparison is called a saliency score which is stored in a saliency map. Recently, deep learning has entered the field of saliency detection and quickly established itself as the de facto benchmark. Their greatest asset relative to traditional * Corresponding author.</p><p>unsupervised approaches is that they can be trained end-toend using simple optimization functions that combine local and deep features.</p><p>While some methods apply a straight forward convolutional neural net (CNN) model <ref type="bibr" target="#b35">[36]</ref>, others have proposed a model tailored to the saliency detection problem <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b49">50]</ref>. To achieve state-of-the-art performance, the top performing CNN models require nontrivial steps such as generating object proposals, applying post-processing, enforcing smoothness through the use of superpixels or defining complex network architectures, all the while making predictions far slower than real-time. As such, there remain opportunities to simplify the model architecture and speed up the computation.</p><p>In this paper, we show that the overarching objectives of state-of-the-art CNN models (enforcing spatial coherence of the predicted saliency map and using both the local and global features in the optimization) can be achieved with a much simplified non-local deep feature (NLDF) model. Spatial coherence is enforced with a Bayesian loss inspired by the Mumford-Shah (MS) functional <ref type="bibr" target="#b34">[35]</ref>. The loss is expressed as the sum of a cross-entropy term and a boundary term. As opposed to conventional implementations of the MS functional, we use non-local features learned by a deep network instead of raw RGB colors. Also, rather than minimizing the boundary length directly (as done by unsupervised MS implementations), we minimize an intersection over union loss computed using predicted and ground truth boundary pixels. This boundary penalty term is shown to contribute significantly to our model's performance.</p><p>Our model's network is composed of convolution and deconvolution blocks organized in a 4 Ã— 5 grid (see <ref type="figure">Figure 1)</ref> where each column of the grid extracts resolutionspecific features. Local contrast processing blocks are also used along each resolution axis in order to promote features with strong local contrast. The resulting local and global features are combined into a "score" processing block that gives the final output at half of the input resolution.</p><p>Since our method does not rely on superpixels, it is fully convolutional and thus achieves best-in-class evaluation speeds. The NLDF model evaluates an input image in 0.08s, a speed gain of 18 to 100 times as compared to other state-of-the-art deep learning methods, while being on par with state-of-the-art evaluation performance on the MSRA-B <ref type="bibr" target="#b29">[30]</ref>, HKU-IS <ref type="bibr" target="#b24">[25]</ref>, PASCAL-S <ref type="bibr" target="#b26">[27]</ref>, DUT-OMRON <ref type="bibr" target="#b48">[49]</ref>, ECSSD <ref type="bibr" target="#b47">[48]</ref> and SOD <ref type="bibr" target="#b31">[32]</ref> benchmark datasets.</p><p>The rest of the paper is organized as follows. Section 2 provides an overview of deep learning based saliency detection techniques. Section 3 describes the theory and practical implementation of our NLDF model. Finally, Section 4 discusses the performance of non-local feature model compared to other state-of-the-art saliency detection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Most previous methods implement an unsupervised model whose goal is to find objects with visual features different than those from the background. Prior efforts have tested simple features such as color and grayscale <ref type="bibr" target="#b1">[2]</ref>, edges <ref type="bibr" target="#b12">[13]</ref> or texture <ref type="bibr" target="#b9">[10]</ref>, as well as more complex features such as objectness, focusness and backgroundness <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b8">9]</ref>. The literature offers a wide variety of unsupervised methods working at the pixel level <ref type="bibr" target="#b1">[2]</ref>, the region level <ref type="bibr" target="#b10">[11]</ref>, with graph-based methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b44">45]</ref>, and with a Bayesian formulation <ref type="bibr" target="#b46">[47]</ref>. The reader shall refer to the survey paper by Borji et al. <ref type="bibr" target="#b4">[5]</ref> for more details on unsupervised methods. While unsupervised methods have their advantages, including simplicity and no need for training, they have been outperformed by machine learning approaches. Although some traditional AI methods such as SVM <ref type="bibr" target="#b40">[41]</ref> perform well, deep learning methods, specifically CNN models, have raised the bar and imposed themselves as the unavoidable standard. With CNNs, the saliency problem has been redefined as a labeling problem where feature selection between salient and non-salient objects is done automatically through gradient descent.</p><p>CNNs were first developed to perform image classification <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23]</ref>. These models are made of a series of convolution layers with non-linear activation functions and max pooling operations all the way to a softmax layer which predicts the likelihood of each class. CNN methods are a priori unfit to predict a saliency map since their output is a k-D vector (where k is the number of classes), and not an N Ã— M map (where N Ã— M is the size of the input image) as one would expect. However, one can alleviate that problem by extracting a square patch around each pixel and use that patch to predict the center pixel's class <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref>. In order for these methods to capture a global context that goes beyond the scope of each patch, they process patches taken from different resolutions of the input image.</p><p>Several deep visual saliency detection methods use this same patch trick for predicting a saliency map <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b42">43]</ref>. Zhao et al. <ref type="bibr" target="#b49">[50]</ref> integrated the global and local context of an image into a single, multi-context network, where the global context helps to model the saliency in the full image, and the local context helps to estimate the saliency of finegrained, feature rich areas. Li et al. <ref type="bibr" target="#b24">[25]</ref> developed a computational model using multi-scale deep features extracted by three CNNs and three fully connected layers to define salient regions of an image. Such a complex model was designed to capture the saliency map of objects with various scales, geometry, spatial positions, irregularities, and contrast levels. Wang et al. <ref type="bibr" target="#b42">[43]</ref> developed a two tier strategy: each pixel is assigned a saliency based upon a local context estimation in parallel to a global search strategy used to identify the salient regions. These two saliency maps are then combined using geodesic object proposal techniques <ref type="bibr" target="#b20">[21]</ref>.</p><p>Another way of having the output resolution of a CNN match the input image resolution is through one (or several) upsampling layer(s). A popular method for doing so is the FCN method by Long et al. <ref type="bibr" target="#b30">[31]</ref> which adds an upsampling layer at the very end of the network. Saliency detection methods using that approach are among the most accurate ones <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b25">26]</ref> most likely because they better capture the local and global context than patch-wise methods.</p><p>In order to enforce spatial coherence, a large number of methods use pre-computed regions or super pixels <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43]</ref>. Roughly speaking, the idea is to set the saliency score of a superpixel as the mean saliency score of each pixel located inside of it. Since superpixels can be inaccurate, some methods <ref type="bibr" target="#b42">[43]</ref> use several object proposals which they combine afterwards while others use more than one CNN stream <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b49">50]</ref>. Spacial coherence can also be enforced by using a CRF or mean-field postprocess <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b22">23]</ref>. The main inconvenience with these approaches is their processing time.</p><p>Our approach differs from these methods as it uses a single and fully convolutional CNN. It uses a series of multiscale convolution and deconvolution blocks organized in a novel 5 Ã— 4 grid. Our CNN model ensures that the output has the right size while capturing the local and global context as well as features at various resolutions. Spatial coherence is enforced with a loss function inspired by the Mumford-Shah model <ref type="bibr" target="#b34">[35]</ref> which we adapted to the context of machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Based Saliency Detection</head><p>Salient region detection as well as image segmentation often boils down to the optimization of a non-convex energy function which consists of a data term and a regularization term. An elegant mathematical global model is the cartoon <ref type="figure">Figure 1</ref>. Architecture of our 4 Ã— 5 grid-CNN network for salient object detection.</p><p>Mumford-Shah (MS) model <ref type="bibr" target="#b34">[35]</ref>, whose fitting energy,</p><formula xml:id="formula_0">F MS = j Î» j vâˆˆâ„¦j I(v) âˆ’ u j 2 dv data fidelity + j Î³ j vâˆˆCj dv boundary length (1)</formula><p>segments an image I as a set of disjoint piece-wise constant functions u j , indexed by j. Here, â„¦ âŠ‚ R N is an open set representing the image domain, I is the observed image, u j is the underlying piece-wise constant segmented image, v is a pixel location, and C is the boundary of the segmented regions. The positive weighting constants Î» j , and Î³ j tune the multi-criteria energy function in terms of data fidelity, and total boundary length. From a Bayesian statistical perspective <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b50">51]</ref>, Eq. (1) can be approximated as,</p><formula xml:id="formula_1">F MS â‰ˆ j Î» j vâˆˆâ„¦j log p j (I(v), v)dv data fidelity + j Î³ j vâˆˆCj dv boundary length .<label>(2)</label></formula><p>As there is no analytic solution to Eqs. <ref type="formula">(1)</ref> and <ref type="formula" target="#formula_1">(2)</ref>, the most common unsupervised approaches to optimize these employ level set base curve evolution techniques <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42]</ref>, generalized Bayesian criteria using the variational principle, and simulated annealing <ref type="bibr" target="#b50">[51]</ref>. Despite their mathematical elegance, these methods are all iterative in nature, making them sensitive to initial conditions and likely to fail in the presence of noise, background clutter, weak image boundaries or image non-uniformity. Furthermore, poor convergence rates in the iterative solution of the level set limits their utility to non real-time applications.</p><p>To address these issues, we propose a supervised deep convolutional network whose loss approximates the MS functional with the sum of a cross entropy data fidelity term between the ground truth and estimated saliency and a boundary loss term:</p><formula xml:id="formula_2">F MS â‰ˆ j Î» j vâˆˆâ„¦j H j (y(v),Å·(v)) cross entropy + j Î³ j (1 âˆ’ IoU(C j ,Äˆ j ))</formula><p>boundary IoU loss <ref type="formula">(3)</ref> where H j is the total cross entropy between ground truth (y) and estimated (Å·) saliency map of all pixels (v) inside region â„¦ j , and IoU(C j ,Äˆ j ) is the intersection over union between the pixels on the true boundary C j and the pixels on the estimated boundaryÄˆ j . Note that since our method implements a supervised version of the MS functional, the use of the IoU allows our method to learn a higher level a priori term, i.e. a term that learns to penalize erroneous boundaries instead of minimizing the total boundary length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture</head><p>Here we provide a deep convolutional network architecture whose goal is to learn discriminant saliency features (our model is shown in <ref type="figure">Figure 1</ref>). As mentioned in Sec. 2, good saliency features must account for both the local and global context of an image and incorporate details from various resolutions. To achieve this goal, we have implemented a novel grid-like CNN network containing 5 columns and 4 rows. Here, each column is geared toward the extraction of features specific to a given input scale. The input I to our model (on the left) is an 352 Ã— 352 image and the output (on the right) is a 176 Ã— 176 saliency map which we resize back to 352 Ã— 352 with a bilinear interpolation.</p><p>The first row of our model contains five convolutional blocks derived from VGG-16 <ref type="bibr" target="#b38">[39]</ref> (CONV-1 to CONV-  <ref type="table" target="#tab_0">Table 1</ref>, these convolution blocks contain a max pooling operation of stride 2 which downsamples their feature maps {X 1 , ..., X 5 } by a factor of 2, e.g. {176 Ã— 176, 88 Ã— 88, ..., 11 Ã— 11}. The last and rightmost convolution block of the first row computes features X G that are specific to the global context of the image.</p><p>The second and third row is a set of ten convolutional blocks, CONV-6 to CONV-10 for row 2 and Contrast-1 to Contrast-5 for row 3. The aim of these blocks is to compute features (X i ) and contrast features (X C i ) specific to each resolution. The contrast features capture the difference of each feature against its local neighborhood favoring regions that are either brighter or darker than their neighbors.</p><p>The last row is a set of deconvolution layers used to upscale the features maps from 11 Ã— 11 (bottom right) all the way to 176 Ã— 176 (bottom left). These UNPOOL layers are a means of combining the feature maps (X i , X C i ) computed at each scale. The lower left block constructs the final local feature maps X L . The SCORE block has 2 convolution layers and a softmax to compute the saliency probability by fusing the local (X L ) and global (X G ) features. Further details of our model are given in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Non-Local Feature Extraction</head><p>Multi-Scale local features: As shown in the second row of <ref type="figure">Figure 1</ref>, the convolutional blocks CONV-6 to CONV-10 are connected to the VGG-16 CONV-1 to CONV-5 processing blocks. The goal of these convolutional layers is to learn multi-scale local feature maps {X 1 , X 2 , ..., X 5 }. Each convolution block has a kernel size 3 Ã— 3 and 128 channels.</p><p>Contrast features: Saliency is the distinctive quality of a foreground object which makes it stand out from its surrounding background. Saliency features must thus be uniform inside the foreground objects and within the background but at the same time be different between foreground and background areas. In order to capture this kind of contrast information, we added a contrast feature associated to each local feature X i . Each contrast feature X c i is computed by subtracting X i from its local average. The kernel size of the average pooling is 3 Ã— 3</p><formula xml:id="formula_3">X c i = X i âˆ’ AvgPool(X i ).<label>(4)</label></formula><p>Note that such contrast feature is similar in spirit to that of Achanta et al. <ref type="bibr" target="#b1">[2]</ref> which computes the difference between the pixel RGB color and the global average color of the image. It is even closer to that of Liu and Gleicher <ref type="bibr" target="#b27">[28]</ref> which computes contrast features from a Gaussian image pyramid. However, our approach is different as our features are learned and not predefined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deconvolution features:</head><p>Since the size of the final output is 176 Ã— 176, we use a series of deconvolution layers to increase the size of the precomputed features maps X i and X C i . Instead of increasing the feature maps by a ratio of {2, 4, 8, 16} as suggested by Long et al. <ref type="bibr" target="#b30">[31]</ref> which results in coarse feature maps, we adopt a step-wise upsampling procedure as showed in the third row in <ref type="figure">Figure 1</ref>. At each UNPOOL processing block, we upsample the previous feature maps by a factor of 2. The resulting unpooled feature map U i is computed by combining the information of its local feature X i , local contrast feature X c i , and the previous block's unpooled feature U i+1</p><formula xml:id="formula_4">U i = UNPOOL(X i , X c i , U i+1 ).<label>(5)</label></formula><p>The UNPOOL operation is implemented with a deconvolution layer with a stride of 2 and a 5 Ã— 5 kernel. The input is the concatenation of X i , X C i and U i+1 . The number of feature channels of U i is equal to the sum of X i and U i+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local feature maps:</head><p>We use a convolution layer with a kernel size 1 Ã— 1 to get the final local feature maps X L . The input of that layer is the concatenation of X 1 , X</p><formula xml:id="formula_5">C 1 and U 2 X L = CONV(X 1 , X c 1 , U 2 ).<label>(6)</label></formula><p>The number of feature channels of X L is equal to the sum of X 1 and U 2 . Note that we tried using another UNPOOL operation to increase the size of X L from 176Ã—176 to 352Ã— 352, but found that this operation doubles the computation time without measurably improving accuracy.</p><p>Capturing global context: Detecting salient objects in an image requires the model to capture the global context of the image before assigning saliency to individual small regions. To account for this, we added three convolutional layers after the CONV-5 block to compute the global feature X G . The first two convolutional layers have a kernel size of 5, and the last convolutional is 3. All three layers have 128 features channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cross Entropy Loss</head><p>The final saliency map is computed as a linear combination of the local features X L and global features X G using two linear operators (W L , b L ) and (W G , b G ). The softmax function is used to compute the probability for each pixel of being salient or not.</p><formula xml:id="formula_6">y(v) = p y(v) = c = e W c L X L (v)+b c L +W c G X G +b c G c â€² âˆˆ{0,1} e W c â€² L X L (v)+b c â€² L +W c â€² G X G +b c â€² G (7)</formula><p>The cross-entropy loss function </p><formula xml:id="formula_7">H j (y(v),Å·(v)) = âˆ’ 1 N N i=1 câˆˆ{0,1} (y(v i ) = c) log Å·(v i ) = c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">IoU Boundary Loss</head><p>Motivated by the significant applications of Dice loss or IoU boundary loss in medical image segmentation <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b33">34]</ref>, our proposed method approximates the penalty on boundary length of Eq. (1) using an IoU boundary loss term. To compute the boundary loss, we approximate the saliency map gradient magnitude (and hence the boundary pixels) using a Sobel operator followed by a tanh activation. The tanh activation projects the gradient magnitude of saliency maps to a probability range of <ref type="bibr">[0,</ref><ref type="bibr" target="#b0">1]</ref>. Given the gradient magnitude of saliency mapsÄˆ j and gradient magnitude of true saliency maps C of region j, the Dice or IoU boundary loss can be computed as</p><formula xml:id="formula_8">IoU Loss = 1 âˆ’ 2|C j âˆ©Äˆ j | |C j | + |Äˆ j | ,<label>(9)</label></formula><p>which has range [0, 1]. Our whole boundary overlapping loss computation procedure is end-to-end trainable, and an example is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Please note that the intersection is implemented using a point-wise multiplication operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Benchmark Datasets</head><p>We have evaluated the performance of our method (NLDF) on six different public benchmark datasets: MSRA-B <ref type="bibr" target="#b29">[30]</ref>, HKU-IS <ref type="bibr" target="#b24">[25]</ref>, DUT-OMRON <ref type="bibr" target="#b48">[49]</ref>, PASCAL-S <ref type="bibr" target="#b26">[27]</ref>, ECSSD <ref type="bibr" target="#b47">[48]</ref> and SOD <ref type="bibr" target="#b31">[32]</ref>.</p><p>MSRA-B: contains 5000 images, and is widely used for visual saliency detection. Most of the images have one salient object and a pixel-wise ground truth <ref type="bibr" target="#b16">[17]</ref>.</p><p>HKU-IS: contains 4447 images, most of which have low contrast and multiple salient objects. This dataset has been split into 2500 training images, 500 validation images and the remaining 1447 test images.</p><p>DUT-OMRON: contains 5168 challenging images, each of which contains one or more salient objects with a relatively cluttered background.</p><p>PASCAL-S: contains 850 natural images which were built from the validation set of the PASCAL-VOC 2010 segmentation challenge. This dataset contains both pixel-wise saliency ground truth and eye fixation ground truth labeled by 12 subjects.</p><p>ECSSD: contains 1000 images with complex structure acquired from the Internet. The ground truth masks were labeled by 5 subjects.  <ref type="bibr" target="#b45">[46]</ref>, MR <ref type="bibr" target="#b48">[49]</ref>, wCtr* <ref type="bibr" target="#b51">[52]</ref>, BSCA <ref type="bibr" target="#b37">[38]</ref>, LEGS <ref type="bibr" target="#b42">[43]</ref>, MC <ref type="bibr" target="#b49">[50]</ref>, MDF <ref type="bibr" target="#b24">[25]</ref> and DCL <ref type="bibr" target="#b25">[26]</ref> methods compared to our NLDF method. The NLDF maps provides clear salient regions and exhibit good uniformity as compared to the saliency maps from the other deep learning methods (LEGS, MC, MDF and DCL). Our method is also more robust to background clutter than the non-deep-learning methods (GS, MR, wCtr* and BSCA).</p><p>SOD: contains 300 images originally designed for image segmentation. Many images contain multiple salient objects with low contrast and overlapping boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation and Experimental Setup</head><p>Our NLDF model was implemented in TensorFlow <ref type="bibr" target="#b0">[1]</ref>. The weights in the CONV-1 to CONV-5 blocks were initialized with the pretrained weights of VGG-16 <ref type="bibr" target="#b38">[39]</ref>. All the weights of newly added convolution and deconvolution layers were initialized randomly with a truncated normal (Ïƒ = 0.01), and the biases were initialized to 0. The Adam optimizer <ref type="bibr" target="#b18">[19]</ref> was used to train our model with an initial learning rate of 10 âˆ’6 , Î² 1 = 0.9, and Î² 2 = 0.999. The Î» j and Î³ j in Eq. (3) were set to 1.</p><p>For fair comparison with other methods, we followed the experimental setup of <ref type="bibr" target="#b16">[17]</ref>, dividing the MSRA-B dataset into 3 parts: 2500 images for training, 500 images for validation and the remaining 2000 images for testing. The training and validation sets were combined together to train our model with horizontal flipping as data augmentation. The inputs were resized to 352 Ã— 352 for training. With an NVIDIA Titan X GPU, it takes âˆ¼ 9 hours to finish the whole training procedure for 20 epochs with a single image batch size. Without further optimization, this trained model was used to compute the saliency maps of the other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Criteria</head><p>Precision-recall (PR) curves, F Î² and mean absolute error (MAE) were used as metrics to evaluate the performance of saliency detection. The PR curve is computed by binarizing the saliency maps under different probability thresholds ranging from 0 to 1 and comparing against the ground truth. As for the F Î² measure, it is defined as,</p><formula xml:id="formula_9">F Î² = (1 + Î² 2 ) Â· Precision Â· Recall Î² 2 Â· Precision + Recall .<label>(10)</label></formula><p>where Î² 2 = 0.3 to emphasize precision over recall as suggested in <ref type="bibr" target="#b1">[2]</ref>. We report the maximum F-Measure computed <ref type="table">Table 2</ref>. Quantitative performance of our model on six benchmark datasets compared with the GS <ref type="bibr" target="#b45">[46]</ref>, MR <ref type="bibr" target="#b48">[49]</ref>, wCtr* <ref type="bibr" target="#b51">[52]</ref>, BSCA <ref type="bibr" target="#b37">[38]</ref>, LEGS <ref type="bibr" target="#b42">[43]</ref>, MC <ref type="bibr" target="#b49">[50]</ref>, MDF <ref type="bibr" target="#b24">[25]</ref> and DCL <ref type="bibr" target="#b25">[26]</ref>  from the PR curve. MAE <ref type="bibr" target="#b36">[37]</ref> is computed as the average pixel-wise absolute difference between the estimated saliency map S and its corresponding ground truth L,</p><formula xml:id="formula_10">MAE = 1 W Ã— H W x=1 H y=1 S(x, y) âˆ’ L(x, y) .<label>(11)</label></formula><p>where W and H is the width and height of a given image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Effectiveness of the Boundary Loss Term</head><p>In addition to our NLDF model, we also trained a model, denoted as NLDF-, which only contains the cross-entropy loss term and excludes the boundary loss term [see Eq. 3]. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, the saliency maps generated from NLDF-are fairly coarse and the boundary of the salient objects are not well preserved. As shown in last two columns of <ref type="table">Table 2</ref>, this qualitative decrease in performance is also mirrored in the quantitative results. The inclusion of the boundary loss in NLDF as compared to NLDF-accounts for increases in max F Î² of 2.1% to 4.4% and decreases in MAE of 5.8% to 20.0% on HKU-IS, DUT-OMRON, PASCAL-S, ECSSD and SOD datasets. Little change is observed for MSRA-B, an expected result, since training and testing samples are drawn from a similar pool of images. Significantly, these results illustrate that the boundary loss term directly enhances the generality of NLDF, making it more robust to variations in input types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with the State of the Art</head><p>We quantitatively compared our NLDF method with several recent state-of-the-art methods: Geodesic Saliency (GS) <ref type="bibr" target="#b45">[46]</ref>, Manifold Ranking (MR) <ref type="bibr" target="#b48">[49]</ref>, optimized Weighted Contrast (wCtr*) <ref type="bibr" target="#b51">[52]</ref>, Background based Singlelayer Cellular Automata (BSCA) <ref type="bibr" target="#b37">[38]</ref>, Local Estimation and Global Search (LEGS) <ref type="bibr" target="#b42">[43]</ref>, Multi-Context (MC) <ref type="bibr" target="#b49">[50]</ref>, Multiscale Deep Features (MDF) <ref type="bibr" target="#b24">[25]</ref> and Deep Contrast Learning (DCL) <ref type="bibr" target="#b25">[26]</ref>. LEGS, MC, MDF and DCL are the latest deep learning based saliency detection methods. Note that since part of the HKU-IS dataset was used to train the MDF model <ref type="bibr" target="#b24">[25]</ref>, we only compute the evaluation metrics on the testing set of HKU-IS. Also the MDF only provided 200 pre-compute saliency maps on SOD dataset, we use the same subset for evaluation.</p><p>In comparison to the top performing method, DCL+, <ref type="bibr" target="#b25">[26]</ref> an extension of DCL that uses a fullyconnected CRF <ref type="bibr" target="#b19">[20]</ref> as a post-processing step to refine the saliency map, we find that NLDF attains nearly identical (or better) performance across the board (see <ref type="table">Table 2</ref>). That this is achieved without a significant post-processing step means that the execution time and implementation complexity are greatly reduced. The computation time reported in <ref type="bibr" target="#b25">[26]</ref> for DCL is 1.5 s per (300 Ã— 400) image and an additional 0.8 s for CRF post-processing (DCL+). In comparison, our NLDF method only requires 0.08 s per image on a Titan X GPU. This substantial speedup enables nearly real-time salient object detection while also delivering state-of-the-art performance.  <ref type="figure">Figure 5</ref>. Precision-recall curves for our model compared to GS <ref type="bibr" target="#b45">[46]</ref>, MR <ref type="bibr" target="#b48">[49]</ref>, wCtr* <ref type="bibr" target="#b51">[52]</ref>, LEGS <ref type="bibr" target="#b42">[43]</ref>, BSCA <ref type="bibr" target="#b37">[38]</ref>, MDF <ref type="bibr" target="#b24">[25]</ref>, MC <ref type="bibr" target="#b49">[50]</ref> and DCL <ref type="bibr" target="#b25">[26]</ref> evaluated on the MASR-B, HKU-IS, DUT-OMRON, PASCAL-S, ECSSD and SOD benchmark datasets. Our NLDF model can deliver state-of-the-art performance on all six datasets.</p><p>A visual comparison of the saliency maps is provided in <ref type="figure" target="#fig_2">Figure 3</ref>. All saliency maps of other methods were either provided by the authors or computed using the authors' released code. Precision-recall curves are shown in <ref type="figure" target="#fig_2">Figure 3</ref> and the maximum F Î² and MAE scores are in <ref type="table">Table 2</ref>. As shown in <ref type="table">Table 2</ref>, our NLDF model achieves superior quantitative max F Î² , MAE and PR performance across the board when compared to GS, MR, wCtr*, BSCA, LEGS, MC, MDF and DCL. NLDF also surpasses DCL+ more times than not in max F Î² and MAE and exhibits equivalent or better PR curves.</p><p>We also compared the average computation time with other four leading deep learning methods for generating the saliency map of one images in <ref type="table" target="#tab_2">Table 3</ref>. On a Titan Black GPU, our approach is 18 to 100 times faster than existing methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>The integration of local and global features has already been shown to be a powerful mechanism for saliency detection. Here we took this approach one step further by adding a boundary loss term to the typical cross entropy loss, in effect implementing the Mumford-Shah functional in a deep neural net framework and training it end to end.</p><p>The resulting model achieves state of the art performance across multiple saliency detection benchmark datasets, does not use any special pre-or post-processing steps and computes saliency maps 18 to 100 times faster than competing systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>to minimize the first data term in Eq. (2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. A single input image (a) together with its groundtruth saliency (b) and boundary (c) is used to train a model only containing the IoU boundary loss term in Eq. (3). The estimated boundary (d) after training for 200 iterations is in excellent agreement with the true boundary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Saliency maps produced by the GS [46], MR [49], wCtr* [52], BSCA [38], LEGS [43], MC [50], MDF [25] and DCL [26] methods compared to our NLDF method. The NLDF maps provides clear salient regions and exhibit good uniformity as compared to the saliency maps from the other deep learning methods (LEGS, MC, MDF and DCL). Our method is also more robust to background clutter than the non-deep-learning methods (GS, MR, wCtr* and BSCA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Visual comparison of saliency detection results with and without the boundary loss term in Eq. (2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Details of the proposed deep convolutional network for predicting salient objects (S: Stride, Pad: zero padding).</figDesc><table>Block 
Layer 
Kernel S Pad 
Output 
CONV-1 
2 conv 
3*3 
1 Yes 
352*352*64 
max-pool 
2*2 
2 Yes 
176*176*64 
CONV-2 
2 conv 
3*3 
1 Yes 176*176*128 
max-pool 
2*2 
2 Yes 
88*88*128 
CONV-3 
3 conv 
3*3 
1 Yes 
88*88*256 
max-pool 
2*2 
2 Yes 
44*44*256 
CONV-4 
3 conv 
3*3 
1 Yes 
44*44*512 
max-pool 
2*2 
2 Yes 
22*22*512 
CONV-5 
3 conv 
3*3 
1 Yes 
22*22*512 
max-pool 
2*2 
2 Yes 
11*11*512 
CONV-6 
conv 
3*3 
1 Yes 176*176*128 
CONV-7 
conv 
3*3 
1 Yes 
88*88*128 
CONV-8 
conv 
3*3 
1 Yes 
44*44*128 
CONV-9 
conv 
3*3 
1 Yes 
22*22*128 
CONV-10 
conv 
3*3 
1 Yes 
11*11*128 
UNPOOL-5 
deconv 
5*5 
2 Yes 
22*22*128 
UNPOOL-4 
deconv 
5*5 
2 Yes 
44*44*256 
UNPOOL-3 
deconv 
5*5 
2 Yes 
88*88*384 
UNPOOL-2 
deconv 
5*5 
2 Yes 176*176*512 
LOCAL 
conv 
1*1 
1 No 176*176*640 
GLOBAL 
conv-1 
5*5 
1 No 
7*7*128 
conv-2 
5*5 
1 No 
3*3*128 
conv-3 
3*3 
1 No 
1*1*128 
SCORE 
conv-L 
1*1 
1 No 
176*176*2 
conv-G 
1*1 
1 No 
1*1*2 

5). As shown in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>models. The latter four are deep learning methods and the former are not. The F Î² and MAE metrics are defined in the text.</figDesc><table>Dataset 
Metric 
GS 
MR 
wCtr* BSCA LEGS 
MC 
MDF DCL DCL+ NLDF-NLDF 

MSRA-B 
max F Î² 0.777 0.824 0.820 
0.830 
0.870 0.894 0.885 0.905 0.916 
0.912 
0.911 
MAE 
0.144 0.127 0.110 
0.130 
0.081 0.054 0.066 0.052 0.047 
0.048 
0.048 

HKU-IS 
max F Î² 0.682 0.715 0.726 
0.723 
0.770 0.798 0.861 0.892 0.904 
0.874 
0.902 
MAE 
0.167 0.174 0.141 
0.174 
0.118 0.102 0.076 0.054 0.049 
0.060 
0.048 

DUT-OMRON 
max F Î² 0.557 0.610 0.630 
0.616 
0.669 0.703 0.694 0.733 0.757 
0.724 
0.753 
MAE 
0.173 0.187 0.144 
0.191 
0.133 0.088 0.092 0.084 0.080 
0.085 
0.080 

PASCAL-S 
max F Î² 0.624 0.666 0.659 
0.666 
0.756 0.740 0.764 0.815 0.822 
0.804 
0.831 
MAE 
0.224 0.223 0.201 
0.224 
0.157 0.145 0.145 0.113 0.108 
0.116 
0.099 

ECSSD 
max F Î² 0.661 0.736 0.716 
0.758 
0.827 0.822 0.832 0.887 0.901 
0.886 
0.905 
MAE 
0.206 0.189 0.171 
0.183 
0.118 0.106 0.105 0.072 0.075 
0.075 
0.063 

SOD 
max F Î² 0.601 0.619 0.632 
0.634 
0.707 0.688 0.745 0.795 0.801 
0.776 
0.810 
MAE 
0.266 0.273 0.245 
0.266 
0.215 0.197 0.192 0.142 0.153 
0.161 
0.143 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Inference time of leading deep learning methods. LEGS MC MDF DCL DCL+ NLDF</figDesc><table>s/img 
2 
1.6 
8 
1.5 
2.3 
0.08 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software available from tensorflow.org</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Frequencytuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Saliency detection for content-aware image resizing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>SÃ¼sstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.5878</idno>
		<title level="m">Salient object detection: A survey</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On local region models and a statistical interpretation of the piecewise smooth mumford-shah functional</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="184" to="193" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A deeper look at saliency: Feature contrast, semantics, and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Catton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Janjic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Active contours without edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Vese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="266" to="277" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fusing generic objectness and visual saliency for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image saliency detection using gabor texture cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust saliency detection via regularized random walks ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A simple method for detecting salient regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2363" to="2371" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="198" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Salient region detection by ufo: Uniqueness, focusness and objectness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Geodesic object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>KrÃ¤henbÃ¼hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Saliency unified: A deep architecture for simultaneous eye fixation prediction and salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gudisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Dholakiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep contrast learning for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Region enhanced scale-invariant saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICME</title>
		<meeting>ICME</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Predicting eye fixations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Saliency segmentation based on learning and graph cut refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mehrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
		<idno>arXiv preprint:1606.04797</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Optimal approximations by piecewise smooth functions and associated variational problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications on pure and applied mathematics</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Shallow and deep convolutional networks for saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">G</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>KrÃ¤henbÃ¼hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Saliency detection via cellular automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arXiv preprint:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Metrics for evaluating 3d medical image segmentation: analysis, selection, and tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC medical imaging</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Salient object detection via bootstrap learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A multiphase level set framework for image segmentation using the mumford and shah model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Vese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="271" to="293" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep networks for saliency detection via local estimation and global search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Salient object detection for searched web images via global saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Grab: Visual saliency via novel graph model and background priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Geodesic saliency using background priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bayesian saliency via low and mid level cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1689" to="1698" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Region competition: Unifying snakes, region growing, and bayes/mdl for multiband image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="884" to="900" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Saliency optimization from robust background detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Statistical validation of image segmentation quality based on a spatial overlap index 1: Scientific reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Warfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Tempany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Kaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Haker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Jolesz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Academic radiology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="178" to="189" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
