<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ring loss: Convex Feature Normalization for Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Zheng</surname></persName>
							<email>yutongzh@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipan</forename><forename type="middle">K</forename><surname>Pal</surname></persName>
							<email>dipanp@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
							<email>marioss@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Ring loss: Convex Feature Normalization for Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning has demonstrated impressive performance on a variety of tasks. Arguably the most important task, that of supervised classification, has led to many advancements. Notably, the use of deeper structures <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b7">7]</ref> and more powerful loss functions <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b15">15]</ref> have resulted in far more robust feature representations. There has also been more attention on obtaining better-behaved gradients through normalization of batches or weights <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b18">18]</ref>.</p><p>One of the most important practical applications of deep networks with supervised classification is face recognition. Robust face recognition poses a huge challenge in the form of very large number of classes with relatively few samples per class for training with significant nuisance transformations. A good understanding of the challenges in this task results  in a better understanding of the core problems in supervised classification, and in general representation learning. However, despite the impressive attention on face recognition tasks over the past few years, there are still many gaps towards such an understanding. Notably, the need and practice of feature normalization. Normalization of features has recently been discovered to provide significant improvement in performance which implicitly results in a cosine embedding <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b25">25]</ref>. However, direct normalization in deep networks explored in these works results in a non-convex formulation resulting in local minima generated by the loss function itself. It is important to preserve convexity in loss functions for more effective minimization of the loss given that the network optimization itself is non-convex. In a separate thrust of work, cosine similarity has also been very recently explored for supervised classification <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b3">4]</ref>. Nonetheless, a concrete justification and principled motivation for the need for normalizing the features itself is also lacking.</p><p>Contributions. In this work, we propose Ring loss, a simple and elegant approach to normalize all sample features through a convex augmentation of the primary loss function (such as Softmax). The value of the target norm is also learnt during training. Thus, the only hyperparameter in Ring loss is the loss weight w.r.t to the primary loss func- tion. We provide an analytical justification illustrating the benefits of feature normalization and thereby cosine feature embeddings. Feature matching during testing in face recognition is typically done through cosine distance creating a gap between testing and training protocols which do not utilize normalization. The incorporation of Ring loss during training eliminates this gap. Ring loss is differentiable that allows for seamless and simple integration into deep architectures trained using gradient based methods. We find that Ring loss provides consistent improvements over a large range of its hyperparameter when compared to other baselines in normalization and indeed other losses proposed for face recognition in general. Interestingly, we also find that Ring loss helps in being robust to lower resolutions through the norm constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Ring loss: Convex Feature Normalization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Intuition and Motivation.</head><p>There have been recent studies on the use of norm constraints right before the Softmax loss <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b17">17]</ref>. However, the formulations investigated are non-convex in the feature representations leading to difficulties in optimization. Further, there is a need for better understanding of the benefits of normalization itself. Wang et.al. <ref type="bibr" target="#b25">[25]</ref> argue that the 'radial' nature of the Softmax features is not a useful property, thereby cosine similarity should be preferred leading to normalized features. A concrete reason was, however, not provided. Ranjan et.al. <ref type="bibr" target="#b17">[17]</ref> show that the Softmax loss encodes the quality of the data (images) into the norm thereby deviating from the ultimate objective of learning a good representation purely for classification. <ref type="bibr" target="#b0">1</ref> Therefore for better classification, normalization forces the network to be invariant to such details. This is certainly not the entire story and in fact overlooks some key properties of feature normalization. We now motivate Ring loss with three arguments. <ref type="bibr" target="#b0">1)</ref> We show that the norm constraint is beneficial to maintain a balance between the angular classification margins of multiple classes.</p><p>2) It removes the disconnect between training and testing metrics.</p><p>3) It minimizes test errors due to angular variation due to low norm features.</p><p>The Angular Classification Margin Imbalance. Consider a binary classification task with two feature vectors x 1 and x 2 from class 1 and 2 respectively, extracted using some model (possibly a deep network). Let the classification weight vector for class 1 and 2 be w 1 ,w 2 respectively (potentially Softmax). An example arrangement is shown in <ref type="figure" target="#fig_2">Fig. 2(a)</ref>. Then in general, in order for the class 1 vector w 1 to pick x 1 and not x 2 for correct classification, we require w</p><formula xml:id="formula_0">T 1 x 1 &gt;w T 1 x 2 )k x 1 k 2 cos θ 1 &gt; kx 2 k 2 cos θ 2 2 .</formula><p>Here, θ 1 and θ 2 are the angles between the weight vector w 1 (class 1 vector only) and x 1 , x 2 respectively 3 . We call the feasible set (range for θ 1 ) for this inequality to hold as the angular classification margin. Note that it is also a function of θ 2 . Setting kx2k2 kx1k2 = r, we observe r&gt;0 and that for correct classification, we need cos θ 1 &gt;rcos θ 2 ) θ 1 &lt; cos −1 (r cos θ 2 ) since cos θ is a decreasing function between [−1, 1] for θ 2 [0,π]. This inequality needs to hold true for any θ 2 . Fixing cos θ 2 = δ, we have θ 1 &lt; cos −1 (rδ). From the domain constraints of cos −1 , we have −1  rδ  1 )</p><formula xml:id="formula_1">−1 δ  r  1 δ .</formula><p>Combining this inequality with r&gt;0, we have 0 &lt;r</p><formula xml:id="formula_2">1 |δ| )kx 2 k 2  1 δ kx 1 k 2 8δ 2 (0 1].</formula><p>For our purposes it suffices to only look at the case δ&gt;0 since the δ&lt;0 doesn't change the inequality −1  rδ  1 and is more interesting.</p><p>Discussion on the angular classification margin. We plot the upper bound on θ 1 (i.e. cos −1 (r cos θ 2 )) for a range of δ ([0.1, 1]) and the corresponding range of r. <ref type="figure" target="#fig_2">Fig. 2(b)</ref> showcases the plot. Consider δ =0 .1 which implies that  <ref type="figure">for varying λ)</ref>. The blue-green dots are the samples before the gradient update and the red dots are the same samples after the update. The dotted blue vector is the target class direction. λ =0fails to converge and does not constrain the norm whereas λ =1 0takes very small steps towards Softmax gradients. A good balance is achieved at λ =1 . In our large scale experiments, a large range of λ achieves this balance.</p><p>the sample x 2 has a large angular distance from w 1 (about 85</p><p>• ). This case is favorable in general since one would expect a lower probability of x 2 being classified to class 1. However, we see that as r increases (difference in norm of x 1 ,x 2 ), the classification margin for x 1 decreases from 90</p><p>• to eventually 0</p><p>• . In other terms, as the norm of x 2 increases w.r.t x 1 , the angular margin for x 1 to be classified correctly while rejecting x 2 by w 1 , decreases. The difference in norm (r&gt;1) therefore will have an adverse effect during training by effectively enforcing smaller angular classification margins for classes with smaller norm samples. This also leads to lop-sided classification margins for multiple classes due to the difference in class norms as can be seen in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>. This effect is only magnified as δ increases (or the sample x 2 comes closer to w 1 ). <ref type="figure" target="#fig_2">Fig. 2(b)</ref> shows that the angular classification margin decreases much more rapidly as δ increases. However, r&lt;1 leads to a larger margin and seems to be beneficial for classifying class 1 (as compared to r&gt;1). One might argue that this suggests that the r&lt;1 should be enforced for better performance. However, note that the same reasoning applies correspondingly to class 2, where we want to classify x 2 to w 2 while rejecting x 1 . This creates a trade off between performance on class 1 versus class 2 based on r which also directly scales to multi-class problems. In typical recognition applications such as face recognition, this is not desirable. Ideally, we would want to represent all classes equally well. Setting r =1or constraining the norms of the samples from both classes to be the same ensures this.</p><p>Effects of Softmax on the norm of MNIST features. We qualitatively observe the effects of vanilla Softmax on the norm of the features (and thereby classification margin) on MNIST in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>. We see that digits 3, 6 and 8 have large norm features which are typically the classes that are harder to distinguish between. Therefore, we observe r&lt;1 for these three 'difficult' classes (w.r.t to the other 'easier' classes) thereby providing a larger angular classification margin to the three classes. On the other hand, digits 1, 9 and 7 have lower norm corresponding to r&gt;1 w.r.t to the other classes, since the model can afford to decrease the margin for these 'easy' classes as a trade off. We also observe that arguably most easily distinguishable class, digit 1, has the lowest norm thereby the highest r. On the other hand, <ref type="figure" target="#fig_1">Fig. 1(b)</ref> showcases the features learned using Softmax augmented with our proposed Ring loss, which forces the network to learn feature normalization through a convex formulation thereby mitigating this imbalance in angular classification margins.</p><p>Regularizing Softmax loss with the norm constraint. The ideal training scenario for a system testing under the cosine metric would be where all features pointing in the same direction have the same loss. However, this is not true for the most commonly used loss function, Softmax and its variants (FC layer combined with the softmax function and the cross-entropy loss). Assuming that the weights are normalized, i.e. kw k k =1 , the Softmax loss for feature vector F(x i ) can be expressed as (for the correct class y i ):</p><formula xml:id="formula_3">L SM = − log exp w k F(x i ) P K k 0 =1 exp w k 0 F(x i ) F (1) = − log exp kF(x i )k cos θ ki P K k 0 =1 exp kF(x i )k cos θ k 0 i (2)</formula><p>Clearly, despite having the same direction, two features with different norms have different losses. From this perspective, the straightforward solution to regularize the loss and remove the influence of the norm is to normalize the features before Softmax as explored in l 2 -constrained Softmax <ref type="bibr" target="#b17">[17]</ref>. However, this approach is effectively a projection method, i.e. it calculates the loss as if the features are normalized to the same scale, while the actual network does not learn to normalize features.</p><p>The need for features normalization in feature space. As an illustration, consider the training and testing set features trained by vanilla Softmax, of the digit 8 from MNIST in <ref type="figure" target="#fig_4">Fig. 4</ref>. <ref type="figure" target="#fig_4">Fig. 4(a)</ref> shows that at the end of training, the features are well behaved with a large variation in the norm of the features with a few samples with low norm. However, <ref type="figure" target="#fig_4">Fig. 4(b)</ref> shows that that the features for the test samples are much more erratic. There is a similar variation in norm but now most of the low norm features have huge variation in angle. Indeed, variation in samples for lower norm features translates to a larger variation in angle than the same for higher norm samples features. This translates to higher errors in classification under the cosine metric (as is common in face recognition). This is yet another motivation to normalize features during training. Forcing the network to learn to normalize the features helps to mitigate this problem during test wherein the network learns to work in the normalized feature space. A related motivation to feature normalization was proposed by Ranjan et.al. <ref type="bibr" target="#b17">[17]</ref> wherein it was argued that low resolution of an image results in a low norm feature leading to test errors. Their solution to project (not implicitly learn) the feature to the scaled unit hypersphere was also aimed at handling low resolution. We find in our large scale experiment with low resolution images (see Exp. 6 <ref type="figure" target="#fig_8">Fig. 8</ref>) that soft normalization by Ring loss achieves better results. In fact hard projection method by l 2 -constrained Softmax <ref type="bibr" target="#b17">[17]</ref> performs worse than Softmax for a downsampling factor of 64.</p><p>Incorporating the norm constraint as a convex problem. Identifying the need to normalize the sample features from the network, we now formulate the problem. We define L S as the primary loss function (for instance Softmax loss). Assuming that F provides deep features for a sample x as F(x), we would like to minimize the loss subject to the normalization constraint as follows,</p><formula xml:id="formula_4">min L S (F(x)) s.t. kF(x)k 2 = R<label>(3)</label></formula><p>Here, R is the scale constant that we would like the features to be normalized to. This is the exact formulation recently studied and implemented by <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b25">25]</ref>. Note that this problem is non-convex in F(x) since the set of feasible solutions is itself non-convex due to the norm equality constraint. Approaches which use standard SGD while ignoring this critical point would not be providing feasible solutions to this problem thereby, the network F would not learn to output normalized features. Indeed, the features obtained using this straightforward approach are not normalized as was found in <ref type="figure" target="#fig_3">Fig. 3b</ref> in <ref type="bibr" target="#b17">[17]</ref> compared to our approach ( <ref type="figure" target="#fig_1">Fig. 1(b)</ref>). One naive approach to get around this problem would be to relax the norm equality constraint to an inequality. This objective will now be convex, however it does not necessarily enforce equal norm features. In order to incorporate the formulation as a convex constraint, the following form is directly useful as we find below. Figure 5: Ring loss improves MNIST testing accuracy across all classes by reducing inter-class norm variance. Norm Ratio is the ratio between average class norm and average norm of all features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Ring loss</head><p>Ring loss Definition. Ring loss L R is defined as</p><formula xml:id="formula_5">L R = λ 2m m X i=1 (kF(x i )k 2 − R) 2<label>(4)</label></formula><p>where F(x i ) is the deep network feature for the sample x i . Here, R is the target norm value which is also learned and λ is the loss weight enforcing a trade-off between the primary loss function. m is the batch-size. The square on the norm difference helps the network to take larger steps when the norm of a sample is too far off from R leading to faster convergence. The corresponding gradients are as follows.</p><formula xml:id="formula_6">∂L R ∂R = − λ m m X i=1 (kF(x i )k 2 − R)<label>(5)</label></formula><formula xml:id="formula_7">∂L R ∂F(x i ) = λ m ✓ 1 − R kF(x i )k 2 ◆ F(x i )<label>(6)</label></formula><p>Ring loss (L R ) can be used along with any other loss function such as Softmax or large-margin Softmax <ref type="bibr" target="#b14">[14]</ref>. The loss encourages norm of samples being value R (a learned parameter) rather than explicit enforcing through a hard normalization operation. This approach provides informed gradients towards a better minimum which helps the network to satisfy the normalization constraint. The network therefore, learns to normalize the features using model weights themselves (rather than needing an explicit non-convex normalization operation as in <ref type="bibr" target="#b17">[17]</ref>, or batch normalization <ref type="bibr" target="#b9">[9]</ref>). In contrast and in connection, batch normalization <ref type="bibr" target="#b9">[9]</ref> enforces the scaled normal distribution for each element in the feature independently. This does not constrain the overall norm of the feature to be equal across all samples and neither addresses the class imbalance problem. As shown in <ref type="figure">Fig. 5</ref>, Ring loss stabilizes the feature norm across all classes, and, in turn, rectifies the classification imbalance for Softmax to perform better overall.</p><p>Ring loss Convergence Visualizations. To illustrate the effect of the Softmax loss augmented with the enforced soft-normalization, we conduct some analytical simulations. We generate a 2D mesh of points from (−1.5, 1.5) in x,y-axis. We then compute the gradients of Ring loss (R =1 ) assuming the dottef blue vertical line (see <ref type="figure" target="#fig_3">Fig. 3</ref>)  <ref type="table">Tables, SM denotes</ref> Softmax, SF denotes SphereFace <ref type="bibr" target="#b14">[14]</ref>,l2-Cons SM denotes <ref type="bibr" target="#b17">[17]</ref>, + CL denotes Center Loss augmentation <ref type="bibr" target="#b26">[26]</ref> and finally + R denotes Ring loss augmentation. Numbers in bracket denote value of hyperparameter (loss weight), i.e. α for <ref type="bibr" target="#b17">[17]</ref>, λ for Center loss and Ring loss.</p><p>as the target class and update each point with a fixed step size for 20 steps. We run the simulation for λ = {0, 1, 10}. Note that λ =0represents pure Softmax. <ref type="figure" target="#fig_3">Fig. 3</ref> depicts the results of these simulations. Sub-figures (a), (b) and (c) in <ref type="figure" target="#fig_3">Fig. 3</ref> show the initial points on the mesh grid (light green) and the final updated points (red). For pure Softmax (λ =0), we see that the updates increases norm of the samples and moreover they do not converge. For a reasonable loss weight of λ =1 , Ring loss gradients can help the updated points converge much faster in the same number of iterations. For heavily weighted Ring loss with λ = 10, we see that the gradients force the samples to a unit norm since R was set to 1 while overpowering Softmax gradients. These figures suggest that there exists a trade off enforced by λ between the Softmax loss L S and the normalization loss. We observe similar trade-offs in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Validation</head><p>We benchmark Ring loss on large scale face recognition tasks while augmenting two loss functions. The first one is the ubiquitous Softmax, and the second being a successful variant of Large-margin Softmax <ref type="bibr" target="#b15">[15]</ref> called SphereFace <ref type="bibr" target="#b14">[14]</ref>. We present results on five large-scale benchmarks of LFW <ref type="bibr" target="#b8">[8]</ref>, IARPA Janus Benchmark IJB-A <ref type="bibr" target="#b11">[11]</ref>, Janus Challenge Set 3 (CS3) dataset (which is a super set of the IJB-A Janus dataset), Celebrities Frontal-Profile (CFP) <ref type="bibr" target="#b20">[20]</ref> and finally the MegaFace dataset <ref type="bibr" target="#b10">[10]</ref>. We also present results of Ring loss augmented Softmax features on low resolution images from Janus CS3 to showcase resolution robust face matching.</p><p>Implementation Details. For all the experiments in this paper, we usethe ResNet 64 (Res64) layer architecture from Liu et. al. <ref type="bibr" target="#b14">[14]</ref>. For Center loss, we utilized the code repository online and used the best hyperparameter setting reported <ref type="bibr" target="#b3">4</ref> . The l 2 -constrained Softmax loss was implemented follwing <ref type="bibr" target="#b17">[17]</ref> by integrating a normalization and scaling layer <ref type="bibr" target="#b4">5</ref> before the last fully-connected layer. For experiments with L-softmax <ref type="bibr" target="#b15">[15]</ref> and SphereFace <ref type="bibr" target="#b14">[14]</ref>, we used the publicly available Caffe implementation. The Resnet 64 layer (Res64) architecture results in a feature dimension of 512 (at the fc5 layer), which is used for matching using the cosine distance. Ring loss and Center loss are both applied on this feature i.e. to the output of the fc5 layer. All models were trained on the MS-Celeb 1M dataset <ref type="bibr" target="#b4">[5]</ref>. The dataset was cleaned to remove potential outliers within each class and also noisy classes before training. To clean the dataset we used a pretrained model to extract features from the MSCeleb 1M dataset. Then, classes that had variance in the MSE, between the sample features and the mean feature of that class, above a certain threshold were discarded. Following this, from the filtered classes, images that have their MSE error between their feature vector and the class mean feature vector higher than a threshold are discarded. After this procedure, we are left with about 31,000 identities and about 3.5 million images. The learning rate was initialized to 0.1 and then decreased by a factor of 10 at 80K and 150K iterations for a total of 165K iterations. All models evaluated were the 165K iteration model <ref type="bibr" target="#b6">6</ref> . Preprocessing. All faces were detected and aligned using <ref type="bibr" target="#b27">[27]</ref> which provided landmarks for the two eye centers, nose and mouth corners (5 points). Since MS-Celeb1M, IJB-A Janus and Janus CS3 have harder faces we use a robust detector i.e. CMS-RCNN <ref type="bibr" target="#b29">[29]</ref> to detect faces and a fast landmarker that is robust to pose <ref type="bibr" target="#b1">[2]</ref>. The faces were then aligned using a similarity transformation and were cropped to 112 ⇥ 96 in the RGB format. The pixel level activations were normalized by subtracting 127.5 and then dividing by 128. For failed detections, the training set images are ignored. In the case of testing, ground truth landmarks were used from the corresponding dataset. Exp 1. Testing Benchmark: LFW. The LFW <ref type="bibr" target="#b8">[8]</ref> database contains about 13,000 images for about 1680 subjects with a total of 6,000 defined matches. The primary nuisance transformations are illumination, pose, color jittering and age. As the field has progressed, LFW has been considered to be saturated and prone to spurious minor variances in performance (in the last % of accuracy) owing to the small size of the protocol. Small differences in accuracy on this protocol do not accurately reflect the generalizing   <ref type="table">Tables, SM denotes</ref> Softmax, SF denotes SphereFace <ref type="bibr" target="#b14">[14]</ref>,l2-Cons SM denotes <ref type="bibr" target="#b17">[17]</ref>, + CL denotes Center Loss augmentation <ref type="bibr" target="#b26">[26]</ref> and finally + R denotes Ring loss augmentation. Numbers in bracket denote value of hyperparameter (loss weight), i.e. α for <ref type="bibr" target="#b17">[17]</ref>, λ for Center loss and Ring loss.</p><p>capabilities of a high performing model. Nonetheless, as a benchmark, we report performance on this dataset.</p><p>Results: LFW. <ref type="table" target="#tab_1">Table 1</ref> showcases the results on the LFW protocol. We find that for Softmax (SM + R), Ring loss normalization seems to significantly improve performance (up from 98.47% to 99.52% using Ring loss with λ =0.01). We find similar trends while using Ring loss with SphereFace. The LFW accuracy of SphereFace improves from 99.47% to 99.50%. We note that since even our baselines are high performing, there is a lot of variance in the results owing to the small size of the LFW protocol (just 6000 matches compared to about 8 million matches in the Janus CS3 protocol which shows clearer trends). Indeed we find clearer trends with MegaFace, IJB-A and CS3 all of which are orders of magnitude larger protocols.</p><p>Exp 2. Testing Benchmark: IJB-A Janus. IJB-A [11] is a challenging dataset which consists of 500 subjects with extreme pose, expression and illumination with a total of 25,813 images. Each subject is described by a template instead of a single image. This allows for score fusion techniques to be developed. The setting is suited for applications which have multiple sources of images/video frames. We report results on the 1:1 template matching protocol containing 10 splits with about 12,000 pair-wise template matches each resulting in a total of 117,420 template matches. The template matching score for two templates T i ,T j is determined by using the following formula,</p><formula xml:id="formula_8">S(T i ,T j )= P K γ=1 P ta 2T i ,t b 2T j s(ta,t b )expγs(ta,t b ) P ta 2T i ,t b 2T j exp γs(ta,t b )</formula><p>where s(t a .t b ) is the cosine similarity score between images t a ,t b and K =8.</p><p>Results: IJB-A Janus. <ref type="table" target="#tab_2">Table.</ref> 3 and <ref type="figure" target="#fig_7">Fig. 7</ref>(a) present these results. We see that Softmax + Ring loss (0.001) outperforms Softmax by a large margin, particularly 60.52% verification rate compared to 78.41% verification at 10 −5 FAR.</p><p>Further, it outperforms Center loss <ref type="bibr" target="#b26">[26]</ref> (46.01%) and l 2 -constrained Softmax (73.29%) <ref type="bibr" target="#b17">[17]</ref>. Although SphereFace performs better than Softmax + Ring loss, an augmentation by Ring loss boosts SphereFace's performance from 78.52% to 82.41% verification rate for λ =0 .01. This matches the state-of-the-art reported in <ref type="bibr" target="#b17">[17]</ref> which uses a 101-layer ResNext architecture despite our system using a much shallower 64-layer ResNet architecture. The effect of high λ akin to the effects simulated in <ref type="figure" target="#fig_3">Fig. 3</ref> show in this setting for λ =0.03 for SphereFace augmentation. We observe this trade-off in Janus CS3, CFP and MegaFace results as well. Nonetheless, we notice that Ring loss augmentation provides consistent improvements over a large range of λ for both Softmax and Sphereface. This is in sharp contrast with l 2 -constrained Softmax whose performance varies significantly with α rendering it difficult to optimize. In fact for α = 10, it performs worse than Softmax. Exp 3. Testing Benchmark: Janus CS3. The Janus CS3 dataset is a super set of the IARPA IJB-A dataset. It contains about 11,876 still images and 55,372 video frames from 7,094 videos. For the CS3 1:1 template verification protocol there are a total of 1,871 subjects and 12,590 templates. The CS3 template verification protocol has over 8 million template matches which amounts to an extremely large number of template verifications. There are about 1,870 templates in the gallery and about 10,700 templates in the probe. The CS3 protocol being a super set of IJB-A, has a large number of extremely challenging images and faces. The challenging conditions range from extreme illumination, extreme pose to significant occlusion. For sample images, please refer to <ref type="figure" target="#fig_4">Fig. 4</ref> and <ref type="figure" target="#fig_1">Fig. 10 in [12]</ref>, <ref type="figure" target="#fig_1">Fig. 1</ref> and <ref type="figure" target="#fig_5">Fig. 6</ref> in <ref type="bibr" target="#b2">[3]</ref>. Since the protocol is template matching, we utilize the same template score fusion technique we utilize in the IJB-A results with K =2.</p><p>Results: Janus CS3. <ref type="table" target="#tab_2">Table. 4</ref>  our results on the CS3 dataset. We report verification rates (VR) at 10 −3 through 10 −6 FAR. We find that our Ring loss augmented Softmax model outperforms the previous best reported results on the CS3 dataset. Recall that the Softmax + Ring loss model (SM + R) was trained only on a subset of the MS-Celeb dataset and achieves a VR of 74.56% at 10</p><formula xml:id="formula_9">−4</formula><p>FAR. This is in contrast to Lin et. al. who train on MS-Celeb plus CASIA-WebFace (an additional 0.5 million images) and achieve 72.52 %. Further, we find that even though our baseline Sphereface Res64 model outperforms the previous state-of-the-art, our Ring loss augmented Sphereface model outperforms all other models to achieve high a VR of 82.74 % at 10 −4 FAR. At very low FAR of 10 −6 our SF + R model achieves VR 35.18 % which to the best of our knowledge is the state-of-the-art on the challenging Janus CS3. In accordance with the results on IJB-A Janus, Ring loss provides consistent improvements over large ranges of λ whereas l 2 -constrained Softmax exhibits significant variation w.r.t. to its hyperparameter.</p><p>Exp 4. Testing Benchmark: MegaFace. The recently released MegaFace benchmark is extremely challenging which defines matching with a gallery of about 1 million distractors <ref type="bibr" target="#b10">[10]</ref>. The aspect of face recognition that this database test is discrimination in the presence of very large number of distractors. The testing database contains two sets of face images. The distractor set contains 1 million distractor subjects (images). The target set contain 100K images from 530 celebrities.</p><p>Result: MegaFace. <ref type="table" target="#tab_2">Table.</ref> 2 showcases our results. Even at this extremely large scale evaluation (evaluating FaceScrub against 1 million), the addition of Ring loss provides significant improvement to the baseline approaches. The identification rate (%) for Softmax upon the addition of Ring loss (λ =0 .001) improves from 56.36% to a high 71.67% and for SphereFace it improves from 74.95% to 75.22% for a single patch model. This is higher than the single patch model reported in the orginal Sphereface paper (72.72% <ref type="bibr" target="#b14">[14]</ref>). We outperform Center loss <ref type="bibr" target="#b26">[26]</ref> augmenting both Softmax (67.24%) and Sphereface (71.15%). We find that though for MegaFace, l 2 -constrained Softmax <ref type="bibr" target="#b17">[17]</ref> for α = 30 achieves 72.22%, there is yet again significant varia-  <ref type="bibr" target="#b20">[20]</ref>. This small dataset has about 7,000 pairs of matches defined with 3,500 same pairs and 3,500 not-same pairs for about 500 different subjects. For sample images please refer to <ref type="figure" target="#fig_1">Fig. 1</ref> of <ref type="bibr" target="#b20">[20]</ref>. The dataset presents a challenge since each of the probe images is almost entirely profile thereby presenting extreme pose along with illumination and expression challenges.</p><p>Result: CFP Frontal vs. Profile. <ref type="figure" target="#fig_5">Fig. 6</ref> showcases the ROC curves for this experiment whereas  <ref type="bibr" target="#b17">[17]</ref> 72.22 82.14 l2-Cons SM (20) <ref type="bibr" target="#b17">[17]</ref> 70.29 83.69 l2-Cons SM (10) <ref type="bibr" target="#b17">[17]</ref> 66.20 76.77 SM + CL <ref type="bibr" target="#b26">[26]</ref> 67.24 78.94 SF <ref type="bibr" target="#b14">[14]</ref> 74.95 89.94 SF + CL <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b14">14]</ref> 71   verification rates at 10 −3 FAR. Ring loss (87.43%) provides consistent and significant boost in performance over Softmax (55.86%). We find however, SphereFace required more careful tuning of λ with λ =0.01 (90.94%) outperforming the baseline. Further, Softmax and Ring loss with λ =0.01 significantly outperforms all runs for l 2 -constrained Softmax <ref type="bibr" target="#b17">[17]</ref> (83.69). Thus, Ring loss helps in providing higher verification rates while dealing with frontal to highly off-angle matches thereby explicitly demonstrating robustness to pose variation.</p><p>Exp 6. Low Resolution Experiments on Janus CS3.  One of the main motivations for l 2 -constrained Softmax was to handle images with varying resolution. Low resolution images were found to result in low norm features and vice versa. Ranjan et.al. <ref type="bibr" target="#b17">[17]</ref> argued normalization (through l 2 -constrained Softmax) would help deal with this issue. In order to test the efficacy of our alternate convex normalization formulation towards handling low resolution faces, we synthetically downsample Janus CS3 from an original size of (112 ⇥ 96) by a factor of 4x, 16x, 25x, 36x and 64x respectively (images were downsampled and resized back up using bicubic interpolation in order to fit the model). We run the Janus CS3 protocol and plot the ROC curves in <ref type="figure" target="#fig_8">Fig. 8</ref>. We find that the Ring loss helps Softmax features be more robust to resolution. Though l 2 -constrained Softmax provides improvement over Softmax, it's performance is lower than Ring loss. Further, at extremely high downsampling of 64x, l 2 -constrained Softmax in fact performs worse than Softmax, whereas Ring loss provides a clear improvement. Center loss fails early on at 16x. We therefore find that our simple convex soft normalization approach is more effective at arresting performance drop due to resolution in accordance with the motivation for as normalization presented in <ref type="bibr" target="#b17">[17]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample MNIST features trained using (a) Softmax and (b) Ring loss on top of Softmax. Ring loss uses a convex norm constraint to gradually enforce normalization of features to a learned norm value R. This results in features of equal length while mitigating classification margin imbalance between classes. Softmax achieves 98.97 % accuracy on MNIST, whereas Ring loss achieves 99.34 % demonstrating the superior performance of the network learned normalized features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) A simple case of binary classification. The shaded regions (yellow, green) denote the classification margin (for class 1 and 2). (b) Angular classification margin for θ 1 for different δ =cosθ 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Ring loss Visualizations: (a), (b) and (c) show the final convergence of the samples (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: MNIST features for digit 8 trained using vanilla Softmax loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: ROC curves on the CFP Frontal vs. Profile verification protocol. For all Figures and Tables, SM denotes Softmax, SF denotes SphereFace [14],l2-Cons SM denotes [17], + CL denotes Center Loss augmentation [26] and finally + R denotes Ring loss augmentation. Numbers in bracket denote value of hyperparameter (loss weight), i.e. α for [17], λ for Center loss and Ring loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: ROC curves on the (a) IJB-A Janus 1:1 verification protocol and the (b) Janus CS3 1:1 verification protocol. For all Figures and Tables, SM denotes Softmax, SF denotes SphereFace [14],l2-Cons SM denotes [17], + CL denotes Center Loss augmentation [26] and finally + R denotes Ring loss augmentation. Numbers in bracket denote value of hyperparameter (loss weight), i.e. α for [17], λ for Center loss and Ring loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: ROC curves for the downsampling experiment on Janus CS3. Ring loss (SM + R λ =0.01) learns the most robust features, whereas l 2 -constrained Softmax (l2-Cons SM α =30) [17] performs poorly (worse than the baseline Softmax) at very high downsampling factor of 64x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Accuracy (%) on LFW.tion in performance that occurs due to a change in the hyper parameter α (66.20% for α = 10 to 72.22% for α = 30). Ring loss hyper parameter (λ), as we find again, is more easily tunable and manageable. This results in a smaller variance in performance for both Softmax and SphereFace augmentations. Exp 5. Testing Benchmark: CFP Frontal vs. Profile. Recently the CFP (Celebrities Frontal-Profile) dataset was released to evaluate algorithms exclusively on frontal versus profile matches</figDesc><table>Method 
Training Data 
Accuracy (%) 
FaceNet [19] 
200M private 
99.65 
Deep-ID2+ [22] 
CelebFace+ 
99.15 
Range loss [28] 
WebFace 
99.52 
+Celeb1M(1.5M) 
Baidu [13] 
1.3M 
99.77 
Norm Face [25] 
WebFace 
99.19 

SM 
MS-Celeb 
98.47 
l2-Cons SM (30) [17] 
MS-Celeb 
99.55 
l2-Cons SM (20) [17] 
MS-Celeb 
99.47 
l2-Cons SM (10) [17] 
MS-Celeb 
99.45 
SM + CL [26] 
MS-Celeb 
99.17 
SF [14] 
MS-Celeb 
99.47 
SF + CL [26, 14] 
MS-Celeb 
99.52 
SM + R (0.01) 
MS-Celeb 
99.52 
SM + R (0.001) 
MS-Celeb 
99.50 
SM + R (0.0001) 
MS-Celeb 
99.28 
SF + R (0.03) 
MS-Celeb 
99.48 
SF + R (0.01) 
MS-Celeb 
99.43 
SF + R (0.001) 
MS-Celeb 
99.42 
SF + R (0.0001) 
MS-Celeb 
99.50 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table . 2</head><label>.</label><figDesc>shows the</figDesc><table>Method 

Acc % (MegaFace) 10 
−3 (CFP) 

SM 
56.36 
55.86 
l2-Cons SM (30) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Identification rates on MegaFace with 1 million distractors (Accu- racy %) and Verification rates at 10 −3 FAR for the CFP Frontal vs. Profile protocol.</figDesc><table>Method 
10 

−5 

10 

−4 

10 

−3 

l2-Cons SM* (101) [17] 
-
87.9 
93.7 
l2-Cons SM* (101x) [17] 
-
88.3 
93.8 
SM 
60.52 69.69 83.10 
l2-Cons SM (30) [17] 
73.29 80.65 90.72 
l2-Cons SM (20) [17] 
67.63 76.88 89.89 
l2-Cons SM (10) [17] 
53.74 68.58 83.42 
SM + CL [26] 
46.01 74.10 88.32 
SF [14] 
78.52 88.0 93.24 
SF + CL [26, 14] 
72.35 81.11 89.26 
SM + R (0.01) 
72.53 79.1 
90.8 
SM + R (0.001) 
78.41 85.0 
91.5 
SM + R (0.0001) 
69.23 82.30 89.20 
SF + R (0.03) 
79.54 85.37 91.64 
SF + R (0.01) 
82.41 
88.5 93.22 
SF + R (0.001) 
79.74 87.71 92.62 
SF + R (0.0001) 
80.13 86.34 92.57 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Verification % on the IJB-A Janus 1:1 verification protocol. l2- Cons SM* indicates the result reported in [17] which uses a 101 layer ResNet/ResNext architecture.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Verification % on the Janus CS3 1:1 verification protocol.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We in fact, find in our pilot study that the Softmax features also encode the 'difficulty' of the class.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Although, it is more common to in turn investigate competition between two weight vectors to classify a single sample, we find that this alternate perspective provide some novel and interesting insights. 3 Note that this reasoning is applicable to any loss function trying to enforce this inequality in some form.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">see https://github.com/ydwen/caffe-face.git 5 see https://github.com/craftGBD/caffe-GBD. In our experiments, for α =50the gradients exploded due the relatively deep Res64 architecture and learning α initialized at 30 did not converge. 6 For all Tables, results reported after double horizontal lines are from models trained during our study. The results above the lines reported directly from the paper as cited.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conclusion. We motivate feature normalization in a principled manner and develop an elegant, simple and straight forward to implement convex approach towards that goal. We find that Ring loss consistently provides significant improvements over a large range of the hyperparameter λ. Further, it helps the network itself to learn normalization thereby being robust to a large range of degradations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Faster than real-time facial alignment: A 3d spatial transformer network approach in unconstrained poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05653</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep heterogeneous feature fusion for templatebased face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Cosine normalization: Using cosine similarity instead of dot product in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chunjie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jianfeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007-10" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4873" to="4882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1931" to="1939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A proximityaware hierarchical clustering of faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04835</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Targeting ultimate accuracy: Face recognition via deep embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07310</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08063</idno>
		<title level="m">Sphereface: Deep hypersphere embedding for face recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="507" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning deep features via congenerous cosine loss for person recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06890</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09507</idno>
		<title level="m">L2-constrained softmax loss for discriminative face verification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Frontal to profile face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1988" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning a metric embedding for face recognition using the multibatch method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tadmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rosenwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1388" to="1389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Normface: L2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06369</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Range loss for deep face recognition with long-tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno>abs/1611.08976</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cms-rcnn: contextual multi-scale region-based cnn for unconstrained face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning for Biometrics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="57" to="79" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
