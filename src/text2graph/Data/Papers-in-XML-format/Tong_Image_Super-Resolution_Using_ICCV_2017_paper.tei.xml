<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader>
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:17+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Super-Resolution Using Dense Skip Connections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Tong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial Vision Technology Fuzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
							<email>ligen@imperial-vision.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Imperial Vision Technology Fuzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiejie</forename><surname>Liu</surname></persName>
							<email>liu.xiejie@imperial-vision.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Imperial Vision Technology Fuzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinquan</forename><surname>Gao</surname></persName>
							<email>gqinquan@imperial-vision.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Imperial Vision Technology Fuzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image Super-Resolution Using Dense Skip Connections</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract xml:lang="en">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text>
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The recovery of a high resolution (HR) image from a low resolution (LR) version is a highly ill-posed problem since the mapping from LR to HR space can have multiple solutions. When the upscaling factor is large, it becomes very challenging to recover the high-frequency details in image super-resolution (SR). Many SR techniques assume that the high-frequency information is redundant and can be accurately predicted from the low-frequency data. Therefore, it is important to collect useful contextual information in large regions from LR images so that sufficient knowledge can be captured for recovering the high-frequency details in HR images.</p><p>Recent works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> have successfully used very deep convolutional neural networks (CNN) to perform single image super-resolution (SISR), and significant improvements over shallow CNN structures <ref type="bibr" target="#b1">[2]</ref> have been observed. One benefit from using deeper networks is that larger receptive field takes more contextual information from LR images to predict data in HR images. However, it is challenging to effectively train a very deep CNN due to the vanishinggradient problem. One good solution to this problem is the use of skip connections, which create short paths from top layers to bottom layers. This helps the flow of information and gradient through the network, making it easy to train. In addition, in previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref>, only high-level features at top layers were used in the reconstruction of HR images. The features at low levels can potentially provide additional information to reconstruct the high-frequency details in HR images. Image SR may benefit from the collective knowledge of features at different levels. Moreover, previous studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref> have shown that redundant feature maps are learnt in different layers of deep networks. The reuse of feature maps from bottom layers is helpful for reducing feature redundancy, thus learning more compact CNN models.</p><p>In this work, we propose a novel super-resolution method termed SRDenseNet in which the dense connected convolutional networks were employed. The introduction of dense connections improves the flow of information through the network, alleviating the gradient vanishing problem. In addition, it allows the reuse of feature maps from preceding layers, avoiding the re-learning of redundant features. Different from previous works, we utilized the dense skip connections to combine the low-level features and high-level features in order to provide rich information for the SR reconstruction. Further, deconvolution layers were integrated to recover the image details and to speedup the reconstruction process. The proposed method has been evaluated on four publicly available benchmark datasets and outperforms the current state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Single image super-resolution</head><p>Many SISR methods have been developed in computer vision community. A detailed review of these methods can be found in <ref type="bibr" target="#b25">[26]</ref>. Among them, interpolation methods are easy to implement and widely adopted. However, these linear models have very limited representation power and often generate blurry high resolution outputs. Sparsity-based techniques <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b23">24]</ref> have recently developed to enhance linear models with rich image priors. These techniques assume that any natural image patch can be sparsely represented by a dictionary of atoms. The dictionary can be formed by a database of patches or learnt from the database <ref type="bibr" target="#b26">[27]</ref>. Such dictionary-based methods <ref type="bibr" target="#b24">[25]</ref> have achieved comparable state-of-the-art results. One drawback of these methods is that it is generally computationally expensive to find the solution of the sparse coding coefficients .</p><p>In addition to sparsity-based methods, other sophisticated learning techniques have been developed to model the mapping from LR to HR space, including neighbor embedding <ref type="bibr" target="#b3">[4]</ref>, random forest <ref type="bibr" target="#b19">[20]</ref> and convolutional neural network <ref type="bibr" target="#b1">[2]</ref>. Among them, the CNN-based approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> have recently set state of the art for SISR. A network with three layers was first developed in <ref type="bibr" target="#b1">[2]</ref> to learn an end-to-end mapping for SR. Subsequently, a deep network with 20 layers was proposed in <ref type="bibr" target="#b10">[11]</ref> to improve the reconstruction accuracy of CNN. The residuals between the HR images and the interpolated LR images were used in <ref type="bibr" target="#b10">[11]</ref> to speedup the converging speed in training and also to improve the reconstruction performance. Instead of using interpolation for upscaling as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref>, recent studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21]</ref> have demonstrated that the SR performance can be further improved both in terms of accuracy and speed by learning the upscaling filters. The upscaling operation can be effectively learnt by using deconvolution layers <ref type="bibr" target="#b2">[3]</ref> or sub-pixel convolution layers <ref type="bibr" target="#b20">[21]</ref>. In our work, we employ the very deep network and also integrate the deconvolution layers to further boost the reconstruction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Skip connections</head><p>As CNNs become increasingly deep, the problem of vanishing gradient hampers the training of networks. Many recent approaches have been proposed to address this problem. ResNets <ref type="bibr" target="#b5">[6]</ref> and Highway Networks <ref type="bibr" target="#b21">[22]</ref> use bypassing path between layers to effectively train networks with more than 100 layers. Stochastic depth <ref type="bibr" target="#b7">[8]</ref> randomly drops layers to improve the training of deep residual networks, which demonstrates a great amount of redundancy in deep residual networks. FractalNets <ref type="bibr" target="#b13">[14]</ref> combines several parallel networks with different depths and many short paths are created in the networks. DenseNets <ref type="bibr" target="#b6">[7]</ref> links all layers in the networks and tries to fully explore the advantages of skip connections. All these networks share a key idea: it is essential to build many skip connections between layers to effectively train a very deep network.</p><p>A skip connection was used in <ref type="bibr" target="#b11">[12]</ref> to link the input data and the final reconstruction layer in SR. State-of-the-art SR results were achieved in <ref type="bibr" target="#b11">[12]</ref>. However, only a single skip connection was adopted in <ref type="bibr" target="#b11">[12]</ref>, which may not fully explore the advantages of skip connections. Many symmetric skip connections were introduced in an encoding-decoding network <ref type="bibr" target="#b16">[17]</ref> for image restoration tasks. However, the improvement of the SR performance over the DRCN method <ref type="bibr" target="#b11">[12]</ref> that used a single skip connection is marginal. An effective way of using a reasonable amount of skip connections in very deep CNNs may potentially improve the SR reconstruction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Contribution</head><p>Skip connections can alleviate the vanishing-gradient problem and enhance the feature propagation in deep networks. In this work, we introduce dense skip connections in a deep network for SISR. Our main contributions are:</p><p>• We demonstrate that the deep CNN framework with the denseNet as basic blocks can achieve good reconstruction performance and that the fusion of features at different levels through dense skip connections can further boost the reconstruction performance for SISR.</p><p>• New state-of-the-art results have been achieved on four benchmark datasets with a upscaling factor of 4 and visual improvements can be easily noticed in the SR results. The proposed framework not only achieves impressive results but also can be implemented very fast.</p><p>The proposed network structure is introduced in Section 3, followed by the experimental results and visual comparsions with state-of-the-art results in Section 4. A further discussion is provided in Section 5 and the paper concludes in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The proposed network aims to learn an end-to-end mapping function F between the LR image I L and the HR image I H . As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, SRDenseNet can be decomposed into several parts: the convolution layer for learning low-level feature, the blocks of DenseNet for learning highlevel features, the deconvolution layers for learning upscaling filters and the reconstruction layer for generating the HR output. Each convolution or deconvolution layer is followed by a ReLu layer for nonlinear mapping except the reconstruction layer. The ReLu activation function is applied element-wise. Let X i−1 be the input, the output of i th convolution or deconvolution layer is expressed as:</p><formula xml:id="formula_0">X i = max(0, w i * X i−1 + b i )<label>(1)</label></formula><p>where W i and B i are the weights and biases in the layer, and * denotes either convolution or deconvolution operation for the convenience of formulation. Let Θ denote all </p><formula xml:id="formula_1">l(Θ) = 1 N N k=1 F (I k L , Θ) − I k H 2 2<label>(2)</label></formula><p>Adam <ref type="bibr" target="#b12">[13]</ref> is used to find the optimum weights and biases in the above equation. In the following, we will describe the details of the proposed network structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">DenseNet blocks</head><p>After applying a convolution layer to the input LR images for learning low-level features, a set of DenseNet blocks are adopted for learning the high-level features. The DenseNet structure was first proposed in <ref type="bibr" target="#b6">[7]</ref>. Different from ResNets as proposed in <ref type="bibr" target="#b5">[6]</ref>, the feature maps are concatenated in DenseNet rather than directly summed. Consequently, the i th layer receives the feature maps of all preceding layers as input:</p><formula xml:id="formula_2">X i = max(0, w i * [X 1 , X 2 , ..., X i−1 ] + b i )<label>(3)</label></formula><p>where [X 1 , X 2 , ..., X i−1 ] represents the concatenation of the feature maps generated in the preceding convolution layers 1, 2, ..., i − 1. In the structure of DenseNet, short paths are created between a layer and every other layer. This strengthens the flow of information through deep networks, thus alleviating the vanishing-gradient problem. In addition, DenseNet can substantially reduce the number of parameters through feature reuse, thus requiring less memory and computation to achieve high performance <ref type="bibr" target="#b6">[7]</ref>. Here, we employ the DenseNet structure as a building block in our network. The structure of each denseNet block can be seen in <ref type="figure" target="#fig_1">Figure 2</ref>. Specifically, there are 8 convolution layers in one DenseNet block in our work. If each convolution layer produce k feature maps as output, the total number of feature maps generated by one DenseNet block is k * 8, where k is refered to as growth rate. The growth rate k regulates how much new information each layer contributes to the final reconstruction. To prevent the network from growing too wide, the growth rate k is set to 16 in this study. This results in a total number of 128 feature maps from one DenseNet block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Deconvolution layers</head><p>In previous SR methods such as SRCNN <ref type="bibr" target="#b1">[2]</ref> and VDSR <ref type="bibr" target="#b10">[11]</ref>, bicubic interpolation is used to upscale LR images to the HR space. After that, the SR process including the computationally expensive convolution is carried out in the HR space. This increases the computational complexity for SR. In addition, interpolation approaches do not bring new information for solving the SR problem. Therefore, recent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref> have employed deconvolution layers to learn the upscaling filters, which can also recover the image details. The deconvolution layer can be considered as an inverse operation of a convolution layer. It can learn diverse upscaling kernels that work jointly for predicting the HR images. There are two advantages in using the deconvolution layers for upscaling. First, it accelerates the SR reconstruction process. After the deconvolution layers are added at the end of networks, the whole computational process is performed in the LR space. If the upscaling factor is r, it will reduce the computational cost by a factor of r 2 . In addition, a large amount of contextual information from the LR images is used to infer the high frequency details. Using the same depth, the receptive field of the network with deconvolution layers at the end is about r 2 times larger than that of the network using interpolation at the beginning. In our work, two successive deconvolution layers with small 3 × 3 kernels and 256 feature maps are trained for upscaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Combination of feature maps</head><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, three different types of network structures were studied and compared in our work. As in previous methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref>, only the feature maps at the top layer are used as input for reconstructing the HR output. We denote this structure as SRDenseNet H which is shown in <ref type="figure" target="#fig_0">Figure 1 (a)</ref>. Further, a skip connection is introduced in the network as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (b) to concatenate the low-level and high-level features, which we term SRDenseNet HL. The concatenated feature maps are then used as input for deconvolution layers. In addition, we use dense skip connections to combine the feature maps produced at all convolution layers for SR reconstruction, and denote this method as SRDenseNet All. A comparison between the SR results using different network structures will be performed in the experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Bottleneck and Reconstruction layers</head><p>In the proposed SRDenseNet All as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (c), all feature maps in the network are concatenated, yielding many inputs for the subsequent deconvolution layers. If the large number of feature maps are directly fed into deconvolution layers, it will significantly increase the computational cost and the model size. Thus, it is necessary to reduce the number of input feature maps in order to keep model compactness and to improve the computational efficiency. It has been demonstrated in previous studies <ref type="bibr" target="#b22">[23]</ref> that a convolution layer with 1 × 1 kernel can be used as a bottleneck layer to reduce the number of input feature maps. To improve the model compactness and computational efficiency, we employ a bottleneck layer to reduce the number of feature maps before feeding them to the deconvolution layers. The number of feature maps is reduced to 256 using 1 × 1 bottleneck layer. After that, the deconvolution layers transform the 256 feature maps from the LR space to the HR space. Finally, the feature maps in the HR space are used to generate HR images via a reconstruction layer. The reconstruction layer is a convolution layer with 3 × 3 kernel and one channel of output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluated the performance of the proposed method on four benchmark datasets. A description of the datasets is first provided, followed by the introduction of the implementation details. The benefit of using different levels of features is then introduced. After that, comparisons with state-of-the-art results are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and metrics</head><p>During the evaluation, we used publicly available benchmark datasets for training and testing. Specifically, 50,000 images were randomly selected from ImageNet for the training. During testing, the dataset Set5 <ref type="bibr" target="#b0">[1]</ref> and Set14 <ref type="bibr" target="#b28">[29]</ref> are often used for SR benchmark. The B100 from the Berkeley segmentation dataset <ref type="bibr" target="#b17">[18]</ref> consisting of 100 natural images were used for testing. In addition, the proposed method was also evaluated using the Urban100 dataset <ref type="bibr" target="#b8">[9]</ref> which includes 100 challenging images. All experiments were performed using a scale factor of 4× between LR and HR images. The peak signal-to-noise ratio (PSNR) and the structural similarity (SSIM) index were used as metrics for evaluation. Since SR was performed in the luminance channel in YCbCr colour space, the PSNR and SSIM were calculated on the Y-channel of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>Non-overlapping sub-images with a size of 100 × 100 were cropped in the HR space. The LR images were obtained by downsampling the HR images using bicubic ker- nel with a scale factor of 4×. As suggested by previous studies, each image has been transformed into YCbCr space and only the Y-channel was used for training. In all networks, 8 DenseNet blocks were used, resulting in 64 convolution layers. Within each block, a growth rate of 16 was set. This generated an output of 128 feature maps from each block. The filter size was set to 3 × 3 in all weight layers. The weights were initialized using the method proposed in <ref type="bibr" target="#b4">[5]</ref> and the biases were initialized to zero. The rectified linear units (ReLu) was used as the activation function. All the networks were optimized using Adam <ref type="bibr" target="#b12">[13]</ref>. The learning rate was initially set to 0.0001 and decreased by a factor of 10 after 30 epoches. A mini-batch size of 32 was set during the training. The training process stopped after no improvements of the loss was observed after 60 epoches. A NVIDIA Titan X GPU was used for training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Benefit of feature combination</head><p>The reconstruction performance using the three types of network structures as shown in <ref type="figure" target="#fig_0">Figure 1</ref> were compared. <ref type="table">Table 1</ref> shows the obtained PSNR and SSIM values on four datasets. As expected, SRDenseNet HL achieved better results than SRDenseNet H after adding a skip connection. This indicates that the combination of low-level features and high-level feature can improve the SR reconstruction performance. A further improvement was observed by concatenating all levels of features. This suggests that there are complementary information among different levels of feature maps for SR. The improvements by combining different levels of features can also be seen in <ref type="figure" target="#fig_3">Figure 4.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with state-of-the-art methods</head><p>We compared the results using the proposed method and those using other SISR methods, including bicubic, Aplus <ref type="bibr" target="#b23">[24]</ref>, SRCNN <ref type="bibr" target="#b1">[2]</ref>, VDSR <ref type="bibr" target="#b10">[11]</ref> and DRCN <ref type="bibr" target="#b11">[12]</ref>. The implementations of these methods have been released online and thus can be carried out on the same datasets for fair comparisons. For SRCNN, the best 9-5-5 image model was used for comparison in this section. As for the Aplus method <ref type="bibr" target="#b23">[24]</ref>, it did not predict image boundaries. To enable a fair comparison, the borders of HR images were cropped so that all the results had the same region. The public code in <ref type="bibr" target="#b8">[9]</ref> was used for calculating the evaluation metrics. <ref type="table">Table 2</ref> shows the average PSNR and SSIM values on four benchmark datasets. In terms of PSNR, the proposed method achieves an improvement of 0.2dB-0.8dB over state-of-theart results on different datasets. On average, an increase of about 1.0 dB using the proposed method was achieved over SRCNN <ref type="bibr" target="#b1">[2]</ref> with 3-layer CNN and an increase of about 0.5 dB over VDSR <ref type="bibr" target="#b10">[11]</ref> with 20-layer CNN. It should be mentioned that the most significant improvement is obtained on the very challenging dataset Urban100.</p><p>Visual comparisons using different methods are given in <ref type="figure" target="#fig_2">Figures 3 and 5</ref>. In <ref type="figure" target="#fig_2">Figure 3</ref>, only the proposed method can well reconstruct the lines and the contours while other methods generate blurry results. In addition, severe distortions are found in some reconstructed results using existing methods (i.e. middle panel in <ref type="figure">Figure 5</ref>) whereas our method can reconstruct the texture pattern and avoid the distortions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and future work</head><p>When the proposed SRDenseNet All is unfolded, the longest chain has 69 weight layers and 68 activation layers. The SR task can benefit from using this very deep network in two aspects: (a) since the size of the receptive field is proportional to the depth, a large amount of contextual information in LR images can be utilized to infer the high frequency information in HR images; (b) due to the use of many ReLu layers, high nonlinearity can be exploited in very deep networks to model the complex mapping functions between LR image and HR images. One challenging problem in very deep network is the vanishing-gradient problem. In this work, we utilized the DenseNet structure as building blocks to alleviate this problem. DenseNets allow layers to use feature maps from their preceding layers. This provides an effective way to reuse feature maps that are already learnt and forces the current layer to learn complementary information, thus avoiding the learning of redundant features. In addition, each layer has a short path  to the loss in the proposed network, leading to an implicit deep supervision <ref type="bibr" target="#b15">[16]</ref>. This can help the training of very deep networks and improve the reconstruction performance in SR <ref type="bibr" target="#b11">[12]</ref>. Several techniques were proposed to improve the accuracy and to speedup the SR process, contributing to the novelty of the proposed framework. In order to improve the reconstruction accuracy, three techniques were proposed and integrated. (a) First, the DenseNet was used as a basic block in our network. This is the first work that uses denseNet for SR. One benefit of using the DenseNet Block is to avoid the gradient vanishing problem, allowing us to train very deep  In addition, the proposed framework not only achieves impressive results but also can be implemented very fast. This was resulted from three aspects: (a) The adoption of the 1*1 convolutional layer significantly reduced the parameters of the network; (b) The use of deconvolutional layers transferred the convolution process from high-resolution space to low-resolution space, thus substantially reducing the computational complexity. (c) A small growth rate of 16 was set in the DenseNet blocks. This means that only 16 new feature maps are required to learn for each convolutional layers. Although the growth rate is low, the total number of features is still large due to the fusion of different levels of features as mentioned above. This enables feature reuse and provides rich information for reconstructing the high-resolution images. In the end, we have achieved an average speed of 36.8ms for super-resolving one single image from the B100 dataset on a Titan X GPU, reaching a real-time SR with a scaling factor of 4×.</p><p>In this work, only the MSE loss is used for guiding the training of networks. The use of MSE loss can lead to results with high PSNR values. However, high PSNR values do not necessarily represent visually pleasing results. Recently, perceptual loss was proposed in <ref type="bibr" target="#b9">[10]</ref> for SR to replace the low-level pixel-wise loss. Further, adversarial loss using a generative adversarial network (GAN) was added to the loss function in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref> and photo-realistic SR images can be generated. Although the generated high frequency details in SRGAN <ref type="bibr" target="#b14">[15]</ref> may be 'fake' texture patterns, it yields visually pleasing high-resolution images. Note that the proposed method can provide a very good generator net- <ref type="figure">Figure 5</ref>. Super-resolution results for "148026" from B100 (top figure),"253027" from B100 (middle figure) and "ppt3" from Set14 (bottom figure) with an upscaling factor of 4. PSNR and SSIM values are shown on the top of each sub-figure. Severe distortions are found in the results of "253027" from B100 using other methods while the proposed method can accurately reconstruct the original pattern.</p><p>work initialized for GAN. It would be very interesting to investigate the integration of perceptual loss in the proposed framework in order to improve the visualized quality of the reconstructed images in future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have presented a novel network that employs dense skip connections for SR. The proposed approach outperforms state-of-the-art methods by a considerable margin on four benchmark datasets in terms of PSNR and SSIM. Noticeable improvement can visually be found in the reconstruction results. In addition, we have demonstrated that the combination of features at different level is helpful for improving SR performance. Future work will focus on the integration of perceptual loss in the proposed network to reconstruct photo-realistic HR images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Different structures of the proposed networks. (a) SRDenseNet H: only the high-level feature maps are used as input for reconstructing the HR images. (b) SRDenseNet HL: the low-level and the high-level features are combined as input for reconstructing the HR images. (c) SRDenseNet All: all levels of features are combined via skip connections as input for reconstructing the HR images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The structure of one DenseNet block. Each block consists of 8 convolution layers. The growth rate is set to 16 and the output of each block has 128 feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Super-resolution results for "img096" (top figure),"img099" (middle figure) and "img004" (bottom figure) from Urban100 with an upscaling factor of 4. PSNR and SSIM values are shown on the top of each sub-figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparison of PSNR and SSIM values on the Urban100 dataset using three different network structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Table 1. Comparison of results in terms of PSNR/SSIM on four benchmark data using three different network structures.</figDesc><table>Dataset 
SRDenseNet H SRDenseNet HL SRDenseNet All 
Urban100 
25.69/0.7700 
25.86/0.7761 
26.05/0.7819 
Set5 
31.66/0.8882 
31.80/0.8907 
32.02/0.8934 
Set14 
28.34/0.7744 
28.40/0.7765 
28.50/0.7782 
B100 
27.42/0.7300 
27.47/0.7318 
27.53/0.7337 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Table 2. Comparison of SR results in terms of PSNR/SSIM using different methods. All means the combination of four datasets including 219 testing images.</figDesc><table>Dataset 

Bicubic 
Aplus [24] 
SRCNN [2] 
VDSR [11] 
DRCN [12] SRDenseNet All 
Urban100 23.14/0.6577 24.32/0.7183 24.52/0.7221 25.18/0.7524 25.14/0.7510 
26.05/0.7819 
Set5 
28.42/0.8104 30.28/0.8603 30.48/0.8628 31.35/0.8838 31.53/0.8854 
32.02/0.8934 
Set14 
26.00/0.7027 27.32/0.7491 27.49/0.7503 28.01/0.7674 28.02/0.7670 
28.50/0.7782 
B100 
25.96/0.6675 26.82/0.7087 26.90/0.7101 27.29/0.7251 27.23/0.7233 
27.53/0.7337 
All 
24.73/0.6685 25.79/0.7191 25.93/0.7216 26.47/0.7439 26.42/0.7424 
27.02/0.7622 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><forename type="middle">A</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Accelerating the superresolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image super-resolution with sparse neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3194" to="3205" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<title level="m">Densely connected convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<title level="m">Fractalnet: Ultra-deep neural networks without residuals. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In press</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image restoration using convolutional auto-encoders with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems</title>
		<meeting>the Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07919</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Naive bayes superresolution forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pérez-Pellitero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="325" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2377" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="111" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Seven ways to improve example-based single image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1865" to="1873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Single-image superresolution: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="372" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Coupled dictionary training for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3467" to="3478" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image superresolution as sparse representation of raw image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on curves and surfaces</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
