<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fisher GAN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Gan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Equal Contribution AI Foundations</orgName>
								<orgName type="institution" key="instit2">IBM Research AI IBM T.J Watson Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Mroueh</surname></persName>
							<email>mroueh@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Equal Contribution AI Foundations</orgName>
								<orgName type="institution" key="instit2">IBM Research AI IBM T.J Watson Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">⇤</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Equal Contribution AI Foundations</orgName>
								<orgName type="institution" key="instit2">IBM Research AI IBM T.J Watson Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Sercu</surname></persName>
							<email>tom.sercu1@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Equal Contribution AI Foundations</orgName>
								<orgName type="institution" key="instit2">IBM Research AI IBM T.J Watson Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Mroueh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Sercu</surname></persName>
						</author>
						<title level="a" type="main">Fisher GAN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Generative Adversarial Networks (GANs) are powerful models for learning complex distributions. Stable training of GANs has been addressed in many recent works which explore different metrics between distributions. In this paper we introduce Fisher GAN which fits within the Integral Probability Metrics (IPM) framework for training GANs. Fisher GAN defines a critic with a data dependent constraint on its second order moments. We show in this paper that Fisher GAN allows for stable and time efficient training that does not compromise the capacity of the critic, and does not need data independent constraints such as weight clipping. We analyze our Fisher IPM theoretically and provide an algorithm based on Augmented Lagrangian for Fisher GAN. We validate our claims on both image sample generation and semi-supervised classification using Fisher GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b0">[1]</ref> have recently become a prominent method to learn high-dimensional probability distributions. The basic framework consists of a generator neural network which learns to generate samples which approximate the distribution, while the discriminator measures the distance between the real data distribution, and this learned distribution that is referred to as fake distribution. The generator uses the gradients from the discriminator to minimize the distance with the real data distribution. The distance between these distributions was the object of study in <ref type="bibr" target="#b1">[2]</ref>, and highlighted the impact of the distance choice on the stability of the optimization. The original GAN formulation optimizes the Jensen-Shannon divergence, while later work generalized this to optimize f-divergences [3], KL [4], the Least Squares objective <ref type="bibr" target="#b4">[5]</ref>. Closely related to our work, Wasserstein GAN (WGAN) [6] uses the earth mover distance, for which the discriminator function class needs to be constrained to be Lipschitz. To impose this Lipschitz constraint, WGAN proposes to use weight clipping, i.e. a data independent constraint, but this comes at the cost of reducing the capacity of the critic and high sensitivity to the choice of the clipping hyper-parameter. A recent development Improved Wasserstein GAN (WGAN-GP) <ref type="bibr" target="#b6">[7]</ref> introduced a data dependent constraint namely a gradient penalty to enforce the Lipschitz constraint on the critic, which does not compromise the capacity of the critic but comes at a high computational cost.</p><p>We build in this work on the Integral probability Metrics (IPM) framework for learning GAN of [8].</p><p>Intuitively the IPM defines a critic function f , that maximally discriminates between the real and fake distributions. We propose a theoretically sound and time efficient data dependent constraint on the critic of Wasserstein GAN, that allows a stable training of GAN and does not compromise the capacity of the critic. Where WGAN-GP uses a penalty on the gradients of the critic, Fisher GAN imposes a constraint on the second order moments of the critic. This extension to the IPM framework is inspired by the Fisher Discriminant Analysis method.</p><p>The main contributions of our paper are:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1. We introduce in Section 2 the Fisher IPM, a scaling invariant distance between distributions. Fisher IPM introduces a data dependent constraint on the second order moments of the critic that discriminates between the two distributions. Such a constraint ensures the boundedness of the metric and the critic. We show in Section 2.2 that Fisher IPM when approximated with neural networks, corresponds to a discrepancy between whitened mean feature embeddings of the distributions. In other words a mean feature discrepancy that is measured with a Mahalanobis distance in the space computed by the neural network. 2. We show in Section 3 that Fisher IPM corresponds to the Chi-squared distance (</p><p>2 ) when the critic has unlimited capacity (the critic belongs to a universal hypothesis function class). Moreover we prove in Theorem 2 that even when the critic is parametrized by a neural network, it approximates the 2 distance with a factor which is a inner product between optimal and neural network critic. We finally derive generalization bounds of the learned critic from samples from the two distributions, assessing the statistical error and its convergence to the Chi-squared distance from finite sample size. 3. We use Fisher IPM as a GAN objective <ref type="bibr" target="#b0">1</ref> and formulate an algorithm that combines desirable properties <ref type="table" target="#tab_0">(Table 1)</ref>: a stable and meaningful loss between distributions for GAN as in Wasserstein GAN <ref type="bibr" target="#b5">[6]</ref>, at a low computational cost similar to simple weight clipping, while not compromising the capacity of the critic via a data dependent constraint but at a much lower computational cost than <ref type="bibr" target="#b6">[7]</ref>. Fisher GAN achieves strong semi-supervised learning results without need of batch normalization in the critic. 2 Learning GANs with Fisher IPM 2.1 Fisher IPM in an arbitrary function space: General framework Integral Probability Metric (IPM). Intuitively an IPM defines a critic function f belonging to a function class F , that maximally discriminates between two distributions. The function class F defines how f is bounded, which is crucial to define the metric. More formally, consider a compact space X in R d . Let F be a set of measurable, symmetric and bounded real valued functions on X . Let P(X ) be the set of measurable probability distributions on X . Given two probability distributions P, Q 2 P(X ), the IPM indexed by a symmetric function space F is defined as follows <ref type="bibr" target="#b9">[10]</ref>:</p><formula xml:id="formula_0">d F (P, Q) = sup f 2F n E x⇠P f (x) E x⇠Q f (x) o .<label>(1)</label></formula><p>It is easy to see that d F defines a pseudo-metric over P(X ). Note specifically that if F is not bounded, sup f will scale f to be arbitrarily large. By choosing F appropriately <ref type="bibr" target="#b10">[11]</ref>, various distances between probability measures can be defined. First formulation: Rayleigh Quotient. In order to define an IPM in the GAN context, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref> impose the boundedness of the function space via a data independent constraint. This was achieved via restricting the norms of the weights parametrizing the function space to a`p ball. Imposing such a data independent constraint makes the training highly dependent on the constraint hyper-parameters and restricts the capacity of the learned network, limiting the usability of the learned critic in a semisupervised learning task. Here we take a different angle and design the IPM to be scaling invariant as a Rayleigh quotient. Instead of measuring the discrepancy between means as in Equation <ref type="formula" target="#formula_0">(1)</ref>, we measure a standardized discrepancy, so that the distance is bounded by construction. Standardizing this discrepancy introduces as we will see a data dependent constraint, that controls the growth of the weights of the critic f and ensures the stability of the training while maintaining the capacity of the critic. Given two distributions P, Q 2 P(X ) the Fisher IPM for a function space F is defined as follows: <ref type="figure">Figure 1</ref>: Illustration of Fisher IPM with Neural Networks. ! is a convolutional neural network which defines the embedding space. v is the direction in this embedding space with maximal mean separation hv, µ</p><formula xml:id="formula_1">d F (P, Q) = sup f 2F E x⇠P [f (x)] E x⇠Q [f (x)] p 1 /2E x⇠P f 2 (x) + 1 /2E x⇠Q f 2 (x) . (2) Real P Fake Q x ! ! (x) 2 R m v</formula><formula xml:id="formula_2">! (P) µ ! (Q)i, constrained by the hyperellipsoid v &gt; ⌃ ! (P; Q) v = 1.</formula><p>While a standard IPM (Equation <ref type="formula" target="#formula_0">(1)</ref>) maximizes the discrepancy between the means of a function under two different distributions, Fisher IPM looks for critic f that achieves a tradeoff between maximizing the discrepancy between the means under the two distributions (between class variance), and reducing the pooled second order moment (an upper bound on the intra-class variance).</p><p>Standardized discrepancies have a long history in statistics and the so-called two-samples hypothesis testing. For example the classic two samples Student's t test defines the student statistics as the ratio between means discrepancy and the sum of standard deviations. It is now well established that learning generative models has its roots in the two-samples hypothesis testing problem <ref type="bibr" target="#b11">[12]</ref>. Non parametric two samples testing and model criticism from the kernel literature lead to the so called maximum kernel mean discrepancy (MMD) <ref type="bibr" target="#b12">[13]</ref>. The MMD cost function and the mean matching IPM for a general function space has been recently used for training GAN <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Interestingly Harchaoui et al <ref type="bibr" target="#b15">[16]</ref> proposed Kernel Fisher Discriminant Analysis for the two samples hypothesis testing problem, and showed its statistical consistency. The Standard Fisher discrepancy used in Linear Discriminant Analysis (LDA) or Kernel Fisher Discriminant Analysis (KFDA) can be written: sup</p><formula xml:id="formula_3">f 2F ✓ E x⇠P [f (x)] E x⇠Q [f (x)] ◆ 2 Var x⇠P (f(x))+Var x⇠Q (f(x)) , where Var x⇠P (f(x)) = E x⇠P f 2 (x) (E x⇠P (f(x))) 2 .</formula><p>Note that in LDA F is restricted to linear functions, in KFDA F is restricted to a Reproducing Kernel Hilbert Space (RKHS). Our Fisher IPM (Eq (2)) deviates from the standard Fisher discrepancy since the numerator is not squared, and we use in the denominator the second order moments instead of the variances. Moreover in our definition of Fisher IPM, F can be any symmetric function class. Second formulation: Constrained form. Since the distance is scaling invariant, d F can be written equivalently in the following constrained form:</p><formula xml:id="formula_4">d F (P, Q) = sup f 2F , 1 2 E x⇠P f 2 (x)+ 1 2 E x⇠Q f 2 (x)=1 E (f ) := E x⇠P [f (x)] E x⇠Q [f (x)].<label>(3)</label></formula><p>Specifying P, Q: Learning GAN with Fisher IPM. We turn now to the problem of learning GAN with Fisher IPM. Given a distribution P r 2 P(X ), we learn a function g ✓ : Z ⇢ R nz ! X , such that for z ⇠ p z , the distribution of g ✓ (z) is close to the real data distribution P r , where p z is a fixed distribution on Z (for instance z ⇠ N (0, I nz )). Let P ✓ be the distribution of g ✓ (z), z ⇠ p z . Using Fisher IPM (Equation <ref type="formula" target="#formula_4">(3)</ref>) indexed by a parametric function class F p , the generator minimizes the IPM: min</p><formula xml:id="formula_5">g ✓ d Fp (P r , P ✓ ).</formula><p>Given samples {x i , 1 . . . N} from P r and samples {z i , 1 . . . M} from p z we shall solve the following empirical problem:</p><formula xml:id="formula_6">min g ✓ sup fp2FpÊ (f p , g ✓ ) := 1 N N X i=1 f p (x i ) 1 M M X j=1 f p (g ✓ (z j )) Subject to⌦(f p , g ✓ ) = 1,<label>(4)</label></formula><formula xml:id="formula_7">where⌦(f p , g ✓ ) = 1 2N P N i=1 f 2 p (x i ) + 1 2M P M j=1 f 2 p (g ✓ (z j )).</formula><p>For simplicity we will have M = N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fisher IPM with Neural Networks</head><p>We will specifically study the case where F is a finite dimensional Hilbert space induced by a neural network ! (see <ref type="figure">Figure 1</ref> for an illustration). In this case, an IPM with data-independent constraint will be equivalent to mean matching <ref type="bibr" target="#b7">[8]</ref>. We will now show that Fisher IPM will give rise to a whitened mean matching interpretation, or equivalently to mean matching with a Mahalanobis distance. Rayleigh Quotient. Consider the function space F v,! , defined as follows</p><formula xml:id="formula_8">F v,! = {f (x) = hv, ! (x)i |v 2 R m , ! : X ! R m }, !</formula><p>is typically parametrized with a multi-layer neural network. We define the mean and covariance (Gramian) feature embedding of a distribution as in McGan <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_9">µ ! (P) = E x⇠P ( ! (x)) and ⌃ ! (P) = E x⇠P ! (x) ! (x) &gt; ,</formula><p>Fisher IPM as defined in Equation <ref type="formula">(2)</ref> on F v,! can be written as follows:</p><formula xml:id="formula_10">d Fv,! (P, Q) = max ! max v hv, µ ! (P) µ ! (Q)i q v &gt; ( 1 2 ⌃ ! (P) + 1 2 ⌃ ! (Q) + I m )v ,<label>(5)</label></formula><p>where we added a regularization term ( &gt; 0) to avoid singularity of the covariances. Note that if Constrained Form. Since the Rayleigh Quotient is not amenable to optimization, we will consider Fisher IPM as a constrained optimization problem. By virtue of the scaling invariance and the constrained form of the Fisher IPM given in Equation <ref type="formula" target="#formula_4">(3)</ref>, d Fv,! can be written equivalently as:</p><formula xml:id="formula_11">d Fv,! (P, Q) = max !,v,v &gt; ( 1 2 ⌃!(P)+ 1 2 ⌃!(Q)+ Im)v=1 hv, µ ! (P) µ ! (Q)i<label>(6)</label></formula><p>Define the pooled covariance:</p><formula xml:id="formula_12">⌃ ! (P; Q) = 1 2 ⌃ ! (P) + 1 2 ⌃ ! (Q) + I m . Doing a simple change of variable u = (⌃ ! (P; Q)) 1 2 v we see that: d Fu,! (P, Q) = max ! max u,kuk=1 D u, (⌃ ! (P; Q)) 1 2 (µ ! (P) µ ! (Q)) E = max ! (⌃ ! (P; Q)) 1 2 (µ ! (P) µ ! (Q)) ,<label>(7)</label></formula><p>hence we see that fisher IPM corresponds to the worst case distance between whitened means. Since the means are white, we don't need to impose further constraints on ! as in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>. Another interpretation of the Fisher IPM stems from the fact that:</p><formula xml:id="formula_13">d Fv,! (P, Q) = max ! q (µ ! (P) µ ! (Q)) &gt; ⌃ 1 ! (P; Q)(µ ! (P) µ ! (Q))</formula><p>, from which we see that Fisher IPM is a Mahalanobis distance between the mean feature embeddings of the distributions. The Mahalanobis distance is defined by the positive definite matrix ⌃ w (P; Q). We show in Appendix A that the gradient penalty in Improved Wasserstein <ref type="bibr" target="#b6">[7]</ref> gives rise to a similar Mahalanobis mean matching interpretation. Learning GAN with Fisher IPM. Hence we see that learning GAN with Fisher IPM:</p><formula xml:id="formula_14">min g ✓ max ! max v,v &gt; ( 1 2 ⌃!(Pr)+ 1 2 ⌃!(P ✓ )+ Im)v=1</formula><p>hv, µ w (P r ) µ ! (P ✓ )i corresponds to a min-max game between a feature space and a generator. The feature space tries to maximize the Mahalanobis distance between the feature means embeddings of real and fake distributions. The generator tries to minimize the mean embedding distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Theory</head><p>We will start first by studying the Fisher IPM defined in Equation <ref type="formula">(2)</ref> when the function space has full capacity i.e when the critic belongs to L 2 (X , 1 2 (P + Q)) meaning that</p><formula xml:id="formula_15">R X f 2 (x) (P(x)+Q(x)) 2</formula><p>dx &lt; 1. Theorem 1 shows that under this condition, the Fisher IPM corresponds to the Chi-squared distance between distributions, and gives a closed form expression of the optimal critic function f (See Appendix B for its relation with the Pearson Divergence). Proofs are given in Appendix D.  <ref type="figure">Fig (b, c)</ref> show the exact 2 distance from numerically integrating Eq (8), together with the estimate obtained from training a 5-layer MLP with layer size = 16 and LeakyReLU nonlinearity on different training sample sizes. The MLP is trained using Algorithm 1, where sampling from the generator is replaced by sampling from Q, and the 2 MLP estimate is computed with Equation <ref type="formula">(2)</ref> on a large number of samples (i.e. out of sample estimate). We see in (b) that for large enough sample size, the MLP estimate is extremely good. In (c) we see that for smaller sample sizes, the MLP approximation bounds the ground truth 2 from below (see Theorem 2) and converges to the ground truth roughly as O( 1 p N ) (Theorem 3). We notice that when the distributions have small 2 distance, a larger training size is needed to get a better estimateagain this is in line with Theorem 3.</p><p>Theorem 1 (Chi-squared distance at full capacity). Consider the Fisher IPM for F being the space of all measurable functions endowed by</p><formula xml:id="formula_16">1 2 (P + Q), i.e. F := L 2 (X , P+Q 2 )</formula><p>. Define the Chi-squared distance between two distributions:</p><formula xml:id="formula_17">2 (P, Q) = s Z X (P(x) Q(x)) 2 P(x)+Q(x) 2 dx (8)</formula><p>The following holds true for any P, Q, P 6 = Q:</p><formula xml:id="formula_18">1) The Fisher IPM for F = L 2 (X ,<label>P+Q</label></formula><p>2 ) is equal to the Chi-squared distance defined above:</p><formula xml:id="formula_19">d F (P, Q) = 2 (P, Q). 2) The optimal critic of the Fisher IPM on L 2 (X , P+Q 2 ) is :</formula><formula xml:id="formula_20">f (x) = 1 2 (P, Q) P(x) Q(x) P(x)+Q(x) 2 .</formula><p>We note here that LSGAN <ref type="bibr" target="#b4">[5]</ref> at full capacity corresponds to a Chi-Squared divergence, with the main difference that LSGAN has different objectives for the generator and the discriminator (bilevel optimizaton), and hence does not optimize a single objective that is a distance between distributions. The Chi-squared divergence can also be achieved in the f -gan framework from <ref type="bibr" target="#b2">[3]</ref>. We discuss the advantages of the Fisher formulation in Appendix C.</p><p>Optimizing over</p><formula xml:id="formula_21">L 2 (X , P+Q 2 )</formula><p>is not tractable, hence we have to restrict our function class, to a hypothesis class H , that enables tractable computations. Here are some typical choices of the space H : Linear functions in the input features, RKHS, a non linear multilayer neural network with a linear last layer (F v,! ). In this Section we don't make any assumptions about the function space and show in Theorem 2 how the Chi-squared distance is approximated in H , and how this depends on the approximation error of the optimal critic f in H . Theorem 2 (Approximating Chi-squared distance in an arbitrary function space H ). Let H be an arbitrary symmetric function space. We define the inner product hf, f i L2(X ,</p><formula xml:id="formula_22">P+Q 2 ) = R X f (x)f (x) P(x)+Q(x) 2</formula><p>dx, which induces the Lebesgue norm. Let S L2(X ,</p><formula xml:id="formula_23">P+Q 2 ) be the unit sphere in L 2 (X , P+Q 2 ): S L2(X , P+Q 2 ) = {f : X ! R, kf k L2(X ,<label>P+Q</label></formula><p>2 ) = 1}. The fisher IPM defined on an arbitrary function space H d H (P, Q), approximates the Chi-squared distance. The approximation quality depends on the cosine of the approximation of the optimal critic f in H . Since H is symmetric this cosine is always positive (otherwise the same equality holds with an absolute value)</p><formula xml:id="formula_24">d H (P, Q) = 2 (P, Q) sup f 2H \ S L 2 (X , P+Q 2 ) hf, f i L2(X , P+Q</formula><p>2 ) , Equivalently we have following relative approximation error:</p><formula xml:id="formula_25">2 (P, Q) d H (P, Q) 2 (P, Q) = 1 2 inf f 2H \ S L 2 (X , P+Q 2 ) kf f k 2 L2(X , P+Q 2 ) .</formula><p>From Theorem 2, we know that we have always d H (P, Q)  2 (P, Q). Moreover if the space H was rich enough to provide a good approximation of the optimal critic f , then d H is a good approximation of the Chi-squared distance 2 . Generalization bounds for the sample quality of the estimated Fisher IPM from samples from P and Q can be done akin to <ref type="bibr" target="#b10">[11]</ref>, with the main difficulty that for Fisher IPM we have to bound the excess risk of a cost function with data dependent constraints on the function class. We give generalization bounds for learning the Fisher IPM in the supplementary material (Theorem 3, Appendix E). In a nutshell the generalization error of the critic learned in a hypothesis class H from samples of P and Q, decomposes to the approximation error from Theorem 2 and a statistical error that is bounded using data dependent local Rademacher complexities <ref type="bibr" target="#b16">[17]</ref> and scales like O( p 1 /n), n = MN /M+N. We illustrate in <ref type="figure" target="#fig_1">Figure 2</ref> our main theoretical claims on a toy problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Fisher GAN Algorithm using ALM</head><p>For any choice of the parametric function class F p (for example F v,! ), note the constraint in Equation <ref type="formula" target="#formula_6">(4)</ref> </p><formula xml:id="formula_26">by⌦(f p , g ✓ ) = 1 2N P N i=1 f 2 p (x i ) + 1 2N P N j=1 f 2 p (g ✓ (z j ))</formula><p>. Define the Augmented Lagrangian <ref type="bibr" target="#b17">[18]</ref> corresponding to Fisher GAN objective and constraint given in Equation (4):</p><formula xml:id="formula_27">L F (p, ✓, ) =Ê (f p , g ✓ ) + (1 ⌦ (f p , g ✓ )) ⇢ 2 (⌦(f p , g ✓ ) 1) 2 (9)</formula><p>where is the Lagrange multiplier and ⇢ &gt; 0 is the quadratic penalty weight. We alternate between optimizing the critic and the generator. Similarly to <ref type="bibr" target="#b6">[7]</ref> we impose the constraint when training the critic only. Given ✓, for training the critic we solve max p min L F (p, ✓, ). Then given the critic parameters p we optimize the generator weights ✓ to minimize the objective min ✓Ê (f p , g ✓ ). We give in Algorithm 1, an algorithm for Fisher GAN, note that we use ADAM <ref type="bibr" target="#b18">[19]</ref> for optimizing the parameters of the critic and the generator. We use SGD for the Lagrange multiplier with learning rate ⇢ following practices in Augmented Lagrangian <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Fisher GAN</head><p>Input: ⇢ penalty weight, ⌘ Learning rate, n c number of iterations for training the critic, N batch size Initialize p, ✓, = 0 repeat for j = 1 to n c do Sample a minibatch g✓ iterations  We hypothesize that this is due to the implicit whitening that Fisher GAN provides. (Note that WGAN-GP does also succesfully converge without BN <ref type="bibr" target="#b6">[7]</ref>). For both models the learning rate was appropriately reduced.</p><formula xml:id="formula_28">x i , i = 1 . . . N, x i ⇠ P r Sample a minibatch z i , i = 1 . . . N, z i ⇠ p z (g p , g ) (r p L F , r L F )(p, ✓, ) p p + ⌘ ADAM (p, g p ) ⇢g {SGD rule on with learning rate ⇢} end for Sample z i , i = 1 . . . N, z i ⇠ p z d ✓ r ✓Ê (f p , g ✓ ) = r ✓ 1 N P N i=1 f p (g ✓ (z i )) ✓ ✓ ⌘ ADAM (✓, d ✓ ) until ✓ converges</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We experimentally validate the proposed Fisher GAN. We claim three main results: (1) stable training with a meaningful and stable loss going down as training progresses and correlating with sample quality, similar to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. <ref type="formula">(2)</ref> very fast convergence to good sample quality as measured by inception score. (3) competitive semi-supervised learning performance, on par with literature baselines, without requiring normalization of the critic.</p><p>We report results on three benchmark datasets: CIFAR-10 <ref type="bibr" target="#b19">[20]</ref>, LSUN <ref type="bibr" target="#b20">[21]</ref> and CelebA <ref type="bibr" target="#b21">[22]</ref>. We parametrize the generator g ✓ and critic f with convolutional neural networks following the model design from DCGAN <ref type="bibr" target="#b22">[23]</ref>. For 64 ⇥ 64 images (LSUN, CelebA) we use the model architecture in Appendix F.2, for CIFAR-10 we train at a 32 ⇥ 32 resolution using architecture in F.3 for experiments regarding sample quality (inception score), while for semi-supervised learning we use a better regularized discriminator similar to the Openai <ref type="bibr" target="#b8">[9]</ref> and ALI <ref type="bibr" target="#b23">[24]</ref> architectures, as given in F.4.We used Adam <ref type="bibr" target="#b18">[19]</ref> as optimizer for all our experiments, hyper-parameters given in Appendix F. Qualitative: Loss stability and sample quality. <ref type="figure" target="#fig_3">Figure 3</ref> shows samples and plots during training. For LSUN we use a higher number of D updates (n c = 5) , since we see similarly to WGAN that the loss shows large fluctuations with lower n c values. For CIFAR-10 and CelebA we use reduced n c = 2 with no negative impact on loss stability. CIFAR-10 here was trained without any label information. We show both train and validation loss on LSUN and CIFAR-10 showing, as can be expected, no overfitting on the large LSUN dataset and some overfitting on the small CIFAR-10 dataset. To back up our claim that Fisher GAN provides stable training, we trained both a Fisher Gan and WGAN where the batch normalization in the critic f was removed <ref type="figure" target="#fig_4">(Figure 4</ref>). Quantitative analysis: Inception Score and Speed. It is agreed upon that evaluating generative models is hard <ref type="bibr" target="#b24">[25]</ref>. We follow the literature in using "inception score" <ref type="bibr" target="#b8">[9]</ref> as a metric for the quality  <ref type="figure">a,b,c)</ref>. The inception score plots are mirroring <ref type="figure" target="#fig_3">Figure 3</ref> from <ref type="bibr" target="#b6">[7]</ref>. Note All inception scores are computed from the same tensorflow codebase, using the architecture described in appendix F.3, and with weight initialization from a normal distribution with stdev=0.02. In Appendix F.1 we show that these choices are also benefiting our WGAN-GP baseline.</p><p>of CIFAR-10 samples. <ref type="figure" target="#fig_5">Figure 5</ref> shows the inception score as a function of number of g ✓ updates and wallclock time. All timings are obtained by running on a single K40 GPU on the same cluster. We see from <ref type="figure" target="#fig_5">Figure 5</ref>, that Fisher GAN both produces better inception scores, and has a clear speed advantage over WGAN-GP. Quantitative analysis: SSL. One of the main premises of unsupervised learning, is to learn features on a large corpus of unlabeled data in an unsupervised fashion, which are then transferable to other tasks. This provides a proper framework to measure the performance of our algorithm. This leads us to quantify the performance of Fisher GAN by semi-supervised learning (SSL) experiments on CIFAR-10. We do joint supervised and unsupervised training on CIFAR-10, by adding a cross-entropy term to the IPM objective, in conditional and unconditional generation. <ref type="table">Table 2</ref>: CIFAR-10 inception scores using resnet architecture and codebase from <ref type="bibr" target="#b6">[7]</ref>. We used Layer Normalization <ref type="bibr" target="#b25">[26]</ref> which outperformed unnormalized resnets. Apart from this, no additional hyperparameter tuning was done to get stable training of the resnets.</p><p>Method Score ALI <ref type="bibr" target="#b23">[24]</ref> 5.34 ± .05 BEGAN <ref type="bibr" target="#b26">[27]</ref> 5.62 DCGAN <ref type="bibr" target="#b22">[23]</ref> (in <ref type="bibr" target="#b27">[28]</ref>)</p><p>6.16 ± .07 Improved GAN (-L+HA) <ref type="bibr" target="#b8">[9]</ref> 6.86 ± .06 EGAN-Ent-VI <ref type="bibr" target="#b28">[29]</ref> 7.07 ± .10 DFM <ref type="bibr" target="#b29">[30]</ref> 7.72 ± .13 WGAN-GP ResNet <ref type="bibr" target="#b6">[7]</ref> 7.86 ± .07 Fisher GAN ResNet (ours)</p><p>7.90 ± .05 Unsupervised Method Score</p><p>SteinGan <ref type="bibr" target="#b30">[31]</ref> 6.35 DCGAN (with labels, in <ref type="bibr" target="#b30">[31]</ref>) 6.58 Improved GAN <ref type="bibr" target="#b8">[9]</ref> 8.09 ± .07 Fisher GAN ResNet (ours)</p><p>8.16 ± .12 AC-GAN <ref type="bibr" target="#b31">[32]</ref> 8.25 ± .07 SGAN-no-joint <ref type="bibr" target="#b27">[28]</ref> 8.37 ± .08 WGAN-GP ResNet <ref type="bibr" target="#b6">[7]</ref> 8.42 ± .10 SGAN <ref type="bibr" target="#b27">[28]</ref> 8.59 ± .12 Supervised Unconditional Generation with CE Regularization. We parametrize the critic f as in F v,! . While training the critic using the Fisher GAN objective L F given in Equation <ref type="formula">(9)</ref>, we train a linear classifier on the feature space ! of the critic, whenever labels are available (K labels). The linear classifier is trained with Cross-Entropy (CE) minimization. Then the critic loss becomes</p><formula xml:id="formula_29">L D = L F D P (x,y)2lab CE(x, y; S, ! ), where CE(x, y; S, ! ) = log [Softmax(hS, ! (x)i) y ]</formula><p>, where S 2 R K⇥m is the linear classifier and hS, ! i 2 R K with slight abuse of notation. D is the regularization hyper-parameter. We now sample three minibatches for each critic update: one labeled batch from the small labeled dataset for the CE term, and an unlabeled batch + generated batch for the IPM. Conditional Generation with CE Regularization. We also trained conditional generator models, conditioning the generator on y by concatenating the input noise with a 1-of-K embedding of the label: we now have g ✓ (z, y). We parametrize the critic in F v,! and modify the critic objective as above. We also add a cross-entropy term for the generator to minimize during its training step:</p><formula xml:id="formula_30">L G =Ê + G P z⇠p(z),y⇠p(y) CE(g ✓ (z, y), y; S, ! )</formula><p>. For generator updates we still need to sample only a single minibatch since we use the minibatch of samples from g ✓ (z, y) to compute both the IPM lossÊ and CE. The labels are sampled according to the prior y ⇠ p(y), which defaults to the discrete uniform prior when there is no class imbalance. We found D = G = 0.1 to be optimal. New Parametrization of the Critic: "K + 1 SSL". One specific successful formulation of SSL in the standard GAN framework was provided in <ref type="bibr" target="#b8">[9]</ref>, where the discriminator classifies samples into K + 1 categories: the K correct clases, and K + 1 for fake samples. Intuitively this puts the real classes in competition with the fake class. In order to implement this idea in the Fisher framework, we define a new function class of the critic that puts in competition the K class directions of the classifier S y , and another "K+1" direction v that indicates fake samples. Hence we propose the following parametrization for the critic:</p><formula xml:id="formula_31">f (x) = P K y=1 p(y|x) hS y , ! (x)i hv, ! (x)i, where p(y|x) = Softmax(hS, ! (x)i)</formula><p>y which is also optimized with Cross-Entropy. Note that this critic does not fall under the interpretation with whitened means from Section 2.2, but does fall under the general Fisher IPM framework from Section 2.1. We can use this critic with both conditional and unconditional generation in the same way as described above. In this setting we found D = 1.5, G = 0.1 to be optimal. Layerwise normalization on the critic. For most GAN formulations following DCGAN design principles, batch normalization (BN) <ref type="bibr" target="#b32">[33]</ref> in the critic is an essential ingredient. From our semisupervised learning experiments however, it appears that batch normalization gives substantially worse performance than layer normalization (LN) <ref type="bibr" target="#b25">[26]</ref> or even no layerwise normalization. We attribute this to the implicit whitening Fisher GAN provides. <ref type="table" target="#tab_1">Table 3</ref> shows the SSL results on CIFAR-10. We show that Fisher GAN has competitive results, on par with state of the art literature baselines. When comparing to WGAN with weight clipping, it becomes clear that we recover the lost SSL performance. Results with the K + 1 critic are better across the board, proving consistently the advantage of our proposed K + 1 formulation. Conditional generation does not provide gains in the setting with layer normalization or without normalization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have defined Fisher GAN, which provide a stable and fast way of training GANs. The Fisher GAN is based on a scale invariant IPM, by constraining the second order moments of the critic. We provide an interpretation as whitened (Mahalanobis) mean feature matching and 2 distance. We show graceful theoretical and empirical advantages of our proposed Fisher GAN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>!</head><label></label><figDesc>was implemented with homogeneous non linearities such as RELU, if we swap (v, !) with (cv, c 0 !) for any constants c, c 0 &gt; 0, the distance d Fv,! remains unchanged, hence the scaling invariance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example on 2D synthetic data, where both P and Q are fixed normal distributions with the same covariance and shifted means along the x-axis, see (a). Fig (b, c) show the exact 2 distance from numerically integrating Eq (8), together with the estimate obtained from training a 5-layer MLP with layer size = 16 and LeakyReLU nonlinearity on different training sample sizes. The MLP is trained using Algorithm 1, where sampling from the generator is replaced by sampling from Q, and the 2 MLP estimate is computed with Equation (2) on a large number of samples (i.e. out of sample estimate). We see in (b) that for large enough sample size, the MLP estimate is extremely good. In (c) we see that for smaller sample sizes, the MLP approximation bounds the ground truth 2 from below (see Theorem 2) and converges to the ground truth roughly as O( 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Samples and plots of the lossÊ (.), lagrange multiplier , and constraint⌦(.) on 3 benchmark datasets. We see that during training as grows slowly, the constraint becomes tight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: No Batch Norm: Training results from a critic f without batch normalization. Fisher GAN (left) produces decent samples, while WGAN with weight clipping (right) does not. We hypothesize that this is due to the implicit whitening that Fisher GAN provides. (Note that WGAN-GP does also succesfully converge without BN [7]). For both models the learning rate was appropriately reduced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: CIFAR-10 inception scores under 3 training conditions. Corresponding samples are given in rows from top to bottom (a,b,c). The inception score plots are mirroring Figure 3 from [7]. Note All inception scores are computed from the same tensorflow codebase, using the architecture described in appendix F.3, and with weight initialization from a normal distribution with stdev=0.02. In Appendix F.1 we show that these choices are also benefiting our WGAN-GP baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Comparison between Fisher GAN and recent related approaches.</figDesc><table>Stability Unconstrained Efficient 
Representation 
capacity 
Computation power (SSL) 
Standard GAN [1, 9] 
7 
3 
3 
3 
WGAN, McGan [6, 8] 
3 
7 
3 
7 
WGAN-GP [7] 
3 
3 
7 
? 
Fisher Gan (Ours) 
3 
3 
3 
3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc>CIFAR-10 SSL results.Improved GAN (FM) [9] 21.83 ± 2.01 19.61 ± 2.09 18.63 ± 2.32 17.72 ± 1.82 ALI [24] 19.98 ± 0.89 19.09 ± 0.44 17.99 ± 1.62 17.05 ± 1.49 WGAN (weight clipping) UncondFisher GAN LN Cond 26.78 ± 1.04 23.30 ± 0.39 20.56 ± 0.64 18.26 ± 0.25 Fisher GAN LN Uncond 24.39 ± 1.22 22.69 ± 1.27 19.53 ± 0.34 17.84 ± 0.15 Fisher GAN LN K+1 Cond 20.99 ± 0.66 19.01 ± 0.21 17.41 ± 0.38 15.50 ± 0.41 Fisher GAN LN K+1, Uncond 19.74 ± 0.21 17.87 ± 0.38 16.13 ± 0.53 14.81 ± 0.16 Fisher GAN No Norm K+1, Uncond 21.15 ± 0.54 18.21 ± 0.30 16.74 ± 0.19 14.80 ± 0.15</figDesc><table>Number of labeled examples 
1000 
2000 
4000 
8000 
Model 
Misclassification rate 

CatGAN [34] 
19.58 
69.01 
56.48 
40.85 
30.56 
WGAN (weight clipping) Cond 
68.11 
58.59 
42.00 
30.91 

Fisher GAN BN Cond 
36.37 
32.03 
27.42 
22.85 
Fisher GAN BN Uncond 
36.42 
33.49 
27.36 
22.82 
Fisher GAN BN K+1 Cond 
34.94 
28.04 
23.85 
20.75 
Fisher GAN BN K+1 Uncond 
33.49 
28.60 
24.19 
21.59 
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code is available at https://github.com/tomsercu/FisherGAN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. The authors thank Steven J. Rennie for many helpful discussions and Martin Arjovsky for helpful clarifications and pointers.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Amortised map inference for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Casper Kaae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huszár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04076</idno>
		<title level="m">Least squares generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<title level="m">Soumith Chintala, and Léon Bottou. Wasserstein gan. ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
		<title level="m">Improved training of wasserstein gans</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcgan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08398</idno>
		<title level="m">Mean and covariance feature matching gan</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Improved techniques for training gans. NIPS</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Integral probability metrics and their generating classes of functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Probability</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the empirical estimation of integral probability metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bharath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gert</forename><forename type="middle">R G</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.03483</idno>
		<title level="m">Learning in implicit generative models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative moment matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Training generative neural networks via maximum mean discrepancy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gintare Karolina Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>UAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Testing for homogeneity with kernel fisher discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaïd</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moulines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahar</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Local rademacher complexities. Ann. Statist</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Numerical Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Aäron van den Oord, and Matthias Bethge. A note on the evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Began: Boundary equilibrium generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10717</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04357</idno>
		<title level="m">Stacked generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Calibrating energy-based generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01691</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improving generative adversarial networks with denoising feature matching. ICLR submissions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning to draw samples: With application to amortized mle for generative adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01722</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09585</idno>
		<title level="m">Conditional image synthesis with auxiliary classifier gans</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Unsupervised and semi-supervised learning with categorical generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06390</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Metrics for probabilistic geometries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandra</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Søren</forename><surname>Hauberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfredo</forename><surname>Vellido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">On integral probability metrics, -divergences and binary classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bharath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gert</forename><forename type="middle">R G</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanckriet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Infinite-dimensional Optimization and Convexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ekeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Turnbull</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<publisher>The University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
