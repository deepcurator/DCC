<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:32+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Differentiable Compositional Kernel Learning for Gaussian Processes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyang</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqi</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaman</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
						</author>
						<title level="a" type="main">Differentiable Compositional Kernel Learning for Gaussian Processes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The generalization properties of Gaussian processes depend heavily on the choice of kernel, and this choice remains a dark art. We present the Neural Kernel Network (NKN), a flexible family of kernels represented by a neural network. The NKN's architecture is based on the composition rules for kernels, so that each unit of the network corresponds to a valid kernel. It can compactly approximate compositional kernel structures such as those used by the Automatic Statistician (Lloyd et al., 2014), but because the architecture is differentiable, it is end-to-end trainable with gradientbased optimization. We show that the NKN is universal for the class of stationary kernels. Empirically we demonstrate NKN's pattern discovery and extrapolation abilities on several tasks that depend crucially on identifying the underlying structure, including time series and texture extrapolation, as well as Bayesian optimization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Gaussian processes (GPs) are a powerful and widely used class of models due to their nonparametric nature, explicit representation of posterior uncertainty, and ability to flexibly model a variety of structures in data. However, patterns of generalization in GP depend heavily on the choice of kernel function <ref type="bibr" target="#b21">(Rasmussen, 1999)</ref>; different kernels can impose widely varying modeling assumptions, such as smoothness, linearity, or periodicity. Capturing appropriate kernel structures can be crucial for interpretability and extrapolation <ref type="bibr" target="#b6">(Duvenaud et al., 2013;</ref><ref type="bibr" target="#b31">Wilson &amp; Adams, 2013)</ref>. Even for experts, choosing GP kernel structures remains a dark art.</p><p>GPs' strong dependence on kernel structures has motivated work on automatic kernel learning methods. Sometimes this   <ref type="figure">Figure 1</ref>. 2-D synthetic data (Left) and extrapolation result using our neural kernel network (Right). The 2-D function is y = (cos(2x1) + cos(2x2)) ⇤ p x1x2. Black dots represent 100 training data randomly sampled from <ref type="bibr">[ 6,</ref><ref type="bibr">6]</ref> 2 . This synthetic experiment illustrates NKN's ability to discover and extrapolate patterns.</p><p>can be done by imposing a specific kind of structure: e.g., <ref type="bibr" target="#b1">Bach (2009)</ref> <ref type="bibr">;</ref><ref type="bibr" target="#b7">Duvenaud et al. (2011)</ref> learned kernel structures which were additive over subsets of the variables. A more expressive space of kernels is spectral mixtures <ref type="bibr" target="#b31">(Wilson &amp; Adams, 2013;</ref><ref type="bibr" target="#b14">Kom Samo &amp; Roberts, 2015;</ref><ref type="bibr" target="#b22">Remes et al., 2017)</ref>, which are based on spectral domain summations. For example, spectral mixture (SM) kernels <ref type="bibr" target="#b31">(Wilson &amp; Adams, 2013</ref>) approximate all stationary kernels using Gaussian mixture models in the spectral domain. Deep kernel learning (DKL)  further boosted the expressiveness by transforming the inputs of spectral mixture base kernel with a deep neural network. However, the expressiveness of DKL still depends heavily on the kernel placed on the output layer.</p><p>In another line of work, <ref type="bibr" target="#b6">Duvenaud et al. (2013)</ref> defined a context-free grammar of kernel structures based on the composition rules for kernels. Due to its compositionality, this grammar could express combinations of properties such as smoothness, linearity, or periodicity. They performed a greedy search over this grammar to find a kernel struture which matched the input data. Using the learned structures, they were able to produce sensible extrapolations and interpretable decompositions for time series datasets. <ref type="bibr" target="#b17">Lloyd et al. (2014)</ref> extended this work to an Automatic Statistician which automatically generated natural language reports. All of these results depended crucially on the compositionality of the underlying space. The drawback was that discrete search over the kernel grammar is very expensive, often requiring hours of computation even for short time series.</p><p>In this paper, we propose the Neural Kernel Network (NKN), a flexible family of kernels represented by a neural network. The network's first layer units represent primitive kernels, including those used by the Automatic Statistician. Subsequent layers are based on the composition rules for kernels, so that each intermediate unit is itself a valid kernel. The NKN can compactly approximate the kernel structures from the Automatic Statistician grammar, but is fully differentiable, so that the kernel structures can be learned with gradient-based optimization. To illustrate the flexibility of our approach, <ref type="figure">Figure 1</ref> shows the result of fitting an NKN to model a 2-D function; it is able to extrapolate sensibly.</p><p>We analyze the NKN's expressive power for various choices of primitive kernels. We show that the NKN can represent nonnegative polynomial functions of its primitive kernels, and from this demonstrate universality for the class of stationary kernels. Our universality result holds even if the width of the network is limited, analogously to <ref type="bibr" target="#b26">Sutskever &amp; Hinton (2008)</ref>. Interestingly, we find that the network's representations can be made significantly more compact by allowing its units to represent complex-valued kernels, and taking the real component only at the end.</p><p>We empirically analyze the NKN's pattern discovery and extrapolation abilities on several tasks that depend crucially on identifying the underlying structure. The NKN produces sensible extrapolations on both 1-D time series datasets and 2-D textures. It outperforms competing approaches on regression benchmarks. In the context of Bayesian optimization, it is able to optimize black-box functions more efficiently than generic smoothness kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Gaussian Process Regression</head><p>A Gaussian process (GP) defines a distribution p(f ) over functions X ! R for some domain X . For any finite set {x 1 , ..., x n } ⇢ X, the function values f = (f (x 1 ), f(x 2 ), ..., f(x n )) have a multivariate Gaussian distribution. Gaussian processes are parameterized by a mean function µ(·) and a covariance function or kernel function k(·, ·). The marginal distribution of function values is given by</p><formula xml:id="formula_0">f ⇠ N (µ, K XX ),<label>(1)</label></formula><p>where K XX denotes the matrix of k(x i , x j ) for all (i, j). Assume we are given a set of training input-output pairs,</p><formula xml:id="formula_1">D = {(x i , y i )} n i=1 = (X, y)</formula><p>, and each target y n is generated from the corresponding f (x n ) by adding independent Gaussian noise; i.e.,</p><formula xml:id="formula_2">y n = f (x n ) + ✏ n , ✏ n ⇠ N (0, 2 )<label>(2)</label></formula><p>As the prior on f is a Gaussian process and the likelihood is Gaussian, the posterior on f is also Gaussian. We can use this to make predictions p(y ⇤ |x ⇤ , D) in closed form:</p><formula xml:id="formula_3">p(y ⇤ |x ⇤ , D) = N (µ ⇤ , 2 ⇤ ) µ ⇤ = K ⇤X (K XX + 2 I) 1 y 2 ⇤ = K ⇤⇤ K ⇤X (K XX + 2 I) 1 K X⇤ + 2<label>(3)</label></formula><p>Here we assume zero mean function for f . Most GP kernels have several hyperparameters ✓ which can be optimized jointly with to maximize the log marginal likelihood,</p><formula xml:id="formula_4">L(✓) = ln p(y|0, K XX + 2 I)<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Bochner's Theorem</head><p>Gaussian Processes depend on specifying a kernel function k(x, x 0 ), which acts as a similarity measure between inputs. Definition 1. Let X be a set, and k be a conjugate symmetric function k : X ⇥ X ! C is a positive definite kernel if</p><formula xml:id="formula_5">8x 1 , · · ·, x n 2 X and 8c 1 , · · ·, c n 2 C, n X i,j=1 c i c j k(x i , x j ) 0,<label>(5)</label></formula><p>where the bar denotes the complex conjugate. Bochner's Theorem <ref type="bibr" target="#b3">(Bochner, 1959)</ref> establishes a bijection between complex-valued stationary kernels and positive finite measures using Fourier transform, thus providing an approach to analyze stationary kernels in the spectral domain <ref type="bibr" target="#b31">(Wilson &amp; Adams, 2013;</ref><ref type="bibr" target="#b14">Kom Samo &amp; Roberts, 2015)</ref>.</p><formula xml:id="formula_6">Theorem 1. (Bochner) A complex-valued function k on R d</formula><p>is the covariance function of a weakly stationary mean square continuous complex-valued random process on R d if and only if it can be represented as</p><formula xml:id="formula_7">k(⌧ ) = Z R P exp(2⇡iw &gt; ⌧ ) (dw)<label>(6)</label></formula><p>where is a positive and finite measure. If has a density S(w), then S is called the spectral density or power spectrum of k. S and k are Fourier duals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Automatic Statistician</head><p>For compositional kernel learning, the Automatic Statistician <ref type="bibr" target="#b17">(Lloyd et al., 2014;</ref><ref type="bibr" target="#b6">Duvenaud et al., 2013</ref>) used a compositional space of kernels defined as sums and products of a small number of primitive kernels. The primitive kernels included:</p><p>• radial basis functions, corresponding to smooth functions. • rational quadratic, corresponding to functions with multiple scale variations.</p><formula xml:id="formula_8">RBF(x, x 0 ) = 2 exp( kx x 0 k 2 2l 2 ) • periodic. PER(x, x 0 ) = 2 exp( 2 sin 2 (⇡kx x 0 k/p) l 2 ) • linear kernel. LIN(x, x 0 ) = 2 x &gt; x 0 ! " ! #</formula><formula xml:id="formula_9">RQ(x, x 0 ) = 2 (1 + kx x 0 k 2 2↵l 2 ) 1 ↵ • white noise. WN(x, x 0 ) = 2 x,x 0 • constant kernel. C(x, x 0 ) = 2</formula><p>The Automatic Statistician searches over the compositional space based on three search operators.</p><p>1. Any subexpression S can be replaced with S + B, where B is any primitive kernel family. 2. Any subexpression S can be replaced with S ⇥ B, where B is any primitive kernel family. 3. Any primitive kernel B can be replaced with any other primitive kernel family B 0 .</p><p>The search procedure relies on a greedy search: at every stage, it searches over all subexpressions and all possible operators, then chooses the highest scoring combination. To score kernel families, it approximates the marginal likelihood using the Bayesian information criterion <ref type="bibr" target="#b24">(Schwarz et al., 1978)</ref> after optimizing to find the maximumlikelihood kernel parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Neural Kernel Networks</head><p>In this section, we introduce the Neural Kernel Network (NKN), a neural net which computes compositional kernel structures and is end-to-end trainable with gradient-based optimization. The input to the network consists of two vectors x 1 , x 2 2 R d , and the output k(x 1 , x 2 ) 2 R (or C) is the kernel value. Our NKN architecture is based on well-known composition rules for kernels:</p><formula xml:id="formula_10">Lemma 2. For kernels k 1 , k 2 • For 1 , 2 2 R + , 1 k 1 + 2 k 2 is a kernel.</formula><p>• The product k 1 k 2 is a kernel.</p><p>We design the architecture such that every unit of the network computes a kernel, although some of those kernels may be complex-valued.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture</head><p>The first layer of the NKN consists of a set of primitive kernels. Subsequent layers alternate between linear combinations and products. Since the space of kernels is closed under both operations, each unit in the network represents a kernel. Linear combinations and products can be seen as OR-like and AND-like operations, respectively; this is a common pattern in neural net design <ref type="bibr" target="#b16">(LeCun et al., 1989;</ref><ref type="bibr" target="#b20">Poon &amp; Domingos, 2011)</ref>. The full architecture is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Primitive kernels. The first layer of the network consists of a set of primitive kernel families with simple functional forms. While any kernels can be used here, we use the RBF, PER, LIN, and RQ kernels from the Automatic Statistician (see Section 2.3) because these express important structural motifs for GPs. Each of these kernel families has an associated set of hyperparameters (such as lengthscales or variances), and instantiating the hyperparameters gives a kernel. These hyperparameters are treated as parameters (weights) in this layer of the network, and are optimized with the rest of the network. Note that it may be advantageous to have multiple copies of each primitive kernel so that they can be instantiated with different hyperparameters.</p><p>Linear layers. The Linear layer closely resembles a fully connected layer in deep neural networks, with each layer</p><formula xml:id="formula_11">h l = W l h l 1</formula><p>representing a nonnegative linear combination of units in the previous layer (i.e. W l is a nonnegative matrix). In practice, we use the parameterization W l = log(1 + exp(A l )) to enforce the nonnegativity constraint. (Here, exp is applied elementwise.)</p><p>The Linear layer can be seen as a OR-like operation: two points are considered similar if either kernel has a high value, while the Linear layer further controls the balance using trainable weights.</p><p>Product layers. The Product layer introduces multiplication, in that each unit is the product of several units in the previous layer. This layer has a fixed connectivity pattern and no trainable parameters. While this fixed structure may appear restrictive, Section 3.3 shows that it does not restrict the expressiveness of the network.</p><p>The Product layer can be seen as an AND-like operation: two points are considered similar if both constituent kernels have large values.</p><p>Activation functions. Analogously to ordinary neural nets, each layer may also include a nonlinear activation function, so that</p><formula xml:id="formula_12">h l = f (z l )</formula><p>, where z l , the pre-activations, are the result of a linear combination or product. However, f must be selected with care in order to ensure closure of the kernels. Polynomials with positive coefficients, as well as the exponential function f (z) = e z , fulfill this requirement.</p><p>Complex-valued kernels. Allowing units in NKN to represent complex-valued kernels as in Definition 1 and take the real component only at the end, can make the network's representations significantly more compact. As complexvalued kernels also maintain closure under summation and multiplication <ref type="bibr" target="#b34">(Yaglom, 2012)</ref>, additional modifications are unnecessary. In practice, we can include exp(iµ &gt; ⌧ ) in our primitive kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning</head><p>Optimization. All trainable parameters can be grouped into two categories: (1) parameters of primitive kernels, e.g., lengthscale in an RBF kernel; (2) parameters of Linear layers. We jointly learn these parameters by maximizing the marginal likelihood L(✓). Since the NKN architecture is differentiable, we can jointly fit all parameters using gradient-based optimization.</p><p>Computational Cost. NKN introduces small computational overhead. Suppose we have N data points and m connections in the NKN; the computational cost of the forward pass is O(N 2 m). Note that a moderately-sized NKN, as we used in our experiments 1 , has only tens of parameters, and the main computational bottleneck in training lies in inverting kernel matrix, which is an O(N</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Universality</head><p>In this section, we analyze the expressive power of the NKN, and in particular its ability to approximate arbitrary stationary kernels. Our analysis provides insight into certain design decisions for the NKN: in particular, we show that the NKN can approximate some stationary kernels much more compactly if the units of the network are allowed to take complex values. Furthermore, we show that the fixed structure of the product layers does not limit what the network can represent.</p><p>Definition 2. For kernels {k j } n j=1 , a kernel k is positiveweighted polynomial (PWP) of these kernels if 9T 2 N and</p><formula xml:id="formula_13">{w t , {p tj } n j=1 |w i 2 R + , p tj 2 N} T t=0 , such that k(x, y) = T X t=1 w t n Y j=1 k ptj j<label>(7)</label></formula><p>holds for all x, y 2 R. Its degree is max</p><formula xml:id="formula_14">t P n j=1 p tj .</formula><p>Composed of summation and multiplication, the NKN naturally forms a positive-weighted polynomial of primitive kernels. Although NKN adopts a fixed multiplication order in the Product layer, the following theorem shows that this fixed architecture doesn't undermine NKN's expressiveness (proof in Appendix D).</p><p>Theorem 3. Given B primitive kernels,</p><p>• An NKN with width 2B + 6 can represent any PWP of primitive kernels.</p><p>• An NKN with width 2 Bp+1 and p Linear-Product modules can represent any PWP with degree no more than</p><formula xml:id="formula_15">2 p .</formula><p>Interestingly, NKNs can sometimes approximate (realvalued) kernels more compactly if the hidden units are allowed to represent complex-valued kernels, and the real part is taken only at the end. In particular, we give an example of a spectral mixture kernel class which can be represented with an NKN with a single complex-valued primitive kernel, but whose real-valued NKN representation requires a primitive kernel for each mixture component (proof in Appendix E).</p><p>Example 1. Define a d-dimensional spectral mixture kernel with n + 1 components, k</p><formula xml:id="formula_16">⇤ (⌧ ) = n+1 P t=1 n 2 2t cos(4 t 1 &gt; ⌧ ). Then 9✏ &gt; 0, such that 8{µ t } n t=1</formula><p>, and any PWP of {cos(µ</p><formula xml:id="formula_17">&gt; t ⌧ )} n t=1 denoted ask, max ⌧ 2R d |k(⌧ ) k ⇤ (⌧ )| &gt; ✏<label>(8)</label></formula><p>In contrast, k ⇤ can be represented as the real part of a PWP of only one complex-valued primitive kernel e i1</p><formula xml:id="formula_18">&gt; ⌧ , k ⇤ (⌧ ) = &lt;{ n+1 X t=1 ✓ n 2 ◆ 2t [e i1 &gt; ⌧ ] 4 t }<label>(9)</label></formula><p>We find that an NKN with small width can approximate any complex-valued stationary kernel, as shown in the following theorem (Proof in Appendix F).</p><formula xml:id="formula_19">Theorem 4. For any d-dimensional complex-valued sta- tionary kernel k ⇤ and ✏ 2 R + , 9{ j } d j=1 , {µ j } 2d j=1</formula><p>, and an NKNk with primitive kernels {exp(</p><formula xml:id="formula_20">2⇡ 2 k⌧ j k 2 )} d j=1 , {exp(iµ &gt; j ⌧ )} 2d j=1</formula><p>, and width no more than 6d+6, such that</p><formula xml:id="formula_21">max ⌧ 2R d |k(⌧ ) k ⇤ (⌧ )| &lt; ✏<label>(10)</label></formula><p>Beyond approximating stationary kernels, NKN can also capture non-stationary structure by incorporating nonstationary primitive kernels. In Appendix G, we prove that with the proper choice of primitive kernels, NKN can approximate a broad family of non-stationary kernels called generalized spectral kernels <ref type="bibr" target="#b14">(Kom Samo &amp; Roberts, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Additive kernels <ref type="bibr" target="#b7">(Duvenaud et al., 2011)</ref> are linear combinations of kernels over individual dimensions or groups of dimensions, and are a promising method to combat the curse of dimensionality. While additive kernels need an exponential number of multiplication terms in the input dimension, hierarchical kernel learning (HKL) <ref type="bibr" target="#b1">(Bach, 2009</ref>) presents a similar kernel except selecting only a subset to get a polynomial number of terms. However, this subset selection imposes additional optimization difficulty.</p><p>Based on Bochner's theorem, there is another a line of work on designing kernels in the spectral domain, including sparse spectrum kernels (SS) (Lázaro-Gredilla et al., 2010); spectral mixture (SM) kernels <ref type="bibr" target="#b31">(Wilson &amp; Adams, 2013)</ref>; generalized spectral kernels (GSK) (Kom <ref type="bibr" target="#b14">Samo &amp; Roberts, 2015)</ref> and generalized spectral mixture (GSM) kernels <ref type="bibr" target="#b22">(Remes et al., 2017)</ref>. Though these approaches often extrapolate sensibly, capturing complex covariance structure may require a large number of mixture components.</p><p>The Automatic Statistician <ref type="bibr" target="#b6">(Duvenaud et al., 2013;</ref><ref type="bibr" target="#b17">Lloyd et al., 2014;</ref><ref type="bibr" target="#b18">Malkomes et al., 2016</ref>) used a compositional grammar of kernel structures to analyze datasets and provide natural language reports. In each stage, it considered all production rules and used the one that resulted in the largest log-likelihood improvement. Their model showed good extrapolation for many time series tasks, attributed to the recovery of underlying structure. However, it relied on greedy discrete search over kernel and operator combinations, making it computational expensive, even for small time series datasets.</p><p>There have been several attempts <ref type="bibr" target="#b11">(Hinton &amp; Salakhutdinov, 2008;</ref><ref type="bibr" target="#b33">Wilson et al., 2016)</ref> to combine neural networks with Gaussian processes. Specifically, they used a fixed kernel structure on top of the hidden representation of a neural network. This is complementary to our work, which focuses on using neural networks to infer the kernel structure itself. Both approaches could potentially be combined.</p><p>Instead of represeting kernel parametrically, <ref type="bibr" target="#b19">Oliva et al. (2016)</ref> modeled random feature dimension with stick breaking prior and <ref type="bibr" target="#b28">Tobar et al. (2015)</ref> generated functions as the convolution between a white noise process and a linear filter drawn from GP. These approaches offer much flexibility but also incur challenges in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We conducted a series of experiments to measure the NKN's predictive ability in several settings: time series, regression benchmarks, and texture images. We focused in particular on extrapolation, since this is a strong test of whether it has uncovered the underlying structure. Furthermore, we tested the NKN on Bayesian Optimization, where model structure and calibrated uncertainty can each enable more efficient exploration. Code is available at git@github.com:</p><p>ssydasheng/Neural-Kernel-Network.git</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Time Series Extrapolation</head><p>We first conducted experiments time series datasets to study extrapolation performance. For all of these experiments, as well as the 2-d experiment in <ref type="figure">Figure 1</ref>, we used the same NKN architecture and training setup (Appendix J.1).</p><p>We validated the NKN on three time series datasets introduced by <ref type="bibr" target="#b6">Duvenaud et al. (2013)</ref>: airline passenger volume (Airline), Mauna Loa atmospheric CO 2 concentration (Mauna), and solar irradiance (Solar). Our focus is on extrapolation, since this is a much better test than interpolation for whether the model has learned the underlying structure.</p><p>We compared the NKN with the Automatic Statistician <ref type="bibr" target="#b6">(Duvenaud et al., 2013)</ref>; both methods used RBF, RQ, PER and LIN as the primitive kernels. In addition, because many time series datasets appear to contain a combination of seasonal patterns, long-term trends, and medium-scale variability, we also considered a baseline consisting of sums of PER, LIN, RBF, and Constant kernels, with trainable weights and kernel parameters. We refer to this baseline as "heuristic".</p><p>The results for Airline are shown in <ref type="figure">Figure 3</ref>, while the  <ref type="figure">Figure 3</ref>. Extrapolation results of NKN on the Airline dataset. "Heuristic" denotes linear combination of RBF, PER, LIN, and Constant kernels. AS represents Automatic Statistician <ref type="bibr" target="#b6">(Duvenaud et al., 2013)</ref>. The red circles are the training points, and the curve after the blue dashed line is the extrapolation result. Shaded areas represent 1 standard deviation.</p><p>results for Mauna and Solar are shown in <ref type="figure">Figures 8 and  9</ref> in the Appendix. All three models were able to capture the periodic and increasing patterns. However, the heuristic kernel failed to fit the data points well or capture the increasing amplitude, stemming from its lack of PER*LIN structure. In comparison, both AS and NKN fit the training points perfectly, and generated sensible extrapolations. However, the NKN was far faster to train because it avoided discrete search: for the Airline dataset, the NKN took only 201 seconds, compared with 6147 seconds for AS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Regression Benchmarks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">RANDOM TRAINING/TEST SPLITS</head><p>To evaluate the predictive performance of NKN, we first conducted experiments on regression benchmark datasets from the UCI collection <ref type="bibr" target="#b0">(Asuncion &amp; Newman, 2007)</ref>. Following the settings in Hernández-Lobato &amp; Adams <ref type="formula" target="#formula_0">(2015)</ref>, the datasets were randomly split into training and testing sets, comprising 90% and 10% of the data respectively. This splitting process was repeated 10 times to reduce variability. We compared NKN to RBF and SM (Wilson &amp; Adams, 2013) kernels, and the popular Bayesian neural network method Bayes-by-Backprop (BBB) <ref type="bibr" target="#b2">(Blundell et al., 2015)</ref>. For the SM kernel, we used 4 mixture components, so we denote it as SM-4. For all experiments, the NKN uses 6 primitive kernels including 2 RQ, 2 RBF, and 2 LIN. The following layers are organized as Linear8-Product4-Linear4-Product2-Linear1. <ref type="bibr">2</ref> We trained both the variance and d-dimensional lengthscales for all kernels. As a result, for d dimensional inputs, SM-4 has 8d+12 trainable parameters and NKN has 4d + 85 parameters.</p><p>As shown in <ref type="table" target="#tab_1">Table 1</ref>, BBB performed worse than the Gaussian processes methods on all datasets. On the other hand, NKN and SM-4 performed consistently better than the standard RBF kernel in terms of both RMSE and log-likelihoods. Moreover, the NKN outperformed the SM-4 kernel on all datasets other than Naval and Kin8nm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">MEASURING EXTRAPOLATION WITH PCA SPLITS</head><p>Since the experiments just presented used random training/test splits, they can be thought of as measuring interpolation performance. We are also interested in measuring extrapolation performance, since this is a better measure of whether the model has captured the underlying structure. In  <ref type="table">Table 2</ref>. Test RMSE and log-likelihood for the PCA-split regression benchmarks. N denotes the number of data points. order to test this, we sorted the data points according to their projection onto the top principal component of the data. The top 1/15 and bottom 1/15 of the data were used as test data, and the remainder was used as training data.</p><formula xml:id="formula_22">TEST RMSE TEST LOG-LIKELIHOOD DATASET N G P -R B F G P -S M G P -N K N G P -R B F G P -S M G P -N K N BOSTON</formula><p>We compared NKN with standard RBF and SM kernels using the same architectural settings as in the previous section. All models were trained for 20,000 iterations. To select the mixture number of SM kernels, we further subdivided the training set into a training and validation set, using the same PCA-splitting method as described above. For each dataset, we trained SM on the sub-training set using {1, 2, 3, 4} mixture components and selected the number based on validation error. (We considered up to 4 mixture components in order to roughly match the number of parameters for NKN.) Then we retrained the SM kernel using the combined training and validation sets. The resulting test RMSE and log-likelihood are shown in <ref type="table">Table 2</ref>.</p><p>As seen in <ref type="table">Table 2</ref>, all three kernels performed significantly worse compared with <ref type="table" target="#tab_1">Table 1</ref>, consistent with the intuition that extrapolation is more difficult that interpolation. However, we can see NKN outperformed SM for most of the datasets. In particular, for small datasets (and hence more chance to overfit), NKN performed better than SM by a substantial margin, with the exception of the Energy dataset. This demonstrates the NKN was better able to capture the underlying structure, rather than overfitting the training points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Bayesian Optimization</head><p>Bayesian optimization <ref type="bibr" target="#b4">(Brochu et al., 2010;</ref><ref type="bibr" target="#b25">Snoek et al., 2012</ref>) is a technique for optimization expensive black-box functions which repeatedly queries the function, fits a surrogate function to past queries, and maximizes an acquisition function to choose the next query. It's important to model both the predictive mean (to query points that are likely to perform well) and the predictive variance (to query points that have high uncertainty). Typically, the surrogate functions are estimated using a GP with a simple kernel, such as Matern. But simple kernels lead to inefficient exploration due to the curse of dimensionality, leading various researchers to consider additive kernels <ref type="bibr" target="#b13">(Kandasamy et al., 2015;</ref><ref type="bibr" target="#b8">Gardner et al., 2017;</ref><ref type="bibr" target="#b29">Wang et al., 2017)</ref>. Since additivity is among the patterns the NKN can learn, we were interested in testing its performance on Bayesian optimization tasks with additive structure. We used Expectated Improvement (EI) to perform BO.</p><p>Following the protocol in <ref type="bibr" target="#b13">Kandasamy et al. (2015)</ref>; <ref type="bibr" target="#b8">Gardner et al. (2017);</ref><ref type="bibr" target="#b29">Wang et al. (2017)</ref>, we evaluated the performance on three toy function benchmarks with additive structure,</p><formula xml:id="formula_23">f (x) = |P | X i=1 f i (x[P i ])<label>(11)</label></formula><p>The For modelling additive functions with GP, the kernel can decompose as a summation between additive groups as well.</p><formula xml:id="formula_24">k(x, x 0 ) = P |P | i=1 k i (x[P i ], x 0 [P i ]).</formula><p>We considered an oracle kernel, which was a linear combination of RBF kernels corresponding to the true additive structure of the function. Both the kernel parameters and the combination coefficients were trained with maximum likelihood. We also tested the standard RBF kernel without additive structure. For the NKN, we used d RBF kernels over individual input dimensions as the primitive kernels. The following layers were arranged as Linear8-Product4-Linear4-Product2-Linear1. Note that, although the primitive kernels corresponded to seperate dimensions, NKN can represent additive structure through these linear combination and product operations. In all cases, we used Expected Improvement as the acquisition function.</p><p>As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, for all three benchmarks, the oracle kernel not only converged faster than RBF kernel, but also found smaller function values by a large margin. In comparsion, we can see that although NKN converged slower than oracle in the beginning, it caught up with oracle eventually and reached the same function value. This suggests that the NKN is able to exploit additivity for Bayesian optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Texture Extrapolation</head><p>Based on <ref type="bibr" target="#b32">Wilson et al. (2014)</ref>, we evaluated the NKN on texture exploration, a test of the network's ability to learn local correlations as well as complex quasi-periodic patterns. From the original 224 ⇥ 224 images, we removed a 60 ⇥ 80 region, as shown in <ref type="figure" target="#fig_3">Figure 5(a)</ref>. From a regression perspective, this corresponds to 45376 training examples and 4800 test examples, where the inputs are 2-D pixel locations and the outputs are pixel intensities. To scale our algorithms to this setting, we used the approach of <ref type="bibr" target="#b32">Wilson et al. (2014)</ref>. In particular, we exploited the grid structure of texture images to represent the kernel matrix for the full image as a Kronecker product of kernel matrices along each dimension <ref type="bibr" target="#b23">(Saatçi, 2012)</ref>. Since some of the grid points are unobserved, we followed the algorithm in <ref type="bibr" target="#b32">Wilson et al. (2014)</ref> complete the grid with imaginary observations, and placed infinite measurement noise on these observations.</p><p>To reconstruct the missing region, we used an NKN with 4 primitive kernels: LIN, RBF, RQ, and PER. As shown in <ref type="figure" target="#fig_3">Figure 5</ref>(c), our NKN was able to learn and extrapolate complex image patterns. As baselines, we tested RBF and PER kernels; those results are shown in <ref type="figure" target="#fig_3">Figure 5</ref>(d) and <ref type="figure" target="#fig_3">Figure 5</ref>(e). The RBF kernel was unable to extrapolate to the missing region, while the PER kernel was able to extrapolate beyond the training data since the image pattern is almost exactly periodic. We also tested the spectral mixture (SM) kernel, which has previously shown promising results in texture extrapolation. Even with 10 mixture components, its extrapolations were blurrier compared to those of the NKN. The second row shows extrapolations on an irregular paved pattern, which we believe is more difficult. The NKN still provided convincing extrapolation. By contrast, RBF and PER kernels were unable to capture enough information to reconstruct the missing region.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Neural Kernel Network: each module consists of a Linear layer and a Product layer. NKN is based on compositional rules for kernels, thus every individual unit itself represents a kernel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Bayesian optimization on three tasks. The oracle kernel has the true additive structure of underlying function. Shaded error bars represent 0.2 standard deviations over 10 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Texture Extrapolation on metal thread plate (top) and paved pattern (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Toronto, ON, CA. Correspondence to: Shengyang Sun &lt;ssy@cs.toronto.edu&gt;.</figDesc><table>1 Department of Computer Science, University of Toronto, 
Toronto, ON, CA. 
2 Vector Institute. 
3 Uber Advanced Technologies 
Group, Proceedings of the 35 
th International Conference on Machine 
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 
by the author(s). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Average test RMSE and log-likelihood for regression benchmarks with random splits.</figDesc><table>TEST RMSE 
TEST LOG-LIKELIHOOD 
DATASET 
BBB 
GP-RBF 
GP-SM4 
GP-NKN 
BBB 
GP-RBF 
GP-SM4 
GP-NKN 

BOSTON 
3.171±0.149 2.753±0.137 2.979±0.162 2.506±0.150 -2.602±0.031 -2.434±0.069 -2.518±0.107 -2.394±0.080 
CONCRETE 
5.678±0.087 4.685±0.137 3.730±0.190 3.688±0.249 -3.149±0.018 -2.948±0.025 -2.662±0.053 -2.842±0.263 
ENERGY 
0.565±0.018 0.471±0.013 0.316±0.018 0.254±0.020 -1.500±0.006 -0.673±0.035 -0.320±0.089 -0.213±0.162 
KIN8NM 
0.080±0.001 0.068±0.001 0.061±0.000 0.067±0.001 1.111±0.007 
1.287±0.007 
1.387±0.006 
1.291±0.006 
NAVAL 
0.000±0.000 0.000±0.000 0.000±0.000 0.000±0.000 6.143±0.032 
9.557±0.001 
9.923±0.000 
9.916±0.000 
POW. PLANT 4.023±0.036 3.014±0.068 2.781±0.071 2.675±0.074 -2.807±0.010 -2.518±0.020 -2.450±0.022 -2.406±0.023 
WINE 
0.643±0.012 0.597±0.013 0.579±0.012 0.523±0.011 -0.977±0.017 0.723±0.067 
0.652±0.060 
0.852±0.064 
YACHT 
1.174±0.086 0.447±0.083 0.436±0.070 0.305±0.060 -2.408±0.007 -0.714±0.449 -0.891±0.523 -0.116±0.270 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>d-dimensional Styblinski-Tang function and Michalewicz function have fully additive structure with independent dimensions. In our experiment, we set d = 10 and explored the function over domain [ 4, 4] d for Styblinski-Tang and [0, ⇡] d for Michalewicz. We also experimented with a transformed Styblinski-Tang function, which applies Styblinski-Tang function on partitioned dimension groups.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">) operation; therefore, NKN incurs only small per-iteration overhead compared to ordinary GP training. 1 In our experiments, we found 1 or 2 modules work very well. But it might be advantageous to use more modules in other tasks.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The number for each layer represents the output dimension.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed the Neural Kernel Network (NKN), a differentiable architecture for compositional kernel learning. Since the architecture is based on the composition rules for kernels, the NKN can compactly approximate the kernel structures from the Automatic Statistician (AS) grammar. But because the architecture is differentiable, the kernel can be learned orders-of-magnitude faster than the AS using gradient-based optimization. We demonstrated the universality of the NKN for the class of stationary kernels, and showed that the network's representations can be made significantly more compact using complex-valued kernels. Empirically, we found the NKN is capable of pattern discovery and extrapolation in both 1-D time series datasets and 2-D textures, and can find and exploit additive structure for Bayesian Optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank David Duvenaud and Jeongseop Kim for their insightful comments and discussions on this project. SS was supported by a Connaught New Researcher Award and a Connaught Fellowship. GZ was supported by an NSERC Discovery Grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploring large feature spaces with hierarchical multiple kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05424</idno>
		<title level="m">Weight uncertainty in neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Lectures on Fourier Integrals: With an Author&apos;s Supplement on Monotonic Functions, Stieltjes Integrals and Harmonic Analysis; Translated from the Original German by</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bochner</surname></persName>
		</author>
		<editor>Morris Tenenbaum and Harry Pollard</editor>
		<imprint>
			<date type="published" when="1959" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brochu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Cora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1012.2599</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Information theory: coding theorems for discrete memoryless systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Csiszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Körner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4922</idno>
		<title level="m">Structure discovery in nonparametric regression through compositional kernel search</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Additive Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="226" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discovering and exploiting additive structure for Bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1311" to="1319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Classes of kernels for machine learning: a statistics perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Genton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="299" to="312" />
			<date type="published" when="2001-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Probabilistic backpropagation for scalable learning of Bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1861" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using deep belief nets to learn covariance kernels for Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1249" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A note on harmonizable and v-bounded processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kakihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="140" to="156" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">High dimensional bayesian optimisation and bandits via additive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Póczos</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="295" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Generalized spectral kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kom</forename><surname>Samo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02236</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sparse spectrum Gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lázaro-Gredilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quiñonero Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1865" to="1881" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Automatic construction and natural-language description of nonparametric regression models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bayesian optimization for automated model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Malkomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2900" to="2908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bayesian nonparametric kernellearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1078" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sum-product networks: A new deep architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCV Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="690" />
		</imprint>
	</monogr>
	<note>2011 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Evaluation of Gaussian processes and other methods for non-linear regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Remes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heinonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08736</idno>
		<title level="m">Non-stationary spectral kernels</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Scalable inference for structured Gaussian process models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Saatçi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Estimating the dimension of a model. The annals of statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schwarz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="461" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep, narrow sigmoid belief networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2629" to="2636" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Variational learning of inducing variables in sparse Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Titsias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="567" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning stationary time series using gaussian processes with nonparametric kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tobar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3501" to="3509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Batched highdimensional bayesian optimization via structural kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01973</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Annals of mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wiener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tauberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Theorems</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1932" />
			<biblScope unit="page" from="1" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gaussian process kernels for pattern discovery and extrapolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1067" to="1075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast kernel learning for multidimensional pattern extrapolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gilboa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nehorai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3626" to="3634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Correlation theory of stationary and related random functions: Supplementary notes and references</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Yaglom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
