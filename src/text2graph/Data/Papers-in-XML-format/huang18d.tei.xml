<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Autoregressive Flows</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
						</author>
						<author>
							<affiliation>
								<orgName>1 3</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Autoregressive Flows</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Normalizing flows and autoregressive models have been successfully combined to produce state-of-the-art results in density estimation, via Masked Autoregressive Flows (MAF) <ref type="bibr" target="#b30">(Papamakarios et al., 2017)</ref>, and to accelerate stateof-the-art WaveNet-based speech synthesis to 20x faster than real-time <ref type="bibr" target="#b29">(Oord et al., 2017)</ref>, via Inverse Autoregressive Flows (IAF) . We unify and generalize these approaches, replacing the (conditionally) affine univariate transformations of MAF/IAF with a more general class of invertible univariate transformations expressed as monotonic neural networks. We demonstrate that the proposed neural autoregressive flows (NAF) are universal approximators for continuous probability distributions, and their greater expressivity allows them to better capture multimodal target distributions. Experimentally, NAF yields state-of-the-art performance on a suite of density estimation tasks and outperforms IAF in variational autoencoders trained on binarized MNIST.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Invertible transformations with a tractable Jacobian, also known as normalizing flows, are useful tools in many machine learning problems, for example: (1) In the context of deep generative models, training necessitates evaluating data samples under the model's inverse transformation . Tractable density is an appealing property for these models, since it allows the objective of interest to be directly optimized; whereas other mainstream methods rely on alternative losses, in the case of intractable density models <ref type="bibr" target="#b19">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b34">Rezende et al., 2014)</ref>, or * Equal contribution 1 MILA, University of Montreal 2 Element AI 3 CIFAR fellow. Correspondence to: Chin-Wei Huang &lt;chin-wei.huang@umontreal.ca&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the 35</head><p>th International Conference on Machine Learning, <ref type="bibr">Stockholm, Sweden, PMLR 80, 2018</ref><ref type="bibr">. Copyright 2018</ref> by the author(s).</p><p>1 Implementation can be found at https://github.com/CWHuang/NAF/ implicit losses, in the case of adversarial models <ref type="bibr" target="#b12">(Goodfellow et al., 2014)</ref>. <ref type="formula">(2)</ref> In the context of variational inference <ref type="bibr" target="#b33">(Rezende &amp; Mohamed, 2015)</ref>, they can be used to improve the variational approximation to the posterior by parameterizing more complex distributions. This is important since a poor variational approximation to the posterior can fail to reflect the right amount of uncertainty, and/or be biased <ref type="bibr" target="#b40">(Turner &amp; Sahani, 2011)</ref>, resulting in inaccurate and unreliable predictions. We are thus interested in improving techniques for normalizing flows.</p><p>Recent work by <ref type="bibr" target="#b20">Kingma et al. (2016)</ref> reinterprets autoregressive models as invertible transformations suitable for constructing normalizing flows. The inverse transformation process, unlike sampling from the autoregressive model, is not sequential and thus can be accelerated via parallel computation. This allows multiple layers of transformations to be stacked, increasing expressiveness for better variational inference  or better density estimation for generative models <ref type="bibr" target="#b30">(Papamakarios et al., 2017)</ref>. Stacking also makes it possible to improve on the sequential conditional factorization assumed by autoregressive models such as PixelRNN or PixelCNN , and thus define a more flexible joint probability.</p><p>We note that the normalizing flow introduced by <ref type="bibr" target="#b20">Kingma et al. (2016)</ref> only applies an affine transformation of each scalar random variable. Although this transformation is conditioned on preceding variables, the resulting flow can still be susceptible to bad local minima, and thus failure to capture the multimodal shape of a target density; see <ref type="figure" target="#fig_0">Figure  1</ref> and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Contributions of this work</head><p>We propose replacing the conditional affine transformation of <ref type="bibr" target="#b20">Kingma et al. (2016)</ref> with a more rich family of transformations, and note the requirements for doing so. We determine that very general transformations, for instance parametrized by deep neural networks, are possible. We then propose and evaluate several specific monotonic neural network architectures which are more suited for learning multimodal distributions. Concretely, our method amounts to using an autoregressive model to output the weights of multiple independent transformer networks, each of which operates on a single random variable, replacing the affine  transformations of previous works.</p><p>Empirically, we show that our method works better than the state-of-the-art affine autoregressive flows of <ref type="bibr" target="#b20">Kingma et al. (2016)</ref> and <ref type="bibr" target="#b30">Papamakarios et al. (2017)</ref>, both as a sample generator which captures multimodal target densities with higher fidelity, and as a density model which more accurately evaluates the likelihood of data samples drawn from an unknown distribution.</p><p>We also demonstrate that our method is a universal approximator on proper distributions in real space, which guarantees the expressiveness of the chosen parameterization and supports our empirical findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>A (finite) normalizing flow (NF), or flow, is an invertible function f θ : X → Y used to express a transformation between random variables 2 . Since f is invertible, the change of variables formula can be used to translate between densities p Y (y) and p X (x):</p><formula xml:id="formula_0">p Y (y) = ∂f (x) ∂ x −1 p X (x)<label>(1)</label></formula><p>The determinant of f 's Jacobian appears on the right hand side to account for the way in which f can (locally) expand or contract regions of X, thereby lowering or raising the resulting density in those regions' images in Y . Since the composition of invertible functions is itself invertible, complex NFs are often formed via function composition (or "stacking") of simpler NFs.</p><p>Normalizing flows are most commonly trained to produce an output distribution p Y (y) which matches a target distribution (or, more generally, energy function) p target (y) as measured by the KL-divergence KL(p Y (y)||p target (y)). When X or Y is distributed by some simple distribution, such as uniform or standard normal, we call it an unstructured noise; and we call it a structured noise when the distribution is complex and correlated. Two common settings are maximum likelihood and variational inference. Note that these two settings are typically viewed as optimizing different directions of the KL-divergence, whereas we provide a unified view in terms of different input and target distributions. A detailed derivation is presented in the appendix (See Section A).</p><p>For maximum likelihood applications <ref type="bibr" target="#b30">Papamakarios et al., 2017)</ref>, p target (y) is typically a simple prior over latent variable y, and f attempts to disentangle the complex empirical distribution of the data, p X (x) into a simple latent representation p Y (y) matching the prior (structured to unstructured)</p><formula xml:id="formula_1">3 .</formula><p>In a typical application of variational inference <ref type="bibr" target="#b33">(Rezende &amp; Mohamed, 2015;</ref><ref type="bibr" target="#b20">Kingma et al., 2016)</ref>, p target (y) is a complex posterior over latent variables y, and f transforms a simple input distribution (for instance a standard normal distribution) over x into a complex approximate posterior p Y (y) (unstructured to structured). In either case, since p X does not depend on θ, the gradients of the KL-divergence are typically estimated by Monte Carlo:</p><formula xml:id="formula_2">∇ θ D KL p Y (y)||p target (y) = ∇ θ Y p Y (y) log p Y (y) p target (y) d y = X p X (x)∇ θ log p Y (y) p target (y) d x (2)</formula><p>Applying the change of variables formula from Equation 1 to the right hand side of Equation 2 yields:</p><formula xml:id="formula_3">E x∼p X (x) y=f θ (x) ∇ θ log ∂f θ (x) ∂ x −1 p X (x) − ∇ θ log p target (y)<label>(3)</label></formula><p>3 It may also be possible to form a generative model from such a flow, by passing samples from the prior ptarget(y) through f −1 , although the cost of doing so may vary. For example, RealNVP  was devised as a generative model, and its inverse computation is as cheap as its forward computation, whereas MAF <ref type="bibr" target="#b30">(Papamakarios et al., 2017)</ref> is designed for density estimation and is much more expensive to sample from. For the NAF architectures we employ, we do not have an analytic expression for f −1 , but it is possible to approximate it numerically.  Thus for efficient training, the following operations must be tractable and cheap:</p><formula xml:id="formula_4">1. Sampling x ∼ p X (x) 2. Computing y = f (x)</formula><p>3. Computing the gradient of the log-likelihood of y = f (x); x ∼ p X (x) under both p Y (y) and p target (y)</p><p>4. Computing the gradient of the log-determinant of the Jacobian of f</p><p>Research on constructing NFs, such as our work, focuses on finding ways to parametrize flows which meet the above requirements while being maximally flexible in terms of the transformations which they can represent. Note that some of the terms of of Equation 3 may be constant with respect to θ 4 and thus trivial to differentiate, such as p X (x) in the maximum likelihood setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Affine autoregressive flows (AAFs)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head><p>, such as inverse autoregressive flows (IAF) , are one 4 There might be some other parameters other than θ that are learnable, such as parameters of pX and ptarget in the variational inference and maximum likelihood settings, respectively.</p><p>5 Our terminology differs from previous works, and hence holds the potential for confusion, but we believe it is apt. Under our unifying perspective, NAF, IAF, AF, and MAF all make use of the same principle, which is an invertible transformer conditioned on the outputs of an autoregressive (and emphatically not an inverse autoregressive) conditioner.</p><p>particularly successful pre-existing approach. Affine autoregressive flows yield a triangular Jacobian matrix, so that the log-determinant can be computed in linear time, as the sum of the diagonal entries on log scale. In AAFs, the components of x and y are given an order (which may be chosen arbitrarily), and y t is computed as a function of x 1:t . Specifically, this function can be decomposed via an autoregressive conditioner, c, and an invertible transformer, τ , as 6 :</p><formula xml:id="formula_5">y t . = f (x 1:t ) = τ (c(x 1:t−1 ), x t )<label>(4)</label></formula><p>It is possible to efficiently compute the output of c for all t in a single forward pass using a model such as MADE <ref type="bibr" target="#b11">(Germain et al., 2015)</ref>, as pointed out by <ref type="bibr" target="#b20">Kingma et al. (2016)</ref>.</p><p>In previous work, τ is taken to be an affine transformation with parameters µ ∈ R, σ &gt; 0 output from c. For instance <ref type="bibr" target="#b7">Dinh et al. (2017)</ref> use:</p><formula xml:id="formula_6">τ (µ, σ, x t ) = µ + σx t<label>(5)</label></formula><p>with σ produced by an exponential nonlinearity. <ref type="bibr" target="#b20">Kingma et al. (2016)</ref> use:</p><formula xml:id="formula_7">τ (µ, σ, x t ) = σx t + (1 − σ)µ<label>(6)</label></formula><p>with σ produced by a sigmoid nonlinearity. Such transformers are trivially invertible, but their relative simplicity also means that the expressivity of f comes entirely from the complexity of c and from stacking multiple AAFs (potentially using different orderings of the variables)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7</head><p>. However, the only requirements on τ are:</p><p>1. The transformer τ must be invertible as a function of x t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>dyt dxt must be cheap to compute.</p><p>This raises the possibility of using a more powerful transformer in order to increase the expressivity of the flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Neural Autoregressive Flows</head><p>We propose replacing the affine transformer used in previous works with a neural network, yielding a more rich family of distributions with only a minor increase in computation and memory requirements. Specifically,</p><formula xml:id="formula_8">τ (c(x 1:t−1 ), x t ) = DNN(x t ; φ = c(x 1:t−1 ))<label>(7)</label></formula><p>6 <ref type="bibr" target="#b6">Dinh et al. (2014)</ref> use m and g −1 to denote c and τ , and refer to them as the "coupling function" and "coupling law", respectively. <ref type="bibr">7</ref> Permuting the order of variables is itself a normalizing flow that does not expand or contract space and can be inverted by another permutation. is a deep neural network which takes the scalar x t as input and produces y t as output, and its weights and biases are given by the outputs of c(x 1:t−1 ) 8 (see <ref type="figure" target="#fig_3">Figure 4</ref>(a)). We refer to these values φ as pseudo-parameters, in order to distinguish them from the statistical parameters of the model.</p><p>We now state the condition for NAF to be strictly monotonic, and thus invertible (as per requirement 1): Proposition 1. Using strictly positive weights and strictly monotonic activation functions for τ c is sufficient for the entire network to be strictly monotonic.</p><p>Meanwhile, dyt dxt and gradients wrt the pseudo-parameters 9 can all be computed efficiently via backpropagation (as per requirement 2). <ref type="figure">Figure 5</ref>. Illustration of the effects of traditional IAF (top), and our proposed NAF (bottom). Areas where the slope of the transformer τc is greater/less than 1, are compressed/expanded (respectively) in the output distribution. Inflection points in τc(xt) (middle) can transform a unimodal p(xt) (left) into a multimodal p(yt) (right); NAF allows for such inflection points, whereas IAF does not.</p><p>Whereas affine transformers require information about multimodality in y t to flow through x 1:t−1 , our neural autoregressive flows (NAFs) are able to induce multimodality more naturally, via inflection points in τ c , as shown in <ref type="figure">Figure 5</ref>. Intuitively, τ c can be viewed as analogous to a cumulative distribution function (CDF), so that its derivative corresponds to a PDF, where its inflection points yield local maxima or minima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Transformer Architectures</head><p>In this work, we use two specific architectures for τ c , which we refer to as deep sigmoidal flows (DSF) and deep dense sigmoidal flows (DDSF) (see <ref type="figure" target="#fig_3">Figure 4</ref>(b), 4(c) for an illustration). We find that small neural network transformers of 1 or 2 hidden layers with 8 or 16 sigmoid units perform well across our experiments, although there are other possibilities worth exploring (see Section 3.3). Sigmoids contain inflection points, and so can easily induce inflection points in τ c , and thus multimodality in p(y t ). We begin by describing the DSF transformation, which is already sufficiently expressive to form a universal approximator for probability distributions, as we prove in section 4.</p><p>The DSF transformation resembles an MLP with a single hidden layer of sigmoid units. Naive use of sigmoid activation functions would restrict the range of τ c , however, and result in a model that assigns 0 density to sufficiently large or small y t , which is problematic when y t can take on arbitrary real values. We address this issue by applying the inverse sigmoid (or "logit") function at the output layer. To ensure that the output's preactivation is in the domain of the logit (that is, (0, 1)), we combine the output of the sigmoid units via an attention-like <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref> softmax-weighted sums:</p><formula xml:id="formula_9">y t = σ −1 (w T 1×d ·σ( a d×1 · x t 1×1 + b d×1 )))<label>(8)</label></formula><p>where 0 &lt; w i,j &lt; 1, i w i,j = 1, a s,t &gt; 0, b ∈ R d and d denotes the number of hidden units 10 .</p><p>Since all of the sigmoid activations are bounded between 0 and 1, the final preactivation (which is their convex combination) is as well. The complete DSF transformation can be seen as mapping the original random variable to a different space through an activation function, where doing affine/linear operations is non-linear with respect to the variable in the original space, and then mapping it back to the original space through the inverse activation.</p><p>However, we realize the composition of multiple DSF layers resembles an MLP with bottleneck as shown by the bottom left of <ref type="figure" target="#fig_3">Figure 4</ref>. A more general alternative is the DDSF transformation, which takes the form of a fully connected MLP:</p><formula xml:id="formula_10">h (l+1) = σ −1 ( w (l+1) d l+1 ×d l+1 · σ(a (l+1) d l+1 u (l+1) d l+1 ×d l · h (l) d l +b (1+1) d l+1 )) (9) for 1 ≤ l ≤ L where h 0 = x and y = h L ; d 0 = d L = 1.</formula><p>We also require j w ij = 1, j u kj = 1 for all i, k, and all parameters except b to be positive. . To compute the log-determinant of Jacobian in a numerically stable way, we need to apply log-sum-exp to the chain rule</p><formula xml:id="formula_11">∇ x y = ∇ h (L−1) h (L) ∇ h (L−2) h (L−1) , · · · , ∇ h (0) h (1)<label>(10)</label></formula><p>We elaborate more on the numerical stability in parameterization and computation of logarithmic operations in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Efficient Parametrization of Larger Transformers</head><p>Multi-layer NAFs, such as DDSF, require c to output O(d 2 ) pseudo-parameters, where d is the number of hidden units in each layer of τ . As this is impractical for large d, we propose parametrizing τ with O(d 2 ) statistical parameters, but only O(d) pseudo-parameters which modulate the computation on a per-unit basis, using a technique such as conditional batch-normalization (CBN) <ref type="bibr" target="#b8">(Dumoulin et al., 2017)</ref>. Such an approach also makes it possible to use minibatch-style 10 Constraints on the variables are enforced via activation functions; w and a are outputs of a softmax, and softplus or exp, respectively. matrix-matrix products for the forward and backwards propagation through the graph of τ c . In particular, we use a technique similar to conditional weight normalization (CWN) <ref type="bibr" target="#b21">(Krueger et al., 2017)</ref> in our experiments with DDSF; see appendix for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Possibilities for Alternative Architectures</head><p>While the DSF and DDSF architectures performed well in our experiments, there are many alternatives to be explored. One possibility is using other (strictly) monotonic activation functions in τ c , such as leaky ReLUs (LReLU) <ref type="bibr" target="#b42">(Xu et al., 2015)</ref> or ELUs <ref type="bibr" target="#b4">(Clevert et al., 2016)</ref>. Leaky ReLUs in particular are bijections on R and so would not require the softmax-weighted summation and activation function inversion tricks discussed in the previous section.</p><p>Finally, we emphasize that in general, τ need not be expressed as a neural architecture; it only needs to satisfy the requirements of invertibility and differentiability given at the end of section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">NAFs are Universal Density Approximators</head><p>In this section, we prove that NAFs (specifically DSF) can be used to approximate any probability distribution over real vectors arbitrarily well, given that τ c has enough hidden units output by generic neural networks with autoregressive conditioning. Ours is the first such result we are aware of for finite normalizing flows.</p><p>Our result builds on the work of <ref type="bibr" target="#b15">Huang et al. (2017)</ref>, who demonstrate the general universal representational capability of inverse autoregressive transformations parameterized by an autoregressive neural network (that transform uniform random variables into any random variables in reals). However, we note that their proposition is weaker than we require, as there are no constraints on the parameterization of the transformer τ , whereas we've constrained τ to have strictly positive weights and monotonic activation functions, to ensure it is invertible throughout training.</p><p>The idea of proving the universal approximation theorem for DSF (1) in the IAF direction (which transforms unstructured random variables into structured random variables) resembles the concept of the inverse transform sampling: we first draw a sample from a simple distribution, such as uniform distribution, and then pass the sample though DSF. If DSF converges to any inverse conditional CDF, the resulting random variable then converges in distribution to any target random variable as long as the latter has positive continuous probability density everywhere in the reals. (2) For the MAF direction, DSF serves as a solution to the nonlinear independent component analysis problem <ref type="bibr" target="#b16">(Hyvärinen &amp; Pajunen, 1999)</ref>, which disentangles structured random variables into uniformly and independently distributed ran-dom variables. (3) Combining the two, we further show that DSF can transform any structured noise variable into a random variable with any desired distribution.</p><p>We define the following notation for the pre-logit of the DSF transformation (compare equation 8): Proposition 2. (DSF universally transforms uniform random variables into any desired random variables) Let Y be a random vector in R m and assume Y has a strictly positive and continuous probability density distribution. Let X ∼ Unif((0, 1) m ). Then there exists a sequence of functions (G n ) n≥1 parameterized by autoregressive neural networks in the following form</p><formula xml:id="formula_12">S(x t , C(x 1:t−1 )) = n j=1 w j (x 1:t−1 )·σ x t − b j (x 1:t−1 ) τ j (x 1:t−1 )<label>(11)</label></formula><formula xml:id="formula_13">G(x) t = σ −1 (S (x t ; C t (x 1:t−1 )))<label>(12)</label></formula><p>where </p><formula xml:id="formula_14">C t = (a tj , b tj , τ tj ) n j=1 are functions of x 1:t−1 , such that Y n . = G n (X) converges in distribution to Y .</formula><formula xml:id="formula_15">H(x) t = S (x t ; C t (x 1:t−1 ))<label>(13)</label></formula><p>where C t = (a tj , b tj , τ tj ) n j=1 are functions of x 1:t−1 , such that Y n . = H n (X) converges in distribution to Y . Theorem 1. (DSF universally transforms any random variables into any desired random variables) Let X be a random vector in an open set U ⊂ R m . Let Y be a random vector in R m . Assume both X and Y have a positive and continuous probability density distribution. Then there exists a sequence of functions (K n ) n≥1 parameterized by autoregressive neural networks in the following form</p><formula xml:id="formula_16">K(x) t = σ −1 (S (x t ; C t (x 1:t−1 )))<label>(14)</label></formula><p>where C t = (a tj , b tj , τ tj ) n j=1 are functions of x 1:t−1 , such that Y n . = K n (X) converges in distribution to Y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related work</head><p>Neural autoregressive flows are a generalization of the affine autoregressive flows introduced by <ref type="bibr" target="#b20">Kingma et al. (2016)</ref> as inverse autoregressive flows (IAF) and further developed by <ref type="bibr" target="#b3">Chen et al. (2017)</ref> and <ref type="bibr" target="#b30">Papamakarios et al. (2017)</ref> as autoregressive flows (AF) and masked autoregressive flows (MAF), respectively; for details on their relationship to our work see Sections 2 and 3. While <ref type="bibr" target="#b6">Dinh et al. (2014)</ref> draw a particular connection between their NICE model and the Neural Autoregressive Density Estimator (NADE) <ref type="bibr" target="#b22">(Larochelle &amp; Murray, 2011)</ref>,  were the first to highlight the general approach of using autoregressive models to construct normalizing flows. <ref type="bibr" target="#b3">Chen et al. (2017)</ref> and then <ref type="bibr" target="#b30">Papamakarios et al. (2017)</ref> subsequently noticed that this same approach could be used efficiently in reverse when the key operation is evaluating, as opposed to sampling from, the flow's learned output density. Our method increases the expressivity of these previous approaches by using a neural net to output pseudoparameters of another network, thus falling into the hypernetwork framework <ref type="bibr" target="#b13">(Ha et al., 2017;</ref><ref type="bibr" target="#b2">Bertinetto et al., 2016;</ref><ref type="bibr" target="#b17">Jia et al., 2016)</ref>.</p><p>There has been a growing interest in normalizing flows (NFs) in the deep learning community, driven by successful applications and structural advantages they have over alternatives. <ref type="bibr" target="#b35">Rippel &amp; Adams (2013)</ref>, <ref type="bibr" target="#b33">Rezende &amp; Mohamed (2015)</ref> and <ref type="bibr" target="#b6">Dinh et al. (2014)</ref> first introduced normalizing flows to the deep learning community as density models, variational posteriors and generative models, respectively. In contrast to traditional variational posteriors, NFs can represent a richer family of distributions without requiring approximations (beyond Monte Carlo estimation of the KL-divergence). The NF-based RealNVP-style generative models  also have qualitative advantages over alternative approaches. Unlike generative adversarial networks (GANs) <ref type="bibr" target="#b12">(Goodfellow et al., 2014)</ref> and varational autoencoders (VAEs) <ref type="bibr" target="#b19">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b34">Rezende et al., 2014)</ref>, computing likelihood is cheap. Unlike autoregressive generative models, such as pixelCNNs , sampling is also cheap. Unfortunately, in practice RealNVP-style models are not currently competitive with autoregressive models in terms of likelihood, perhaps due to the more restricted nature of the transformations they employ.</p><p>Several promising recent works expand the capabilities of NFs for generative modeling and density estimation, however. Perhaps the most exciting example is <ref type="bibr" target="#b29">Oord et al. (2017)</ref>, who propose the probability density distillation technique to train an IAF  based on the autoregressive WaveNet (van den <ref type="bibr" target="#b28">Oord et al., 2016)</ref> as a generative model using another pretrained WaveNet model to express the target density, thus overcoming the slow sequential sampling procedure required by the original WaveNet (and characteristic of autoregressive models in general), and reaching super-real-time speeds suitable for production. The previously mentioned MAF technique (Pa-   <ref type="bibr">, 2017)</ref> further demonstrates the potential of NFs to improve on state-of-the-art autoregressive density estimation models; such highly performant MAF models could also be "distilled" for rapid sampling using the same procedure as in <ref type="bibr" target="#b29">Oord et al. (2017)</ref>.</p><p>Other recent works also find novel applications of NFs, demonstrating their broad utility. <ref type="bibr" target="#b24">Loaiza-Ganem et al. (2017)</ref> use NFs to solve maximum entropy problems, rather than match a target distribution. <ref type="bibr" target="#b25">Louizos &amp; Welling (2017)</ref> and <ref type="bibr" target="#b21">Krueger et al. (2017)</ref> apply NFs to express approximate posteriors over parameters of neural networks. <ref type="bibr" target="#b37">Song et al. (2017)</ref> use NFs as a proposal distribution in a novel Metropolis-Hastings MCMC algorithm.</p><p>Finally, there are also several works which develop new techniques for constructing NFs that are orthogonal to ours <ref type="bibr" target="#b39">(Tomczak &amp; Welling, 2017;</ref><ref type="bibr" target="#b10">Gemici et al., 2016;</ref><ref type="bibr" target="#b9">Duvenaud et al., 2016;</ref><ref type="bibr" target="#b1">Berg et al., 2018)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>Our experiments evaluate NAFs on the classic applications of variational inference and density estimation, where we outperform IAF and MAF baselines. We first demonstrate the qualitative advantage NAFs have over AAFs in energy function fitting and density estimation (Section 6.1). We then demonstrate the capability of NAFs to capture a multimodal Bayesian posterior in a limited data setting (Section 6.2). For larger-scale experiments, we show that using NAF instead of IAF to approximate the posterior distribution of latent variables in a variational autoencoder <ref type="bibr" target="#b19">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b34">Rezende et al., 2014)</ref> yields better likelihood results on binarized MNIST <ref type="bibr" target="#b22">(Larochelle &amp; Murray, 2011)</ref> (Section 6.3). Finally, we report our experimental results on density estimation of a suite of UCI datasets (Section 6.4).</p><p>6.1. Toy energy fitting and density estimation 6.1.1. EXPRESSIVENESS First, we demonstrate that, in the case of marginally independent distributions, affine transformation can fail to fit the true distribution. We consider a mixture of Gaussian density estimation task. We define the modes of the Gaussians to be laid out on a 2D meshgrid within the range <ref type="bibr">[-5,5]</ref>, and consider 2, 5 and 10 modes on each dimension. While the affine flow only produces a single mode, the neural flow matches the target distribution quite well even up to a 10x10 grid with 100 modes (see <ref type="figure">Figure 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2.">CONVERGENCE</head><p>We then repeat the experiment that produces <ref type="figure" target="#fig_0">Figure 1</ref> and 2 16 times, smooth out the learning curve and present average convergence result of each model with its corresponding standard deviation. For affine flow, we stack 6 layers of transformation with reversed ordering. For DSF and DDSF we used one transformation. We set d = 16 for both, L = 2 for DDSF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Sine Wave experiment</head><p>Here we demonstrate the ability of DSF to capture multimodal posterior distributions. To do so, we create a toy experiment where the goal is to infer the posterior over the frequency of a sine wave, given only 3 datapoints. We fix the form of the function as y(t) = sin(2πf · t) and specify a Uniform prior over the frequency: p(f ) = U ([0, 2]). The task is to infer the posterior distribution p(f |T, Y ) given the dataset (T, Y ) = ((0, 5 / 6, 10 / 6), (0, 0, 0)), as represented by the red crosses of <ref type="figure" target="#fig_9">Figure 8</ref> (left). We assume the data likelihood given the frequency parameter to be p(y i |t i , f ) = N (y i ; y f (t i ), 0.125), where the variance σ 2 = 0.125 represents the inherent uncertainty of the data. <ref type="figure" target="#fig_9">Figure 8</ref> (right) shows that DSF learns a good posterior in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Amortized Approximate Posterior</head><p>We evaluate NAF's ability to improve variational inference, in the context of the binarized MNIST <ref type="bibr" target="#b22">(Larochelle &amp; Murray, 2011</ref>) benchmark using the well-known variational autoencoder <ref type="bibr" target="#b19">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b34">Rezende et al., 2014)</ref> ( <ref type="table" target="#tab_1">Table 1)</ref>. Here again the DSF architecture outperforms both standard IAF and the traditional independent Gaussian posterior by a statistically significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Density Estimation with Masked Autoregressive Flows</head><p>We replicate the density estimation experiments of Papamakarios et al. <ref type="formula" target="#formula_0">(2017)</ref>, which compare MADE <ref type="bibr" target="#b11">(Germain et al., 2015)</ref> and RealNVP  to their proposed MAF model (using either 5 or 10 layers of MAF) on BSDS300 <ref type="bibr" target="#b26">(Martin et al., 2001</ref>) as well as 4 UCI datasets <ref type="bibr" target="#b23">(Lichman, 2013)</ref> processed as in <ref type="bibr" target="#b41">Uria et al. (2013)</ref>. Simply replacing the affine transformer with our DDSF architecture in their best performing architecture for each task (keeping all other settings fixed) results in substantial performance gains, and also outperforms the more recent Transformation Autoregressive Networks (TAN) <ref type="bibr" target="#b27">Oliva et al. (2018)</ref>, setting a new state-of-the-art for these tasks. Results are presented in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this work we introduce the neural autoregressive flow (NAF), a flexible method of tractably approximating rich families of distributions. In particular, our experiments show that NAF is able to model multimodal distributions and outperform related methods such as inverse autoregressive flow in density estimation and variational inference. Our work emphasizes the difficulty and importance of capturing multimodality, as previous methods fail even on simple toy tasks, whereas our method yields significant improvements in performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Energy function fitting using IAF. Left: true distribution. Center: IAF-affine. Right: IAF-DSF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Density estimation using MAF. Left: true distribution. Center: MAF-affine. Right: MAF-DSF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Difference between autoregressive and inverse autoregressive transformations (left), and difference between IAF and MAF (right). Upper left: sample generation of an autoregressive model. Unstructured noise is transformed into structured noise. Lower left: inverse autoregressive transformation of structured data. Structured variables are transformed into unstructured variables. Upper right: IAF-style sampling. Lower right: MAFstyle evaluation of structured data. represents unstructured noise and s represents structured noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Top: In neural autoregressive flows, the transformation of the current input variable is performed by an MLP whose parameters are output from an autoregressive conditioner model, ct . = c(x1:t−1), which incorporates information from previous input variables. Bottom: The architectures we use in this work: deep sigmoidal flows (DSF) and deep dense sigmoidal flows (DDSF). See section 3.1 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>We use either DSF (Equation 8) or DDSF (Equation 9) to define the transformer function τ in Equation 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>where C = (w j , b j , τ j ) n j=1 are functions of x 1:1−t param- eterized by neural networks. Let b j be in (r 0 , r 1 ); τ j be bounded and positive; n j=1 w j = 1 and w j &gt; 0. See Appendix F and G for the proof.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Proposition 3. (DSF universally transforms any random variables into uniformly distributed random variables) Let X be a random vector in an open set U ⊂ R m . Assume X has a positive and continuous probability density distribution. Let Y ∼ Unif((0, 1) m ). Then there exists a sequence of functions (H n ) n≥1 parameterized by autoregressive neural networks in the following form</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Fitting grid of Gaussian distributions using maximum likelihood. Left: true distribution. Center: affine autoregressive flow (AAF). Right: neural autoregressive flow (NAF)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Learning curve of MAF-style and IAF-style training. q denotes our trained model, and p denotes the target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. The DSF model effectively captures the true posterior distribution over the frequency of a sine wave. Left: The three observations (marked with red x's) are compatible with sine waves of frequency f ∈ 0.0, 0.6, 1.2, 1.8. Right: a histogram of samples from the DSF approximate posterior ("counts") and a Kernel Density Estimate of the distribution it represents (KDE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Using DSF to improve variational inference.</figDesc><table>We report 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Test log-likelihood and error bars of 2 standard deviations on the 5 datasets (5 trials of experiments). Neural autoregressive flows (NAFs) produce state-of-the-art density estimation results on all 5 datasets. The numbers (5 or 10) in parantheses indicate the number of transformations which were stacked; for TAN (Oliva et al., 2018), we include their best results, achieved using different architectures on different datasets. We also include validation results to give future researchers a fair way of comparing their methods with ours during development.</figDesc><table>Model 
POWER 
GAS 
HEPMASS 
MINIBOONE 
BSDS300 

MADE MoG 
0.40 ± 0.01 
8.47 ± 0.02 
−15.15 ± 0.02 −12.27 ± 0.47 153.71 ± 0.28 
MAF-affine (5) 
0.14 ± 0.01 
9.07 ± 0.02 
−17.70 ± 0.02 −11.75 ± 0.44 155.69 ± 0.28 
MAF-affine (10) 
0.24 ± 0.01 
10.08 ± 0.02 
−17.73 ± 0.02 −12.24 ± 0.45 154.93 ± 0.28 
MAF-affine MoG (5) 
0.30 ± 0.01 
9.59 ± 0.02 
−17.39 ± 0.02 −11.68 ± 0.44 156.36 ± 0.28 

TAN (various architectures) 0.48 ± 0.01 
11.19 ± 0.02 
−15.12 ± 0.02 −11.01 ± 0.48 157.03 ± 0.07 

MAF-DDSF (5) 
0.62 ± 0.01 11.91 ± 0.13 −15.09 ± 0.40 −8.86 ± 0.15 157.73 ± 0.04 
MAF-DDSF (10) 
0.60 ± 0.02 11.96 ± 0.33 −15.32 ± 0.23 
−9.01 ± 0.01 
157.43 ± 0.30 

MAF-DDSF (5) valid 
0.63 ± 0.01 
11.91 ± 0.13 
15.10 ± 0.42 
−8.38 ± 0.13 
172.89 ± 0.04 
MAF-DDSF (10) valid 
0.60 ± 0.02 
11.95 ± 0.33 
15.34 ± 0.24 
−8.50 ± 0.03 
172.58 ± 0.32 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use x and y to denote inputs and outputs of a function, not the inputs and targets of a supervised learning problem.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We'll sometimes write τc for τ (c(x1:t−1), ·). 9 Gradients for pseudo-parameters are backpropagated through the conditioner, c, in order to train its parameters.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Tegan Maharaj, Ahmed Touati, Shawn Tan and Giancarlo Kerg for helpful comments and advice. We also thank George Papamakarios for providing details on density estimation task's setup.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V D</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hasenclever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05649</idno>
		<title level="m">Sylvester normalizing flows for variational inference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning feed-forward one-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Variational lossy autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbeel</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematics of control, signals and systems</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">NICE: Non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Density estimation using real nvp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Early stopping as nonparametric variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Gemici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02304</idno>
		<title level="m">Normalizing flows on riemannian manifolds</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Masked autoencoder for distribution estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Made</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Hypernetworks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learnable explicit density for continuous latent space and variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Touati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02248</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nonlinear independent component analysis: Existence and uniqueness results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pajunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courville</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04759</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">A. Bayesian hypernetworks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The neural autoregressive distribution estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of the 14th International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Maximum entropy flow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Loaiza-Ganem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiplicative normalizing flows for variational bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malik</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09819</idno>
		<title level="m">Transformation autoregressive networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V D</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stimberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10433</idno>
		<title level="m">Parallel wavenet: Fast high-fidelity speech synthesis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Masked autoregressive flow for density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pavlakou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the convergence of adam and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">High-dimensional probability estimation with deep density models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.5125</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A-nice-mc: Adversarial training for mcmc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Improving variational auto-encoders using householder flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09630</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Improving variational auto-encoders using convex combination linear inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<editor>Benelearn</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Two problems with variational expectation maximisation for time-series models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Time series models</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The realvalued neural autoregressive density-estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rnade ; Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wavenet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013-09" />
		</imprint>
	</monogr>
	<note>A Generative Model for Raw Audio. ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
