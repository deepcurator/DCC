<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Colorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zezhou</forename><surname>Cheng</surname></persName>
							<email>chengzezhou@sjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingxiong</forename><surname>Yang</surname></persName>
							<email>qiyang@cityu.edu.hk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Sheng</surname></persName>
							<email>shengbin@sjtu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Shanghai Jiao Tong University</orgName>
								<orgName type="institution" key="instit2">City University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Colorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image colorization assigns a color to each pixel of a target grayscale image. Colorization methods can be roughly divided into two categories: scribble-based colorization <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref> and example-based colorization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23]</ref>. The scribble-based methods typically require substantial efforts from the user to provide considerable scribbles on the target grayscale images. It is thus time-assuming to colorize a grayscale image with finescale structures, especially for a rookie user.</p><p>To reduce the burden on user, <ref type="bibr" target="#b22">[23]</ref> proposes an examplebased method which is later further improved by <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref>. The example-based method typically transfers the color information from a similar reference image to the target * Correspondence author. grayscale image. However, finding a suitable reference image becomes an obstacle for a user. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref> simplify this problem by utilizing the image data on the Internet and propose filtering schemes to select suitable reference images. However, they both have additional constraints. <ref type="bibr" target="#b13">[14]</ref> requires identical Internet object for precise per-pixel registration between the reference images and the target grayscale image. It is thus limited to objects with a rigid shape (e.g. landmarks). <ref type="bibr" target="#b1">[2]</ref> requires user to provide a semantic text label and segmentation cues for the foreground object. In practice, manual segmentation cues are hard to obtain as the target grayscale image may contain multiple complex objects (e.g. building, car, tree, elephant). These methods share the same limitation âˆ’ their performance highly depends on the selected reference image(s).</p><p>A fully-automatic colorization method is proposed to address this limitation. Intuitively, one reference image cannot include all possible scenarios in the target grayscale image. As a result, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23]</ref> require similar reference image(s). A more reliable solution is locating the most similar image patch/pixel in a huge reference image database and then transferring color information from the matched patch/pixel to the target patch/pixel. However, the matching noise is too high when a large-scale database is adopted in practice.</p><p>Deep learning techniques have achieved amazing success in modeling large-scale data recently. It has shown powerful learning ability that even outperforms human to some extent (e.g. <ref type="bibr" target="#b6">[7]</ref>) and deep learning techniques have been demonstrated to be very effective on various computer vision and image processing applications including image classification <ref type="bibr" target="#b11">[12]</ref>, pedestrian detection <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26]</ref>, image super-resolution <ref type="bibr" target="#b3">[4]</ref>, photo adjustment <ref type="bibr" target="#b23">[24]</ref> etc. The success of deep learning techniques motivates us to explore its potential application in our context. This paper formulates image colorization as a regression problem and deep neural networks are used to solve the problem. A large database of reference images comprising all kinds of objects (e.g. tree, animal, building, sea, mountain etc.) is used for training the neural networks. Some example reference images are presented in <ref type="figure" target="#fig_0">Figure 1 (b)</ref>. Although the training is very slow due to the adoption of a large database, the learned model can be directly used to colorize a target grayscale image efficiently. The state-of-the-art coloriza- tion methods normally require matching between the target and reference images and thus are slow. It has recently been demonstrated that high-level understanding of an image is very useful for low-level vision problems (e.g. image enhancement <ref type="bibr" target="#b23">[24]</ref>, edge detection <ref type="bibr" target="#b26">[27]</ref>). Because image colorization is typically semanticaware, we propose a new semantic feature descriptor to incorporate the semantic-awareness into our colorization model.</p><p>To demonstrate the effectiveness of the presented approach, we train our deep neural network using a large set of reference images from different categories as can be seen in <ref type="figure" target="#fig_0">Figure 1</ref> (b). The learned model is then used to colorize various grayscale images in <ref type="figure" target="#fig_0">Figure 1 (a)</ref>. The colorization results shown in <ref type="figure" target="#fig_0">Figure 1</ref> (c) demonstrate the robustness and effectiveness of the proposed method.</p><p>The major contributions of this paper are as follows:</p><p>1. it proposes the first deep learning based image colorization method and demonstrates its effectiveness on various scenes.</p><p>2. it carefully analyzes informative yet discriminative image feature descriptors from low to high level, which is key to the success of the proposed colorization method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section gives a brief overview of the previous colorization methods.</p><p>Scribble-based colorization Levin et al. <ref type="bibr" target="#b12">[13]</ref> propose an effective approach that requires the user to provide colorful scribbles on the grayscale target image. The color information on the scribbles are then propagated to the rest of the target image using least-square optimization. Huang et al. <ref type="bibr" target="#b9">[10]</ref> develop an adaptive edge detection algorithm to reduce the color bleeding artifact around the region boundaries. Yatziv et al. <ref type="bibr" target="#b24">[25]</ref> colorize the pixels using a weighted combination of user scribbles. Qu et al. <ref type="bibr" target="#b19">[20]</ref> and Luan et al. <ref type="bibr" target="#b15">[16]</ref> utilize the texture feature to reduce the amount of required scribbles.</p><p>Example-based colorization Unlike scribble-based colorization methods, the example-based methods transfer the color information from a reference image to the target grayscale image. The example-based colorization methods can be further separated into two categories according to the source of reference images:</p><p>(1) Colorization using user-supplied example(s). This type of methods requires the user to provide a suitable reference image. Inspired by image analogies <ref type="bibr" target="#b7">[8]</ref> and the color transfer technology <ref type="bibr" target="#b20">[21]</ref>, Welsh et al. <ref type="bibr" target="#b22">[23]</ref> employ the pixel intensity and neighborhood statistics to find a similar pixel in the reference image and then transfer the color of the matched pixel to the target pixel. It is later improved in <ref type="bibr" target="#b10">[11]</ref> by taking into account the texture feature. Charpiat et al. <ref type="bibr" target="#b0">[1]</ref> propose a global optimization algorithm to colorize a pixel. Gupta et al. <ref type="bibr" target="#b5">[6]</ref> develop an colorization method based on superpixel to improve the spatial coherency. These methods share the limitation that the colorization quality relies heavily on example image(s) provided by the user. However, there is not a standard criteria on the example image(s) and thus finding a suitable reference image is a difficult task.</p><p>(2) Colorization using web-supplied example(s). To release the users' burden of finding a suitable image, Liu et al. <ref type="bibr" target="#b13">[14]</ref> and Chia et al. <ref type="bibr" target="#b1">[2]</ref> utilize the massive image data on the Internet. Liu et al. <ref type="bibr" target="#b13">[14]</ref> compute an intrinsic image using a set of similar reference images collected from the Internet. This method is robust to illumination difference between the target and reference images, but it requires the images to contain identical object(s)/scene(s) for precise per-pixel registration between the reference images and the target grayscale image. It is unable to colorize the dynamic factors (e.g. person, car) among the reference and target images, since these factors are excluded during the computation of the intrinsic image. As a result, it is limited to static scenes and the objects/scenes with a rigid shape (e.g. famous landmarks). Chia et al. <ref type="bibr" target="#b1">[2]</ref> propose an image fil- <ref type="figure">Figure 2</ref>. Overview of the proposed colorization method and the architecture of the adopted deep neural network. The feature descriptors will be extracted at each pixel and serve as the input of the neural network. Each connection between pairs of neurons is associated with a weight to be learned from a large reference image database. The output is the chrominance of the corresponding pixel which can be directly combined with the luminance (grayscale pixel value) to obtain the corresponding color value. The chrominance computed from the trained model is likely to be a bit noisy around low-texture regions. The noise can be significantly reduced with a joint bilateral filter (with the input grayscale image as the guidance).</p><p>ter framework to distill suitable reference images from the collected Internet images. It requires the user to provide semantic text label to search for suitable reference image on the Internet and human-segmentation cues for the foreground objects.</p><p>In contrast to the previous colorization methods, the proposed method is fully automatic by utilizing a large set of reference images from different categories (e.g., animal, outdoor, indoor) with various objects (e.g., tree, person, panda, car etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Metric</head><p>An overview of the proposed colorization method is presented in <ref type="figure">Figure 2</ref>. Similar to the other learning based approaches, the proposed method has two major steps: (1) training a neural network using a large set of example reference images; (2) using the learned neural network to colorize a target grayscale image. These two steps are summarized in Algorithm 1 and 2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Image Colorization âˆ’ Training Step</head><p>Input: Pairs of reference images:</p><formula xml:id="formula_0">Î› = { G, C}. Output: A trained neural network. ----------------------- 1.</formula><p>Compute feature descriptors x at sampled pixels in G and the corresponding chrominance values y in C; 2. Construct a deep neural network; 3. Train the deep neural network using the training set Î¨ = { x, y}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Image Colorization âˆ’ Testing Step</head><p>Input: A target grayscale image I and the trained neural network.</p><p>Output: A corresponding color image:ÃŽ.</p><formula xml:id="formula_1">----------------------- 1.</formula><p>Extract a feature descriptor at each pixel location in I; 2. Send feature descriptors extracted from I to the trained neural network to obtain the corresponding chrominance values; 3. Refine the chrominance values to remove potential artifacts; 4. Combine the refined chrominance values and I to obtain the color imageÃŽ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">A deep learning model for image colorization</head><p>This section formulates image colorization as a regression problem and solves it using a regular deep neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Formulation</head><p>A deep neural network is a universal approximator that can represent arbitrarily complex continuous functions <ref type="bibr" target="#b8">[9]</ref>. Given a set of exemplars Î› = { G, C}, where G are grayscale images and C are corresponding color images respectively, our method is based on a premise: there exists a complex gray-to-color mapping function F that can map the features extracted at each pixel in G to the corresponding chrominance values in C. We aim at learning such a mapping function from Î› so that we can use F to convert a new gray image to color image. In our model, we employ the YUV color space, since this color space minimizes the correlation between the three coordinate axes of the color space. For a pixel p in G , the output of F is simply the U and V channels of the corresponding pixel in C and the input of F is the feature descriptors we compute at pixel p. The feature descriptors are introduced in detail in Sec. 3.2. We reformulate the gray-to-color mapping function as c p = F(Î˜, x p ), where x p is the feature descriptor extracted at pixel p and c p are the corresponding chrominance values. Î˜ are the parameters of the mapping function F to be learned from Î›.</p><p>We solve the following least squares minimization problem to learn the parameters Î˜:</p><formula xml:id="formula_2">argmin Î˜âŠ†Î¥ n p=1 F(Î˜, x p ) âˆ’ c p 2 (1)</formula><p>where n is the total number of training pixels sampled from Î› and Î¥ is the function space of F(Î˜, x p ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Architecture</head><p>Deep neural networks (DNNs) typically consist of one input layer, multiple hidden layers and one output layer. Each layer can comprise various number of neurons. In our model, the number of neurons in the input layer is equal to the dimension of the feature descriptor extracted from each pixel location in a grayscale image and the output layer has two neurons which output the U and V channels of the corresponding color value, respectively. We perceptually set the number of neurons in the hidden layer to half of that in the input layer. Each neuron in the hidden or output layer is connected to all the neurons in the proceeding layer and each connection is associated with a weight. Let o l j denote the output of the j-th neuron in the l-th layer. o l j can be expressed as follows:</p><formula xml:id="formula_3">o l j = f (w l j0 b + i&gt;0 w l ji o lâˆ’1 i )<label>(2)</label></formula><p>where w l ji is the weight of the connection between the j th neuron in the l th layer and the i th neuron in the (l âˆ’1) th layer, the b is the bias neuron which outputs value one constantly and f (z) is an activation function which is typically nonlinear (e.g., tanh, sigmoid, ReLU <ref type="bibr" target="#b11">[12]</ref>). The output of the neurons in the output layer is just the weighted combination of the outputs of the neurons in the proceeding layer.</p><p>In our method, we utilize ReLU <ref type="bibr" target="#b11">[12]</ref> as the activation function as it can speed up the convergence of the training process. The architecture of our neural network is presented in <ref type="figure">Figure 2</ref>. We apply the classical error back-propagation algorithm to train the connected power of the neural network, and the weights of the connections between pairs of neurons in the trained neural network are the parameters Î˜ to be learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature descriptor</head><p>Feature design is key to the success of the proposed colorization method. There are massive candidate image features that may affect the effectiveness of the trained model (e.g. SIFT, SURF, Gabor, Location, Intensity histogram etc.). We conducted numerous experiments to test various features and kept only the features that have practical impacts on the colorization results. We separate the adopted features into low-, mid-and high-level features. Let</p><note type="other">x L p , x M p , x H p denote different-level feature descriptors extracted from a pixel location p, we concatenate these features to construct our feature descriptor x p = x L p ; x M p ; x H p . The adopted image features are discussed in detail in the following sections. 3.2.1 Low-level patch feature Intuitively, there exists too many pixels with same luminance but fairly different chrominance in a color image, thus it's far from being enough to use only the luminance value to represent a pixel. In practice, different pixels typically have different neighbors, using a patch centered at a pixel p tends to be more robust to distinguish pixel p from other pixels in a grayscale image. Let x p denote the array containing the sequential grayscale values in a 7 Ã— 7 patch center at p, x p</note><p>is used as the low-level feature descriptor in our framework. This feature performs better than traditional features like SIFT and DAISY at low-texture regions when used for image colorization. <ref type="figure" target="#fig_1">Figure 3</ref> shows the impact of patch feature on our model. Note that our model will be insensitive to the intensity variation within a semantic region when the patch feature is missing (e.g., the entire sea region is assigned with one color in <ref type="figure" target="#fig_1">Figure 3(b)</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Mid-level DAISY feature</head><p>DAISY is a fast local descriptor for dense matching <ref type="bibr" target="#b21">[22]</ref>. Unlike the low-level patch feature, DAISY can achieve a more accurate discriminative description of a local patch and thus can improve the colorization quality on complex scenarios. A DAISY descriptor is computed at a pixel location p in a grayscale image and is denote as x M p . <ref type="figure" target="#fig_3">Figure  4</ref> demonstrates the performance with and without DAISY feature on a fine-structure object and presents the comparison with the state-of-the-art colorization methods. As can be seen, the adoption of DAISY feature in our model leads to a more detailed and accurate colorization result on complex regions. However, DAISY feature is not suitable for matching low-texture regions/objects and thus will reduce the performance around these regions as can be seen in <ref type="figure" target="#fig_3">Figure 4(c)</ref>. A post-processing step will be introduced in Section 3.2.5 to reduce the artifacts and result is presented in <ref type="figure" target="#fig_3">Figure 4(d)</ref>. Furthermore, we can see that our result is comparable to Liu et al. <ref type="bibr" target="#b13">[14]</ref> (which requires a rigid-shape target object and identical reference objects) and Chia et al. <ref type="bibr" target="#b1">[2]</ref> (which requires manual segmentation and identification of the foreground objects), although our method is fullyautomatic.  Although the proposed method is fully-automatic, its performance is comparable to the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">High-level Semantic feature</head><p>Patch and DAISY feature are low-level and mid-level features indicating the geometric structure of the neighbors of a pixel. The existing state-of-art methods typically employ such features to match pixels between the reference and target images. Recently, high-level properties of a image have demonstrated its importance and virtues in some fields (e.g. image enhancement <ref type="bibr" target="#b23">[24]</ref>, edge detection <ref type="bibr" target="#b26">[27]</ref>). Consider that the image colorization is typically a semantic-aware process, we extract a semantic feature at each pixel to express its category (e.g. sky, sea, animal) in our model.</p><p>We utilize the state-of-art scene parsing algorithm <ref type="bibr" target="#b14">[15]</ref> to annotate each pixel with its category label, and obtain a semantic map for the input image. The semantic map is not accurate around region boundaries. As a result, it is smoothed using an efficient edge-preserving filter <ref type="bibr" target="#b4">[5]</ref> with the guidance of the original gray scale image. An Ndimension probability vector will be computed at each pixel location, where N is the total number of object categories and each element is the probability that the current pixel belongs to the corresponding category. This probability vector is used as the high-level descriptor denoted as x H .</p><p>(a)Input (b)Patch+DAISY (c)+Sementic  <ref type="figure" target="#fig_4">Figure 5</ref> shows that the colorization result may change significantly with and without the semantic feature. The adoption of semantic feature can significantly reduce matching/training ambiguities. For instance, if a pixel is detected to be inside a sky region, only sky color values resideing in the reference image database will be used. The colorization problem is thus simplified after integrating the semantic information and colorization result is visually much better as can be seen in <ref type="figure" target="#fig_4">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Global features</head><p>Unlike the previous example-based methods, we don't incorporate the global image features (e.g., gist, histogram etc.) into our model. State-of-the-art methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b13">[14]</ref> typically utilize the global features to construct an image filter, and then use it to select similar reference images from a large image set automatically. However, it is very likely that the image filter selects a reference image that is globally similar but semantically different from the target image. When we incorporate the global features into our model, it would produce an unnatural colorization result as shown in <ref type="figure" target="#fig_5">Figure 6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Chrominance Refinement</head><p>The proposed method adopts the patch feature and DAISY feature, and we hope to use patch feature to describe lowtexture simple regions and DAISY to describe fine-structure regions. However, we simply concatenate the two features instead of digging out a better combination. This will result in potential artifacts especially around the low-texture objects (e.g., sky, sea). This is because DAISY is vulnerable to these objects and presents a negative contribution. The artifacts around low-texture regions can be significantly reduced using joint bilateral filtering technique <ref type="bibr" target="#b18">[19]</ref>. It was first introduced to remove image noise of a no-flash image with the help of a noise-free flash image. Our problem is similar, the chrominance values obtained from the trained neural network is noisy (and thus results in visible artifacts) while the target grayscale image is noise-free. As a result, to ensure artifact-free quality, we apply joint bilateral filtering to smooth/refine the chrominance values (computed by the trained neural network) with the target grayscale image as the guidance. <ref type="figure">Figure 7</ref> presents the result before and after chrominance refinement. Note that most of the visible artifacts can be successfully removed.</p><p>(a)Input (b)Before (c)After <ref type="figure">Figure 7</ref>. Chrominance refinement using joint bilateral filtering <ref type="bibr" target="#b18">[19]</ref>. From (a) to (c): target grayscale image, colorization results before and after chrominance refinement, respectively. Note that the artifacts in (b) are successfully removed from (c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Difference from the state-of-the-art colorization methods</head><p>The previous algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23]</ref> typically use one similar reference image or a set of similar reference images from which transfer color values to the target gray image. <ref type="bibr" target="#b5">[6]</ref> is the state-of-art example-based method as it outperforms others in performance and application scope. However, its performance highly depends on given reference image as demonstrated in <ref type="figure" target="#fig_0">Figure 10</ref>. <ref type="bibr" target="#b5">[6]</ref> can obtain a very good colorization result using a reference image containing identical object(s) as the target grayscale image.</p><p>However, when the reference image is different from the target, its performance is quite low as shown in <ref type="figure" target="#fig_0">Figure 10</ref> (h)-(i). To minimize the high dependence on a suitable reference image, our method utilizes a large reference image database. It "finds" the most similar pixel from the database and "transfers" its color to the target pixel. This is why our approach is robust to different grayscale target images. Intuitively, one reference image cannot comprise all suitable correspondences for pixels in the target grayscale image. This is why the performance of <ref type="bibr" target="#b5">[6]</ref> highly depends on a suitable reference image. As shown in <ref type="figure" target="#fig_0">Figure 11</ref>, using a couple of similar reference images could improve their colorization result. However, when the reference images contain multiple objects (e.g. door, window, building etc.), their colorization result becomes unnatural, although some of the reference images is similar to the target. This is due to the significant amount of noise residing in feature matching (between the reference images and the target image). For instance, we noticed that the grassland in <ref type="figure" target="#fig_0">Figure 10</ref>(a) was matched to the wall in <ref type="figure" target="#fig_0">Figure 11</ref>(e)), and the sky was matched to the building in <ref type="figure" target="#fig_0">Figure 11</ref>  <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23]</ref>. (c)-(f) use (g) as the reference image, while the proposed method adopts a large reference image dataset. The reference image contains similar objects as the target grayscale image (e.g., road, trees, building, cars). It is seen that the performance of the state-of-art colorization methods is lower than the proposed method when the reference image is not "optimal". The segmentation masks used by <ref type="bibr" target="#b10">[11]</ref> are computed by mean shift algorithm <ref type="bibr" target="#b2">[3]</ref>. The PSNR values computed from the colorization results and ground truth are presented under the colorized images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Proposed 30dB 31dB 23dB 23dB 29dB 27dB 25dB</p><p>Ground truth is well-suited for a large reference image database. The deep neural network helps to combine the various features of a pixel and computes the corresponding chrominance values. Additionally, the state-of-the-art methods are very slow because they have to find the most similar pixels (or super-pixels) from massive candidates. In comparison, the deep neural network is tailored to massive data. Although the training of neural network is slow especially when the database is large, colorizing a 256Ã—256 grayscale image using the trained neural network takes only 4.9 seconds in Matlab.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>The proposed colorization neural network is trained on 2688 images from the Sun database <ref type="bibr" target="#b17">[18]</ref>. Each image is segmented into a number of object regions and a total of 47 object categories are used (e.g. building, car, sea etc.). The neural network has an input layer, three hidden layers and one output layer. According to our experiments, using more hidden layers cannot further improve the colorization results. A 49-dimension (7 Ã— 7) patch feature, a 32-dimension DAISY feature <ref type="bibr" target="#b21">[22]</ref> (4 locations and 8 orientations) and a 47-dimension semantic feature are extracted at each pixel location. Thus, there are a total of 128 neurons in the input layer. This paper perceptually set the number of neurons in the hidden layer to half of that in the input layer and 2 neurons in the output layer (which correspond to the chrominance values). <ref type="figure" target="#fig_7">Figure 8</ref> compares our colorization results with the stateof-the-art colorization methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23]</ref>. The performance of these colorization methods is very high when an "optimal" reference image is used (e.g., containing the same objects as the target grayscale image), as shown in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23]</ref>. However, the performance may drop significantly when the reference image is only similar to the target grayscale image. The proposed method does not have this limitation due to the use of a large reference image database as shown in <ref type="figure" target="#fig_0">Figure 1(b)</ref>.</p><p>To test the influence of sample size on our model, we utilized different sizes of samples to train our DNN, and then colorized 1344 grayscale images. Let Î¨ denote our training set. The sample size is denoted as Î´ = size(Î¨). As shown in <ref type="figure" target="#fig_0">Figure 12</ref>, we can see that more samples will result in more accurate colorization results. Figure 9 presents more colorization results obtained from the proposed method with respect to the ground-truth color images. <ref type="figure" target="#fig_8">Figure 9</ref> demonstrates that there is almost not visible artifact in the color images generated using the proposed method, and these images are visually very similar to the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations</head><p>The proposed colorization is fully-automatic and thus is normally more robust than the traditional methods. However, it relies on machine learning techniques and has its own limitations. For instance, it is supposed to be trained on a huge reference image database which contains all possible objects. This is impossible in practice. For instance, the current model was trained on real images and thus is invalid for the synthetic image. It is also impossible to recover the color information lost due to color to grayscale transformation. Nevertheless, this is a limitation to all state-of-theart colorization method. Two failure cases are presented in <ref type="figure" target="#fig_0">Figure 13</ref>.</p><p>(a) Input (b) Our method (c) Groundtruth <ref type="figure" target="#fig_0">Figure 13</ref>. Limitations of our method. Our method is not suitable for synthetic images and cannot recover the information lost during color to grayscale conversion. Note that the green number in the last row of (c) disappears in the corresponding grayscale image in (a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Concluding Remarks</head><p>This paper presents a novel, full-automatic colorization method using deep neural networks to minimize user effort and the dependence on the example color images. Informative yet discriminative features including patch feature, DAISY feature and a new semantic feature are extracted and serve as the input to the neural network. The output chrominance values are further refined using joint bilateral filtering to ensure artifact-free colorization quality. Numerous experiments demonstrate that our method outperforms the state-of-art algorithms both in terms of quality and speed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The colorization results of our full-automatic method. Our system utilizes a large database of colorful reference images as shown in (b). After the training of neural networks, the learned model is used directly to colorize the target gray scale images in (a). The colorization results are presented in (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Evaluation of patch feature. (a) is the target grayscale image. (b) removes the low-level patch feature and (c) includes all the proposed features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Evaluation of DAISY feature. (a) is the target gray scale image. (b) is our result without DAISY feature. (c) is our result after incorporating DAISY feature into our model. (d) is the final result after artifact removal (see Sec. 3.2.5 for details). (e)-(h) presents results obtained with the state-of-the-art colorizations. Although the proposed method is fully-automatic, its performance is comparable to the state-of-the-art.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Importance of semantic feature. (a) is the target grayscale image. (b) is the colorization result using patch and DAISY features only. (c) is the result using patch, DAISY and semantic features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Evaluation of global features. (a) is the target gray scale image. (b) is our final result using patch, DAISY and semantic features. (c) is the colorization result when we incorporate the global features into our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. The high dependence on a suitable reference image of Gupta et al. [6]. (a) is the input grayscale image. (b) is the color image obtained from the proposed method which is visually more accurate. (c) is the ground truth of (a). (d) is the first reference image for [6]. It has a similar scene as the (a). (e) is the second reference image. It is semantically different from (a). (f) is the last reference image that is complete different from (a). The color images obtained from [6] w.r.t. the reference images in (d)-(f) are presented in (g)-(i), respectively. The PSNR values computed from the colorization results and the ground truth are presented under the colorized images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Comparison with the state-of-art colorization methods [1, 6, 11, 23]. (c)-(f) use (g) as the reference image, while the proposed method adopts a large reference image dataset. The reference image contains similar objects as the target grayscale image (e.g., road, trees, building, cars). It is seen that the performance of the state-of-art colorization methods is lower than the proposed method when the reference image is not "optimal". The segmentation masks used by [11] are computed by mean shift algorithm [3]. The PSNR values computed from the colorization results and ground truth are presented under the colorized images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Comparison with the ground truth. The first row presents the input grayscale images from different categories. Colorization results obtained from the proposed method are presented in the second row. The last row presents the corresponding ground-truth color images, and the PSNR values computed from the colorization results and the ground truth are presented under the colorized images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Gupta et al.[6] with multiple reference images. The target grayscale image is the same as Figure 10(a). (a)-(c) are different reference images and (d)-(f) are the corresponding colorization results. Note that the best performance can be achieved when sufficient similar reference images are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Influence of sample size on the colorization results. (a) and (b) presents the average and the distribution of PSNR using different sample size. The yellow, blue and green curves in (b) represent the PSNR distribution when log(Î´) are assigned with 7.6, 8.6 and 11.6 respectively.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic image colorization via multimodal predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Charpiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="126" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic colorization with internet images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y S</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TOG</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="603" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain transform for edge-aware image and video processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TOG</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">69</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image colorization using similar images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y S</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhiyong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM international conference on Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="369" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;01</title>
		<meeting>the 28th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;01</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="327" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An adaptive edge detection based colorization algorithm and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual ACM International Conference on Multimedia, MULTIMEDIA &apos;05</title>
		<meeting>the 13th Annual ACM International Conference on Multimedia, MULTIMEDIA &apos;05</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="351" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Colorization by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Irony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Symp. on Rendering</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Colorization using optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2004 Papers</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="689" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Intrinsic colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TOG</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">152</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4038</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Natural image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Eurographics Conference on Rendering Techniques, EGSR&apos;07</title>
		<meeting>the 18th Eurographics Conference on Rendering Techniques, EGSR&apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="309" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint deep learning for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2056" to="2063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sun attribute database: Discovering, annotating, and recognizing scene attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2751" to="2758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Petschnigg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
		<title level="m">Digital photography with flash and no-flash image pairs. ToG</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Manga colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2006 Papers, SIGGRAPH &apos;06</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1214" to="1220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Color transfer between images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ashikhmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A fast local descriptor for dense matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transferring color to greyscale images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Welsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ashikhmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="277" to="280" />
			<date type="published" when="2002-07-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Automatic photo adjustment using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7725</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast image and video colorization using chrominance blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yatziv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Img. Proc</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1120" to="1129" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-stage contextual deep learning for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Detecting object boundaries using low-, mid-, and high-level information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
