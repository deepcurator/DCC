<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Spatially-aware Fashion Concept Discovery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
							<email>xintong@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
							<email>zxwu@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phoenix</forename><forename type="middle">X</forename><surname>Huang</surname></persName>
							<email>phoenix@snap.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Snap Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
							<email>menglong@google.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
							<email>yzhao@google.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Spatially-aware Fashion Concept Discovery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The exponential growth of online fashion shopping websites has encouraged techniques that can effectively search for a desired product from a massive collection of clothing items. However, this remains a particularly challenging problem since, unlike generic objects, clothes are usually subject to severe deformations and demonstrate significant variations in style and texture, and, most importantly, the long-standing semantic gap between low-level visual features and high-level intents of customers is very large. To overcome the difficulty, researchers have proposed interactive search to refine retrieved results with humans in the loop. Given candidate results, customers can provide We propose a concept discovery approach to automatically cluster spatially-aware attributes into meaningful concepts. The discovered spatially-aware concepts are further utilized for (b) structured product browsing (visualizing images according to selected concepts) and (c) attribute-feedback product retrieval (refining search results by providing a desired attribute).</p><p>various feedback, including the relevance of displayed images <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b3">4]</ref>, or tuning parameters like color and texture, and then results are updated correspondingly. However, relevance feedback is limited due to its slow convergence to meet the customer requirements. In addition to color and texture, customers often wish to exploit higher-level features, such as neckline, sleeve length, dress length, etc. Semantic attributes <ref type="bibr" target="#b12">[13]</ref>, which have been applied effectively to object categorization <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27]</ref> and fine-grained recognition <ref type="bibr" target="#b11">[12]</ref> could potentially address such challenges. They are mid-level representations that describe semantic properties. Recently, researchers have annotated clothes with semantic attributes <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b10">11]</ref> (e.g., material, pattern) as intermediate representations or supervisory signals to bridge the semantic gap. However, annotating semantic attributes is costly. Further, attributes conditioned on object parts have achieved good performance in fine-grained recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33]</ref>, confirming that spatial information is critical for attributes. This also holds for clothing images. For example, the neckline attribute usually corresponds to the top part in images while the sleeve attribute ordinarily relates to the left and right side of images. To address the above limitations, we jointly model clothing images and their product descriptions with a visualsemantic embedding, and propose a novel approach that automatically discovers spatially-aware concepts, each of which is a collection of attributes describing the same characteristic (e.g., if the concept is color then the attributes could contain yellow and blue, as shown in <ref type="figure" target="#fig_0">Figure 1(a)</ref>). In addition, we learn a subspace embedding for each discovered concept to facilitate a structured exploration of the dataset based on the concept of interest <ref type="figure" target="#fig_0">(Figure 1(b)</ref>). More importantly, inspired by <ref type="bibr" target="#b9">[10]</ref>, we leverage the learned visual-semantic space to exploit multimodal linguistic regularities for attribute-feedback product retrieval. For example, an image of a "white sleeveless dress" − "sleeveless" + "long-sleeve" would be near images of "white long-sleeve dress". In contrast to <ref type="bibr" target="#b9">[10]</ref> which requires explicitly specifying the attribute to remove, we implicitly remove corresponding attributes based on the discovered concepts <ref type="figure" target="#fig_0">(Figure 1(c))</ref>. <ref type="figure">Figure 2</ref> provides an overview of the framework. Specifically, our framework contains the following three steps <ref type="bibr" target="#b0">(1)</ref> we first train a joint visual-semantic embedding space using clothing images and their product descriptions. Given an image, we compute its features with GoogleNet, which are further projected into the embedding space to minimize the distance to its product description encoded by bag-ofwords of attributes. By fine-tuning GoogleNet in an endto-end fashion, we train a discriminative model that contains localization information of attributes; (2) we then obtain the spatial representation for each attribute, indicating where in images the attribute mostly corresponds to, from the attribute activation maps. These spatial representations are further utilized to augment their corresponding semantic word representations (word vectors) produced from a skipgram model. Further, clustering is performed to discover concepts, each of which contains semantically related attributes (e.g., maxi, midi, mini are all different dress length); (3) we further disentangle the trained visual-semantic embedding by training a subspace embedding for each discovered concept, in which the similarities among items can be measured based on the corresponding concept only. The transformation of images into a subspace embedding facilitates attribute-feedback clothing search and structured browsing of fashion images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAP</head><p>Given the fact that existing datasets only contain images and annotated attributes (which are often very sparse) rather than image and product description pairs, we con-structed the Fashion200K dataset, which contains more than 200,000 clothing images of five categories (dress, top, pants, skirt and jacket) and their associated product descriptions from online shopping websites. These five classes are the most important verticals in fashion due to their various styles and occasions. Thus, we focus on these categories in our dataset, but our method is applicable to any fashion categories. We conduct extensive experiments on this dataset to validate the efficacy of the automatically discovered concepts in attribute-feedback product retrieval as well as structured fashion image browsing.</p><p>Our main contributions are two-fold. First, we demonstrate that the augmentation of semantic word vectors for attributes with their spatial representations can be used to effectively cluster attributes into semantically meaningful and spatially-aware concepts. Second, we leverage semantic regularities in the visual-semantic space for attributefeedback clothing retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Interactive image search. Extensive studies have been conducted on interactive image search, aiming to improve retrieved results from search engines with user feedback <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b3">4]</ref> (See <ref type="bibr" target="#b34">[35]</ref> for a comprehensive review). The basic idea is to refine the results by incorporating feedback from users, including the relevance of the candidates, and tuning low-level parameters like color and texture. In practice, relevance feedback requires a large number of iterations to converge to user intent. Also, it requires manual annotations to define the relative attributes, which limits its scalability. In addition, when searching clothing images, customers generally focus on certain higher-level characteristics, such as neckline, sleeve length, etc., thus rendering relevance feedback less useful.</p><p>Attributes for clothing modeling. There have been numerous works focusing on utilizing semantic attributes as mid-level representations for clothing modeling. For instance, Chen et al. <ref type="bibr" target="#b1">[2]</ref> learned semantic attributes for clothing on the human upper body. Huang et al. <ref type="bibr" target="#b7">[8]</ref> built treestructured layers for all attribute categories to form a semantic representation for clothing images. Veit et al. <ref type="bibr" target="#b28">[29]</ref> learned visually relevant semantic subspaces using a multiquery triplet network. Kovashka et al. <ref type="bibr" target="#b10">[11]</ref> utilized relative attributes with ranking functions instead of using binary feedback for retrieval tasks. In contrast, we propose a novel concept discovery framework, in which a concept is a collection of automatically identified attributes derived by jointly modeling image and text.</p><p>Visual concept discovery. To exploit the substantial amounts of weakly labeled data, researchers have proposed various approaches to discover concepts. Sun et al. <ref type="bibr" target="#b23">[24]</ref> combined visual and semantic similarities of concepts to cluster concepts while ensuring their discrimination and compactness. Vittayakorn et al. <ref type="bibr" target="#b29">[30]</ref> and Berg et al. <ref type="bibr" target="#b0">[1]</ref> verified the visualness of attributes, and <ref type="bibr" target="#b29">[30]</ref> also uses neural activations to learn the characteristics of each attribute. Vaccaro et al. <ref type="bibr" target="#b27">[28]</ref> utilized a topic model to learn latent concepts and retrieve fashion items based on textual specifications. Singh et al. <ref type="bibr" target="#b21">[22]</ref> discovered pair-concepts for event detection and discard irrelevant concepts by the co-occurrences of concepts. Recently, some works discovered the spatial extents of concepts. Xiao and Lee <ref type="bibr" target="#b31">[32]</ref> discovered visual chains for locating the image regions that are relevant to one attribute. Singh and Lee <ref type="bibr" target="#b22">[23]</ref> introduced a deep network to jointly localize and rank relative attributes. However, these approaches involve training a single model for each individual attribute, which is not scalable.</p><p>Visual-semantic joint embedding. Our work is also related to visual-semantic embedding models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b6">7]</ref>. Frome et al. <ref type="bibr" target="#b4">[5]</ref> recognize objects with a deep visualsemantic embedding model. Kiros et al. <ref type="bibr" target="#b9">[10]</ref> adopted an encoder-decoder framework coupled with a contrastive loss to train a joint visual-semantic embedding. Wang et al. <ref type="bibr" target="#b30">[31]</ref> combined cross-view ranking loss and within-view structure preservation loss to map images and their descriptions. Beyond training a joint visual-semantic embedding with image and text pairs as in these works, we further decompose the trained embedding space into multiple conceptspecific subspaces, which facilitates structured browsing and attribute-feedback product retrieval by exploiting multimodal linguistic regularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Fashion200K Dataset</head><p>There have been several clothing datasets collected recently <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. However, none of these datasets are suitable for our task because they do not contain descriptions of images. This prevents us from learning semantic representations for attributes using word2vec <ref type="bibr" target="#b17">[18]</ref>. Thus, we collected the Fashion200K dataset and automatically discover concepts from it.</p><p>We first crawled more than 300,000 product images and their product descriptions from online shopping websites and removed the ones whose product descriptions contain fewer than four words, resulting in over 200,000 images. We then split them into 172,049 images for training, 12,164 for validation, and 25,331 for testing. For cleaning product descriptions, we deleted stop words, symbols, as well as words that occur fewer than 5 times. Each remaining word is regarded as an attribute. Finally, there are 4,404 attributes for training the joint embedding.</p><p>Example clothing image and description pairs are shown in <ref type="figure" target="#fig_1">Figure 3</ref>. Since we wish to automatically discover concepts from this noisy dataset and learn concept-level subspace features, we do not conduct any manual annotations for this dataset. Note that as a preprocessing step, we trained a detector using the MultiBox model <ref type="bibr" target="#b24">[25]</ref> for all five categories and run them on all images. Only the detected foregrounds are cropped and used as input to our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our Approach</head><p>In this section, we present the key components of the proposed concept discovery approach shown in <ref type="figure">Fig. 2</ref>, including visual-semantic embedding learning, spatiallyaware concept discovery and concept subspace learning. Since our method leverages spatial information of an attribute, and the same attribute in different types of clothing (e.g., "short" in "short dress" and "short pants") will have different spatial characteristics, we train an individual model for each category in our dataset. For simplicity in notation and illustration, we only show the concept discovery approach for dresses, while the same pipeline is applied to other categories in the same fashion. Results of all categories are shown and evaluated in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Visual-semantic Embedding</head><p>To fully explore the substantial weakly labeled web data for mining concepts, we first train a joint visual-semantic embedding model with image-text pairs by projecting a product image and its associated text into a joint embedding space. Following <ref type="bibr" target="#b9">[10]</ref>, we also utilize a stochastic bidirectional contrastive loss to achieve good convergence.</p><p>More formally, let I denote an image and S = {w 1 , w 2 , ..., w N } its corresponding text, where w i is the i-th attribute (word) in the product description. Let W I denote the image embedding matrix, and W T denote the attribute embedding matrix. We first represent the i-th word w i with one-hot vector e i , which is further encoded into the embedding space by v i = W T · e i . We then represent the product description with bag-of-words v = 1 N i v i . Similarly, for the image I, we first compute its feature vector f ∈ R 2048 with a GoogleNet model <ref type="bibr" target="#b25">[26]</ref> parameterized by weights V after the global average pooling (GAP) layer as shown in <ref type="figure">Figure 2</ref>. Then we project the feature vector into the embedding space, in which the original image is represented as x = W I · f .</p><p>The similarity between an image and its description is computed with cosine similarity, i.e., d(x, v) = x·v, where x and v are normalized to unit norm. Finally, the joint embedding space is trained by minimizing the following contrastive loss:</p><formula xml:id="formula_0">min Θ x,k max(0, m − d(x, v) + d(x, v k ))+ v,k max(0, m − d(v, x) + d(v, x k )),<label>(1)</label></formula><p>where Θ = {W I , W T , V} contains the parameters to be optimized, and v k denotes non-matching descriptions for image x while x k are non-matching images for description v. By minimizing this loss function, the distance between x and its corresponding text v is forced to be smaller than the distance from unmatched descriptions v k by some margin m. Vice versa for description v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Spatially-aware Concept Discovery</head><p>The training process of a joint visual-semantic embedding will lead to a discriminative CNN model, which contains not only the semantic information (i.e., the last embedding layer) but also important spatial information that is hidden in the network. We now discuss how to obtain spatially-aware concepts from the network.</p><p>Attribute spatial representation. Spatial information of an attribute is crucial for understanding what part of a clothing item the attribute refers to. Motivated by <ref type="bibr" target="#b33">[34]</ref>, we generate embedded attribute activation maps (EAAM), which can localize the salient regions of attributes for an image by a single forward pass with the trained network.</p><p>Given an image I, let q k (i, j) be the activation of unit k in the last convolutional layer at location (i, j). After the global average pooling (GAP) operation, f k = i,j q k (i, j) is the k-th dimension feature of the image representation f . For a given attribute a, the cosine distance d(x, W a ) between image embedding x and attribute embedding W a indicates the probability that attribute a is present in this image. If we plug f k into the cosine distance we obtain:</p><formula xml:id="formula_1">d(x, W a ) = m W a m x m = m W a m k W I m,k f k = m W a m k W I m,k i,j q k (i, j) = i,j m W a m k W I m,k q k (i, j)<label>(2)</label></formula><p>where W a m and W I m,k are entries of the attribute embedding W a and image embedding matrix W I , respectively. Thus, the embedded attribute activation map (EAAM) for attribute a of image I can be defined as:</p><formula xml:id="formula_2">M I a (i, j) = m W a m k W I m,k q k (i, j)<label>(3)</label></formula><p>one-shoulder$ knot$ sleeveless$ v-neck$ embroidered$ belted$ <ref type="figure">Figure 4</ref>. Embedding attribute activation map for a given attribute. The generated activation maps successfully highlight the discriminative regions for the given attribute.</p><formula xml:id="formula_3">Since d(x, W a ) = i,j M I a (i, j), M I a (i, j)</formula><p>indicates how likely the attribute appears at spatial location (i, j). <ref type="figure">Figure 4</ref> shows sample EAAMs of images. We can see the activation maps indicate where the joint embedding model looks to identify an attribute. Product images on shopping websites usually have clean backgrounds and are displayed in an aligned frontal view. Thus, for a particular attribute a and its positive training set (i.e., images whose product descriptions contain a) P a , we average EAAMs for all images in P a to generate an activation map A a . We refer to it as the attribute activation map (AAM) of a: <ref type="figure">Figure 5</ref> shows AAMs of some attributes for the dress category. From this figure, we can discover that for attributes that have clear spatial information in a dress image, their AAMs capture the spatial patterns. For example, belt is most likely to occur in the middle part of dress images, long-sleeve often occurs on two sides of dress images, and off-shoulder is around the shoulder region of a dress. However, for some attributes whose locations are not certain for different dress images, like floral, stripe, and colors, their AAMs span almost the entire image.</p><formula xml:id="formula_4">A a = 1 |P a | I∈Pa M I a (4)</formula><p>Therefore, for each attribute in a clothing category, its AAM can serve as a spatial representation. If two attributes describe the similar spatial part of a clothing category, e.g., sleeveless and long-sleeve, or v-neck and mockneck, their spatial information should also be similar.</p><p>Attribute semantic representation. Only using spatial information is not sufficient for effective concept discovery, especially for those attributes that do not have a discriminative spatial representation. Thus, we train a skip-gram model <ref type="bibr" target="#b17">[18]</ref>  attributes in our dataset. We denote the semantic representation of attribute a as E a .</p><p>Attribute clustering. Ideally, attributes belonging to the same concept describe the same characteristic of a clothing category; that means they should be both spatially consistent and semantically similar. Thus, for an attribute a, by simply flattening its spatial representation A a and concatenating it with its semantic representation E a , we can generate a feature vector:</p><formula xml:id="formula_5">F a = [vec(A a ), E a ]<label>(5)</label></formula><p>where vec(·) is vectorization operation and we normalize vec(A a ) and E a to have unit norm before concatenation. As a result, this attribute feature is aware of the spatial information of the attribute and can also capture its semantic meaning. K-means clustering algorithm is then used to cluster all the attributes into attribute groups, such that the attributes within a group form a concept. Unlike <ref type="bibr" target="#b23">[24]</ref>, we do not directly use visual similarity between attributes because attributes describing the same characteristic might be visually dissimilar. For example, blue and red are both color attributes, but they are visually very different. <ref type="table">Table 1</ref> presents some concepts discovered by our method for different categories. We find that the attributes describing the same characteristic are grouped into one cluster. For example, all attributes describing colors are in one concept because they are very close in the semantic embedding space (they are often the first word in product descriptions) and their AAMs do not provide much useful information (the right two AAMs in <ref type="figure">Figure 5)</ref>. Thus, the semantic representations of those attributes dominate in this case and place them in the same concept. Different kinds of sleeves also form a concept, since their AAMs are very similar (along with the two sides of dresses or tops) and their word vectors are also close. We also observe that our method can successfully group noisy (not visually perceptible) attributes together, because the semantic and spatial information of these attributes is not discriminative. These noisy clusters will be discovered by our method and not affect the attribute-feedback, since customers will not provide an attribute with low visualness for retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Concept Subspace Learning</head><p>The discovered concepts are further utilized to refine the learned joint visual-semantic space, so that similarities between items can be measured by each individual concept (e.g., color and neckline could result in different similarities). This is crucial for cases when customers want to modify attributes to refine the search results or hope to browse products based on a particular concept. Therefore, given the concepts discovered by the attribute clustering process, we further train a sub-network for each concept, constructing a concept-specific subspace.</p><p>For a concept C = {a 1 , a 2 , ..., a n } where a i is an attribute in this concept, we build a fully-connected layer and a softmax layer on top of the image embedding features to classify the a i . The number of neurons in the softmax layer is n + 1 (each attribute corresponds to one neuron with an additional one for none-of-above). This network is trained only on images with a i in their product descriptions plus a small number of randomly sampled negative images. We denote S C (x) to be the softmax output of the sub-network for concept C given the input image x.</p><p>After the subspace training stage, the concept subspace features (hidden layer representations) are aware of the attributes of this particular concept, and hence enable the similarity measurement among images based only on this concept. For example, a "blue maxi dress" is more similar to a "blue mini dress" than a "red maxi dress" in the color subspace. However, a "red maxi dress" is closer to "blue maxi dress" in dress length subspace. As a result, customers can choose the desired similarity measure during online shopping so they can better explore the clothing gallery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Attribute-feedback Product Retrieval</head><p>Based on the discovered concepts and learned concept subspaces, we leverage multimodal linguistic regularities to help perform attribute-feedback product retrieval task. Some example results can be found in <ref type="figure">Figure 7</ref>.</p><p>Given a retrieved image ("red sleeveless mini dress", for example), customers may want to change one attribute of the image while keeping others fixed, say "I want this dress to have long-sleeves". As we already trained a visualsemantic embedding (VSE), a baseline method would be sorting database images based on their cosine distances with the query image + query attribute (long-sleeve). In this way, the retrieved images have a high score for the query attribute and are similar to the query image at the same time. For a query image x q and a query attribute w p , the attributefeedback retrieval task to find image x o is defined as:</p><formula xml:id="formula_6">x o = arg max x (x q + w p ) · x<label>(6)</label></formula><p>However, one problem with this approach is that it retrieves images which are closest to "red sleeveless long-sleeve mini dress" instead of "red long-sleeve mini dress". To overcome this, we note that by providing a query attribute, customers implicitly intend to remove an existing attribute (sleeveless in this case) that describes the same characteristic of the product as the query attribute. Since the attributes within one discovered concept describe the same characteristic, we detect the implicit negative attribute w n and use it to search image x o :</p><formula xml:id="formula_7">w n = arg max w∈C S C (x q ) x o = arg max x (x q + w p − w n ) · x<label>(7)</label></formula><p>where C is the concept to which w p belongs and S C (x q ) is the softmax output of the sub-network for C. Thus, w n is the attribute in C that is most likely to be present in the query image x q . By subtracting the detected negative attribute w n from the query embedding, we remove the negative attribute to avoid two visually contradictory attributes (e.g., sleeveless and long-sleeve) hurting the retrieval performance. Eqn. 7 indicates that our method actually uses multimodal linguistic regularities <ref type="bibr" target="#b9">[10]</ref> with automatic negative attribute detection.</p><p>Because the subspace networks are trained with a noneof-above class, it might predict that x q does not have any attributes in concept C. In this case, our method degenerates to the baseline method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results and Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiment Setup</head><p>Clothing detection. Some works have shown that using detected clothing segmentations instead of entire images can achieve better performance in various tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>, so we also train a detector for each clothing category using MultiBox model <ref type="bibr" target="#b24">[25]</ref> to detect and crop clothing items in our dataset. Because the product images on shopping websites have clean backgrounds, the detectors work very well.</p><p>Visual-semantic embedding. We use GoogleNet InceptionV3 model <ref type="bibr" target="#b25">[26]</ref>  (e) jacket <ref type="figure">Figure 6</ref>. Top-k retrieval accuracy of different methods for attribute-feedback product retrieval for dresses, tops, pants, skirt, and jacket.</p><p>pooling (GAP) layer after the last convolutional layer enables us to directly use it without changing the structure of the network as in <ref type="bibr" target="#b33">[34]</ref>. We use the 2048D features right after GAP as the image features. The dimension of the joint embedding space is set to 512, thus W I is a 2048×512 matrix, and W T is an M ×512 matrix, where M is the number of attributes. We set the margin m = 0.2 in Eqn. 1. The initial learning rate is 0.05 and is decayed by a factor of 2 after every 8 epochs. The batch size is set to 32. Finally, we fine-tune all layers of the network pretrained on ImageNet.</p><p>Spatiallly-aware concept discovery. The feature map size of the last convolutional layer in the InceptionV3 model is 8 × 8 × 2048, hence the attribute activation map is of size 8 × 8. After vectorizing the activation map, an attribute will have a 64D feature vector as its spatial representation. We also set the dimension of word vectors to 64 to have the same dimentionality when training the Word2vec <ref type="bibr" target="#b17">[18]</ref> model. The number of clusters is fixed to 50 for clustering.</p><p>Subspace feature learning. We set the hidden layer of each concept subspace to have 128 neurons. The learning rate is fixed to be 0.1 and we stop training after 10 epochs. Note that during training subspace networks, the visualsemantic embedding weights are fixed, only the parameters after the image embedding layer are updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation of Discovered Concepts</head><p>To evaluate the quality of our discovered concepts, a fashion professional manually assigned around 300 attributes into different categories (e.g., color, pattern, neckline, sleeve, etc.). We use this information as ground truth concept assignments of the attributes and compare our approach with the following methods: Automatic Concept Discovery (ACD) <ref type="bibr" target="#b23">[24]</ref>, only using semantic representations of attributes for clustering (Word2vec <ref type="bibr" target="#b17">[18]</ref>) and only using spatial information (Our AAM). In all methods, we set the number of clusters to 50. Homogeneity, completeness and V-measure <ref type="bibr" target="#b18">[19]</ref> are used to evaluate the clustering quality.</p><p>Results are shown in <ref type="table">Table 2</ref>. Only using semantic information gives reasonable results. However, just relying on spatial information performs worst, since for many attributes, their spatial information is not discriminative and thus fails to discover informative concepts. ACD performs similarly to Word2vec because it combines semantic and visual similarities of attributes but visually dissimilar atHomogeneity Completeness V-measure ACD <ref type="bibr" target="#b23">[24]</ref> 0  <ref type="table">Table 2</ref>. Comparison among concept discovery methods. Homogeneity, completeness and V-measure <ref type="bibr" target="#b18">[19]</ref> are between 0 and 1, higher is better.</p><p>tributes may also describe the same characteristic. By jointly clustering the semantic and spatial representations of attributes, our concept discovery approach outperforms other methods by 0.03 in V-measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Attribute-feedback Product Retrieval</head><p>To evaluate how the discovered concepts help attributefeedback product retrieval, we collected 3,167 product pairs from the test set. The two products in each pair have one attribute that differs in their product descriptions, e.g., "blue geometric long-sleeve shirt" vs. "blue paisley longsleeve shirt", "blue off-shoulder floral dress" vs. "blue oneshoulder floral dress", etc. In each pair, we use the image of one product and the differing attribute in their descriptions as the query to retrieve the images of the other product. Topk retrieval accuracy is used for evaluation.</p><p>As shown in <ref type="figure">Figure 6</ref>, we compare our full method for all five categories with other methods. We also include the baseline method (VSE w/o concept discovery as in Eqn. 6), where no negative attribute is used.</p><p>We can see that using only attribute activation maps (AAM) significantly reduces performance of retrieval due to lack of semantic information. Only using semantic information (Word2vec) helps for most categories, but is worse than the baseline when retrieving tops. By adding visual information, ACD performs slightly worse than Word2vec because the visual similarity of attributes is not suitable for discovering concepts. After combining both semantic and spatial information, our concept discovery approach achieves the highest retrieval accuracy for all five categories, especially for the categories top, dress and jacket whose attributes have strong spatial information (e.g., collar shape, sleeve length, sleeve shape). However, for clothing items like pants, whose attributes do not present informative spatial cues, our method only yields a marginal improvement over Word2vec.  <ref type="figure">Figure 7</ref>. Examples of our attribute-feedback product retrieval results. Sleeve type changes from sleeveless to cap-sleeve in the first example, and shoulder changes from one-shoulder to strapless in the third example, according to customer feedback attributes. The attributes in parentheses are the negative attributes automatically detected by our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dress length decreases</head><p>Maxi dress</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mini dress</head><p>Midi dress <ref type="figure">Figure 8</ref>. Subspace embedding corresponding to concept {maxi, midi, mini} for dresses. Images are mapped to a grid for better visualization. <ref type="figure">Figure 7</ref> illustrates some examples which show that our retrieval model can accurately detect the negative attribute and give satisfying results with the desired attributes added to the original results. <ref type="figure">Figure 8</ref>,9 use t-SNE <ref type="bibr" target="#b16">[17]</ref> to visualize two subspace embeddings based on two discovered concepts. In <ref type="figure">Figure 8</ref>, the subspace network is trained to distinguish {maxi, midi, mini} for dresses, and it learns a continuous representation of the length of dresses -dress length decreases from left to right on the 2D visualization plane. <ref type="figure" target="#fig_3">Figure 9</ref> illustrates the embedding corresponding to the attributes describing colors for tops. Tops with different colors are well separated in the embedding subspace. Although Veit et al. <ref type="bibr" target="#b28">[29]</ref> also learns concept subspaces based on an attention mechanism, they heavily rely on richly annotated data, while our method is fully automatic and annotation free. By training a subspace embedding for each discovered concept, we can project images into the appropriate subspace and explore the images according to this specific concept, while a general embedding (like the visual-semantic embedding) cannot automatically adjust its representations based on user-specified characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Structured Browsing of Products</head><p>Thus, the subspace features enable structured browsing during online shopping. For example, when a customer finds a mini dress and wants to see other dresses that share similar length with this dress, she may choose the subspace of {maxi, midi, mini}, so she can find the other mini dresses near her initial choice and as she explores the left side of the subspace, she can find dresses with longer length.</p><p>We should note that it is also possible to concatenate subspace embeddings of two concepts, hence clothing items sharing the same characteristics according to two concepts will be close in the concatenated subspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We automatically discover spatially-aware concepts with clothing images and their product descriptions. By projecting images and their attributes into a joint visual-semantic embedding space, we are able to learn attribute spatial representations. We then combine spatial representations and semantic representations of attributes, and cluster attributes into spatially-aware concepts, such that the attributes in one concept describe the same characteristic. Finally, a subspace embedding is trained for each concept to capture the concept-specific information. Experiments on clustering quality evaluation and attribute-feedback product retrieval for five clothing categories show the effectiveness of the discovered concepts and the learned subspace features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (a) We propose a concept discovery approach to automatically cluster spatially-aware attributes into meaningful concepts. The discovered spatially-aware concepts are further utilized for (b) structured product browsing (visualizing images according to selected concepts) and (c) attribute-feedback product retrieval (refining search results by providing a desired attribute).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Examples of the image-text pairs in Fashion200K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Subspace embedding corresponding to concept {black, blue, white, red, gray, green, purple, beige, ...} for tops.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Figure 2. Overview of our approach. Our approach mainly contains three parts: 1. Joint embedding space training. A joint visual-semantic embedding space is trained using clothing images and their product descriptions. 2. Spatially-aware concept discovery. We use neural activations provided by global pooling (GAP) layer to generate attribute activation maps (AAMs) of attributes. The AAM captures the spatial information of attributes (i.e., what is the spatial location an attribute usually refers to). By combining attributes' spatial information and their semantic representations obtained from a word2vec model, we cluster attributes into concepts. 3. Concept subspace learning. For each discovered concept, we further train a sub-network to effectively measure the similarity of images according to this concept only.</figDesc><table>White sleeveless 
bird-print shift dress 

white 

sleeveless 

bird-print 

shift 

Bag-of-words 
Joint Embedding Space 

Attribute Activation Maps 

v-neck 
short-sleeve 
off-shoulder 

1. Visual-Semantic 
Embedding Training 

2. Spatially-aware 
Concept Discovery 

3. Concept Subspace 
Learning 

sleeveless 
short-sleeve 
long-sleeve 
… 

maxi 
midi 
mini 
… 

Discovered concept list: 
Dress length: 
{maxi, midi, mini, …} 
Sleeve type: 
{short-sleeve, sleeveless, 
cap-sleeve, …} 
Color: 
{blue, black, red, …} 
… 

black printed bell-sleeve mini dress 
multicolor embroidered maxi dress 
white striped a-line dress 
… 

Word2vec model 

Sentence embedding 

v 

f 
x 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>on the descriptions of clothing products to ob- tain the semantic representations (Word2vec vectors) for alloff-the-shoulder, one-shoulder, strapless, ...decoration: lace, embellished, embroidered, beaded, ... sleeve length: sleeveless, long-sleeve, short-sleeve, ... sleeve shape: kimono, cap, dolman, bell, flutter, ...straight-leg, slim-leg, tapered-leg, bootcut, ... pattern: check, geometric, leopard, palm, abstract, ... Table 1. Concept discovered by our method. Each row contains the attributes belong to one concept. Ellipsis is used when the attribute list is too long to show.</figDesc><table>off-shoulder 
belt 
asymmetric 
long-sleeve 
floral 
stripe 

Figure 5. Attribute activation map for a given attribute of the dress 
category. The most frequency locations an attribute corresponds 
to in an image are highlighted. 

concepts discovered by our method 

dress 

dress length: maxi, midi, mini 
neckline: v, plunge, deep, high, scoop 
shoulder: top 

pants 

color: black, blue, multicolor, gray, white, green, ... 
pant cut: </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>for the image CNN. Its global average</figDesc><table>1 

10 
20 
30 
40 
50 

Number of retrieved images 

0.05 

0.09 

0.13 

0.17 

0.21 

0.25 

0.29 

0.33 

0.37 

0.41 

Retrieval accuracy 

VSE (w/o Concept Discovery) 
ACD [22] 
Word2vec [17] (semantic only) 
AAM (spatial only) 
Ours Joint (semantic + spatial) 

(a) dress 

1 
10 
20 
30 
40 
50 

Number of retrieved images 

0.05 

0.08 

0.11 

0.14 

0.17 

0.2 

0.23 

0.26 

0.29 

0.32 

Retrieval accuracy 

VSE (w/o Concept Discovery) 
ACD [22] 
Word2vec [17] (semantic only) 
AAM (spatial only) 
Ours Joint (semantic + spatial) 

(b) top 

1 
10 
20 
30 
40 
50 

Number of retrieved images 

0.05 

0.09 

0.13 

0.17 

0.21 

0.25 

0.29 

0.33 

0.37 

0.41 

Retrieval accuracy 

VSE (w/o Concept Discovery) 
ACD [22] 
Word2vec [17] (semantic only) 
AAM (spatial only) 
Ours Joint (semantic + spatial) 

(c) pants 

1 
10 
20 
30 
40 
50 

Number of retrieved images 

0.05 

0.09 

0.13 

0.17 

0.21 

0.25 

0.29 

0.33 

0.37 

0.41 

Retrieval accuracy 

VSE (w/o Concept Discovery) 
ACD [22] 
Word2vec [17] (semantic only) 
AAM (spatial only) 
Ours Joint (semantic + spatial) 

(d) skirt 

1 
10 
20 
30 
40 
50 

Number of retrieved images 

0.05 

0.09 

0.13 

0.17 

0.21 

0.25 

0.29 

0.33 

0.37 

0.41 

Retrieval accuracy 

VSE (w/o Concept Discovery) 
ACD [22] 
Word2vec [17] (semantic only) 
AAM (spatial only) 
Ours Joint (semantic + spatial) 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors acknowledge the Maryland Advanced Research Computing Center (MARCC) for providing computing resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic attribute discovery and characterization from noisy web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Describing clothing by semantic attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discovering localized attributes for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Interactive search for image categories by mental matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ferecatu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Where to buy it: Matching street clothing photos in online shops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadi Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning fashion compatibility with bidirectional lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cross-domain image retrieval with a dual attribute-aware ranking network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hipster wars: Discovering elements of fashion styles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Whittlesearch: Image search with relative attribute feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kovashka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combining language and vision with a multimodal skip-gram model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NACCL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object bank: A high-level image representation for scene classification &amp; semantic feature sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">V-measure: A conditional entropy-based external cluster evaluation measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relevance feedback: a power tool for interactive content-based image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neuroaesthetics in fashion: Modeling the perception of fashionability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Selecting relevant web trained concepts for automated event retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end localization and ranking for relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic concept discovery from parallel text and visual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scalable</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1441</idno>
		<title level="m">high-quality object detection</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient object category recognition using classemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The elements of fashion style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vaccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karahalios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Disentangling nonlinear perceptual embeddings with multi-query triplet networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karaletsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic attribute discovery with neural activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vittayakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Umeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning deep structurepreserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Discovering the spatial extent of relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Panda: Pose aligned networks for deep attribute modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Relevance feedback in image retrieval: A comprehensive review. Multimedia systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
