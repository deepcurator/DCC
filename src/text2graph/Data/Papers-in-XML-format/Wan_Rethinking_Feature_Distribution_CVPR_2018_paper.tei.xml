<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Feature Distribution for Loss Functions in Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weitao</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyi</forename><surname>Zhong</surname></persName>
							<email>yuanyiz2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of at Urbana-Champaign</orgName>
								<address>
									<settlement>Illinois</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianpeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiansheng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Feature Distribution for Loss Functions in Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We propose a large-margin Gaussian Mixture (L-GM) loss for deep neural networks in classification tasks. Different from the softmax cross-entropy loss, our proposal is established on the assumption that the deep features of the training set follow a Gaussian Mixture distribution. By involving a classification margin and a likelihood regularization, the L-GM loss facilitates both a high classification performance and an accurate modeling of the training feature distribution. As such, the L-GM loss is superior to the softmax loss and its major variants in the sense that besides classification, it can be readily used to distinguish abnormal inputs, such as the adversarial examples, based on their features' likelihood to the training feature distribution. Extensive experiments on various recognition benchmarks like MNIST, CIFAR, ImageNet and LFW, as well as on adversarial examples demonstrate the effectiveness of our proposal.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, deep neural networks have substantially improved the state-of-the-art performances of various challenging classification tasks, including image based object recognition <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b9">10]</ref>, face recognition <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b35">36]</ref> and speech recognition <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. In these tasks, the softmax cross-entropy loss, or the softmax loss for short, has been widely adopted as the classification loss function for various deep neural networks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b11">12]</ref>. For example in image classification, the affinity score of an input sample to each class is first computed by a linear transformation on the extracted deep feature. Then the posterior probability is modeled as the normalized affinity scores using the softmax function. Finally, the cross-entropy between the posterior probability and the class label is used as the loss function. The softmax loss has its probabilistic interpretation in that, for a large class of distributions, the posterior distribution complies with the softmax transformation of linear functions of the feature vectors <ref type="bibr" target="#b0">[1]</ref>. It can also be derived from a binary Markov Random Field or a Boltzmann Machine model <ref type="bibr" target="#b2">[3]</ref>. However, the relationship between the affinity score and the probability distribution of the training feature space is vague. In other words, for an extracted feature, its likelihood to the training feature distribution is not well formulated.</p><p>Several variants have been proposed to enhance the effectiveness of the softmax loss. The Euclidean distances between each pair <ref type="bibr" target="#b35">[36]</ref> or among each triplet <ref type="bibr" target="#b24">[25]</ref> of extracted features are added as an additional loss to the softmax loss. Alternatively, in <ref type="bibr" target="#b31">[32]</ref> the Euclidean distance between each feature vector and its class centroid is used. However, under the softmax loss formulation, the cosine distance based similarity metrics is more appropriate, indicating that using the Euclidean distance based additional losses may not be the most ideal choice. Based on this understanding, an angular distance based margin is introduced in <ref type="bibr" target="#b21">[22]</ref> to force extra intra-class compactness and inter-class separability, leading to better generalization of the trained models. Nevertheless, the softmax loss is still indispensable and mostly dominates the training process in these proposals. Therefore, the probabilistic modeling of the training feature space is still not explicitly considered.</p><p>In this paper we propose a Gaussian Mixture loss (GM loss) under the intuition that it is reasonable as well as tractable to assume the learned features of the training set to follow a Gaussian Mixture (GM) distribution, with each component representing a class. As such, the posterior probability can be computed using the Bayes' rule. The classification loss is then calculated as the cross-entropy between the posterior probability and the corresponding class labels. To force the training samples to obey the assumed GM distribution, we further add a likelihood regularization term to the classification loss. As such, for a well trained model, the probability distribution of the training features can now be (f) <ref type="figure">Figure 1</ref>. Two-dimensional feature embeddings on MNIST training set. (a) Softmax loss. (b) Softmax loss + center loss <ref type="bibr" target="#b31">[32]</ref>. (c) Largemargin softmax loss <ref type="bibr" target="#b21">[22]</ref>. (d) GM Loss without margin (α =0). (e) Large-margin GM loss (α =1). (f) Heatmap of the learned likelihood corresponding to (e). Higher values are brighter. Several adversarial examples generated by the Fast Gradient Sign Method <ref type="bibr" target="#b7">[8]</ref>h a v e extremely low likelihood according to the learned GM distribution and thus can be easily distinguished. This figure is best viewed in color.</p><p>explicitly formulated. It can be observed from <ref type="figure">Fig. 1</ref> that the learned training features spaces using the proposed GM loss are intrinsically different from those learned using the softmax loss and its invariants, by approximately following a GM distribution.</p><p>The GM loss is not just an alternative, it bears several essential merits comparing to the softmax loss and its invariants. First, incorporating a classification margin into the GM loss is simple and straightforward so that there is no need to introduce an additional complicated distance function as is practiced in the large-margin softmax loss <ref type="bibr" target="#b21">[22]</ref>. Second, it can be proved that the center loss <ref type="bibr" target="#b31">[32]</ref> is formally equivalent to a special case of the likelihood regularization in the GM loss. However, the classification loss and the regularization now share identical feature distance measurements in the GM loss since they are both induced from the same GM assumption. Last but not the least, in addition to the classification result, the GM loss can be readily used to estimate the likelihood of an input to the learned training feature distribution, leading to the possibility of improving the model's robustness, for example, towards adversarial examples.</p><p>We discuss mathematic details of the GM loss in Section 3. Extensive experimental results on object classification, face verification and adversarial examples are shown in Section 4. We conclude this work in Section 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The previous efforts for overcoming certain deficiencies of the softmax loss are inspiring. One of the most widely studied technical route is to explicitly encourage stronger intra-class compactness and larger inter-class separability while using the softmax loss. Y. Sun et al. introduced the contrastive loss in training a Siamese network for face recognition by simultaneously minimizing the distances between positive face image pairs and enlarging the distances between negative face image pairs by a predefined margin <ref type="bibr" target="#b35">[36]</ref>. Similarly, F. Schroff et al. proposed to apply such inter sample distance regularizations on image triplets rather than on image pairs <ref type="bibr" target="#b24">[25]</ref>. A major drawback of the contrastive loss and the triplet loss is the combinatoric explosion in the number of image pairs or triplets especially for large-scale data sets, leading to the significant increase in the required number of training iterations. The center loss proposed in <ref type="bibr" target="#b31">[32]</ref> effectively circumvents the pair-wise or triplet-wise computation by minimizing the Euclidean distance between the features and the corresponding class centroids. However, such a formulation brings about inconsistency of distance measurements in the feature space. W. Liu et al. solved this problem by explicitly introduced an angular margin into the softmax loss through the designing of a sophisticated differentiable angular distance function <ref type="bibr" target="#b21">[22]</ref>. Another technical route mainly aims at improving the numerical stability of the softmax loss. Along this line, the label smoothing <ref type="bibr" target="#b30">[31]</ref> and the knowledge distilling <ref type="bibr" target="#b6">[7]</ref> are two typical methods of which the basic idea is to replace the one-hot ground truth distribution with other distributions that are probabilistically more reasonable. An interesting recent work proposed by B. Chen et al. focused on mitigating the early saturation problem of the softmax loss by injecting annealed noise in the softmax function during each training iteration <ref type="bibr" target="#b1">[2]</ref>. Generally speaking, all these works aim at improving the softmax loss rather than reformulating its fundamental assumption.</p><p>It has been revealed that deep neural networks with high classification accuracies are vulnerable to adversarial examples <ref type="bibr" target="#b7">[8]</ref>. Previous methods for solving this dilemma either directly included the adversarial samples in the training set <ref type="bibr" target="#b17">[18]</ref> or introduced an new model for detecting the spoofing samples <ref type="bibr" target="#b23">[24]</ref>. Intuitively, however, the features of adversarial examples should follow a probability distribution quite different from that of the learned training feature space. In other words, it is possible to better distinguish the adversarial examples if the distribution of the training feature space can be explicitly modeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Gaussian Mixture Loss</head><p>In this section, we will formulate the GM loss from a probability perspective. We will also describe how to efficiently add a classification margin to the GM loss, after which the likelihood regularization term in the GM loss is further discussed. The optimization of the GM loss is also presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Intuitions</head><p>Considering a K class classification task in which the softmax loss is used. For an input sample with x as its extracted deep feature vector, its posterior probability of belonging to a certain class j ∈ [1,K] can be expressed by Eq. 1, in which the affinity score (logit) f k (x) is usually calculated by linearly transforming the feature vector x as is shown in Eq. 2. In practice, the linear functions of all the K classes are combined to form a linear transformation layer with all the w k ,b k as the trainable parameters. A larger value of the affinity score f k (x) indicates a higher posterior probability of x belonging to the class k. However, f k (x) cannot be directly used to evaluate x's likelihood to the distribution of the training features which is not explicitly formulated at all.</p><formula xml:id="formula_0">p(j|x)= e fj (x) K k=1 e f k (x)</formula><p>(1)</p><formula xml:id="formula_1">f k (x)=w T k x + b k ,k ∈ [1,K]<label>(2)</label></formula><p>What is more, since f k (x) is computed through inner product, the similarity between features in the learned feature space should be measured using the cosine distance. However, in the Euclidean distance based regularization is more widely adopted in softmax variants probably due to its mathematical simplicity. For example, the Euclidean distance between the extracted feature and the corresponding class centroid was used to formulate the center loss L C in Eq. 3 <ref type="bibr" target="#b31">[32]</ref>, in which N is the number of training samples; x i and z i are the extracted feature and the class label of the i-th sample respectively; and μ zi is the feature centroid (mean) for class z i . Intuitively, such a regularization should be more reasonable if the similarity measurement can be coherent to that in the classification loss.</p><formula xml:id="formula_2">L C = 1 2 N i=1 x i − μ zi 2 2<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">GM loss formulation</head><p>Different from the softmax loss, we hereby assume that the extracted deep feature x on the training set follows a Gaussian mixture distribution expressed in Eq. 4, in which μ k and Σ k are the mean and covariance of class k in the feature space; and p(k) is the prior probability of class k.</p><formula xml:id="formula_3">p(x)= K k=1 N (x; μ k , Σ k )p(k)<label>(4)</label></formula><p>Under such an assumption, the conditional probability distribution of a feature x i given its class label z i ∈ [1,K] can be expressed in Eq. 5. Consequently, the corresponding posterior probability distribution can be expressed in Eq. 6.</p><formula xml:id="formula_4">p(x i |z i )=N (x i ; μ zi , Σ zi ) (5) p(z i |x i )= N (x i ; μ zi , Σ zi )p(z i ) K k=1 N (x i ; μ k , Σ k )p(k)<label>(6)</label></formula><p>As such, a classification loss L cls can be computed as the cross-entropy between the posterior probability distribution and the one-hot class label as is shown in Eq. 7, in which the indicator function ✶() equals 1 if z i equals k; or 0 otherwise.</p><formula xml:id="formula_5">L cls = − 1 N N i=1 K k=1 ✶(z i = k)logp(k|x i ) = − 1 N N i=1 log N (x i ; μ zi , Σ zi )p(z i ) K k=1 N (x i ; μ k , Σ k )p(k)<label>(7)</label></formula><p>Optimizing the classification loss only cannot explicitly drive the extracted training features towards the GM distribution. For example, a feature x i can be far away from the corresponding class centroid μ zi while still being correctly classified as long as it is relatively closer to μ zi than to the feature means of the other classes. To solve this problem, we further introduce a likelihood regularization term for measuring to what extent the training samples fit the assumed distribution. The likelihood for the complete data set {X, Z} is expressed in Eq. 8. We define the likelihood regularization term as the negative log likelihood shown in Eq. 9.B y reasonably assuming constant prior probabilities p(z i ), the likelihood regularization L lkd can be simplified as Eq. 10.</p><formula xml:id="formula_6">p(X, Z|μ, Σ) = N i=1 K k=1 ✶(z i = k)N (x i ; μ zi , Σ zi )p(z i ) (8) log p(X, Z|μ, Σ) = − N i=1 (log N (x i ; μ zi , Σ zi )+logp(z i )) (9) L lkd = − N i=1 log N (x i ; μ zi , Σ zi )<label>(10)</label></formula><p>Finally the proposed GM loss L GM is defined in Eq. 11, in which λ is a non-negative weighting coefficient.</p><formula xml:id="formula_7">L GM = L cls + λL lkd<label>(11)</label></formula><p>By definition, for the training feature space, the classification loss L cls is mainly related to its discriminative capability while the likelihood regularization L lkd is related to its probabilistic distribution. Under the GM distribution assumption, L cls and L lkd share all the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Large-Margin GM Loss</head><p>It has been widely recognized in statistical machine learning that large classification margin on the training set usually helps generalization, which is also believed to be applicable in deep learning <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b21">22]</ref>. Denote x i 's contribution to the classification loss to be L cls,i , of which an expansion form is in Eq. 12 and Eq. 13.</p><formula xml:id="formula_8">L cls,i = − log p(z i )|Σ zi | − 1 2 e −dz i k p(k)|Σ k | − 1 2 e −d k (12) d k =(x i − μ k ) T Σ −1 k (x i − μ k )/2<label>(13)</label></formula><p>Since the squared Mahalanobis distance d k is by definition non-negative, a classification margin m ≥ 0 can be easily introduced to achieve the large-margin GM loss as in Eq. 14. Obviously, adding the classification margin to the GM loss is more straightforward than to the softmax loss <ref type="bibr" target="#b21">[22]</ref>. It should be emphasized that such a simple formulation cannot be directly applied to the softmax loss since an inner product can be negative, whereas a margin generally has to be non-negative to make sense.</p><formula xml:id="formula_9">L m cls,i = − log p(z i )|Σ zi | − 1 2 e −dz i −m k p(k)|Σ k | − 1 2 e −d k −✶(k=zi)m (14)</formula><p>To understand m's role in the large-margin GM loss, one may consider the simplest case in which p(k) and Σ k are identical for all the classes. Then x i is classified to the class z i if and only if Eq. 15 holds, indicating that x i should be closer to the feature mean of class z i than to that of the other classes by at least m.</p><formula xml:id="formula_10">e −dz i −m &gt;e −d k ⇐⇒ d k − d zi &gt;m ,∀k = z i (15)</formula><p>To design the margin, we adopt an adaptive scheme by letting the value of m to be proportional to each sample's distance to its corresponding class feature mean, i.e., m = αd zi , in which α is a non-negative parameter controlling the size of the expected margin between two classes on the training set. <ref type="figure" target="#fig_0">Fig. 2</ref> shows a schematic interpretation of α; and <ref type="figure">Fig. 1 (d)</ref> and (e) illustrate how the training feature space changes when increasing α from 0 to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">A Discussion on L lkd</head><p>Although the likelihood regularization L lkd defined in Eq. 10 is proposed from a probability perspective, it has a strong connection with the empirical center loss L C defined in Eq. 3 <ref type="bibr" target="#b31">[32]</ref> as is described in Lemma 1, of which the proof is quite straightfoward.</p><formula xml:id="formula_11">Lemma 1. If Σ k = I (identity matrix), p(k)=1/K, ∀k ∈ [1,K]</formula><p>, the center loss L C and the likelihood regularization L lkd satisfy Eq. 16, in which D is the feature dimension.</p><formula xml:id="formula_12">L lkd = L C + N 2 D log(2π)<label>(16)</label></formula><p>Lemma 1 shows that L C is identical to L lkd except for a constant under certain conditions. In other words, the center loss <ref type="bibr" target="#b31">[32]</ref> is basically equivalent to a special case of the proposed likelihood regularization. This indicates that it might be more appropriate to use the center loss, or the proposed likelihood L lkd as regularization in a GM distributed feature space, as is practiced in this work.</p><p>More importantly, L lkd can be readily used to estimate the likelihood of a sample feature to the learned GM distribution. Simply put, a model trained using our GM loss can now both generate a classification result and provide a likelihood estimation. In case that the likelihood is too low, one may refuse to make the classification decision. Such a choice may be favorable, for example, when an adversarial example <ref type="bibr" target="#b7">[8]</ref> is generated to attack the trained classification model. In fact, the center loss L C could also be used to estimate such a likelihood. However, when being combined with the softmax loss during training, the center loss may produce inaccurate likelihood estimation since the generated training feature space probably deviates from the GM distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Optimization</head><p>The GM loss can be optimized using the typical stochastic gradient descent (SGD) algorithm. In practice, updating the covariance matrix with gradient descent is feasible but may suffer from singularity problems. Hence, for simplicity, we assume that the covariance matrix Σ k is diagonal, denoted by Λ k ; and the prior probability p(k)=1/K. As such, the contribution of a sample x i to the large-margin GM loss can be rewritten in Eq. 17 and Eq. 18.</p><formula xml:id="formula_13">L m GM,i = − log |Λ zi | − 1 2 e −dz i (1+α) k |Λ k | − 1 2 e −d k (1+✶(k=zi)α) + λ(d zi + 1 2 log |Λ zi |)<label>(17)</label></formula><formula xml:id="formula_14">d k = 1 2 (x i − μ k ) T Λ −1 k (x i − μ k ),k∈ [1,K]<label>(18)</label></formula><p>The</p><note type="other">gradient computations for the GM loss of the i-th sample are given in Eqs. 19 to 23. For conciseness, we denote p(k|x i ) as p k and (</note><formula xml:id="formula_15">x i − μ k )(x i − μ k ) T as C k in all these equations. ∂L m GM,i ∂μ zi = 1 − p zi (1 + α)+λ Λ −1 zi (μ zi − x i ) (19) ∂L m GM,i ∂μ k = p k Λ −1 k (x i − μ k ), ∀k = z i (20) ∂L m GM,i ∂Λ zi = − 1 2 (1 − p zi )(1 + α)+λ Λ −1 zi C k − (1 − p zi + λ)I Λ −1 zi (21) ∂L m GM,i ∂Λ k = − 1 2 p k (I − Λ −1 k C k )Λ −1 k , ∀k = z i (22) ∂L m GM,i ∂x i = 1 − p zi (1 + α)+λ Λ −1 zi (x − μ zi ) − k =zi p k Λ −1 k (x i − μ k )<label>(23)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Two sets of experiments are presented in this section. In the first set, we conduct the image classification and face verification experiments to verify the effectiveness of the large-margin GM loss (L-GM loss for short). We report mean and standard deviation of 3 tries. In the second set, we demonstrate the feasibility of distinguishing adversarial examples using the likelihood regularization term L lkd . All experiments are carried out using the Caffe framework <ref type="bibr" target="#b32">[33]</ref> on NVIDIA TitanX GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Functions 2-D (%)</head><p>100-D (%) Center <ref type="bibr" target="#b31">[32]</ref> 1.45 ± 0.01 0.47 ± 0.01 L-Softmax <ref type="bibr" target="#b21">[22]</ref> 1.30 ± 0.02 0.43 ± 0.01 Softmax 1.82 ± 0.01 0.68 ± 0.01 L-GM (α =0)) 1.44 ± 0.01 0.49 ± 0.01 L-GM (α =0.3) 1.32 ± 0.01 0.42 ± 0.02 L-GM (α =1.0) 1.17 ± 0.01 0.39 ± 0.01 <ref type="table">Table 1</ref>. Recognition error rates (%) on MNIST test set using a 6-layer CNN with different loss functions.</p><p>For the margin parameter α, a larger value may lead to a more difficult optimization objective. Therefore intuitively, α should be smaller when the number of classes gets larger. In our experiments, we empirically set α to 1.0, 0.3, 0.1, 0.01 and 0.01 for MNIST, CIFAR-10, CIFAR-100, ImageNet and face verification, respectively. Also, we set the likelihood regularization parameter λ to a small value, e.g. 0.1 in our experiments, so that the likelihood regularization starts to play a major role when the training accuracy is approaching saturation, or when p zi approaches 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Classification</head><p>MNIST We first compare the softmax loss, the center loss (with the softmax loss) <ref type="bibr" target="#b31">[32]</ref>, the large-margin softmax loss (L-Softmax loss for short) <ref type="bibr" target="#b24">[25]</ref> and the L-GM loss by visualizing their learned 2D feature spaces for the MNIST Handwritten Digit dataset <ref type="bibr" target="#b19">[20]</ref>. We adopt a network with 6 convolution layers and a fully connected layer with a two dimensional output. The feature embeddings on the training set with different loss functions are illustrated in <ref type="figure">Fig. 1</ref>.A s we can see, different from the softmax loss and its variants, the features generated using the L-GM loss roughly follow the GM distribution, which is consistent with the assumption. The heatmap of the learned likelihood is shown in <ref type="figure">Fig. 1(f)</ref>. Also, as is shown in <ref type="figure">Fig. 1 (d)</ref>-(e), with an increasing α, larger margin sizes can be observed among different classes.</p><p>For the quantitative evaluation, we also increase the output dimension of the fully connected layer from 2 to 100 and add a ReLU activation after it. For fair comparison, we train the same network with different loss functions using identical training parameters including the learning rate, weight decay, etc.. The classification accuracies on the test set are presented in <ref type="table">Table 1</ref>.</p><p>CIFAR CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b15">[16]</ref> each consists of of 32 × 32 pixel colored images, with 50,000 training images and 10,000 testing images. We adopt the standard data augmentation scheme including mirroring and 32 × 32 random cropping after 4 pixel zero-paddings on each side <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref>. ResNet-56 ResNet-110 Softmax <ref type="bibr" target="#b9">[10]</ref> 8.75 ± 0.04 6.97 ± 0.05 6.43 ± 0.04 Center <ref type="bibr" target="#b31">[32]</ref> 7.77 ± 0.05 5.94 ± 0.02 5.32 ± 0.03 L-Softmax <ref type="bibr" target="#b21">[22]</ref> 7.73 ± 0.03 6.05 ± 0.04 5.79 ± 0.02 L-GM(α =0.3) 7.21 ± 0.04 5.61 ± 0.02 4.96 ± 0.03 <ref type="table">Table 2</ref>. Recognition error rates (%) on CIFAR-10 using ResNet models with different loss functions.</p><p>learning rate is set to 0.1 and then divided by 10 at the 150 th epoch and the 225 th epoch respectively. We use a weight decay of 5 × 10 −4 and the Nesterov optimization algorithm <ref type="bibr" target="#b29">[30]</ref> with a momentum of 0.9. The network weights are initialized using the method introduced in <ref type="bibr" target="#b8">[9]</ref>. The recognition accuracies are shown in <ref type="table">Table 2</ref>. Results in the first row were reported in the original RestNet paper <ref type="bibr" target="#b9">[10]</ref>. For the center loss and the large-margin softmax loss, we train the models by ourselves since the ResNet was not used on CIFAR-10 in the original papers <ref type="bibr" target="#b31">[32]</ref> and <ref type="bibr" target="#b21">[22]</ref>. The proposed L-GM loss outperforms the softmax loss and its two variants for different ResNet models with various depths.</p><p>For CIFAR-100, we adopt the same CNN architecture used by the large-margin softmax loss <ref type="bibr" target="#b21">[22]</ref>, which follows the design philosophy of the VGG-net <ref type="bibr" target="#b26">[27]</ref> consisting of 13 convolutional layers and 1 fully connected layer. Bach normalization <ref type="bibr" target="#b14">[15]</ref> is used after each convolutional layer and no dropout is used. To achieve better recognition performances, we replace the fully connected layer in this network with Global Average Pooling <ref type="bibr" target="#b20">[21]</ref>. We report the recognition performances with or without the data augmentation in <ref type="table">Table 3</ref>, denoted by C100+ and C100 respectively. Several points can be observed from <ref type="table">Table 3</ref>. First, the proposed L-GM loss consistently outperforms the softmax based losses on both C100+ and C100. Second, for the augmented data set C100+, increasing the margin parameter α consistently benefits the recognition performance. However, this is not true for C100 without data augmentation. This is probably related to the fact that the number of training samples for each object class is as low as 500 on C100. The margin size on the training set and the model generalization capability is less correlated.</p><p>Loss Functions C100 C100+ Center <ref type="bibr" target="#b31">[32]</ref> 24.85 ± 0.06 21.05 ± 0.03 L-Softmax <ref type="bibr" target="#b21">[22]</ref>   <ref type="table">Table 3</ref>. Recognition error rates (%) on CIFAR-100 using a VGGlike 13 layer CNN with different loss functions.</p><p>ImageNet We investigate the performance on large-scale image classification using the ImageNet dataset <ref type="bibr" target="#b3">[4]</ref>. We perform experiments on ImageNet (ILSVRC2012) using ResNet-101 <ref type="bibr" target="#b9">[10]</ref> combined with different loss functions. To make fair comparison, all the models are trained for 100 epochs on 6 Titan GPUs with a mini-batch size of 16 for each GPU. The learning rate is initialized as 0.01 and divided by 10 at the 50 th epochs and 75 th epochs respectively. We use a weight decay of 0.0002 and a momentum of 0.9; and no dropout <ref type="bibr" target="#b10">[11]</ref> is used. We evaluate the performances for 1-crop and 10-crop practices on the ILSVRC2012 validation set. Results in <ref type="table">Table 4</ref> show that our proposal is also effective on the large-scale dataset.  <ref type="table">Table 4</ref>. Error rates (%) on ILSVRC2012 validation set. For L-GM, we set α=0.01 and λ=0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Face Verification</head><p>We conduct the face verification experiments on the Labeled Face in the Wild (LFW) dataset <ref type="bibr" target="#b12">[13]</ref>, which contains 13,233 face images from 5749 different identities with large variations in pose, expression and illumination. The officially provided 6,000 pairs are used for face verification test. We follow the standard unrestricted, labeled outside data protocol of LFW and use only the CASIA-WebFace dataset <ref type="bibr" target="#b33">[34]</ref> for training. The CASIA-WebFace dataset consists of 494,414 face images from 10,575 subjects. The training and testing images are aligned using MTCNN <ref type="bibr" target="#b36">[37]</ref> and resized to 128 × 128 pixel. A simple data augmentation scheme is adopted including horizontal mirroring and 120 × 120 random crop from the aligned 128 × 128 pixel face images.</p><p>We train the ResNet <ref type="bibr" target="#b9">[10]</ref> based face recognition model with 27 convolutional layers. The PReLU activations <ref type="bibr" target="#b8">[9]</ref> are used after each convolutional layer and no batch normalization or Dropout is used. We train with a batch size of 256 for 20 epochs. The learning rate is initially set to 0.1 and divided by 10 at the 10th, 14th and 16th epochs. The</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Training Data Accuracy FaceNet <ref type="bibr" target="#b25">[26]</ref> 200M 99.65 Deepid2+ <ref type="bibr" target="#b28">[29]</ref> 0.3M 98.70 Softmax 0.49M 98.56 ± 0.03 L-Softmax <ref type="bibr" target="#b21">[22]</ref> 0.49M 98.92 ± 0.03 Center <ref type="bibr" target="#b31">[32]</ref> 0.49M 99.05 ± 0.02 LGM (α =0.001) 0.49M 99.03 ± 0.03 LGM (α =0.005) 0.49M 99.08 ± 0.02 LGM (α =0.01) 0.49M 99.20 ± 0.03 <ref type="table">Table 5</ref>. Face verification performances on LFW of a single model. The 6 models at bottom are trained on our scheme while the 2 results on top are reported from the original paper.</p><p>networks are trained using stochastic gradient descent (SGD) with a momentum of 0.9 and a weight decay of 5 × 10 −4 . For the L-GM loss, we perform PCA on the 512-dimensional feature embeddings and then compute the Mahalanobis distance for verification. For fair comparison, the verification performance is evaluated on single models and model ensemble is not used. In <ref type="table">Table 5</ref>, the accuracies for the Deepid2+ (contrastive loss) <ref type="bibr" target="#b28">[29]</ref> and the FaceNet (triplet loss) <ref type="bibr" target="#b25">[26]</ref> are reported in the original papers. The FaceNet achieves the highest accuracy of 99.65% by using a very large training set of 200M images. In <ref type="bibr" target="#b31">[32]</ref>, Y. Wen et al. reported a higher accuracy of 99.28% for the center loss by using both the CASIA-Webface and the Celebrity+ <ref type="bibr" target="#b22">[23]</ref> dataset for training, with 0.7M training images in total. When using the CASIA-Webface training dataset only, the L-GM loss outperforms the other loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Beyond Classification</head><p>As we have discussed in Sect. 3.4, the proposed L-GM loss enables the likelihood estimation for a given input in addition to the class prediction. During training, the L-GM loss drives the deep model to generate features that follow the assumed GM distribution as well as possible, while guaranteeing the inter class separability. In other words, the training feature distribution is supposed to be well established for a trained deep model using the L-GM loss. We will validate this claim through experiments on distinguishing adversarial examples from normal inputs in this section.</p><p>Adversarial Examples For a deep neural network, adversarial examples are inputs formed by intentionally adding small but worst-case perturbations which cause the model to make incorrect classifications with high confidence <ref type="bibr" target="#b7">[8]</ref>. We generate the adversarial examples using the fast gradient sigh method (FGSM) <ref type="bibr" target="#b7">[8]</ref>, which uses gradient backpropagation to perturb the inputs so as to maximize the classification loss. The perturbation P is generated by P = ·sign(∇ I L(I,z)), in which L is the classification loss function (e.g. L cls in L-GM loss), I is the input image, z is the true class label, and &gt;0 is called the magnitude of perturbation. Then the adversarial example is formed by adding P to the original image I. An extension of FGSM called the Targeted FGSM aims at misclassifying an input sample to a target class by minimizing the loss for the pre-set target labelz. The targeted perturbation P t is given by</p><formula xml:id="formula_16">P t = ·sign(−∇ I L(I,z)), in which L is L GM in our experiments.</formula><p>By using the FGSM, we first generate one adversarial example for each MNIST test image in order to evaluate the classification performance using different loss functions. As such there are altogether 10,000 adversarial examples and 10,000 original normal MNIST test images in the experiment. We use the CNN architecture as described in Sect. <ref type="bibr" target="#b3">4</ref>  <ref type="table">Table 6</ref>. Classification error rates (%) on adversarial examples generated from the MNIST test set using FGSM. =0means that the inputs are normal MNIST test images. are trained on the standard MNIST training set by using the softmax loss, the center loss and the proposed L-GM loss respectively. The classification error rates on the adversarial examples are presented in <ref type="table">Table 6</ref>, which shows that all three models seem to be vulnerable to adversarial attacks. We then investigate the posterior probability (p max =max k p(k|x)) corresponding to the predicted class for both the normal inputs and the adversarial examples ( =0.3). For adversarial examples, the histograms of p max are shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. For normal inputs, it is unnecessary to plot the histograms since the p max &gt; 0.98 for over 95% of the samples for all the three losses. Obviously, for the L-GM loss, the overlap between the histograms of p max of the normal inputs and the adversarial examples is the smallest among three loss functions. This means that even by only considering the posterior probability in classification, the L-GM loss already outperforms the other two loss functions in distinguishing adversarial examples. Nevertheless, a more effective way for distinguishing adversarial examples is to directly consider the likelihood to the learned training feature distribution. We therefore design the following experiment.</p><p>Adversarial Verification Intuitively, in the feature space, the adversarial examples should follow a distribution different from that of the normal inputs. Based on this understanding, we design an experiment called the adversarial verification to distinguish the adversarial examples from normal inputs based on the feature likelihood. Let the predicted class beẑ i =a r g m a x k p(k|x i ). For the L-GM loss, we now assume identity covariance matrix and equal priors for simplicity. Then the likelihood of x i is l GM,i = exp(− x i − μẑ i 2 /2) based on Eq. 8 by omitting the constant coefficient. And it can also be rewritten as l GM,i = exp(−L lkd,i ) according to Eq.10. For the center loss, the likelihood can be computed similarly according to Lemma 1, leading to l C,i = exp(− x i − μẑ i 2 /2). For the softmax loss, the likelihood is not explicitly established in its formulation. A reasonable way is to estimate the likelihood as l S,i = w T zi x i +bẑ i . After all, the affinity score w T zi x i +bẑ i represents the similarity between x i and classẑ i .</p><p>In the adversarial verification experiment, the FGSM is used to generate adversarial examples for the MNIST test set, with =0 .3. Then for the three models, we compute the likelihood of the normal test images and the adversarial examples. For the softmax loss, we normalize the likelihood l S to (0, 1] for comparison. The histograms of the likelihood for three loss functions are illustrated in <ref type="figure" target="#fig_4">Fig. 4</ref>. For the L-GM loss, the adversarial examples have very low likelihood in the feature space and the normal inputs can be easily distinguished from them. The softmax loss, however, clearly suffers from a serious overlap between the two likelihood histograms. The center loss lies in between by being superior to the softmax loss while inferior to the L-GM loss in terms of the capability of adversarial verification.</p><p>Quantitatively, we evaluate the adversarial verification performances by thresholding the likelihood, and resultant ROC curves are demonstrated in <ref type="figure" target="#fig_5">Fig. 5</ref>. The equal error rate (EER) for the softmax loss is 37.7%, which is practically too high in a binary classification task. The center loss performs much better with an EER of 10.2%. The proposed L-GM loss achieves the lowest EER of 3.1%. This experiment demonstrates that comparing to the other two loss functions, the L-GM loss can be effectively used for distinguishing adversarial examples. This validates our claim that the L-GM loss can well establish the training feature distribution while maintaining a satisfactory classification performance.</p><p>Discussions Theoretically speaking, it is possible to generate adversarial examples with high likelihood in the L-GM loss by jointly optimizing the classification loss and the likelihood regularization term. It can be verified that under the L-GM loss formulation, such a joint optimization can be approximately realized using the Targeted FGSM, in which the targeted perturbation P t actually helps to reduce the distance between the feature and the center of the targeted class, or increase the likelihood. We test this approach by using the class with the second largest posterior probability as the  target labelz for a given input. We still set =0.3 and only test the L-GM loss. The classification error rate is 81.37%, which is similar to that in <ref type="table">Table.</ref> 6. The likelihood histogram is illustrated in <ref type="figure" target="#fig_6">Fig. 6</ref>. Compared to <ref type="figure" target="#fig_4">Fig. 4</ref>, the number of adversarial examples with very low likelihood (e.g. smaller than 0.2) is decreased, leading to a slightly higher EER of 4.3%. Nevertheless, most of the adversarial examples can still be distinguished using the likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We proposed a loss function by assuming a Gaussian Mixture (GM) distribution of the deep features on the training set. Besides the classification loss, a log likelihood regularization term is added to explicitly drive the deep model for generating GM distributed features. To further improve the generalization capability of the trained model, a classification margin is introduced. Extensive experiments demonstrate that the proposed L-GM loss outperforms the softmax loss and its variants in in both small and large-scale datasets when combined with different deep models. Besides, the L-GM loss facilitates a more effective distinguishment of abnormal inputs of which the extracted features follow a distribution different from the one learned during training. This can be practically useful, for example, to improve an deep model's robustness towards adversarial examples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. A geometry interpretation of the relationship between α and the margin size in the training feature space using (a) GM loss without margin α =0; (b) large-margin GM loss with α&gt;0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>For CIFAR- 10 ,</head><label>10</label><figDesc>We train the ResNet [10] of depth 20, 56 and 110 with different loss functions. The networks are trained with a batch size of 128 for 300 epochs;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>24.83 ± 0.05 20.98 ± 0.04 Softmax 25.61 ± 0.07 21.60 ± 0.04 LGM(α =0.1) 23.74 ± 0.08 20.94 ± 0.03 LGM(α =0.2) 23.04 ± 0.08 20.85 ± 0.04 LGM(α =0.3) 23.80 ± 0.06 20.76 ± 0.03</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Histograms of the predicted posterior probability of the adversarial examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Histograms of the likelihood for adversarial examples (Adv.) and normal inputs (Normal).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. ROC curves of the adversarial verification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Histogram of the likelihood for adversarial examples generated by the Targeted FGSM against the L-GM loss.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by the National Natural Science Foundation of China (61673234).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Noisy softmax: Improving the generalization ability of dcnn via postponing the early softmax saturation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large-scale object classification using label relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="48" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Context-dependent pretrained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alex</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdelrahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Navdeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jeff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<title level="m">Explaining and harnessing adversarial examples</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>University of Massachusetts, Amherst</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01236</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fractalnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07648</idno>
		<title level="m">Ultra-deep neural networks without residuals</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="507" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bischoff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04267</idno>
		<title level="m">On detecting adversarial perturbations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the depth of deep neural networks: A theoretical view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2066" to="2072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00567</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conferenc on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yangqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Evan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jeff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sergio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Trevor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
