<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stochastic Generative Hashing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niao</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
						</author>
						<title level="a" type="main">Stochastic Generative Hashing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Learning-based binary hashing has become a powerful paradigm for fast search and retrieval in massive databases. However, due to the requirement of discrete outputs for the hash functions, learning such functions is known to be very challenging. In addition, the objective functions adopted by existing hashing techniques are mostly chosen heuristically. In this paper, we propose a novel generative approach to learn hash functions through Minimum Description Length principle such that the learned hash codes maximally compress the dataset and can also be used to regenerate the inputs. We also develop an efficient learning algorithm based on the stochastic distributional gradient, which avoids the notorious difficulty caused by binary output constraints, to jointly optimize the parameters of the hash function and the associated generative model. Extensive experiments on a variety of large-scale datasets show that the proposed method achieves better retrieval results than the existing state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Search for similar items in web-scale datasets is a fundamental step in a number of applications, especially in image and document retrieval. Formally, given a reference dataset X = {x i } N i=1 with x 2 X ⇢ R d , we want to retrieve similar items from X for a given query y according to some similarity measure sim(x, y). When the negative Euclidean distance is used, i.e., sim(x, y) = kx yk 2 , this corresponds to L 2 Nearest Neighbor Search (L2NNS) problem; when the inner product is used, i.e., sim(x, y) = x &gt; y, it becomes a Maximum Inner Product Search (MIPS) problem. In this work, we focus on L2NNS for simplicity, however our method handles MIPS problems as well, as shown in the supplementary material D. Brute-force linear search is expensive for large datasets. To alleviate the time and storage bottlenecks, two research directions have been studied extensively: (1) partition the dataset so that only a subset of data points is searched; (2) represent the data as codes so that similarity computation can be carried out more efficiently. The former often resorts to search-tree or bucket-based lookup; while the latter relies on binary hashing or quantization. These two groups of techniques are orthogonal and are typically employed together in practice.</p><p>In this work, we focus on speeding up search via binary hashing. Hashing for similarity search was popularized by influential works such as Locality Sensitive Hashing <ref type="bibr" target="#b21">(Indyk and Motwani, 1998;</ref><ref type="bibr" target="#b8">Gionis et al., 1999;</ref><ref type="bibr" target="#b4">Charikar , 2002)</ref>. The crux of binary hashing is to utilize a hash function, f (·) : X ! {0, 1} l , which maps the original samples in X 2 R d to l-bit binary vectors h 2 {0, 1} l while preserving the similarity measure, e.g., Euclidean distance or inner product. Search with such binary representations can be efficiently conducted using Hamming distance computation, which is supported via POPCNT on modern CPUs and GPUs. Quantization based techniques <ref type="bibr" target="#b0">(Babenko and Lempitsky, 2014;</ref><ref type="bibr" target="#b22">Jegou et al., 2011;</ref><ref type="bibr" target="#b42">Zhang et al., 2014b)</ref> have been shown to give stronger empirical results but tend to be less efficient than Hamming search over binary codes <ref type="bibr">(Douze et al., 2015;</ref><ref type="bibr" target="#b16">He et al., 2013)</ref>.</p><p>Data-dependent hash functions are well-known to perform better than randomized ones <ref type="bibr" target="#b36">(Wang et al., 2014)</ref>. Learning hash functions or binary codes has been discussed in several papers, including spectral hashing <ref type="bibr" target="#b37">(Weiss et al., 2009)</ref>, semi-supervised hashing <ref type="bibr" target="#b35">(Wang et al., 2010)</ref>, iterative quantization <ref type="bibr" target="#b9">(Gong and Lazebnik, 2011)</ref>, and others <ref type="bibr" target="#b28">(Liu et al., 2011;</ref><ref type="bibr" target="#b11">Gong et al., 2013;</ref><ref type="bibr" target="#b39">Yu et al., 2014;</ref><ref type="bibr" target="#b34">Shen et al., 2015;</ref><ref type="bibr" target="#b15">Guo et al., 2016)</ref>. The main idea behind these works is to optimize some objective function that captures the preferred properties of the hash function in a supervised or unsupervised fashion.</p><p>Even though these methods have shown promising performance in several applications, they suffer from two main drawbacks: (1) the objective functions are often heuristically constructed without a principled characterization of goodness of hash codes, and (2) when optimizing, the binary constraints are crudely handled through some relaxation, leading to inferior results <ref type="bibr" target="#b29">(Liu et al., 2014)</ref>. In this work, we introduce Stochastic Generative Hashing (SGH) to address these two key issues. We propose a generative model which captures both the encoding of binary codes h from input x and the decoding of input x from h. This provides a principled hash learning framework, where the hash function is learned by Minimum Description Length (MDL) principle. Therefore, its generated codes can compress the dataset maximally. Such a generative model also enables us to optimize distributions over discrete hash codes without the necessity to handle discrete variables. Furthermore, we introduce a novel distributional stochastic gradient descent method which exploits distributional derivatives and generates higher quality hash codes. Prior work on binary autoencoders (Carreira-Perpinán and Raziperchikolaei, 2015) also takes a generative view of hashing but still uses relaxation of binary constraints when optimizing the parameters, leading to inferior performance as shown in the experiment section. We also show that binary autoencoders can be seen as a special case of our formulation. In this work, we mainly focus on the unsupervised setting 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Stochastic Generative Hashing</head><p>We start by first formalizing the two key issues that motivate the development of the proposed algorithm.</p><p>Generative view. Given an input x 2 R d , most hashing works in the literature emphasize modeling the forward process of generating binary codes from input, i.e., h(x) 2 {0, 1} l , to ensure that the generated hash codes preserve the local neighborhood structure in the original space. Few works focus on modeling the reverse process of generating input from binary codes, so that the reconstructed input has small reconstruction error. In fact, the generative view provides a natural learning objective for hashing. Following this intuition, we model the process of generating x from h, p(x|h), and derive the corresponding hash function q(h|x) from the generative process. Our approach is not tied to any specific choice of p(x|h) but can adapt to any generative model appropriate for the domain. In this work, we show that even using a simple generative model (Section 2.1) already achieves the state-of-the-art performance.</p><p>Binary constraints. The other issue arises from dealing with binary constraints. One popular approach is to relax the constraints from {0, 1} <ref type="bibr" target="#b37">(Weiss et al., 2009)</ref>, but this often leads to a large optimality gap between the relaxed and non-relaxed objectives. Another approach is to enforce the model parameterization to have a particular structure so that when applying alternating optimization, the algorithm can alternate between updating the parameters and binarization efficiently. For example, <ref type="bibr" target="#b9">(Gong and Lazebnik, 2011;</ref><ref type="bibr" target="#b10">Gong et al., 2012)</ref> imposed an orthogonality constraint on the projection matrix, while <ref type="bibr" target="#b39">(Yu et al., 2014)</ref> proposed to use circulant constraints, and <ref type="bibr" target="#b41">(Zhang et al., 2014a)</ref> introduced Kronecker Product structure. Although such constraints alleviate the difficulty with optimization, they substantially reduce the model flexibility. In contrast, we avoid such constraints and propose to optimize the distributions over the binary variables to avoid directly working with binary variables. This is attained by resorting to the stochastic neuron reparametrization (Section 2.4), which allows us to back-propagate through the layers of weights using the stochsastic gradient estimator.</p><p>Unlike <ref type="bibr" target="#b3">(Carreira-Perpinán and Raziperchikolaei, 2015)</ref> which relies on solving expensive integer programs, our model is end-to-end trainable using distributional stochastic gradient descent (Section 3). Our algorithm requires no iterative steps unlike iterative quantization (ITQ) <ref type="bibr" target="#b9">(Gong and Lazebnik, 2011)</ref>. The training procedure is much more efficient with guaranteed convergence compared to alternating optimization for ITQ.</p><p>In the following sections, we first introduce the generative hashing model p(x|h) in Section 2.1. Then, we describe the corresponding process of generating hash codes given input x, q(h|x) in Section 2.2. Finally, we describe the training procedure based on the Minimum Description Length (MDL) principle and the stochastic neuron reparametrization in Sections 2.3 and 2.4. We also introduce the distributional stochastic gradient descent algorithm in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Generative model p(x|h)</head><p>Unlike most works which start with the hash function h(x), we first introduce a generative model that defines the likelihood of generating input x given its binary code h, i.e., p(x|h). It is also referred as a decoding function. The corresponding hash codes are derived from an encoding function q(h|x), described in Section 2.2.</p><p>We use a simple Gaussian distribution to model the generation of x given h:</p><formula xml:id="formula_0">p(x, h) = p(x|h)p(h), where p(x|h) = N (Uh,⇢ 2 I)<label>(1)</label></formula><p>and</p><formula xml:id="formula_1">U = {u i } l i=1 , u i 2 R d is a codebook with l code- words. The prior p(h) ⇠ B(✓) = Q l i=1 ✓ hi i (1 ✓ i ) 1 hi</formula><p>is modeled as the multivariate Bernoulli distribution on the hash codes, where</p><formula xml:id="formula_2">✓ = [✓ i ] l i=1 2 [0, 1] l .</formula><p>Intuitively, this is an additive model which reconstructs x by summing the selected columns of U given h, with a Bernoulli prior on the distribution of hash codes. The joint distribution can be written as:</p><formula xml:id="formula_3">p(x, h) / exp ( 1 2⇢ 2 x &gt; x + h &gt; U &gt; Uh 2x &gt; Uh | {z } kx U &gt; hk 2 2 (log ✓ 1 ✓ ) &gt; h )<label>(2)</label></formula><p>This generative model can be seen as a restricted form of general Markov Random Fields in the sense that the parameters for modeling correlation between latent variables h and correlation between x and h are shared. However, it is more flexible compared to Gaussian Restricted Boltzmann machines <ref type="bibr" target="#b26">(Krizhevsky, 2009;</ref><ref type="bibr" target="#b30">Marc'Aurelio and Geoffrey, 2010)</ref> due to an extra quadratic term for modeling correlation between latent variables. We first show that this generative model preserves local neighborhood structure of the x when the Frobenius norm of U is bounded. Proposition 1 If kU k F is bounded, then the Gaussian reconstruction error, kx Uh x k 2 is a surrogate for Euclidean neighborhood preservation. Proof Given two points x, y 2 R d , their Euclidean distance is bounded by kx yk 2</p><formula xml:id="formula_4">= k(x U &gt; h x ) (y U &gt; h y ) + (U &gt; h x U &gt; h y )k 2 6 kx U &gt; h x k 2 + ky U &gt; h y k 2 + kU &gt; (h x h y )k 2 6 kx U &gt; h x k 2 + ky U &gt; h y k 2 + kU k F kh x h y k 2 where h</formula><p>x and h y denote the binary latent variables corresponding to x and y, respectively. Therefore, we have</p><formula xml:id="formula_5">kx yk 2 kU k F kh x h y k 2 6 kx U &gt; h x k 2 +ky U &gt; h</formula><p>y k 2 which means minimizing the Gaussian reconstruction error, i.e., log p(x|h), will lead to Euclidean neighborhood preservation.</p><p>A similar argument can be made with respect to MIPS neighborhood preservation as shown in the supplementary material D. Note that the choice of p(x|h) is not unique, and any generative model that leads to neighborhood preservation can be used here. In fact, one can even use more sophisticated models with multiple layers and nonlinear functions. In our experiments, we find complex generative models tend to perform similarly to the Gaussian model on datasets such as SIFT-1M and GIST-1M. Therefore, we use the Gaussian model for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Encoding model q(h|x)</head><p>Even with the simple Gaussian model (1), computing the posterior</p><formula xml:id="formula_6">p(h|x) = p(x,h) p(x)</formula><p>is not tractable, and finding the MAP solution of the posterior involves solving an expensive integer programming subproblem. Inspired by the recent work on variational auto-encoder <ref type="bibr" target="#b25">(Kingma and Welling, 2013;</ref>, we propose to bypass these difficulties by parameterizing the encoding function as</p><formula xml:id="formula_7">q(h|x) = l Y k=1 q(h k = 1|x) h k q(h k = 0|x) 1 h k ,<label>(3)</label></formula><p>to approximate the exact posterior p(h|x). With the linear</p><formula xml:id="formula_8">parametrization, h = [h k ] l k=1 ⇠ B( (W &gt; x)) with W = [w k ] l k=1</formula><p>. At the training step, a hash code is obtained by sampling from B( (W &gt; x)). At the inference step, it is still possible to sample h. More directly, the MAP solution of the encoding function (3) is readily given by</p><formula xml:id="formula_9">h(x) = argmax h q(h|x) = sign(W &gt; x) + 1</formula><p>2 This involves only a linear projection followed by a sign operation, which is common in the hashing literature. Computing h(x) in our model thus has the same amount of computation as ITQ <ref type="bibr" target="#b9">(Gong and Lazebnik, 2011)</ref>, except without the orthogonality constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Training Objective</head><p>Since our goal is to reconstruct x using the least information in binary codes, we train the variational auto-encoder using the Minimal Description Length (MDL) principle, which finds the best parameters that maximally compress the training data. The MDL principle seeks to minimize the expected amount of information to communicate x:</p><formula xml:id="formula_10">L(x) = X h q(h|x)(L(h) + L(x|h))</formula><p>where L(h) = log p(h) + log q(h|x) is the description length of the hashed representation h and L(x|h) = log p(x|h) is the description length of x having already communicated h in (Hinton and <ref type="bibr" target="#b18">Van Camp, 1993;</ref><ref type="bibr" target="#b19">Hinton and Zemel, 1994;</ref>. By summing over all training examples x, we obtain the following training objective, which we wish to minimize with respect to the parameters of p(x|h) and q(h|x):</p><formula xml:id="formula_11">min ⇥={W,U, ,⇢} H(⇥) := X x L(x; ⇥) = X x X h q(h|x)(log p(x, h) log q(h|x)),<label>(4)</label></formula><p>where U, ⇢ and := log ✓ 1 ✓ are parameters of the generative model p(x, h) as defined in <ref type="formula" target="#formula_0">(1)</ref>, and W comes from the encoding function q(h|x) defined in (3). This objective is sometimes called Helmholtz (variational) free energy <ref type="bibr" target="#b38">(Williams, 1980;</ref><ref type="bibr" target="#b40">Zellner, 1988;</ref><ref type="bibr" target="#b5">Dai et al., 2016)</ref>. When the true posterior p(h|x) falls into the family of (3), q(h|x) becomes the true posterior p(h|x), which leads to the shortest description length to represent x.</p><p>We emphasize that this objective no longer includes binary variables h as parameters and therefore avoids optimizing with discrete variables directly. This paves the way for continuous optimization methods such as stochastic gradient descent (SGD) to be applied in training. As far as we are aware, this is the first time such a procedure has been used in the problem of unsupervised learning to hash. Our methodology serves as a viable alternative to the relaxation-based approaches commonly used in the past.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Reparametrization via Stochastic Neuron</head><p>Using the training objective of (4), we can directly compute the gradients w.r.t. parameters of p(x|h). However, we cannot compute the stochastic gradients w.r.t. W because it depends on the stochastic binary variables h. In order to back-propagate through stochastic nodes of h, two possible solutions have been proposed. First, the reparametrization trick (Kingma and Welling, 2013) which works by introducing auxiliary noise variables in the model. However, it is difficult to apply when the stochastic variables are discrete, as is the case for h in our model. On the other hand, the gradient estimators based on REINFORCE trick <ref type="bibr" target="#b1">(Bengio et al., 2013)</ref> suffer from high variance. Although some variance reduction remedies have been proposed <ref type="bibr" target="#b14">Gu et al., 2015)</ref>, they are either biased or require complicated extra computation in practice.</p><p>In next section, we first provide an unbiased estimator of the gradient w.r.t. W derived based on distributional derivative, and then, we derive a simple and efficient approximator. Before we derive the estimator, we first introduce the stochastic neuron for reparametrizing Bernoulli distribution. A stochastic neuron reparameterizes each Bernoulli variable h k (z) with z 2 (0, 1). Introducing random variables ⇠ ⇠ U(0, 1), the stochastic neuron is defined ash</p><formula xml:id="formula_12">(z, ⇠) := ( 1 if z &gt; ⇠ 0 if z &lt; ⇠ .<label>(5)</label></formula><p>Because P(h(z, ⇠) = 1) = z, we haveh(z, ⇠) ⇠ B(z). We use the stochastic neuron (5) to reparameterize our binary variables h by replacing</p><formula xml:id="formula_13">[h k ] l k=1 (x) ⇠ B( (w &gt; k x)) with [h k ( (w &gt; k x), ⇠ k )] l k=1</formula><p>. Note thath now behaves deterministically given ⇠. This gives us the reparameterized version of our original training objective (4):</p><formula xml:id="formula_14">H(⇥) = X xH (⇥; x) := X x E ⇠ h`(h , x) i ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_15">`(h, x) := log p(x,h( (W &gt; x), ⇠)) + log q(h( (W &gt; x), ⇠)|x) with ⇠ ⇠ U(0, 1).</formula><p>With such a reformulation, the new objective can now be optimized by exploiting the distributional stochastic gradient descent, which will be explained in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Distributional Stochastic Gradient Descent</head><p>For the objective in (6), given a point x randomly sampled from {x i } N i=1 , the stochastic gradient b r U, ,⇢H (⇥; x) can be easily computed in the standard way. However, with the reparameterization, the functionH(⇥; x) is no longer differentiable with respect to W due to the discontinuity of the stochastic neuronh(z, ⇠). Namely, the SGD algorithm is not readily applicable. To overcome this difficulty, we will adopt the notion of distributional derivative for generalized functions or distributions <ref type="bibr" target="#b13">(Grubb, 2008)</ref>.  <ref type="formula" target="#formula_23">(8)</ref> and <ref type="formula" target="#formula_0">(10)</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Distributional derivative of Stochastic Neuron</head><formula xml:id="formula_16">Let ⌦ ⇢ R d be an open set. Denote C 1 0 (⌦)</formula><formula xml:id="formula_17">Sample x i uniformly from {x i } N i=1 . 4: Sample ⇠ i ⇠ U([0, 1] l ). 5: Compute stochastic gradients b r ⇥H (⇥ i ; x i ) or b r ⇥H (⇥ i ; x i ), defined in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Update parameters as We emphasize this definition of distributions is more general than that of traditional probability distributions.</p><formula xml:id="formula_18">⇥ i+1 = ⇥ i i b r ⇥H (⇥ i ; x i ), or ⇥ i+1 = ⇥ i i b r ⇥H (⇥ i ; x i ),</formula><formula xml:id="formula_19">Definition 2 (Distributional derivative) (Grubb, 2008) Let u 2 D 0 (⌦), then a distribution v is called the distri- butional derivative of u, denoted as v = Du, if it satisfies Z ⌦ v dx = Z ⌦ u@ dx, 8 2 C 1 0 (⌦).</formula><p>It is straightforward to verify that for given ⇠, the func-</p><formula xml:id="formula_20">tionh(z, ⇠) 2 D 0 (⌦) and moreover, D zh (z, ⇠) = ⇠ (z)</formula><p>, which is exactly the Dirac-function. Based on the definition of distributional derivatives and chain rules, we are able to compute the distributional derivative of the functioñ H(⇥; x), which is provided in the following lemma.</p><p>Lemma 3 For a given sample x, the distributional derivative of functionH(⇥; x) w.r.t. W is given by</p><formula xml:id="formula_21">D WH (⇥; x) = (7) E ⇠ h h`(h ( (W &gt; x), ⇠)) (W &gt; x) • (1 (W &gt; x))x &gt; i</formula><p>where • denotes point-wise product and h`(h ) denotes the finite difference defined as</p><formula xml:id="formula_22">h h`(h ) i k =`(h 1 k ) `(h 0 k ), where [h i k ] l =h l if k 6 = l, otherwise [h i k ] l = i, i 2 {0, 1}</formula><p>. We can therefore combine distributional derivative estimators (7) with stochastic gradient descent algorithm (see e.g., <ref type="bibr" target="#b32">(Nemirovski et al., 2009</ref>) and its variants <ref type="bibr" target="#b24">(Kingma and Ba, 2014;</ref><ref type="bibr" target="#b2">Bottou et al., 2016)</ref>), which we designate as Distributional SGD. The detail is presented in Algorithm 1, where we denote</p><formula xml:id="formula_23">b r ⇥H (⇥ i ; x i ) = h b D WH (⇥ i ; x i ), b r U, ,⇢H (⇥ i ; x i ) i<label>(8)</label></formula><p>as the unbiased stochastic estimator of the gradient at ⇥ i constructed by sample x i , ⇠ i . Compared to the existing algorithms for learning to hash which require substantial effort on optimizing over binary variables, e.g., (CarreiraPerpinán and Raziperchikolaei, 2015), the proposed distri-butional SGD is much simpler and also amenable to online settings <ref type="bibr" target="#b20">(Huang et al., 2013;</ref><ref type="bibr" target="#b27">Leng et al., 2015)</ref>.</p><p>In general, the distributional derivative estimator (7) requires two forward passes of the model for each dimension. To further accelerate the computation, we approximate the distributional derivative D WH (⇥; x) by exploiting the mean value theorem and Taylor expansion bỹ</p><formula xml:id="formula_24">D WH (⇥; x) := (9) E ⇠ h rh`(h( (W &gt; x), ⇠)) (W &gt; x) • (1 (W &gt; x))x &gt; i ,</formula><p>which can be computed for each dimension in one pass. Then, we can exploit this estimator</p><formula xml:id="formula_25">b r ⇥H (⇥ i ; x i ) = h b D WH (⇥ i ; x i ), b r U, ,⇢H (⇥ i ; x i ) i<label>(10)</label></formula><p>in Algorithm 1. Interestingly, the approximate stochastic gradient estimator of the stochastic neuron we established through the distributional derivative coincides with the heuristic "pseudo-gradient" constructed <ref type="bibr" target="#b33">(Raiko et al., 2014)</ref>. Please refer to the supplementary material A for details for the derivation of the approximate gradient estimator (9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Convergence of Distributional SGD</head><p>One caveat here is that due to the potential discrepancy of the distributional derivative and the traditional gradient, whether the distributional derivative is still a descent direction and whether the SGD algorithm integrated with distributional derivative converges or not remains unclear in general. However, for our learning to hash problem, one can easily show that the distributional derivative in (7) is indeed the true gradient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 4</head><p>The distributional derivative D WH (⇥; x) is equivalent to the traditional gradient r W H(⇥; x). Proof First of all, by definition, we haveH(⇥; x) = H(⇥; x). One can easily verify that under mild condition, both D WH (⇥; x) and r W H(⇥; x) are continuous and 1-norm bounded. Hence, it suffices to show that for any distribution u 2 C Consequently, the distributional SGD algorithm enjoys the same convergence property as the traditional SGD algorithm. Applying theorem 2.1 in <ref type="bibr" target="#b7">(Ghadimi and Lan, 2013)</ref>, we arrive at Theorem 5 Under the assumption that H is L-Lipschitz smooth and the variance of the stochastic distributional gradient (8) is bounded by 2 in the distributional SGD, for the solution ⇥ R sampled from the trajectory</p><formula xml:id="formula_26">{⇥ i } t i=1</formula><p>with probability</p><formula xml:id="formula_27">P (R = i) = 2 i L 2 i P t i=1 2 i L 2 i where i ⇠ O 1/ p t , we have E  r⇥H(⇥R) 2 ⇠ O ✓ 1 p t ◆ .</formula><p>We emphasize that although the estimator proposed in (7) and the REINFORCE gradient estimator are both unbiased, the latter is known to suffer from high variance. Hence, our algorithm is expected to converge faster even without extra variance reduction techniques, e.g., <ref type="bibr" target="#b14">Gu et al., 2015)</ref>.</p><p>In fact, even with the approximate gradient estimators (9), the proposed distributional SGD is also converging in terms of first-order conditions. For the detailed proof of theorem 5 and the convergence with approximate distributional derivative, please refer to the supplementary material B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Connections</head><p>The proposed stochastic generative hashing is a general framework. In this section, we reveal the connection to several existing algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iterative Quantization (ITQ).</head><p>If we fix some ⇢ &gt; 0, and U = W R where W is formed by eigenvectors of the covariance matrix and R is an orthogonal matrix, we have U &gt; U = I. If we assume the joint distribution as</p><formula xml:id="formula_28">p(x, h) / N (W Rh, ⇢ 2 I)B(✓),</formula><p>and parametrize q(h|x i ) = bi (h), then from the objective in (4) and ignoring the irrelevant terms, we obtain the optimization</p><formula xml:id="formula_29">min R,b N X i=1 kx i W Rb i k 2 ,<label>(11)</label></formula><p>which is exactly the objective of iterative quantization <ref type="bibr" target="#b9">(Gong and Lazebnik, 2011)</ref>.</p><p>Binary Autoencoder (BA). If we use the deterministic linear encoding function, i.e., q(h|x) = 1+sign(W &gt; x) 2 (h), and prefix some ⇢ &gt; 0, and ignore the irrelevant terms, the optimization (4) reduces to</p><formula xml:id="formula_30">min U,W N X i=1 x i Uh 2 , s.t. h = 1 + sign(W &gt; x) 2 ,<label>(12)</label></formula><p>which is the objective of a binary autoencoder <ref type="bibr">(CarreiraPerpinán and Raziperchikolaei, 2015)</ref>.</p><p>In BA, the encoding procedure is deterministic, therefore, the entropy term E q(h|x) [log q(h|x)] = 0. In fact, the entropy term, if non-zero, performs like a regularization and helps to avoid wasting bits. Moreover, without the stochasticity, the optimization (12) becomes extremely difficult due to the binary constraints. While for the proposed algorithm, we exploit the stochasticity to bypass such difficulty in optimization. The stochasticity enables us to accelerate the optimization as shown in section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we evaluate the performance of the proposed distributional SGD on commonly used datasets in hashing. Due to the efficiency consideration, we conduct the experiments mainly with the approximate gradient estimator (9). We evaluate the model and algorithm from several aspects to demonstrate the power of the proposed SGH: (1) Reconstruction loss. To demonstrate the flexibility of generative modeling, we compare the L2 reconstruction error to that of ITQ <ref type="bibr" target="#b9">(Gong and Lazebnik, 2011)</ref>, showing the benefits of modeling without the orthogonality constraints. (2) Nearest neighbor retrieval. We show Recall K@N plots on standard large scale nearest neighbor search benchmark datasets of MNIST, SIFT-1M, GIST-1M and SIFT-1B, for all of which we achieve state-of-the-art among binary hashing methods. (3) Convergence of the distributional SGD. We evaluate the reconstruction error showing that the proposed algorithm indeed converges, verifying the theorems. (4) Training time. The existing generative works require a significant amount of time for training the model. In contrast, our SGD algorithm is very fast to train both in terms of number of examples needed and the wall time. (5) Reconstruction visualization. Due to the generative nature of our model, we can regenerate the original input with very few bits. On MNIST and CIFAR10, we qualitatively illustrate the templates that correspond to each bit and the resulting reconstruction.</p><p>We used several benchmarks datasets, i.e., (1) MNIST which contains 60,000 digit images of size 28 ⇥ 28 pixels, (2) CIFAR-10 which contains 60,000 32 ⇥ 32 pixel color images in 10 classes, (3) SIFT-1M and (4) SIFT-1B which contain 10 6 and 10 9 samples, each of which is a 128 dimensional vector, and (5) GIST-1M which contains 10 6 samples, each of which is a 960 dimensional vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Reconstruction loss</head><p>Because our method has a generative model p(x|h), we can easily compute the regenerated inputx = argmax p(x|h), and then compute the L 2 loss of the regenerated input and the original x, i.e., kx xk 2 2 . ITQ also trains by minimizing the binary quantization loss, as described in Equation <ref type="formula" target="#formula_3">(2)</ref> in <ref type="bibr" target="#b9">(Gong and Lazebnik, 2011)</ref>, which is essentially L 2 reconstruction loss when the magnitude of the feature vectors is compatible with the radius of the binary cube. We plotted the L 2 reconstruction loss of our method and ITQ on SIFT-1M in <ref type="figure" target="#fig_1">Figure 1</ref>(a) and on MNIST and GIST-1M in <ref type="figure">Figure 4</ref>, where the x-axis indicates the number of examples seen by the training algorithm and the y-axis shows the average L 2 reconstruction loss. The training time comparison is listed in <ref type="table" target="#tab_1">Table 1</ref>. Our method (SGH) arrives at a better reconstruction loss with comparable or even less time compared to ITQ. The lower reconstruction loss demonstrates our claim that the flexibility of the proposed  model afforded by removing the orthogonality constraints indeed brings extra modeling ability. Note that ITQ is generally regarded as a technique with fast training among the existing binary hashing algorithms, and most other algorithms <ref type="bibr" target="#b16">(He et al., 2013;</ref><ref type="bibr" target="#b17">Heo et al., 2012;</ref><ref type="bibr" target="#b3">Carreira-Perpinán and Raziperchikolaei, 2015)</ref> take much more time to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Large scale nearest neighbor retrieval</head><p>We compared the stochastic generative hashing on an L2NNS task with several state-of-the-art unsupervised algorithms, including K-means hashing (KMH) <ref type="bibr" target="#b16">(He et al., 2013)</ref>, iterative quantization (ITQ) <ref type="bibr" target="#b9">(Gong and Lazebnik, 2011)</ref>, spectral hashing (SH) <ref type="bibr" target="#b37">(Weiss et al., 2009)</ref>, spherical hashing (SpH) <ref type="bibr" target="#b17">(Heo et al., 2012)</ref>, binary autoencoder (BA) <ref type="bibr" target="#b3">(Carreira-Perpinán and Raziperchikolaei, 2015)</ref>, and scalable graph hashing (GH) <ref type="bibr" target="#b23">(Jiang and Li, 2015)</ref>. We demonstrate the performance of our binary codes by doing standard benchmark experiments of Approximate Nearest Neighbor (ANN) search by comparing the retrieval recall. In particular, we compare with other unsupervised techniques that also generate binary codes. For each query, linear search in Hamming space is conducted to find the approximate neighbors.</p><p>Following the experimental setting of <ref type="bibr" target="#b16">(He et al., 2013)</ref>, we plot the Recall10@N curve for MNIST, SIFT-1M, GIST-1M, and SIFT-1B datasets under varying number of bits <ref type="bibr">(16, 32 and 64)</ref> in <ref type="figure" target="#fig_3">Figure 2</ref>. On the SIFT-1B datasets, we only compared with ITQ since the training cost of the other competitors is prohibitive. The recall is defined as the fraction of retrieved true nearest neighbors to the total number of true nearest neighbors. The Recall10@N is the recall of 10 ground truth neighbors in the N retrieved samples. Note that Recall10@N is generally a more chal- lenging criteria than Recall@N (which is essentially Recall1@N), and better characterizes the retrieval results. For completeness, results of various Recall K@N curves can be found in the supplementary material which show similar trend as the Recall10@N curves. <ref type="figure" target="#fig_3">Figure 2</ref> shows that the proposed SGH consistently performs the best across all bit settings and all datasets. The searching time is the same, because all algorithms use the same optimized implementation of POPCNT based Hamming distance computation and priority queue. We point out that many of the baselines need significant parameter tuning for each experiment to achieve a reasonable recall, except for ITQ and our method, where we fix hyperparameters for all our experiments and used a batch size of 500 and learning rate of 0.01 with stepsize decay. Our method is less sensitive to hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Empirical study of Distributional SGD</head><p>We demonstrate the convergence of the Adam (Kingma and Ba, 2014) with distributional derivative numerically on SIFT-1M, GIST-1M and MINST from 8 bits to 64 bits. The convergence curves on SIFT-1M are shown in <ref type="figure" target="#fig_1">Figure 1 (a)</ref>. The results on GIST-1M and MNIST are similar and shown in <ref type="figure">Figure 4</ref> in supplementary material C. Obviously, the proposed algorithm converges quickly, no matter how many bits are used. It is reasonable that with more bits, the model fits the data better and the reconstruction error can be reduced further.</p><p>In line with the expectation, our distributional SGD trains much faster since it bypasses integer programming. We benchmark the actual time taken to train our method to convergence and compare that to binary autoencoder hashing (BA) (Carreira-Perpinán and Raziperchikolaei, 2015) on SIFT-1M, GIST-1M and MINST. We illustrate the performance on SIFT-1M in <ref type="figure" target="#fig_1">Figure 1(b)</ref> . The results on GIST-1M and MNIST datasets follow a similar trend as shown in the supplementary material C. Empirically, BA takes significantly more time to train on all bit settings due to the expensive cost for solving integer programming subproblem. Our experiments were run on AMD 2.4GHz Opteron CPUs⇥4 and 32G memory. Our implementation of stochastic generative hashing as well as the whole training procedure was done in TensorFlow. We have released our code on GitHub 2 . For the competing methods, we di- The SGH reconstruction tends to be much better than that of ITQ, and is on par with PCA which uses 32 times more bits! rectly used the code released by the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Visualization of reconstruction</head><p>One important aspect of utilizing a generative model for a hash function is that one can generate the input from its hash code. When the inputs are images, this corresponds to image generation, which allows us to visually inspect what the hash bits encode, as well as the differences in the original and generated images.</p><p>In our experiments on MNIST and CIFAR-10, we first visualize the "template" which corresponds to each hash bit, i.e., each column of the decoding dictionary U . This gives an interesting insight into what each hash bit represents. Unlike PCA components, where the top few look like averaged images and the rest are high frequency noise, each of our image template encodes distinct information and looks much like filter banks of convolution neural networks. Empirically, each template also looks quite different and encodes somewhat meaningful information, indicating that no bits are wasted or duplicated. Note that we obtain this representation as a by-product, without explicitly setting up the model with supervised information, similar to the case in convolution neural nets.</p><p>We also compare the reconstruction ability of SGH with the that of ITQ and real valued PCA in <ref type="figure">Figure 3</ref>. For ITQ and SGH, we use a 64-bit hash code. For PCA, we kept 64 components, which amounts to 64 ⇥ 32 = 2048 bits. Visually comparing with SGH, ITQ reconstructed images look much less recognizable on MNIST and much more blurry on CIFAR-10. Compared to PCA, SGH achieves similar visual quality while using a significantly lower (32⇥ less) number of bits!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have proposed a novel generative approach to learn binary hash functions. We have justified from a theoretical angle that the proposed algorithm is able to provide a good hash function that preserves Euclidean neighborhoods, while achieving fast learning and retrieval. Extensive experimental results justify the flexibility of our model, especially in reconstructing the input from the hash codes. Comparisons with approximate nearest neighbor search over several benchmarks demonstrate the advantage of the proposed algorithm empirically. We emphasize that the proposed generative hashing is a general framework which can be extended to semi-supervised settings and other learning to hash scenarios as detailed in the supplementary material. Moreover, the proposed distributional SGD with the unbiased gradient estimator and its approximator can be applied to general integer programming problems, which may be of independent interest.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>⌦), which can be considered as the dual space. The elements in space D 0 (⌦) are often called gen- eral distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 (</head><label>1</label><figDesc>⌦) and Du, ru 2 L 1 (⌦), Du = ru. For any 2 C 1 0 (⌦), by definition of the distributional deriva- tive, we have R ⌦ Du dx = R ⌦ u@ dx. On the other hand, we always have R ⌦ ru dx = R u@ dx. Hence, R ⌦ (Du ru) dx = 0 for all 2 C 1 0 (⌦). By the Du Bois- Reymond's lemma (see Lemma 3.2 in (Grubb, 2008)), we have Du = ru.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Convergence of reconstruction error with number of samples seen by SGD, and (b) training time comparison of BA and SGH on SIFT-1M over the course of training with varying number of bits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: L2NNS comparison on MNIST, SIFT-1M, and GIST-1M and SIFT-1B with the length of binary codes varying from 16 to 64 bits. We evaluate the performance with Recall 10@M (fraction of top 10 ground truth neighbors in retrieved M), where M increases up to 1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( a ) 10 Figure 3 :</head><label>a103</label><figDesc>Figure 3: Illustration of MNIST and CIFAR-10 templates (left) and regenerated images (right) from different methods with 64 hidden binary variables. In MNIST, the four rows and their number of bits used to encode them are, from the top: (1) original image, 28 ⇥ 28 ⇥ 8 = 6272 bits; (2) PCA with 64 components 64 ⇥ 32 = 2048 bits; (3) SGH, 64 bits; (4) ITQ, 64 bits. In CIFAR : (1) original image, 30 ⇥ 30 ⇥ 24 = 21600 bits; (2) PCA with 64 components 64 ⇥ 32 = 2048 bits; (3) SGH, 64 bits; (4) ITQ, 64 bits. The SGH reconstruction tends to be much better than that of ITQ, and is on par with PCA which uses 32 times more bits!</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>as the space of the functions that are infinitely differentiable with compact</figDesc><table>Algorithm 1 Distributional-SGD 
Input: {x 
i } N 

i=1 

1: Initialize ⇥ 0 = {W, U, , ⇢} randomly. 
2: for i = 1, . . . , t do 

3: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Training time on SIFT-1M in second. 
Method 8 bits 16 bits 32 bits 64 bits 
SGH 
28.32 29.38 
37.28 
55.03 
ITQ 
92.82 121.73 173.65 259.13 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The proposed algorithm can be extended to supervised/semisupervised setting easily as described in the supplementary material E.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/doubling/Stochastic Generative Hashing</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>LS is supported in part by NSF IIS-1218749, NIH BIG-DATA 1R01GM108341, NSF CAREER IIS-1350983, NSF IIS-1639792 EAGER, ONR N00014-15-1-2340, NVIDIA, Intel and Amazon AWS.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Additive quantization for extreme vector compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">roceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Optimization methods for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nocedal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04838</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hashing with binary autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Miguel A Carreira-Perpinán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raziperchikolaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="557" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Similarity estimation techniques from rounding algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moses</forename><forename type="middle">S</forename><surname>Charikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thiry-fourth annual ACM symposium on Theory of computing</title>
		<meeting>the thiry-fourth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="380" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Provable bayesian inference via particle mirror descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 19th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="985" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Polysemous codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stochastic first-and zeroth-order methods for nonconvex stochastic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2341" to="2368" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristides</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Angular quantization-based binary codes for fast similarity search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning binary codes for highdimensional data using bilinear projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="484" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep autoregressive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 31st International Conference on Machine Learning</title>
		<meeting>The 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1242" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Distributions and operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerd</forename><surname>Grubb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">252</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Muprop</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05176</idno>
		<title level="m">Unbiased backpropagation for stochastic neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quantization based fast inner product search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Simcha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">K-means hashing: An affinity-preserving quantization method for learning binary compact codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2938" to="2945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Junfeng He, Shih-Fu Chang, and Sung-Eui Yoon. Spherical hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Pil</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwoon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2957" to="2964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Keeping the neural networks simple by minimizing the description length of the weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Camp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth annual conference on Computational learning theory</title>
		<meeting>the sixth annual conference on Computational learning theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="5" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autoencoders, minimum description length and helmholtz free energy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Online hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long-Kai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third international joint conference on Artificial Intelligence</title>
		<meeting>the Twenty-Third international joint conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1422" to="1428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbors: towards removing the curse of dimensionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirtieth annual ACM symposium on Theory of computing</title>
		<meeting>the thirtieth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scalable Graph Hashing with Feature Transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing-Yuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu-Jun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Stochastic Generative Hashing Diederik Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Online sketching hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2503" to="2511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hashing with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discrete graph hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cun</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling pixel means and covariances using factorized third-order boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranzato</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename><surname>Aurelio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2551" to="2558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Neural variational inference and learning in belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.0030</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust stochastic approximation approach to stochastic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkadi</forename><surname>Nemirovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoli</forename><surname>Juditsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on optimization</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1574" to="1609" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2989</idno>
		<title level="m">Techniques for learning binary stochastic feedforward neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning binary codes for maximum inner product search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4148" to="4156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semisupervised hashing for scalable image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Hashing for similarity search: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Heng Tao Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.2927</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spectral hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1753" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bayesian conditionalisation and the principle of minimum information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal for the Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="144" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Circulant binary embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Optimal Information Processing and Bayes&apos;s Theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><surname>Zellner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1988-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Supervised hashing with latent factor models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu-Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</title>
		<meeting>the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Composite quantization for approximate nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="838" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep hashing network for efficient similarity retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
