<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Lovász-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
							<email>maxim.berman@esat.kuleuven.be</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. ESAT</orgName>
								<orgName type="department" key="dep2">Center for Processing Speech and Images KU</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amal</forename><surname>Rannen</surname></persName>
							<email>amal.rannen@esat.kuleuven.be</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. ESAT</orgName>
								<orgName type="department" key="dep2">Center for Processing Speech and Images KU</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triki</forename><surname>Matthew</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. ESAT</orgName>
								<orgName type="department" key="dep2">Center for Processing Speech and Images KU</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Blaschko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. ESAT</orgName>
								<orgName type="department" key="dep2">Center for Processing Speech and Images KU</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Lovász-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The Jaccard index, also referred to as the intersectionover-union score, is commonly   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We consider the task of semantic image segmentation, where each pixel i of a given image has to be classified into an object class c ∈ C. Most of the deep network based segmentation methods rely on logistic regression, optimizing the cross-entropy loss <ref type="bibr" target="#b9">[10]</ref> loss(f ) = − 1 p</p><formula xml:id="formula_0">p i=1 log f i (y * i ),<label>(1)</label></formula><p>with p the number of pixels in the image or minibatch considered, y * i ∈ C the ground truth class of pixel i, f i (y * i ) the network probability estimate of the ground truth probability of pixel i, and f a vector of all network outputs f i (c). This supposes that the unnormalized scores F i (c) of the network have been mapped to probabilities through a softmax unit </p><formula xml:id="formula_1">f i (c) = e</formula><p>Loss <ref type="formula" target="#formula_0">(1)</ref> generalizes the logistic loss and leads to smooth optimization. During testing, the decision function commonly used consists in picking the class of maximum score: the predicted class for a given pixel i isỹ i = arg max c∈C F i (c).</p><p>The measure of the cross-entropy loss on a validation set is often a poor indicator of the quality of the segmentation. A better performance measure commonly used for evaluating segmentation masks is the Jaccard index, also called the intersection-over-union (IoU) score. Given a vector of ground truth labels y * and a vector of predicted labelsỹ, the Jaccard index of class c is defined as <ref type="bibr" target="#b13">[14]</ref> J c (y * ,ỹ) = |{y * = c} ∩ {ỹ = c}| |{y * = c} ∪ {ỹ = c}| ,</p><p>which gives the ratio in [0, 1] of the intersection between the ground truth mask and the evaluated mask over their union, with the convention that 0/0 = 1. A corresponding loss function to be employed in empirical risk minimization is ∆ Jc (y * ,ỹ) = 1 − J c (y * ,ỹ).</p><p>For multilabel datasets, the Jaccard index is commonly averaged across classes, yielding the mean IoU (mIoU). We develop here a method for optimizing the performance of a discriminatively trained segmentation system with respect to the Jaccard index. We show that a piecewise linear convex surrogate to the Jaccard loss based on the Lovász extension of submodular set functions yields a consistent improvement of predicted segmentation masks as measured by the Jaccard index.</p><p>Although the Jaccard index is often computed globally, over every pixel of the evaluated segmentation dataset <ref type="bibr" target="#b7">[8]</ref>, it can also be computed independently for each image. Using the per-image Jaccard index is known to have better perceptual accuracy by reducing the bias towards large instances of the object classes in the dataset <ref type="bibr" target="#b5">[6]</ref>. Due to these favorable properties, and the empirical risk minimization principle of optimizing the loss of interest at training time <ref type="bibr" target="#b24">[25]</ref>, optimization of the Jaccard loss during training has been frequently considered in the literature. However, in contrast to the present work, existing methods all have significant shortcomings that do not allow plug-and-play application to a wide range of learning architectures.</p><p>[20] provides a Bayesian framework for optimization of the Jaccard index. The author proposes an approximate algorithm using parametric linear programming to optimize a statistical approximation to the objective. <ref type="bibr" target="#b0">[1]</ref> optimize IoU by selecting among a few candidate segmentations, instead of directly optimizing the model with respect to the loss. <ref type="bibr" target="#b2">[3]</ref> optimize the Jaccard loss in a structured output SVM, but are only able to do so with a branch-and-bound optimization over bounding boxes and not full segmentations.</p><p>Alternative approaches train binary classifiers, but on data that are sampled to capture high Jaccard index. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref> use IoU and related overlap measures to define training sets for binary classifiers in a complex multi-stage training. Such sampling-based approaches clearly induce suboptimality in the empirical risk approximation and do not lend themselves to convenient modular application in a deep learning setting.</p><p>Still other recent high-impact research has highlighted the need for optimization of the Jaccard index, but resort to binary training as a proxy, presumably for lack of a convenient and flexible method of directly optimizing the loss of interest. <ref type="bibr" target="#b17">[18]</ref> train with logistic loss and test with the Jaccard index. The paper introducing the highly influential OverFeat network specifically addresses the shortcoming in the discussion section <ref type="bibr" target="#b22">[23]</ref>: "We are using ℓ 2 loss, rather than directly optimizing the intersection-over-union (IoU) criterion on which performance is measured. Swapping the loss to this should be possible...." However, this is left to future work. In this paper, we develop the necessary plug-and-play loss layer to enable flexible direct minimization of the Jaccard loss in a deep learning setting, while demonstrating its applicability for training state-of-the-art image segmentation networks.</p><p>Our approach is based on the recent development of general strategies for generating convex surrogates to submodular loss functions, including the Lovász hinge <ref type="bibr" target="#b25">[26]</ref>. Based on the result that the Jaccard loss is submodular, this strategy is directly applicable. We moreover generalize this approach to a multiclass setting by considering a regression-based variant, using a softmax activation layer to naturally map network probability estimates to the Lovász extension of the Jaccard loss. In this work, we (i) apply the Lovász hinge with Jaccard loss to the problem of binary image segmentation (Sec. 2.1), (ii) propose a surrogate for the multi-class setting, the Lovász-Softmax loss (Sec. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Optimization surrogates for submodular loss functions</head><p>In order to optimize the Jaccard index in a continuous optimization framework, we consider smooth extensions of this discrete loss. The extensions are based on submodular analysis of set functions, where the set function maps from a set of mispredictions to the set of real numbers <ref type="bibr" target="#b25">[26,</ref><ref type="bibr">Equation (6)</ref>].</p><p>For a segmentation outputỹ and ground truth y * , we define the set of mispredicted pixels for class c as</p><formula xml:id="formula_5">M c (y * ,ỹ) = {y * = c,ỹ = c} ∪ {y * = c,ỹ = c}. (5)</formula><p>For a fixed ground truth y * , the Jaccard loss in Eq. (4) can be rewritten as a function of the set of mispredictions</p><formula xml:id="formula_6">∆ Jc : M c ∈ {0, 1} p → |M c | |{y * = c} ∪ M c | .<label>(6)</label></formula><p>Note that for ease of notation, we naturally identify subsets of pixels with their indicator vector in the discrete hypercube {0, 1} p . In a continuous optimization setting, we want to assign a loss to any vector of errors m ∈ R p + , and not only to discrete vectors of mispredictions in {0, 1} p . A natural candidate for this loss is the convex closure of function <ref type="bibr" target="#b5">(6)</ref> in R p . In general, computing the convex closure of set functions is NP-hard. However, the Jaccard set function <ref type="bibr" target="#b5">(6)</ref> has been shown to be submodular <ref type="bibr" target="#b26">[27,</ref><ref type="bibr">Proposition 11]</ref>.</p><formula xml:id="formula_7">Definition 1 [9]. A set function ∆ : {0, 1} p → R is sub- modular if for all A, B ∈ {0, 1} p ∆(A) + ∆(B) ≥ ∆(A ∪ B) + ∆(A ∩ B).<label>(7)</label></formula><p>The convex closure of submodular set functions is tight and computable in polynomial time <ref type="bibr" target="#b18">[19]</ref>; it corresponds to its Lovász extension. Let ∆ be a set function encoding a submodular loss such as the Jaccard loss defined in Equation <ref type="bibr" target="#b5">(6)</ref>. By submodularity ∆ is the tight convex closure of ∆ <ref type="bibr" target="#b18">[19]</ref>. ∆ is piecewise linear and interpolates the values of ∆ in R p \ {0, 1} p , while having the same values as ∆ on {0, 1} p , i.e. on any set of mispredictions (Equation <ref type="formula">(5)</ref>). Intuitively, if m is a vector of all pixel errors, ∆(m) is a sum weighting these errors according to the interpolated discrete loss. By its convexity and continuity, ∆ is a natural surrogate for the minimization of ∆ with first-order continuous optimization, such as in neural networks. The elementary operations involved to compute ∆ (sort, dot product, . . . ) are differentiable and implemented on GPU in current deep learning frameworks. The vector g(m) of which the components are defined in Equation <ref type="formula" target="#formula_9">(9)</ref> directly corresponds to the derivative of ∆ with respect to m.</p><formula xml:id="formula_8">Definition 2 [2, Def. 3.1]. The Lovász extension of a set function ∆ : {0, 1} p → R such that ∆(0) = 0 is defined by ∆ : m ∈ R p → p i=1 m i g i (m)<label>(8)</label></formula><formula xml:id="formula_9">with g i (m) = ∆({π 1 , . . . , π i }) − ∆({π 1 , . . . , π i−1 }),<label>(9)</label></formula><p>In the following, we consider two different settings in which we construct surrogate losses by using the Lovász extension and specifying the vector of errors m that we use:</p><p>1. The foreground-background segmentation problem, which leads to the Lovász hinge, as described in <ref type="bibr" target="#b26">[27]</ref>;</p><p>2. The multiclass segmentation problem, which leads to the Lovász-Softmax loss, incorporating the softmax operation in the Lovász extension. In the binary case, we consider the optimization of the Jaccard index for the foreground class ∆ J1 . We use a maxmargin classifier: for an image x, we define</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Foreground-background segmentation</head><formula xml:id="formula_10">(a) GT = [−1, −1] (b) GT = [−1, 1] (c) GT = [1, −1] (d) GT = [1, 1]</formula><p>• y * i ∈ {−1, 1} the ground truth label of pixel i, • F i (x) the i-th element of the output scores F of the model, such that the predicted labelỹ i = sign(F i (x)),</p><formula xml:id="formula_11">• m i = max(1 − F i (x) y * i , 0)</formula><p>the hinge loss associated with the prediction of pixel i.</p><p>In this setting, the vector of hinge losses m ∈ R + is the vectors of errors discussed before. With ∆ J1 the Lovász extension to ∆ J1 , the resulting loss surrogate</p><formula xml:id="formula_12">loss(F ) = ∆ J1 (m(F ))<label>(10)</label></formula><p>is the Lovász hinge applied to the Jaccard loss, as described in <ref type="bibr" target="#b25">[26]</ref>. It is piecewise linear in the output scores F as a composition of piecewise linear functions. Moreover, by choice of the hinge loss for the vector m, the Lovász hinge reduces to the standard hinge loss <ref type="bibr" target="#b23">[24]</ref> in the case of a single prediction, or when using the Hamming distance instead of the Jaccard loss as a basis for the construction. <ref type="figure" target="#fig_2">Figure 1</ref> illustrates the extension of the Jaccard loss in the case of the prediction of two pixels, illustrating the convexity and the tightness of the surrogate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multiclass semantic segmentation</head><formula xml:id="formula_13">(a) GT = [−1, −1] (b) GT = [−1, 1] (c) GT = [1, −1] (d) GT = [1, 1]</formula><p>Figure 2: Lovász-Softmax for the foreground class, with two classes {−1, 1} and two pixels, for each ground truth labeling GT. The loss is plotted against the difference of unnormalized scores</p><formula xml:id="formula_14">d i = F i (y * i ) − F i (1 − y * i ) for i = 1, 2.</formula><p>In a segmentation setting with more than two classes, we propose a surrogate based on a logistic output instead of using a max-margin setting. Specifically we map the output scores of the model to probability distributions using a softmax unit as is done traditionally in the case of the cross-entropy loss.</p><p>We use the class probabilities f i (c) ∈ [0, 1] defined in Equation (2) to construct a vector of pixel errors m(c) for class c ∈ C defined by</p><formula xml:id="formula_15">m i (c) = 1 − f i (c) if c = y * i , f i (c) otherwise.<label>(11)</label></formula><p>We use the vector of errors m(c) ∈ [0, 1] p to construct the loss surrogate to ∆ Jc , the Jaccard index for class c:</p><formula xml:id="formula_16">loss(f (c)) = ∆ Jc (m(c))<label>(12)</label></formula><p>When considering the class-averaged mIoU metric, common in semantic segmentation, we average the class-specific surrogates; hence we define the Lovász-Softmax loss as</p><formula xml:id="formula_17">loss(f ) = 1 |C| c∈C ∆ Jc (m(c))<label>(13)</label></formula><p>which is piecewise linear in f , the normalized network outputs. <ref type="figure">Figure 2</ref> show this loss as a function of the unnormalized vector outputs F for a prediction of two pixels. In the limit of large scores (confident outputs), the probability vectors at each pixel (f i (c)) c∈C are close to an indicator vector, and we recover the values of the discrete Jaccard index for the corresponding discrete labeling with respect to the ground truth, as seen on the figure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Optimization of intersection over union</head><p>Naïve computation of the Lovász extension (Equation <ref type="formula" target="#formula_8">(8)</ref>) applied to ∆ Jc can be achieved by sorting the elements of m in O(p log p) time and doing O(p) calls to ∆ Jc . However, if we compute ∆ Jc by Equation <ref type="formula" target="#formula_3">(3)</ref>, each call will cost O(n). As π is known in advance, we may simply keep track of the cumulative number of false positives and negatives in {π 1 , . . . , π i } for increasing i yielding an amortized O(1) cost per evaluation of ∆ Jc (cf. [27, Equation <ref type="formula" target="#formula_3">(43)]</ref>). This computation also yields the gradient g(m) at the same computational cost. This is a powerful result implying that a tight surrogate function for the Jaccard loss is available and computable in time O(p log p). The algorithm for computing the gradient of the loss surface resulting from this procedure is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Gradient of the Jaccard loss extension ∆ Jc</head><p>Inputs: vector of errors m(c) ∈ R p + class foreground pixels δ = {y</p><formula xml:id="formula_18">* = c} ∈ {0, 1} p Output: g(m) gradient of ∆ Jc (Equation (9)) 1: π ← decreasing sort permutation for m 2: δ π ← (δ πi ) i∈[1,p] 3: intersection ← sum(δ) − cumulative sum(δ π ) 4: union ← sum(δ) + cumulative sum(1 − δ π ) 5: g ← 1 − intersection/union 6: if p &gt; 1 then 7: g[2 : p] ← g[2 : p] − g[1 : p − 1] 8: end if 9: return g π −1</formula><p>3.1. Image-mIoU vs. dataset-mIoU</p><p>The official metric of the semantic segmentation task in Pascal VOC <ref type="bibr" target="#b6">[7]</ref> and numerous other popular competitions is the dataset-mIoU,</p><formula xml:id="formula_19">dataset-mIoU = 1 |C| c∈C J c (y * ,ỹ),<label>(14)</label></formula><p>where y * andỹ contain the ground truth and predicted labels of all pixels in the testing dataset.</p><p>The Lovász-Softmax loss considers an ensemble of pixel predictions for the computation of the surrogate to the Jaccard loss. In a stochastic gradient descent setting, only a small numbers of pixel predictions are taken into account in one optimization step. Therefore, the Lovász-Softmax loss cannot directly optimize the dataset-mIoU. We can compute this loss over individual images, optimizing for the expected image-mIoU, or over each minibatch, optimizing for the expected batch-mIoU. However, it is not true in general that</p><formula xml:id="formula_20">E intersection union ≈ E(intersection) E(union) ,<label>(15)</label></formula><p>and we found in our experiments that optimizing the imagemIoU or batch-mIoU generally degrades the dataset-mIoU compared with optimizing the standard cross-entropy loss.</p><p>The main difference between the dataset and imagemIoU measures resides in the absent classes. When the network wrongly predicts a single pixel belonging to a class that is absent from an image, the image intersection over union loss corresponding to that class changes from 0 to 1. By contrast, a single pixel misprediction does not substantially affect the dataset-mIoU metric.</p><p>Given this insight, we propose as an heuristic for optimizing the dataset-mIoU to compute the batch Lovász-Softmax surrogate by taking the average in Equation <ref type="bibr" target="#b12">(13)</ref> only over the classes present in the batch's ground truth. As a result, the loss is more stable to single predictions in absent classes, mimicking the dataset-mIoU. As outlined in our experiments, the optimization of the Lovász-Softmax restricted to classes present in each batch, effectively translates into gains for the dataset-mIoU metric.</p><p>We propose an additional trick for the optimization of the dataset-mIoU. Since the mIoU gives equal importance to each class, and to make the expectation of the batch-mIoU closer to the dataset-mIoU, it seems important to ensure that we feed the network with samples from all classes during training. In order to enforce this requirement, we sample the patches from the training by cycling over every classes, such that each class is visited at least once every |C| patches. This method is referred to as equibatch in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Synthetic experiment</head><p>We demonstrate the relevance of using the Jaccard loss for binary segmentation with a synthetic binary image segmentation experiment. We generate N = 10 binary images of size 50 × 50 representing circles of various radius, and extract for each pixel i a single feature using a unit variance Gaussian perturbation of the ground truth,  <ref type="figure" target="#fig_3">Figure 3a</ref>.</p><p>We consider a model classifying pixels in the foreground class for f p &gt; −b, and we learn the bias term b. An exhaustive search, illustrated in <ref type="figure" target="#fig_3">Figure 3b</ref>, shows that among the losses considered, only the Lovász hinge efficiently captures the absolute minimum of the Jaccard loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Binary segmentation on Pascal VOC</head><p>We base our Pascal VOC experiments on the DeeplabV2-single-scale semantic segmentation network <ref type="bibr" target="#b15">[16]</ref>. The network uses a Resnet-101 <ref type="bibr" target="#b12">[13]</ref> based architecture, re-purposed for image segmentation, notably using dilated (or atrous) convolutions. We use the initialization weights provided by the authors. These weights were pre-trained on MS-COCO <ref type="bibr" target="#b16">[17]</ref> using cross-entropy loss and weight decay. We further fine-tune these weights on a segmentation dataset consisting of Pascal VOC 2012 training images <ref type="bibr" target="#b7">[8]</ref> and the extra images provided by <ref type="bibr" target="#b10">[11]</ref>, as is common in recent semantic image segmentation applications.</p><p>For our binary segmentation experiments, we perform an initial fine-tuning of the weights using cross-entropy loss alone jointly on the 21 classes of Pascal VOC (including the background class); this constitutes our basis network. We then turn to binary segmentation by selecting one particular class and finetune the output of the network for the selected class. In order to consider a realistic binary segmentation setting, for each class, we sample the validation set such that half of the images contain at least one foreground pixel. The training is done on random crops of size 321 × 321 extracted from the training set, with random scale and horizontal flipping. Training batches are randomly sampled from the training set such that half of the selected images contain the foreground class on average.</p><p>Our experiments revolve around the choice of the training loss during fine-tuning to binary segmentation. We do a fine-tuning of 2 epoch iterations, with an initial learning rate of 5 · 10 −4 , reduced to 1 · 10 −4 after 1 epoch.</p><p>Performance of the surrogate <ref type="table" target="#tab_1">Table 1</ref> shows the average of the losses considered after a training with different loss objectives. Evidently, training with a particular loss leads generally to a better objective value of this loss on the validation set. Moreover, we see that the Lovász hinge acts as a  good surrogate of the discrete image-IoU, leading to a better validation accuracy for this measure. <ref type="figure" target="#fig_4">Figure 4</ref> shows example binary segmentation mask outputs. We notice that the Jaccard loss tends to fill gaps in segmentation, recover small objects, and lead to a more sensible segmentation globally, than other losses considered.</p><p>Comparison to prior work <ref type="bibr" target="#b21">[22]</ref> propose separately ap-</p><formula xml:id="formula_21">proximating I ≃ p i=1 F i [y * i = 1] and U ≃ n i=1 (p i + [y * i = 1]</formula><p>) − I for optimization of binary IoU ≃ I/U . In our experiments, we were not able to observe a consistent improvement of the IoU using this surrogate, contrary to the Lovász hinge. Details on this comparison are included in the Supplementary Material, Section A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Multi-class segmentation on Pascal VOC</head><p>We again use Deeplab-resnet-v2. This time, we exactly replicate the training procedure of the authors and following the same learning rate schedule, simply swapping out the loss for our multiclass surrogate, the Lovász-Softmax loss as described in Equation <ref type="formula" target="#formula_0">(13)</ref>, with the mean being restricted to the classes present in a given batch.</p><p>As in the reference implementation, we use a stochastic gradient descent optimizer with momentum 0.9 and weight decay 5 · 10 −4 ; the learning rate at training iteration k is  included in the training only for experiments evaluated on the official test evaluation server. We train Deeplab-resnet at a single input scale, which fits the memory constraints of a single GPU. We optionally evaluate the learned weights in a multiscale setting by taking the mean of the probabilities given by the network at scales 1, 0.75, and 0.5, and also include the Gaussian CRF post-processing step used by Deeplab-v2. In this evaluation setting, we found that the baseline performance of the network trained with cross-entropy reaches 76.44% datasetmIoU on the test set of Pascal VOC. <ref type="table" target="#tab_2">Tables 2 and 3</ref> present the scores obtained after training the network with cross-entropy or Lovász-Softmax loss, with and without equibatch, under various evaluation regimes. For a given training and evaluation setting, our loss achieves higher mIoU. <ref type="figure" target="#fig_5">Figure 5</ref> shows some example outputs. <ref type="figure" target="#fig_7">Figure 6a</ref> shows the evolution of the validation mIoU over the course of the training. We notice that the performance gain manifests itself especially in the last epochs of the optimization. Therefore, we also experiment with the same training setting with 30K iterations, to further benefit from the effects of the loss at these smaller learning rates. In agreement with our intuition, we see in <ref type="table" target="#tab_2">Table 2</ref> that training with our surrogate benefits from a larger number of iterations, in contrast to the original training with cross-entropy.</p><formula xml:id="formula_22">lr (k) = lr base 1 − k max iter</formula><p>The CRF post-processing step of Deeplab appears to bring complementary improvements to the use of our mIoU surrogate. While using equibatch (batches with cyclic sampling from each class) does significantly help the crossentropy loss with respect to the dataset-mIoU, its effect on the performance with Lovász-softmax seems marginal. This may be linked with the fact that our loss ignores classes absent from the minibatch ground truth, and therefore relies less on the order of appearance of the classes across batches.   We found however that using equibatch facilitates the convergence of the training, as it helps the network to consider all classes during the course of the optimization. This is especially important in the early stages of the optimization, where a class absent for too long can end up being dropped by the classifier in favor of the other classes.  <ref type="figure" target="#fig_7">Figure 6b</ref> shows the joint evolution of the dataset-mIoU, and the batch-mIoU computed over present classes, during training. The correlation between these two measures justifies our choice of restricting the Lovász-Softmax to present classes as a proxy for optimizing the dataset-mIoU. As highlighted by <ref type="figure" target="#fig_7">Figure 6c</ref>, the image-mIoU is a poor surrogate for the dataset-mIoU, as discussed in Section 3.1: optimizing one measure is generally detrimental to the other. <ref type="figure" target="#fig_8">Figure 7</ref> illustrates some qualitative differences between segmentations predicted by the network optimized for batchmIoU and the network optimized for image-mIoU. The biggest difference between batch-mIoU and image-mIoU is the penalty associated with predicting a class that is absent from the ground truth. Accordingly, we notice that optimizing for image-mIoU tends to produce more sparse outputs, and output less extraneous classes, sometimes at the price of not including classes that are harder to detect. Comparison to prior work Instead of changing the learning, Nowozin <ref type="bibr" target="#b19">[20]</ref> designs a test-time decision function for mIoU based on the assumption of independent classifiers with calibrated probabilities. We applied this method on the Softmax output probabilities of the best model trained with cross-entropy loss (cross-entropy + equibatch), and compare with the outputs from Lovász-Softmax (Lovász + equibatch 30K). Since <ref type="bibr" target="#b19">[20]</ref> performs a local optimization (batches), we randomly select 20 batches of 21 images with every class represented, optimize the decision function, and compare the optimized mIoU of the batch with the mIoU of the selected batch in our output. The baseline has an average mIoU of 68.7±1.2, our method significantly improves it to 72.5±1.2, while <ref type="bibr" target="#b19">[20]</ref> significantly degrades it to 65.1 ± 1.4. We believe this comes from the miscalibration of the neural network's probabilities, which adversely affects the assumptions of the decision function, as discussed in [20, Sec. 5].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Cityscapes segmentation with ENet</head><p>We experiment with ENet, a segmentation architecture optimized for speed <ref type="bibr" target="#b20">[21]</ref>, on the Cityscapes dataset <ref type="bibr" target="#b4">[5]</ref>. We fine-tune the weights provided by the authors, obtained after convergence of weighted cross-entropy loss, a loss that biases the cross-entropy loss to account for class inbalance in the training set. We do not need such a reweighing as our method inherently captures the class balancing of the mIoU.</p><p>We finetune ENet using an Adam optimizer <ref type="bibr" target="#b14">[15]</ref> with the same learning rate and schedule as in Equation <ref type="bibr" target="#b15">(16)</ref>.  Consistent with <ref type="bibr" target="#b20">[21]</ref>, we use images of size 512 × 1024 with no data augmentation. We snapshot every 1K iterations and report the test performance of snapshot 9K with batches of size 10, which corresponds to the highest validation score. <ref type="figure" target="#fig_10">Fig. 9</ref> shows that our fine-tuning leads to a higher validation mIoU, while further training with weighted crossentropy barely affects the performance -as expected. Higher batch sizes generally lead to more improvement thanks to a better approximation of the dataset IoU. Equibatch training did not make a difference in our experiments, which can be explained by the fact that the dataset is more uniform than Pascal VOC in terms of class representation. Note that we optimize for the mIoU measure, named Class IoU in Cityscapes. Accordingly, we observe a substantial gain in performance in Cityscapes IoU metrics, with the Class IoU increasing from 58.29% to 63.06%. Reweighting the different classes in the average of the Lovász-Softmax loss (Equation <ref type="formula" target="#formula_0">(13)</ref>) could allow us to target IoU-based measures which are weighted differently, such as CityScapes' iIoU metrics. <ref type="figure" target="#fig_9">Figure 8</ref> presents some example output masks; we find that our fine-tuning generally reduces false positives and leads to finer details. Of course, our improved segmentation accuracy does not impact the high inference speed for which ENet is designed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Conclusions</head><p>In this work, we have demonstrated a versatile approach for optimizing the Jaccard loss for image segmentation. Our proposed method can be flexibly applied to a large number of function classes for segmentation, and we have demonstrated their effectiveness on state-of-the-art deep network architectures, substantially improving accuracies on semantic segmentation datasets simply by optimizing the correct loss during training. Qualitatively, we see greatly improved segmentation quality, in particular on small objects, while large objects tend to have consistent but smaller improvement in accuracy.</p><p>This work shows that submodular measures such as the Jaccard index can be readily optimized in a continuous optimization setting. Further work includes the application of the approach to different tasks and losses exhibiting submodularity, and a derivation of specialized optimization routines given the piecewise-linear nature of the Lovász extension.</p><p>The code associated with this publication, with replication of the experiments and implementations of the Lovász-Softmax loss, is released on https://github.com/ bermanmaxim/LovaszSoftmax.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>∈C e Fi(c ′ ) ∀i ∈ [1, p], ∀c ∈ C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>2.2), (iii) design a batch-based IoU surrogate that acts as an efficient proxy to the dataset IoU measure (Sec. 3.1), (iv) analyze and compare the proper- ties of different IoU-based measures, and (v) demonstrate a substantial and consistent improvement in performance mea- sured by the Jaccard index in state-of-the-art deep learning based segmentation systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Lovász hinge in the case of two pixel predictions for the four possible ground truths GT, as a function of the relative margins r i = 1 − F i (x) y * i for i = 1, 2. The red dots indicate the values of the discrete Jaccard index.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Synthetic model studied in 4.1 and loss objectives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Binary bicycle masks predicted on a validation image after training the network under various losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Multiclass segmentations after training with the Lovász-Softmax or the cross-entropy loss, and post-processed with Gaussian CRF. The color scheme follows the standard convention of the Pascal VOC dataset [8].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>dataset-mIoU and image- mIoU during training with Lovász-Softmax optimizing for image-mIoU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Evolution of some validation measures over the course of the training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Details of predicted masks after training with Lovász-Softmax per-batch vs. Lovász-Softmax per-image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: ENet: parts of output masks before and after fine-tuning with Lovász-Softmax (using the Cityscapes color palette).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Convergence of ENet on the validation set under fine-tuning with Lovász-Softmax, with various batch sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>π being a permutation ordering the components of m in decreasing order, i.e. x π1 ≥ x π2 . . . ≥ x πp .</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Average of mean validation binary losses over the 20 Pascal VOC categories, after a training with cross-entropy, hinge, and Lovász hinge loss. The image-mIoU of the basis network, trained for all categories, is equal to 78.29.</figDesc><table>Training loss → Cross-entropy Hinge Lovász hinge 

Cross-entropy 
6.84 
6.96 
7.91 
Hinge 
7.81 
6.95 
7.11 
Lovász hinge 
8.37 
7.45 
5.44 
Image-IoU 
77.14 
75.8 
80.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Performance of Deeplab-v2 single-scale trained with cross-entropy (x-loss) vs. Lovász-Softmax loss, for different network evaluations: raw single-scale network output, multi-scale, and Gaussian CRF post-processing.</figDesc><table>validation mIoU (%) 
test mIoU (%) 

single-scale multi-scale multi-scale + CRF multi-scale + CRF 

x-loss 
74.64 
76.23 
76.53 
76.44 
x-loss + equibatch 
75.53 
76.70 
77.31 
78.05 
x-loss + equibatch -30K iterations 
74.97 
76.24 
76.73 

Lovász 
76.56 
77.24 
77.99 
Lovász + equibatch 
76.53 
77.28 
78.49 
Lovász + equibatch -30K iterations 
77.41 
78.22 
79.12 
79.00 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Per-class test IoU (%) corresponding to the best-performing variants in Table 2. airplane cycle bird boat bottle bus car cat chair cow d. table dog horse mbike person plant sheep sofa train tv</figDesc><table>x-loss 
92.95 41.06 87.06 61.23 77.6 91.99 88.11 92.45 32.84 82.48 59.6 90.13 89.83 86.77 85.79 58.06 85.31 52.00 84.47 71.26 
x-loss-equi. 
93.32 40.29 91.47 63.74 77.03 93.10 86.70 93.37 34.79 87.92 69.74 89.53 90.61 84.70 85.13 59.23 87.71 64.46 82.89 68.57 
Lovász-equi 30K 92.63 41.55 87.87 68.41 77.75 94.71 86.71 90.37 38.59 86.24 74.50 89.02 91.69 87.28 86.37 65.92 87.13 65.21 83.69 68.64 

(a) Dataset mIoU on the validation set over 
the course of the Lovász-Softmax or cross-
entropy optimization. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Cityscapes results with Lovász-Softmax finetuning Class IoU Class iIoU Cat. IoU Cat. iIoU</figDesc><table>ENet [21] 
58.29 
34.36 
80.40 
63.99 
Finetuned 
63.06 
34.06 
83.58 
61.05 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work is partially funded by Internal Funds KU Leuven and FP7-MC-CIG 334380. We acknowledge support from the Research Foundation -Flanders (FWO) through project number G0A2716N. The authors thank J. Yu, X. Jia and Y. Huang for valuable comments and discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optimizing expected intersection-over-union with candidate-constrained CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning with submodular functions: A convex optimization perspective. Foundations and Trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="145" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to localize objects with structured output regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<editor>D. Forsyth, P. Torr, and A. Zisserman</editor>
		<imprint>
			<biblScope unit="volume">5302</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2" to="15" />
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting people using mutually consistent poselet activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, Part VI</title>
		<editor>K. Daniilidis, P. Maragos, and N. Paragios</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="168" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What is a good evaluation measure for semantic segmentation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meylan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The PASCAL Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/.1" />
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Submodular Functions and Optimization. Annals of Discrete Mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fujishige</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Elsevier Science</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV, Part VII</title>
		<editor>D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Étude comparative de la distribution florale dans une portion des Alpes et des Jura</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jaccard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin de la Société Vaudoise des Sciences Naturelles</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="547" to="579" />
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang-Chieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Submodular functions and convexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lovász</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Programming The State of the Art</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1983" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimal decisions from probabilistic models: The intersection-over-union case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">ENet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimizing intersection-overunion in deep neural networks for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="234" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Nature of Statistical Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning submodular losses with the Lovász hinge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The Lovász hinge: A convex surrogate for submodular losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.07797</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
