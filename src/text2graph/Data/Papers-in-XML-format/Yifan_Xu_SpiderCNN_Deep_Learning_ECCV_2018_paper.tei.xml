<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
							<email>tq.fan@siat.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Guangdong Key Lab of Computer Vision and Virtual Reality, SIAT-SenseTime Joint Lab, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Guangdong Key Lab of Computer Vision and Virtual Reality, SIAT-SenseTime Joint Lab, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zeng</surname></persName>
							<email>zenglong@sz.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>yu.qiao@siat.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Guangdong Key Lab of Computer Vision and Virtual Reality, SIAT-SenseTime Joint Lab, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>Convolutional neural network · Parametrized convolutional filters · Point clouds</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Deep neural networks have enjoyed remarkable success for various vision tasks, however it remains challenging to apply CNNs to domains lacking a regular underlying structures such as 3D point clouds. Towards this we propose a novel convolutional architecture, termed SpiderCNN, to efficiently extract geometric features from point clouds. SpiderCNN is comprised of units called SpiderConv, which extend convolutional operations from regular grids to irregular point sets that can be embedded in R n , by parametrizing a family of convolutional filters. We design the filter as a product of a simple step function that captures local geodesic information and a Taylor polynomial that ensures the expressiveness. SpiderCNN inherits the multi-scale hierarchical architecture from classical CNNs, which allows it to extract semantic deep features. Experiments on ModelNet40 demonstrate that SpiderCNN achieves stateof-the-art accuracy 92.4% on standard benchmarks, and shows competitive performance on segmentation task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional neural networks are powerful tools for analyzing data that can naturally be represented as signals on regular grids, such as audio and images <ref type="bibr" target="#b9">[10]</ref>. Thanks to the translation invariance of lattices in R n , the number of parameters in a convolutional layer is independent of the input size. Composing convolution layers and activation functions results in a multi-scale hierarchical learning pattern, which is shown to be very effective for learning deep representations in practice.</p><p>With the recent proliferation of applications employing 3D depth sensors <ref type="bibr" target="#b22">[23]</ref> such as autonomous navigation, robotics and virtual reality, there is an increasing demand for algorithms to efficiently analyze point clouds. However, point clouds are distributed irregularly in R 3 , lacking a canonical order and translation invariance, which prohibits using CNNs directly. One may circumvent this problem by converting point clouds to 3D voxels and apply 3D convolutions <ref type="bibr" target="#b12">[13]</ref>. However, volumetric methods are computationally inefficient because point clouds are sparse in 3D as they usually represent 2D surfaces. Although there are studies that improve the computational complexity, it may come with a performance trade off <ref type="bibr" target="#b17">[18]</ref>  <ref type="bibr" target="#b1">[2]</ref>. Various studies are devoted to making convolution neural networks applicable for learning on non-Euclidean domains such as graphs or manifolds by trying to generalize the definition of convolution to functions on manifolds or graphs, enriching the emerging field of geometric deep learning <ref type="bibr" target="#b2">[3]</ref>. However, it is challenging theoretically because convolution cannot be naturally defined when the space does not carry a group action, and when the input data consists of different shapes or graphs, it is difficult to make a choice for convolutional filters. The integral formula for convolution between a signal f and a filter g is f * g(p) = q∈R n f (q)g(p − q)dq. Discretizing the integral formula on a set of points P in R n gives f * g(p) = q∈P, p−q ≤r f (q)g(p − q) if g is supported in a ball of radius r.</p><p>(a) when P can be represented by regular grids, only 9 values of a filter g are needed to compute the convolution due to the translation invariance of the domain. (b) when the signal is on point clouds, we choose the filter g from a parameterized family of function on R 3 .</p><p>In light of the above challenges, we propose an alternative convolutional architecture, SpiderCNN, which is designed to directly extract features from point clouds. We validate its effectiveness on classification and segmentation benchmarks. By discretizing the integral formula of convolution as shown in <ref type="figure">Figure 1</ref>, and using a special family of parametrized non-linear functions on R 3 as filters, we introduce a novel convolutional layer, SpiderConv, for point clouds.</p><p>The family of filters is designed to be expressive while still being feasible to optimize. We combine simple step functions, which are used to capture the coarse geometry described by local geodesic distance, with order-3 Taylor expansions, which ensure the filters are complex enough to capture intricate local geometric variations. Experiments in Section 4 show that SpiderCNN with a relatively simple network architecture achieves the state-of-the-art performance for classification on ModelNet40 <ref type="bibr" target="#b3">[4]</ref>, and shows competitive performance for segmentation on ShapeNet-Part <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>First we discuss deep neural network based approaches that target point clouds data. Second, we give a partial overview of geometric deep learning. Point clouds as input: PointNet <ref type="bibr" target="#b14">[15]</ref> is a pioneering work in using deep networks to directly process point sets. A spatial encoding of each point is learned through a shared MLP, and then all individual point features aggregate to a global signature through max-pooling, which is a symmetric operation that doesn't depend on the order of input point sequence.</p><p>While PointNet works well to extract global features, its design limits its efficacy at encoding local structures. Various studies addressing this issue propose different grouping strategies of local features in order to mimic the hierarchical learning procedure at the core of classical convolutional neural networks. PointNet++ <ref type="bibr" target="#b16">[17]</ref> uses iterative farthest point sampling to select centroids of local regions, and PointNet to learn the local pattern. Kd-Network <ref type="bibr" target="#b8">[9]</ref> subdivides the space using K-d trees, whose hierarchical structure serves as the instruction to aggregate local features at different scales. In SpiderCNN, no additional choice for grouping or sampling is needed, for our filters handle the issue automatically.</p><p>The idea of using permutation-invariant functions for learning on unordered sets is further explored by DeepSet <ref type="bibr" target="#b21">[22]</ref>. We note that the output of SpiderCNN does not depend on the input order by design. Voxels as input: VoxNet <ref type="bibr" target="#b12">[13]</ref> and Voxception-ResNet <ref type="bibr" target="#b1">[2]</ref> apply 3D convolution to a voxelization of point clouds. However, there is a high computational and memory cost associated with 3D convolutions. A variety of work <ref type="bibr" target="#b17">[18]</ref> [6] <ref type="bibr" target="#b6">[7]</ref> has aimed at exploiting the sparsity of voxelized point clouds to improve the computational and memory efficiency. OctNet <ref type="bibr" target="#b17">[18]</ref> modified and implemented convolution operations to suit a hybrid grid-octree data structure. Vote3Deep <ref type="bibr" target="#b5">[6]</ref> uses a feature-centric voting scheme so that the computational cost is proportional to the number of points with non-zero features. Sparse Submanifold CNN <ref type="bibr" target="#b6">[7]</ref> computes the convolution only at activated points whose number does not increase when the convolution layers are stacked. In comparison, SpiderCNN can use point clouds as input directly and can handle very sparse input. Convolution on non-Euclidean domain: There are two main philosophically different approaches to define convolutions for non-Euclidean domains: one is spatial and the other is spectral. The recent work ECC <ref type="bibr" target="#b19">[20]</ref> defines convolutionlike operations on graphs where filter weights are conditioned on edge labels. Viewing point clouds as a graph, and taking the filters to be MLPs, SpiderCNN and ECC <ref type="bibr" target="#b19">[20]</ref> result in similar convolution. However, we show that our proposed family of filters outperforms MLPs. Spatial methods: GeodesicCNN <ref type="bibr" target="#b11">[12]</ref> is an early attempt at applying neural networks to shape analysis. The philosophy behind GeodesicCNN is that for a Riemannian manifold, the exponential map identifies a local neighborhood of a point to a ball in the tangent space centered at the origin. The tangent plane is isomorphic to R d where we know how to define convolution. Let M be a mesh surface, and let F : M → R be a function, GeodesicCNN first uses the patch operator D to map a point p and its neighbors N (p) to the lattice Z 2 ⊆ R 2 , and applies Equation 2. Explicitly,</p><formula xml:id="formula_0">F * g(p) = j∈J g j ( q∈N (p) w j (u(p, q))F (q))</formula><p>, where u(p, q) represents the local polar coordinate system around p, w j (u) is a function to model the effect of the patch</p><formula xml:id="formula_1">operator D = {D j } j∈J . By definition D j = q∈N (p) w j (u(p, q))F (q)</formula><p>. Later, AnisotrpicCNN <ref type="bibr" target="#b0">[1]</ref> and MoNet <ref type="bibr" target="#b13">[14]</ref> further explore this framework by improving the choice for u and w j . MoNet <ref type="bibr" target="#b13">[14]</ref> can be understood as using mixtures of Gaussians as convolutional filters. We offer an alternative viewpoint. Instead of finding local parametrizations of the manifold, we view it as an embedded submanifold in R n and design filters, which are more efficient for point clouds processing, in the ambient Euclidean space. Spectral methods: We know that Fourier transform takes convolutions to multiplications. Explicitly, If f, g : R n → C, then f * g =f ·ĝ. Therefore, formally</p><p>we have f * g = (f ·ĝ) ∨ , 4 which can be used as a definition for convolution on non-Euclidean domains where we know how to take Fourier transform.</p><p>Although we do not have Fourier theory on a general space without any equivariant structure, on Riemannian manifolds or graphs there are generalized notions of Laplacian operator. Taking Fourier transform in R n could be formally viewed as finding the coefficients in the expansion of the eigenfunctions of the Laplacian operator. To be more precise, recall that</p><formula xml:id="formula_2">f (ξ) = R n f (x) exp (−2πix · ξ)dξ,<label>(1)</label></formula><p>and {exp (−2πix · ξ)} ξ∈R n are eigen-functions for the Laplacian operator ∆ = n i=1 ∂ ∂xi . Therefore, if U is the matrix whose columns are eigenvectors of the graph Laplacian matrix and Λ is the vector of corresponding eigenvalues, for F, g two functions on the vertices of the graph, then</p><formula xml:id="formula_3">F * g = U (U T F ⊙ U T g), where U</formula><p>T is the transpose of U and ⊙ is the Hadamard product of two matrices. Since being compactly supported in the spatial domain translates into being smooth in the spectral domain, it is natural to choose U T g to be smooth functions in Λ. For instance, ChebNet <ref type="bibr" target="#b4">[5]</ref> uses Chebyshev polynomials that reduces the complexity of filtering, and CayleyNet <ref type="bibr" target="#b10">[11]</ref> uses Cayley polynomials which allows efficient computations for localized filters in restricted frequency bands of interest.</p><p>When analyzing different graphs or shapes, spectral methods lack abstract motivations, because different spectral domains cannot be canonically identified. SyncSpecCNN <ref type="bibr" target="#b20">[21]</ref> proposes a weight sharing scheme to align spectral domains using functional maps. Viewing point clouds as data embedded in R 3 , SpiderCNN can learn representations that are robust to spatial rigid transformations with the aid of data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SpiderConv</head><p>In this section, we describe SpiderConv, which is the fundamental building block for SpiderCNN. First, we discuss how to define a convolutional layer in neural network when the inputs are features on point sets in R n . Next we introduce a special family of convolutional filters. Finally, we give details for the implementation of SpiderConv with multiple channels and the approximations used for computational speedup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Convolution on point sets in R n</head><p>An image is a function on regular grids F : Z 2 → R. Let W be a (2m + 1) × (2m + 1) filter matrix, where m is a positive integer, the convolution in classical CNNs is</p><formula xml:id="formula_4">F * W (i, j) = m s=−m m t=−m F (i − s, j − t)W (s, t),<label>(2)</label></formula><p>which is the discretization of the following integration</p><formula xml:id="formula_5">f * g(p) = R 2 f (q)g(p − q)dq,<label>(3)</label></formula><p>if f, g :</p><formula xml:id="formula_6">R 2 → R, such that f (i, j) = F (i, j) for (i, j) ∈ Z 2 and g(s, t) = W (s, t) for s, t ∈ {−m, −m + 1, ..., m − 1, m} and g is supported in [−m, m] × [−m, m].</formula><p>Now suppose that F is a function on a set of points P in R n . Let g : R n → R be a filter supported in a ball centered at the origin of radius r. It is natural to define SpiderConv with input F and filter g to be the following:</p><formula xml:id="formula_7">F * g(p) = q∈P, q−p ≤r F (q)g(p − q).<label>(4)</label></formula><p>Note that when P = Z 2 is a regular grid, Equation 4 reduces to Equation 3. Thus the classical convolution can be seen as a special case of SpiderConv. Please see <ref type="figure">Figure 1</ref> for an intuitive illustration.</p><p>In SpiderConv, the filters are chosen from a parametrized family {g w } (See <ref type="figure" target="#fig_6">Figure 3</ref>.2 for a concrete example) which is piece-wise differentiable in w. During the training of SpiderCNN, the parameters w ∈ R d are optimized through SGD algorithm, and the gradients are computed through the formula</p><formula xml:id="formula_8">∂ ∂wi F * g w (p) = q∈P, q−p ≤r F (q) ∂ ∂wi g w (p − q)</formula><p>, where w i is the i-th component of w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A special family of filters {g w }</head><p>A natural choice is to take g w to be a multilayer perceptron (MLP) network, because theoretically an MLP with one hidden layer can approximate an arbitrary continuous function <ref type="bibr" target="#b7">[8]</ref>. However, in practice we find that MLPs do not work well. One possible reason is that MLP fails to account for the geometric </p><formula xml:id="formula_9">+ y + z + xy + xz + yz + xyz. (b) is the scatter plot of g step (x, y, z) = i+1 8 if i 8 ≤ x 2 + y 2 + z 2 &lt; i+1 8</formula><p>, when i = 0, 1, ..., 7. (c) is the scatter plot of the product g = g T aylor · g step . In the second row, (d) (e) (f ) are the graphs of g T aylor , g step and g respectively when restricting their domain to the plane z = 0 (the Z-axis represents the value of the function).</p><p>prior of 3D point clouds. Another possible reason is that to ensure sufficient expressiveness the number of parameters in a MLP needs to be sufficiently large, which makes the optimization problem difficult.</p><p>To address the above issues, we propose the following family of filters {g w }:</p><formula xml:id="formula_10">g w (x, y, z) = g Step w S (x, y, z) · g T aylor w T (x, y, z),<label>(5)</label></formula><p>with w = (w S , w T ) is the concatenation of two vectors w S = (w S i ) and</p><formula xml:id="formula_11">w T = (w T i ), 5 where g Step w S (x, y, z) = w S i if r i ≤ x 2 + y 2 + z 2 &lt; r i+1 ,<label>(6)</label></formula><p>with r 0 = 0 &lt; r 1 &lt; r 2 ... &lt; r N , and</p><formula xml:id="formula_12">g T aylor w T (x, y, z) = w T 0 + w T 1 x + w T 2 y + w T 3 z + w T 4 xy + w T 5 yz + w T 6 xz + w T 7 x 2 + w T 8 y 2 + w T 9 z 2 + w T 10 xy 2 + w T 11 x 2 y + w T 12 y 2 z + w T 13 yz 2 + w T 14 x 2 z + w T 15 xz 2 + w T 16 xyz + w T 17 x 3 + w T 18 y 3 + w T 19 z 3 .<label>(7)</label></formula><p>The first component g</p><p>Step w S is a step function in the radius variable of the local polar coordinates around a point. It encodes the local geodesic information, which is a critical quantity to describe the coarse local shape. Moreover, step functions are relatively easy to optimize using SGD.</p><p>The order-3 Taylor term g T aylor w T further enriches the complexity of the filters, complementary to g</p><p>Step w S since it also captures the variations of the angular component. Let us be more precise about the reason for choosing Taylor expansions here from the perspective of interpolation. We can think of the classical 2D convolutional filters as a family of functions interpolating given values at 9 points {(i, j)} i,j∈{−1,0,1} , and the 9 values serve as the parametrization of such a family. Analogously, in 3D consider the vertices of a cube {(i, j, k)} i,j,k=0,1 , assume that at the vertex (i, j, k) the value a i,j,k is assigned. The trilinear interpolation algorithm gives us a function of the form </p><formula xml:id="formula_13">f w T (x, y, z) = w T 0 + w T 1 x + w T 2 y + w T 3 z + w T 4 xy + w T 5 yz + w T 6 xz + w</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation</head><p>The following approximations are used based on the uniform sampling process constructing the point clouds:</p><p>1. K-nearest neighbors are used to measure the locality instead of the radius, so the summation in Equation 4 is over the K-nearest neighbors of p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The step function g</head><p>Step w T is approximated by a permutation. Explicitly, let X be the 1×K matrix indexed by the K-nearest neighbors of p including p, and X(1, i) is a feature at the i-th K-nearest neighbors of p. Then F * g</p><p>Step w T (p) is approximated by Xw, where w is a K × 1 matrix with w(i, 1) corresponds to w T i in Equation <ref type="bibr" target="#b5">6</ref>.</p><p>Later in the article, we omit the parameters w, w S and w T , and just write g = g</p><p>Step · g T aylor to simplify our notations. The input to SpiderConv is a c 1 -dimensional feature on a point cloud P , and is represented as F = (F 1 , F 2 , ..., F c1 ) where F v : P → R. The output of a SpiderConv is a c 2 -dimensional feature on the point cloudF = (F 1 ,F 2 , ...,F c2 ) whereF i : P → R. Let p be a point in the point cloud, and q 1 , q 2 , ..., q K are its K-nearest neighbors in order. Assume g   </p><formula xml:id="formula_14">F i (p) = c1 v=1 K j=1 g i (p − q j )F v (q j ), where g i (p − q j ) = b t=1 g T aylor t (p − q j )w</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We analyze and evaluate SpiderCNN on 3D point clouds classification and segmentation. We empirically examine the key hyper-parameters of a 3-layer SpiderCNN, and compare our models with the state-of-the-art methods. Implementation Details: All models are prototyped with Tensorflow 1.3 on 1080Ti GPU and trained using the Adam optimizer with a learning rate of 10 −3 . A dropout rate of 0.5 is used with the the fully connected layers. Batch normalization is used at the end of each SpiderConv with decay set to 0.5. On a GTX 1080Ti, the forward-pass time of a SpiderConv layer (batch size 8) with in-channel 64 and out-channel 64 is 7.50 ms. For the 4-layer SpiderCNN (batch size 8), the total forward-pass time is 71.68 ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Classification on ModelNet40</head><p>ModelNet40 <ref type="bibr" target="#b3">[4]</ref> contains 12,311 CAD models which belong to 40 different categories with 9,843 used for training and 2,468 for testing. We use the source code for PointNet <ref type="bibr" target="#b14">[15]</ref> to sample 1,024 points uniformly and compute the normal vectors from the mesh models. The same data augmentation strategy as <ref type="bibr" target="#b14">[15]</ref> is applied: the point cloud is randomly rotated along the up-axis and the position of each point is jittered by a Gaussian noise with zero mean and 0.02 standard deviation. The batch size is 32 for all the experiments in Section 4.1. We use the (x, y, z)-coordinates and normal vectors of the 1,024 points as the input for SpiderCNN for the experiments on ModelNet40 unless otherwise specified.  <ref type="figure" target="#fig_6">Figure 3</ref> illustrates a SpiderCNN with 3 layers of SpiderConvs each with 3 Taylor terms, and the respective out-channels for each layer being 32, 64, 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3-layer SpiderCNN:</head><p>6 ReLU activation function is used here. The output features of the three SpiderConvs are concatenated in the end. Top-k pooling among all the points is used to extract global features. Two important hyperparameters in SpiderCNN are studied: the number of nearest neighbors K chosen in SpiderConv, and the number of pooled features k after the concatenation. The results are summarized in <ref type="figure" target="#fig_7">Figure 4</ref>. The number of nearest-neighbors K is analogous to size of the filter in the usual convolution. We see that 20 is the optimal choice among 12, 16, 20, and 24-nearest neighbors. In <ref type="figure" target="#fig_8">Figure 5</ref> we provide visualization for top-2 pooling. The points that contribute to the top-2 pooling features are plotted. We see that similar to PointNet, Spider CNN picks up representative critical points. To prevent overfitting, while training we apply the data augmentation method DP (random input dropout) introduced in <ref type="bibr" target="#b16">[17]</ref>. <ref type="table" target="#tab_0">Table 1</ref> shows a comparison between SpiderCNN and other models. The 4-layer SpiderCNN achieves accuracy of 92.4% which improves over the best reported result of models with input 1024 points and normals. For 5 runs, the mean accuracy of a 4-layer SpiderCNN is 92.0%. Ablative Study: Compared to max-pooling, top-2 pooling enables the model to learn richer geometric information. For example, in <ref type="figure" target="#fig_9">Figure 6</ref>, we see top-2 pooling preserves more points where the curvature is non-zero. Using max-pooling, the classification accuracy is 92.0% for a 4-layer SpiderCNN, and is 90.4% for a 3-layer SpiderCNN. In comparison, using top-2 pooling, the accuracy is 92.4% for a 4-layer SpiderCNN, and is 91.5% for a 3-layer SpiderCNN. MLP filters do not perform as well in our setting. The accuracy of a 3-layer SpiderCNN is 71.3% with g w = MLP <ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b0">1)</ref>, and is 72.8% with g w = MLP <ref type="bibr" target="#b15">(16,</ref><ref type="bibr">32,</ref><ref type="bibr" target="#b0">1)</ref>.</p><p>Without normals, the accuracy of a 4-layer SpiderCNN using only the 1,024 points is 90.5%. Using normals extracted from the 1,024 input points via orthogonal distance regression, the accuracy of a 4-layer SpiderCNN is 91.8%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Classification on SHREC15</head><p>SHREC15 is a dataset for non-rigid 3D shape retrieval. It consists of 1,200 watertight triangle meshes divided in 50 categories. On average 10,000 vertices are stored in one mesh model. Comparing to ModelNet40, SHREC15 contains more complicated local geometry and non-rigid deformation of one object. See <ref type="figure" target="#fig_10">Figure 7</ref> for a comparison. 1,192 meshes are used with 895 for training and 297 for testing. We compute three intrinsic shape descriptors (Heat Kernel Signature, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Segmentation on ShapeNet Parts</head><p>ShapeNet Parts consists of 16,880 models from 16 shape categories and 50 different parts in total, with a 14,006 training and 2,874 testing split. Each part is annotated with 2 to 6 parts. The mIoU is used as the evaluation metric, computed by taking the average of all part classes. A 4-layer SpiderCNN whose architecture is shown in <ref type="figure" target="#fig_11">Figure 8</ref> is trained with batch of 16. We use points with their normal vectors as the input and assume that the category labels are known. The results are summarized in <ref type="table" target="#tab_2">Table 3</ref>. For 4 runs, the mean of mean IoU of SpiderCNN is 85.24. We see that SpiderCNN achieves competitive results despite a relatively simple network architecture.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>In this section, we conduct additional analysis and evaluations on the robustness of SpiderCNN, and provide visualization for some of the typical learned filters from the first layer of SpiderCNN. Robustness: We study the effect of missing points on SpiderCNN. Following the setting for experiments in Section 4.1, we train a 4-layer SpiderCNN and PointNet++ with 512, 248, 128, 64 and 32 points and their normals as input. The results are summarized in <ref type="figure" target="#fig_13">Figure 10</ref>. We see that even with only 32 points, SpiderCNN obtains 87.7% accuracy. Visualization: In <ref type="figure" target="#fig_14">Figure 11</ref>, we scatter plot the convolutional filters g w (x, y, z) learned in the first layer of SpiderCNN and the color of a point represents the value of g w at the point. In <ref type="figure" target="#fig_1">Figure 12</ref> we choose a plane passing through the origin, and project the points that lie on one side of the plane of the scatter graph onto the plane. We see some similar patterns that appear in 2D image filters. The visualization gives some hints about the geometric features that the convolutional filters in SpiderCNN learn. For example, the first row in <ref type="figure" target="#fig_1">Figure 12</ref> corresponds to 2D image filters that can capture boundary information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>A new convolutional neural network SpiderCNN that can directly process 3D point clouds with parameterized convolutional filters is proposed. More complex network architectures and more applications of SpiderCNN can be explored.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 Fig. 1 .</head><label>31</label><figDesc>Fig. 1. The integral formula for convolution between a signal f and a filter g is f * g(p) = q∈R n f (q)g(p − q)dq. Discretizing the integral formula on a set of points P in R n gives f * g(p) = q∈P, p−q ≤r f (q)g(p − q) if g is supported in a ball of radius r.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization of a filter in the family {gw}. (a) is the scatter plot (color represents the value of the function) of g T aylor (x, y, z) = 1 + x + y + z + xy + xz + yz + xyz. (b) is the scatter plot of g step (x, y, z) =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>}</head><label></label><figDesc>s are linear functions in c ijk . Therefore f w T is a special form of gcan interpolate arbitrary values at the vertexes of a cube and capture rich spatial information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>t (p − q j ) = w</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>..., b and v = 1, 2, ..., c 1 and i = 1, 2, ...c 2 . Then a SpiderConv with c 1 in- channels, c 2 out-channels and b Taylor terms is defined via the formula:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>}</head><label></label><figDesc>for t = 1, 2, ..., b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The architecture of a 3-layer SpiderCNN used in ModelNet40 classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. On ModelNet40 (a) shows the effect of number of pooled features on the accuracy of 3-layer SpiderCNN with 20-nearest neighbors. (b) shows the effect of nearest neighbors in SpiderConv on the accuracy of 3-layer SpiderCNN with top-2 pooling .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Visualization of the effect of top-k pooling. Edge points and points with nonzero curvature are preserved after pooling. (a) (b) (c) (d) are the original input point clouds. (e) (f ) (g) (h) are points contributing to features extracted via top-2 pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Top-2 pooling learns rich features and fine geometric details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. (b) and (c) are shapes in SHREC15. (d) is a shape in ModelNet40. (a) is the point cloud sampled from (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The SpiderCNN architecture used in the ShapeNet Part segmentation task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Some examples of the segmentation results of SpiderCNN on ShapeNet Part.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Classification accuracy of SpiderCNN and PointNet++ with different number of input points on ModelNet40.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Visualization of for the convolutional filters learned in in the first layer of SpiderCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Visualization for the convolutional filters learned in in the first layer of SpiderCNN. The 3D filters are shown as scatter plots projected on to the planes x = 0 or y = 0 or z = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Classification accuracy of SpiderCNN and other models on ModelNet40.</figDesc><table>Method 
Input 
Accuracy 

Subvolume [16] 
voxels 
89.2 
VRN Single [2] 
voxels 
91.3 
OctNet [18] 
hybrid grid octree 
86.5 
ECC [20] 
graphs 
87.4 
Kd-Network [9] (depth 15) 1024 points 
91.8 
PointNet [15] 
1024 points 
89.2 
PointNet++ [17] 
5000 points+normal 91.9 
SpiderCNN + PointNet 
1024 points+normal 92.2 
SpiderCNN (4-layer) 
1024 points+normal 92.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Classification accuracy on SHEREC15.Wave Kernel Signature and Fast Point Feature Histograms) for deformable shape analysis from the mesh models. 1,024 points are sampled uniformly randomly from the vertices of a mesh model, and the (x, y, z)-coordinates are used as the input for SpiderCNN, PointNet and PointNet++. We use SVM with linear kernel when the inputs are classical shape descriptors. Table 2 summarizes the results. We see that SpiderCNN outperforms the other methods.</figDesc><table>Method 
Input 
Accuracy 

SVM + HKS 
features 56.9 
SVM + WKS 
features 87.5 
SVM + FPFH 
features 80.8 
PointNet 
points 
69.4 
PointNet++ [17] 
points 
60.2 
PointNet++(our implementation) points 
94.1 
SpiderCNN (4-layer) 
points 
95.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Segmentation results on ShapeNet Part dataset. Mean IoU and IoU for each categories are reported.aero bag cap car chair ear guitar knife lamp laptop motor mug pistol rocket skate table</figDesc><table>mean ph 
board 

PN [15] 
83.7 83.4 78.7 82.5 74.9 89.6 73.0 91.5 
85.9 80.8 95.3 
65.2 
93.0 81.2 57.9 
72.8 80.6 

PN++[17] 
85.1 82.4 79.0 87.7 77.3 90.8 71.8 91.0 
85.9 83.7 95.3 
71.6 94.1 81.3 58.7 
76.4 82.6 

Kd-Net [9] 82.3 80.1 74.6 74.3 70.3 88.6 73.5 90.2 
87.2 81.0 94.9 
57.4 
86.7 78.1 51.8 
69.9 80.3 

SSCNN [21] 84.7 81.6 81.7 81.9 75.2 90.2 74.9 93.0 86.1 84.7 95.6 
66.7 
92.7 81.6 60.6 
82.9 82.1 

SpiderCNN 85.3 83.5 81.0 87.2 77.5 90.7 76.8 91.1 
87.3 83.3 95.8 
70.2 
93.5 82.7 59.7 
75.8 82.8 </table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">There is no canonical choice of a domain for these filters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">If h is a function, thenĥ is the Fourier transform, and h ∨ is its inverse Fourier transform.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Here we use the notation v = (vi) to represent that vi ∈ R is the i-th component of the vector v.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">See Section 3.3 for the definition of a SpiderConv with c1 in-channels, c2 out-channels and b Taylor terms.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by Shenzhen Basic Research Program (JCYJ201509251 63005055, JCYJ20170818164704758), National Natural Science Foundation of China (U1613211, 61633021, 61502263) and External Cooperation Program of BIC Chinese Academy of Sciences (172644KYSB20150019). We would like to thank Zhikai Dong for his technical assistance and helpful discussion.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3189" to="3197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04236</idno>
		<title level="m">Generative and discriminative voxel modeling with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">Shapenet: An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1355" to="1361" />
		</imprint>
	</monogr>
	<note>Robotics and Automation (ICRA</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10275</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07664</idno>
		<title level="m">Cayleynets: Graph convolutional neural networks with complex rational spectral filters</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
	<note>Geodesic convolutional neural networks on riemannian manifolds</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5105" to="5114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (fpfh) for 3d registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation, 2009. ICRA&apos;09. IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3212" to="3217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3394" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
