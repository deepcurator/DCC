<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader>
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:24+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Deep Representation for Imbalanced Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
							<email>chuang@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<email>ccloy@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
							<email>xtang@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Deep Representation for Imbalanced Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract xml:lang="en">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Data in vision domain often exhibit highly-skewed class distribution, i.e., most data belong to a few majority classes, while the minority classes only contain a scarce amount of instances. To mitigate this issue, contemporary classification methods based on deep convolutional neural network (CNN) typically follow classic strategies such as class re-sampling or cost-sensitive training. In this paper, we conduct extensive and systematic experiments to validate the effectiveness of these classic schemes for representation learning on class-imbalanced data. We further demonstrate that more discriminative deep representation can be learned by enforcing a deep network to maintain both intercluster and inter-class margins. This tighter constraint effectively reduces the class imbalance inherent in the local data neighborhood. We show that the margins can be easily deployed in standard deep learning framework through quintuplet instance sampling and the associated triple-header hinge loss. The representation learned by our approach, when combined with a simple k-nearest neighbor (kNN) algorithm, shows significant improvements over existing methods on both high-and low-level vision classification tasks that exhibit imbalanced class distribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text>
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many data in computer vision domain naturally exhibit imbalance in their class distribution. For instance, the number of positive and negative face pairs in face verification is highly skewed since it is easier to obtain face images of different identities (negative) than faces with matched identity (positive) during data collection. In face attribute recognition <ref type="bibr" target="#b24">[25]</ref>, it is comparatively easier to find persons with "normal-sized nose" attribute on web images than that of "big-nose". For image edge detection, the image edge structures intrinsically follow a power-law distribution, e.g., horizontal and vertical edges outnumber those with "Y" shape. Without handling the imbalance issue conventional methods tend to be biased toward the majority class with poor accuracy for the minority class <ref type="bibr" target="#b17">[18]</ref>.</p><p>Deep representation learning has recently achieved great success due to its high learning capacity, but still cannot escape from such negative impact of imbalanced data. To counter the negative effects, one often chooses from a few available options, which have been extensively studied in the past <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48]</ref>. The first option is re-sampling, which aims to balance the class priors by under-sampling the majority class or over-sampling the minority class (or both). For instance, Oquab et al. <ref type="bibr" target="#b31">[32]</ref> resample the number of foreground and background image patches for learning a convolutional neural network (CNN) for object classification. The second option is cost-sensitive learning, which assigns higher misclassification costs to the minority class than to the majority. In image edge detection, for example, Shen et al. <ref type="bibr" target="#b34">[35]</ref> regularize the softmax loss to cope with the imbalanced edge class distribution. Other alternatives exist, e.g., learning rate adaptation <ref type="bibr" target="#b17">[18]</ref>.</p><p>Are these methods the most effective way to deal with imbalance data in the context of deep representation learning? The aforementioned options are well studied for the so called 'shallow' model <ref type="bibr" target="#b11">[12]</ref> but their implications have not yet been systematically studied for deep representation learning. Importantly, such schemes are well-known for some inherent limitations. For instance, over-sampling can easily introduce undesirable noise with overfitting risks, and under-sampling is often preferred <ref type="bibr" target="#b10">[11]</ref> but may remove valuable information. Such nuisance factors can be equally applicable to deep representation learning.</p><p>In this paper, we wish to investigate a better approach for learning a deep representation given class-imbalanced data. Our method is motivated by the observation that the minority class often contains very few instances with high degree of visual variability. The scarcity and high variability make the genuine neighborhood of these instances easy to be invaded by other imposter nearest neighbors <ref type="bibr" target="#b0">1</ref> . To this end, <ref type="bibr" target="#b0">1</ref> An imposter neighbor of a data point x i is another data point x j with a different class label, y i = y j .</p><p>we propose to learn an embedding f (x) âˆˆ R d with a CNN to ameliorate such invasion. The CNN is trained with instances selected through a new quintuplet sampling scheme and the associated triple-header hinge loss. The learned embedding produces features that preserve not only locality across the same-class clusters but also discrimination between classes. We demonstrate that such "quintuplet loss" introduces a tighter constraint for reducing imbalance in the local data neighborhood when compared to existing triplet loss. We also study the effectiveness of classic schemes of class re-sampling and cost-sensitive learning in our context.</p><p>Our key contributions are as follows: <ref type="formula">(1)</ref> we show how to learn deep feature embeddings for imbalanced data classification, which is understudied in the literature; (2) we formulate a new quintuplet sampling method with the associated triple-header loss that preserves locality across clusters and discrimination between classes. Using the learned features, we show that classification can be simply achieved by a fast cluster-wise kNN search followed by a local large margin decision. The proposed method, called Large Margin Local Embedding (LMLE)-kNN, achieves state-of-the-art results in the large-scale imbalanced classification tasks of (binary) face attributes and (multi-class) image edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Previous efforts to tackle the class imbalance problem can be mainly divided into two groups: data re-sampling <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30]</ref> and cost-sensitive learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48]</ref>. The former group aims to alter the training data distribution to learn equally good classifiers for the majority and minority classes, usually by random under-sampling and over-sampling techniques. The latter group, instead of manipulating samples at the data level, operates at the algorithmic level by adjusting misclassification costs. A comprehensive literature survey can be found in <ref type="bibr" target="#b17">[18]</ref>.</p><p>A well-known issue with replication-based random oversampling is its tendency to overfit. More radically, it does not actually increase any information, and fails in solving the fundamental "lack of data" problem. To address this, SMOTE <ref type="bibr" target="#b6">[7]</ref> creates new non-replicated examples by interpolating neighboring minority class instances. Several variants of SMOTE <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30]</ref> followed for improvements. However, their broadened decision regions are still error-prone by synthesizing noisy and borderline examples. Therefore under-sampling is often preferred to over-sampling <ref type="bibr" target="#b10">[11]</ref>, although potentially valuable information may be removed. Cost-sensitive alternatives avoid these problems by directly imposing heavier penalty on misclassifying the minority class. For example, in <ref type="bibr" target="#b39">[40]</ref> the classic SVM is made cost-sensitive to improve classification on highly skewed datasets. Zadrozny et al. <ref type="bibr" target="#b45">[46]</ref> combined cost sensitivity with ensemble approaches to further improve classification accuracy. Many other methods follow this practice of designing classifier ensemble to combat imbalance (e.g., <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b40">41]</ref>), and boosting <ref type="bibr" target="#b40">[41]</ref> offers an easy way to embed the costs by updating example weights. Chen et al. <ref type="bibr" target="#b8">[9]</ref> resorted to bagging which is less vulnerable to noise than boosting, and generated a cost-sensitive version of random forest.</p><p>None of the above works addresses the class imbalance learning using CNN. They rely on shallow models and hand-crafted features. To our knowledge, only few works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b47">48]</ref> approach imbalanced classification via deep learning. Jeatrakul et al. <ref type="bibr" target="#b20">[21]</ref> treated the Complementary Neural Network as an under-sampling technique, and combined it with SMOTE over-sampling to balance training data. Zhou and Liu <ref type="bibr" target="#b47">[48]</ref> studied data resampling in training cost-sensitive neural networks. Khan et al. <ref type="bibr" target="#b21">[22]</ref> further seek for joint optimization of the class-sensitive costs and deep features. These works can be seen as natural extensions to existing imbalanced learning techniques, while neglecting the underlying data structure for discriminating imbalanced data. Motivated from this, we propose a "data structureaware" deep learning approach with built-in margins for imbalanced classification, where the classic schemes of data resampling and cost-sensitive learning are also studied systematically. Attribute recognition: Face attributes are useful as midlevel features for many applications like face verification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. It is challenging to predict them from unconstrained face images due to the large facial variations such as pose and lighting. Most existing methods for attribute recognition extract hand-crafted features from images, which are then fed into some classifiers to predict the presence of an array of face attributes, e.g., "male", "smile", etc. Examples are <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> where HOG-like features are extracted on various local face regions to predict attributes. Recent deep learning methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b46">47]</ref> excel by learning powerful features. Zhang et al. <ref type="bibr" target="#b46">[47]</ref>, for instance, trained pose-normalized CNNs for deep attribute modeling. These studies, however, share a common drawback: they neglect the class imbalance issue in those relatively rare attributes like "big nose" and "bald". Thus skewed results are expected when predicting the positive class of these underrepresented attributes. Edge detection: State-of-the-art edge detection methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33]</ref> mostly use engineered gradient features to classify edge patches against non-edges. Due to the large variety of edge structures, such a binary classification problem is usually transformed to a multi-class one. <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">35]</ref> first cluster edge patches into hundreds of subclasses, then the goal becomes predicting whether an input patch belongs to each edge subclass or the non-edge class. The final binary task can be accomplished by ensembling classification scores, which works well under the condition of equal amounts of edge and non-edge samples used. Built on the same assumption, recent CNN-based methods  <ref type="figure">Figure 1</ref>. Embeddings by (a) triplet vs. by (b) quintuplet. We exemplify the class imbalance by two different sized classes, where the clusters are formed by k-means. Our quintuplets enforce both inter-cluster and inter-class margins, while triplets only enforce interclass margins irrespective of different class sizes and variations. This difference leads the unique capability of quintuplets in preserving discrimination in any local neighborhood, and forming a local classification boundary that is insensitive to imbalanced class sizes. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b44">45]</ref> achieve better results by learning deep features. However, all these methods would face the problem of data imbalance between each edge subclass and the dominant non-edge class, which is barely addressed properly. Only Shen et al. <ref type="bibr" target="#b34">[35]</ref> attempt to regularize the multiway softmax loss with a balanced weighting between the "positive" super-class and "negative" class, which is a compromise between the binary and multi-class tasks. Here we propose to explicitly learn discriminative features from imbalanced data.</p><formula xml:id="formula_0">x i x p i x n i x i x p+ i x pâˆ’ i x pâˆ’âˆ’ i x n i (x i ,x p i ,x n i ) (x i ,x p+ i ,x pâˆ’ i ,x pâˆ’âˆ’ i ,x n i )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning Deep Representation from ClassImbalanced Data</head><p>Given an imagery dataset with imbalanced class distribution, our goal is to learn a Euclidean embedding f (x) from an image x into a feature space R d , such that the embedded features are discriminative without any possible local class imbalance. We constrain this embedding to live on a d-dimensional hypersphere, i.e., ||f (x)|| 2 = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Quintuplet Sampling</head><p>To achieve the aforementioned goal, we select quintuplets from the imbalanced data as illustrated in <ref type="figure">Fig. 1</ref>. Each quintuplet is defined as:</p><formula xml:id="formula_1">â€¢ xi : an anchor, â€¢ x p+ i</formula><p>: the anchor's most distant within-cluster neighbor,</p><formula xml:id="formula_2">â€¢ x pâˆ’ i</formula><p>: the nearest within-class neighbor of the anchor, but from a different cluster,</p><formula xml:id="formula_3">â€¢ x pâˆ’âˆ’ i</formula><p>: the most distant within-class neighbor of the anchor,</p><formula xml:id="formula_4">â€¢ x n i</formula><p>: the nearest between-class neighbor of the anchor.</p><p>We wish to ensure that the following relationship holds in the embedding space:</p><formula xml:id="formula_5">D(f (x i ), f (x p+ i )) &lt; D(f (x i ), f (x pâˆ’ i )) &lt; D(f (x i ), f (x pâˆ’âˆ’ i )) &lt; D(f (x i ), f (x n i )), (1) where D(f (x i ), f (x j )) = f (x i ) âˆ’ f (x j ) 2 2 is the Eu- clidean distance.</formula><p>Such a fine-grained similarity defined by quintuplets has two merits: 1) The ordering in Eq. <ref type="formula">(1)</ref> provides richer information and a stronger constraint than the conventional class-level image similarity. In the latter, two images are considered similar as long as they belong to the same category. In contrast, we require two instances to be close at both class-and cluster-levels to be considered similar. This actually helps build a local classification boundary with the most discriminative local samples. Other irrelevant samples in a class are effectively "ignored" for class separation, making the local boundary robust and insensitive to imbalanced class sizes.</p><p>2) The quintuplet sampling is repeated during CNN training, thus avoiding large information loss as in traditional random under-sampling. When compared with over-sampling strategies, it introduces no artificial noise. In practice, to ensure adequate learning for all classes, we collect quintuplets for equal numbers of minority-and majority-class samples x i in one mini-batch. Section 5 will quantify the efficacy of this re-sampling scheme.</p><p>Note in the above, we implicitly assume the imbalanced data are already clustered so that quintuplets can be sampled. In practice, we obtain the initial clusters for each class by applying k-means on some prior features (e.g., for face attribute recognition, we employ the pre-trained DeepID2 features <ref type="bibr" target="#b38">[39]</ref> on the face verification task). To make the clustering more robust, an alternating scheme is formulated to refine the clusters using features extracted from the proposed model itself every 5000 iterations. The overall pipeline will be summarized in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Triple-Header Hinge Loss</head><p>To enforce the relationship in Eq. 1 during feature learning, we apply the large margin idea using the sampled quintuplets. A triple-header hinge loss is formulated to constrain three margins between the four distances, and we de-  fine the following objective function with slack allowed:</p><formula xml:id="formula_6">R 2 space x pâˆ’âˆ’ i x i x p+ i x pâˆ’ i x n i f (x i ) f (x p+ i ) f (x pâˆ’ i ) f (x pâˆ’âˆ’ i ) f (x n i )</formula><formula xml:id="formula_7">min i (Îµ i + Ï„ i + Ïƒ i ) + Î» W 2 2 , s.t. : max 0, g 1 + D(f (x i ), f (x p+ i )) âˆ’ D(f (x i ), f (x pâˆ’ i )) â‰¤ Îµ i , max 0, g 2 + D(f (x i ), f (x pâˆ’ i )) âˆ’ D(f (x i ), f (x pâˆ’âˆ’ i )) â‰¤ Ï„ i , max 0, g 3 + D(f (x i ), f (x pâˆ’âˆ’ i )) âˆ’ D(f (x i ), f (x n i )) â‰¤ Ïƒ i , âˆ€i, Îµ i â‰¥ 0, Ï„ i â‰¥ 0, Ïƒ i â‰¥ 0<label>(2)</label></formula><p>where Îµ i , Ï„ i , Ïƒ i are the slack variables, g 1 , g 2 , g 3 are the margins, W represents the parameters of the CNN embedding function f (Â·), and Î» is a regularization parameter. This formulation effectively regularizes the deep representation learning based on the ordering specified in quintuplets, imposing a tighter constraint than triplets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42]</ref>. Ideally, in the hypersphere embedding space, clusters should collapse into small neighborhoods with safe margin g 1 between one another, and g 2 being the largest within a class, and their containing class is also well separated by a large margin g 3 from other classes (see <ref type="figure" target="#fig_2">Fig. 2(a)</ref>). A merit of our learning algorithm is that the margins can be explicitly determined by a geometric intuition. Suppose there are L training samples in total, class c is of size L c , c = 1, . . . , C. Let all the classes constitute s âˆˆ [0, 1] of the entire hypersphere, and we generate clusters of equalsize l for each class. Obviously, the margins' lower bounds are zero. For their upper bounds, g = 2 sin(Ï€/C) when all classes collapse into single points. In practice, we try several decreasing margins by a coarse grid search before actual training.</p><p>The learning network architecture is shown in <ref type="figure" target="#fig_2">Fig. 2(b)</ref>. Given a class evenly re-sampled mini-batch, we retrieve for each x i in it a quintuplet by using a lookup table computed offline. To generate a table of meaningful and discriminative quintuplets, instead of selecting the "hardest" ones from the entire training set, we select "semi-hard" ones by computing distances on a random subset (50%) of training data to avoid those mislabelled or poor quality data. Then each quintuplet member is fed independently into five identical CNNs with shared parameters. Finally, the output feature embeddings are L 2 normalized and used to compute a triple-header hinge loss by Eq. 2. Back-propagation is used to update the CNN parameters.</p><p>To further ensure equal learning for the imbalanced classes, we assign samples in each mini-batch costs such that the class weights therein are identical. The specific cost-sensitive schemes for different classification tasks will be detailed in Section 5 with supporting experiments. Below we summarize the learning steps of the proposed LMLE approach:</p><p>1. Cluster for each class by k-means using the learned features from previous round of alternation. For the first round, we use hand-crafted features or prior features obtained from other pre-trained network. 2. Generate a quintuplet table using the cluster and class labels from a random subset (50%) of training data. 3. For CNN training, repeatedly sample mini-batches equally from each class and retrieve the corresponding quintuplets from the offline table. 4. Feed all quintuplets into five identical CNNs to compute the loss in Eq. 2 with cost-sensitivities. 5. Back-propagate the gradients to update the CNN parameters and feature embeddings. 6. Alternate between steps 1-2 and 3-5 every 5000 iterations until convergence (empirically within 4 rounds when we observe no improvements on validation set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Differences between "Quintuplet" Loss and Triplet Loss</head><p>The triplet loss is inspired by Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) <ref type="bibr" target="#b14">[15]</ref> and Large Margin Nearest Neighbor (LMNN) <ref type="bibr" target="#b43">[44]</ref>. It is widely used in many recent vision studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42]</ref>, aiming to bring data of the same class closer, while data of different classes further away (see <ref type="figure">Fig. 1</ref>). To enforce such a relationship, one needs to generate mini-batches of triplets, i.e., an anchor x i , a positive instance x p i of the same class, and a negative instance x n i of different class, for deep feature learning. We argue that they are rather limited in capturing the embedding structure of imbalanced data. Specifically, the similarity information is only extracted at the class-level, which would homogeneously collapse each class irrespective of their different degrees of variation. As a result, the class structures are lost. When a class has high data variability, it is also hard to maintain the class-wise margin, leading to potential invasion of imposter neighbors or even domination of the majority class in local neighborhood. By contrast, the proposed LMLE generates diverse quintuplets that differ in the membership of both clusters and classes. It thus captures the considerable data variability within each class, and can easily enforce the local margin to reduce any local class imbalance. It is worth noting that Wang et al. <ref type="bibr" target="#b41">[42]</ref> also aim to learn fine-grained similarity within classes but they do not explicitly model the within-class variations by clustering like us. <ref type="figure" target="#fig_4">Fig. 3</ref> illustrates our advantage. Section 5 will further quantify the benefits of our quintuplet loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Nearest Neighbor Imbalanced Classification</head><p>The above LMLE approach offers crucial feature representations for the following classification to perform well on imbalanced data. We choose the simple kNN classifier to show the efficacy of our learned features. Better performance is expected with the use of more elaborated classifiers.</p><p>A traditional kNN classifier predicts the class label of query q as the majority label among its kNN in the training set</p><formula xml:id="formula_8">P = {(x i , y i )} L i=1</formula><p>, where y i = 1, . . . , C is the (binary or multi-way) class label of sample x i . Such kNN rule is appealing due to its non-parametric nature, and it is easy to extend to new classes without retraining. However, the underlying equal-class-density assumption is not satisfied and will greatly degrade performance in our imbalanced case.</p><p>Hence we modify the kNN classifier in two ways: 1) In the well-clustered LMLE space learned in training stage, we treat each cluster as a single class-specific exemplar 2 , <ref type="bibr" target="#b1">2</ref> Clustering to aid classification is common in the literature <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>and perform a fast cluster-wise kNN search. 2) Let Ï†(q) be query q's local neighborhood defined by its kNN cluster centroids</p><formula xml:id="formula_9">{m i } k i=1</formula><p>. We seek a large margin local boundary among Ï†(q), labelling q as the class to which the maximum cluster distance is smaller than the minimum cluster distance to any other class by the largest margin:</p><formula xml:id="formula_10">y q = arg max c=1,...,C ï£« ï£¬ ï£­ min mj âˆˆÏ†(q) yj =c D(f (q), f (m j )) âˆ’ max miâˆˆÏ†(q) yi=c D(f (q), f (m i )) ï£¶ ï£¸ .<label>(3)</label></formula><p>This large margin local decision offers two advantages: (i) More resistance to data imbalance: Recall that we fix the cluster size l rather than the number of clusters for each class (to avoid large quantization errors for the minority classes). Thus all the âŒŠL c /lâŒ‹ clusters from different classes c = 1, . . . , C still exhibit class imbalance. But the large margin rule in Eq. 3 can well solve this issue because it is independent of the cluster number in each class. It is also very suited to our LMLE representation which is learned under the same large margin rule.</p><p>(ii) Fast speed: Decision by cluster-wise kNN search is much faster than by sample-wise search. Finally we summarize the steps for our kNN-based imbalanced classification: for query q,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Find its kNN cluster centroids {m</head><formula xml:id="formula_11">i } k i=1</formula><p>from all classes learned in the training stage. 2. If all the k cluster neighbors belong to the same class, q is labelled by that class and exit. 3. Otherwise, label q as y q using Eq. 3.</p><p>Note the cluster-wise kNN search can be further sped up using some conventional tricks. For instance, we utilize the KD-tree <ref type="bibr" target="#b35">[36]</ref> whose runtime is logarithmic in the number of all clusters (âŒŠL/lâŒ‹) with a complexity of O(L/l log(L/l)) in comparison to O(L log L) by sample-wise search. This leads to up to three orders of magnitude speedup over standard kNN classification in practice, making it easy to scale to large datasets with a large number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Discussion</head><p>There are many previous studies of identifying nearest neighbors that focus on distance metric learning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b43">44]</ref>, but barely on imbalanced learning. Distance metric learning improves kNN search by optimizing parametric distance functions. For instance, Globerson and Roweis <ref type="bibr" target="#b13">[14]</ref> learn the Mahalanobis distance which requires all the same-class samples collapse to one point. However, those classes with high data variability cannot be effectively squeezed to single points <ref type="bibr" target="#b30">[31]</ref>. In comparison, our LMLE-kNN captures within-class variations across clusters, only which are "collapsed" in the embedding space, thus offering both accuracy and speed advantages for the kNN search. In <ref type="bibr" target="#b43">[44]</ref>, the Mahalanobis distance is learned to directly improve the large margin kNN classification, but not under the imbalanced circumstances. Liu and Chawla <ref type="bibr" target="#b27">[28]</ref> provide a weighting scheme over kNN to correct the classification bias. We "correct" this bias more effectively by a large margin kNN classifier with accordingly learned deep features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We study the high-level face attribute classification task and low-level edge detection task, both with large-scale imbalanced data. The originally balanced MNIST digit classification is also studied to experiment with controlled class imbalance. Our face attribute is binary, with severely imbalanced positive and negative samples (e.g., "Bald" attribute: 2% vs. 98%). Our approach predicts 40 attributes simultaneously in a multi-task framework. Edge detection is cast as a multi-class classification problem to address the diversity of edges, i.e., to predict whether an input image patch belongs to any edge class (shape) or the non-edge class. Since the ultimate goal is still binary, the "positive" edge patches and "negative" non-edge patches are usually equally sampled for training. Thus severe imbalance exists between the edge classes (power-law) and dominant non-edge class.</p><p>Datasets and evaluation metrics: For face attributes, we use CelebA dataset <ref type="bibr" target="#b28">[29]</ref> that contains 10000 identities, each with about 20 images. Every face image is annotated with 40 attributes and 5 key points to align it to 55 Ã— 47 pixels. We partition the dataset following <ref type="bibr" target="#b28">[29]</ref>: the first 160 thousand images (i.e., 8000 identities) for training (10 thousand images for validation), the following 20 thousand for training SVM classifiers for the PANDA <ref type="bibr" target="#b46">[47]</ref> and ANet <ref type="bibr" target="#b28">[29]</ref> methods, and remaining 20 thousand for testing. To account for the imbalanced positive and negative attribute samples, a balanced accuracy is adopted, that is accuracy = 0.5(t p /N p +t n /N n ), where N p and N n are the numbers of positive and negative samples, while t p and t n are the numbers of true positive and true negative. Note that this evaluation metric differs from that employed in <ref type="bibr" target="#b28">[29]</ref>, where accuracy = ((t p + t n )/(N p + N n )), which can be biased to the majority class.</p><p>For edge detection, we use the BSDS500 <ref type="bibr" target="#b0">[1]</ref> dataset that contains 200 training, 100 validation and 200 testing images. We sample 2 million 45 Ã— 45 pixel training patches, where the numbers of edge and non-edge ones are the same and there are 150 edge classes formed by k-means. Note the ultimate goal of edge detection is to produce a full-scale edge map for one input image, instead of predicting edge classes for local patches by Eq. 3. To estimate the edge map in a robust way, we follow <ref type="bibr" target="#b12">[13]</ref> to simply transfer and fuse the overlapping edge label patches of the nearest neigh- bors found using our learned representations. After applying non-maximal suppression, we evaluate edge detection accuracy by: fixed contour threshold (ODS), per-image best threshold (OIS), and average precision (AP) <ref type="bibr" target="#b0">[1]</ref>.</p><p>The MNIST experiments are carried out on the challenging dataset of MNIST-rot-back-image <ref type="bibr" target="#b25">[26]</ref>. This extension dataset contains 28 Ã— 28 digit images with large rotations and random backgrounds. There are 12000 training images and 50000 testing images, and we augment the training set 10 times by randomly rotating, mirroring and resizing images, leaving 10% in it for validation. We report the mean per-class accuracy in the artificial imbalanced settings.</p><p>Parameters: Our CNN is trained using batch size 40, momentum 0.9, and Î» = 0.0005 in Eq. 2. We form clusters for each class with sizes around l = 200. For those homogeneous classes (with only one possible cluster), our withinclass margins actually become nearly zero, which hurts no performance but only reduces quintuplets to the triplets as a lower bound baseline. We search k = 20 nearest clusters (i.e., |Ï†(q)| = 20 in Eq. 3) for querying. Other taskspecific settings are summarized in <ref type="table" target="#tab_0">Table 1</ref>. Note the prior features for clustering are not critical to the final results because we will gradually learn deep features in alternation with their clustering every 5000 iterations. Different prior features generally converge to similar results, but at different speeds. <ref type="table">Table 2</ref> compares our LMLE-kNN method for multiattribute classification with the state-of-the-art TripletkNN <ref type="bibr" target="#b33">[34]</ref>, PANDA <ref type="bibr" target="#b46">[47]</ref> and ANet <ref type="bibr" target="#b28">[29]</ref> methods, which are trained using the same images and tuned to their best performance. The attributes and their mean per-class accuracies are given in the order of ascending class imbalance level (= |positive class rate-50|%) to reflect its impact on performance. It is shown that LMLE-kNN consistently outperforms other methods across all face attributes, with an average gap of 4% over the runner-up ANet. Considering most face attributes exhibit high class imbalance with an average positive class rate of only 23%, such improvements are nontrivial and prove our features' representation power on imbalanced data. Although the competitive PANDA and <ref type="table">Table 2</ref>. Mean per-class accuracy (%) and class imbalance level (= |positive class rate-50|%) of each of the 40 face attributes on CelebA dataset <ref type="bibr" target="#b28">[29]</ref>. Attributes are sorted in an ascending order by the imbalance level. To account for the imbalanced positive and negative attribute samples, a balanced accuracy is adopted, unlike <ref type="bibr" target="#b28">[29]</ref>. The results of ANet are therefore different from that reported in <ref type="bibr" target="#b28">[29]</ref>.  Relative accuracy gain (%)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison with State-of-the-Art Methods</head><p>Class imbalance level (%)</p><p>Face attribute</p><p>Over PANDA <ref type="bibr" target="#b31">[32]</ref> Over Triplet-kNN <ref type="bibr" target="#b21">[22]</ref> M or e im ba la nc ed   <ref type="table">Table 2</ref>.</p><p>ANet are capable of learning a robust representation by ensembling and multi-task learning respectively, they ignore the imbalance issue and thus struggle for highly imbalanced attributes, e.g., "Bald". Compared with the closely related triplet sampling method <ref type="bibr" target="#b33">[34]</ref>, our quintuplet sampling better preserves the embedding discrimination on imbalanced data. The advantage is more evident while observing the relative accuracy gains over other methods in <ref type="figure" target="#fig_7">Fig. 4</ref>. The gains tend to increase with a higher class imbalance level. <ref type="table">Table 3</ref> reports our edge detection results under three metrics. Compared with the classification-based Sketch Token <ref type="bibr" target="#b26">[27]</ref> and DeepContour <ref type="bibr" target="#b34">[35]</ref> methods, our LMLEkNN outperforms by a large margin due to the explicit handling of class imbalance during feature learning. Our ap- <ref type="table">Table 3</ref>. Edge detection results on BSDS500 <ref type="bibr" target="#b0">[1]</ref>. In the bottom cell are deep learning-based methods. *ICCV paper results.</p><p>Method ODS OIS AP gPb-owt-ucm <ref type="bibr" target="#b0">[1]</ref> 0.73 0.76 0.73 Sketch Token <ref type="bibr" target="#b26">[27]</ref> 0.73 0.75 0.78 SCG <ref type="bibr" target="#b32">[33]</ref> 0.74 0.76 0.77 PMI+sPb <ref type="bibr" target="#b19">[20]</ref> 0.74 0.77 0.78 SE <ref type="bibr" target="#b9">[10]</ref> 0.75 0.77 0.80 OEF <ref type="bibr" target="#b15">[16]</ref> 0.75 0.77 0.82 SE+multi-ucm <ref type="bibr" target="#b1">[2]</ref> 0.75 0.78 0.76 DeepNet <ref type="bibr" target="#b22">[23]</ref> 0.74 0.76 0.76 N 4 -Fields <ref type="bibr" target="#b12">[13]</ref> 0.75 0.77 0.78 DeepEdge <ref type="bibr" target="#b3">[4]</ref> 0.75 0.77 0.81 DeepContour <ref type="bibr" target="#b34">[35]</ref> 0.76 0.77 0.80 HFL <ref type="bibr" target="#b4">[5]</ref> 0.77 0.79 0.80 CS-SE+DSNA <ref type="bibr" target="#b18">[19]</ref>  proach provides more robust feature representations than only weighting in <ref type="bibr" target="#b34">[35]</ref>. This is partially validated by the fact that while <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b34">[35]</ref> need to train random forests on features to achieve competitive results, ours only relies on kNN label transfer as in <ref type="bibr" target="#b12">[13]</ref>, but with better results. <ref type="figure">Fig. 5</ref> shows their visual differences, where LMLE-kNN can accurately discover fine rare edges as well as the majority nonedges that make edge maps clean. Sketch Token <ref type="bibr" target="#b26">[27]</ref> and DeepContour <ref type="bibr" target="#b34">[35]</ref> suffer from the prediction bias with noisy edges and relatively low recall of fine edges respectively. LMLE-kNN also outperforms <ref type="bibr" target="#b12">[13]</ref> and our previous work in <ref type="bibr" target="#b18">[19]</ref> by learning effective deep features from imbalanced data. Recent HFL <ref type="bibr" target="#b4">[5]</ref>, HED <ref type="bibr" target="#b44">[45]</ref> methods utilize large VG- <ref type="figure">Figure 5</ref>. Edge detection examples on BSDS500 <ref type="bibr" target="#b0">[1]</ref>. From left to right: input image, ground truth, Sketch Token <ref type="bibr" target="#b26">[27]</ref>, DeepContour <ref type="bibr" target="#b34">[35]</ref>, LMLE-kNN. Note the visual differences in the red box.</p><p>GNet <ref type="bibr" target="#b36">[37]</ref> (13-16 layers) for good performance. In contrast, we only use a lightweight network as in <ref type="bibr" target="#b34">[35]</ref> (6 layers).</p><p>Computation time: LMLE-kNN takes about 4 days to train for 4 alternative rounds on GPU, and 10ms per sample to extract features. The cluster-wise kNN search is typically 1000 times faster than standard kNN. This enables real-time application to large-scale problems, e.g. the above two with hundreds of thousands to millions of samples. <ref type="table" target="#tab_3">Table 4</ref> quantifies the benefits of our quintuplet loss and the re-sampling and cost-sensitive schemes. On both imbalanced tasks, we find favorable performance using the classic schemes, while the proposed quintuplet loss leads to much larger performance gains over baselines. This strongly supports the necessity of imposing additional cluster-wise relations in our quintuplets. Such constraints can better preserve local class structures than triplets, which is critical for ameliorating the invasion of imposter neighbors. Note when the cost-sensitive scheme is applied to strictly balanced resampled mini-batches, it would have no effects since the class weights are already equal. However in the case of predicting multiple face attributes, class-balanced data for one attribute will be almost certainly imbalanced for the other attributes, whose class costs can help then. In the case of edge detection, costs can help by further regularizing the impacts of the positive super-class and negative class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Tests and Control Experiments</head><p>To highlight the effects of our large margin clusterwise kNN classifier, we replace it with the regular instance-wise kNN classifier in our full method 'Quintuplet+resample+cost'. As expected, we observe much lower speed and worse results (81% for attributes, 0.77 for edge).</p><p>We finally conduct control experiments on the MNISTrot-back-image dataset <ref type="bibr" target="#b25">[26]</ref>. It is originally balanced among 10 digit classes, and we form a Gaussian-like imbalanced class distribution by randomly removing data with increasing amounts (thus more imbalanced). We compare the mean per-class accuracies between 3 baselines in <ref type="table">Table 5</ref>, where our full method degrades much more gracefully with increasing imbalance than others. This again validates the efficacy of our quintuplet loss and applied schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Class imbalance is common in many vision tasks. Contemporary deep representation learning methods typically adopt class re-sampling or cost-sensitive learning. Through extensive experiments, we have validated their usefulness and further demonstrated that the proposed quintuplet sampling with triple-header loss works remarkably well for imbalanced learning. Our method has been shown superior to the triplet loss, which is commonly adopted for large margin learning but does not enforce the inter-cluster margins in quintuplets. Generalization to higher-order relationships beyond explicit clusters is a future direction to explore.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. (a) Feature distribution in 2D space and the geometric intuition of margins. (b) Our learning network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>when all âŒŠL/lâŒ‹ clusters are squeezed into single points on a propor- tion s of the sphere. Hence g max 1 = 2 sin(Ï€ * sl/L), and g max 2 can be approximated as 2 sin(Ï€ * s(L c âˆ’ l)/L) via tri- angle inequality. g max 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. From left to right: 2D feature embedding of one imbalanced binary face attribute using DeepID2 model [39] (i.e., prior feature model), triplet-based embedding, quintuplet-based LMLE. We only show 2 Positive Clusters (PC) and 5 Negative Clusters (NC) out of a total of 499 clusters to represent the imbalance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Relative accuracy gains over competitors on the sorted 40 face attributes in Table 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Implementation details for the imbalanced classification tasks considered. For each task, we list its used CNN architecture</figDesc><table>(top left), prior features for clustering (top right), and class-specific 
cost (bottom). 
Face 
same w.r.t. [39] 
DeepID2 features in [39] 
attributes 
inv. to class size in batches 
same w.r.t. [35] 
low-level features in [10] 
Image 
inv. to class size in batches &amp; 
edges 
positive sharing weight [35] 
MNIST 
same w.r.t. [38] pre-trained features by softmax 
digits 
inv. to class size in batches 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Ablation tests on attributes classification (average accu- racy -%) and edge detection (ODS).Table 5. Control experiments on class imbalance on MNIST-rot- back-image dataset [26]. The mean per-class accuracy (%) is re- ported for each case.</figDesc><table>Methods 
Softmax 
+resample +resample+cost 
Attribute 
68.07 
69.43 
70.16 
Edge 
0.72 
0.73 
0.73 
Methods 
Triplet 
+resample +resample+cost 
Attribute 
71.29 
71.75 
72.43 
Edge 
0.73 
0.73 
0.74 
Methods Quintuplet +resample +resample+cost 
Attribute 
81.31 
83.39 
84.26 
Edge 
0.76 
0.77 
0.78 

Remove (%) 
0 
20 
40 
Triplet+resample+cost 
76.12 67.18 56.49 
Quintuplet 
77.62 72.26 65.27 
Quintuplet+resample+cost 77.64 75.58 70.13 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This work is partially supported by SenseTime Group Limited and the Hong Kong Innovation and Technology Support Programme.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>ArbelÃ¡ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">POOF: Part-Based One-vsOne Features for fine-grained categorization, face verification, and attribute estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deepedge: A multiscale bifurcated deep network for top-down contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">High-for-low and lowfor-high: Efficient boundary detection from deep object features and its applications to high-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ask the locals: Multi-way local pooling for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An online algorithm for large scale image similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Using random forest to learn imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<pubPlace>Berkeley</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of California</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fast edge detection using structured forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">C4.5, class imbalance, and cost sensitivity: Why under-sampling beats over-sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Holte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMLW</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Why does unsupervised pre-training help deep learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>JMLR</publisher>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">N4-fields: Neural network nearest neighbor fields for image transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Metric learning by collapsing classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Oriented edge forests for boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hallman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Borderline-SMOTE: A new over-sampling method in imbalanced data sets learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-H</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIC</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Discriminative sparse neighbor approximation for imbalanced learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.01197</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Crisp boundary detection using pointwise mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Classification of imbalanced data by combining the complementary neural network and SMOTE algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jeatrakul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICONIP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Cost sensitive learning of deep feature representations from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.03422v1</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual boundary prediction: A deep neural prediction network and quality dissection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Kivinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Describable visual attributes for face verification and image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1962" to="1977" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sketch tokens: A learned mid-level representation for contour and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Class confidence weighted kNN algorithms for imbalanced data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAKDD</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Local neighbourhood extension of SMOTE for mining imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stefanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDM</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A deep non-linear feature mapping for large-margin kNN classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Discriminatively Trained Sparse Code Gradients for Contour Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deepcontour: A deep convolutional feature learned by positivesharing loss for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Optimised KD-trees for fast image descriptor matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Silpa-Anan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Discriminative transfer learning with tree-based priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SVMs modeling for highly imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TSMC</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="281" to="288" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A comparative study of cost-sensitive boosting algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Ting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Locality-constrained linear coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cost-sensitive learning by cost-proportionate example weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zadrozny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">PANDA: Pose aligned networks for deep attribute modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Training cost-sensitive neural networks with methods addressing the class imbalance problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="77" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
