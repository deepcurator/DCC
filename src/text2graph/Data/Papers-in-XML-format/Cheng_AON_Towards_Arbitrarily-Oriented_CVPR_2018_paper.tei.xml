<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AON: Towards Arbitrarily-Oriented Text Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hikvision Research Institute</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangliu</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tongji University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bai</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Niu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hikvision Research Institute</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
							<email>pushiliang@hikvision.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hikvision Research Institute</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuigeng</forename><surname>Zhou</surname></persName>
							<email>sgzhou@fudan.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AON: Towards Arbitrarily-Oriented Text Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recognizing text from natural images is a hot research topic in computer vision</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene text recognition has attracted much research interest of the computer vision community <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">39]</ref> because of its various applications such as road sign recognition and navigation reading for advanced driver assistant system (ADAS). Though Optical Character Recognition (OCR) has been extensively studied for several decades, recognizing texts from natural images is still a challenging task due to complicated environments (e.g. uneven lighting, blurring, perspective distortion and orientation). * Corresponding author.</p><p>In the past years, there have been many works to solve scene text recognition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">39]</ref>. Although these approaches have shown promising results, most of them can effectively handle only regular texts that are often tightlybounded, horizontal and frontal. However, in real-world applications, many scene texts are in irregular arrangements (e.g. arbitrarily-oriented, curved, slant and perspective etc.) as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, so most existing methods cannot be widely applied in practice. Recently, there are two related works aiming at irregular texts: the spatial transformer network (STN) <ref type="bibr" target="#b17">[18]</ref> based method by <ref type="bibr" target="#b31">[32]</ref> and the attention-based method with fully convolutional network (FCN) <ref type="bibr" target="#b22">[23]</ref> by <ref type="bibr" target="#b38">[39]</ref>. Shi et al. <ref type="bibr" target="#b31">[32]</ref> attempted to first rectify irregular (e.g. curved or perspectively distorted) texts to approximately regular texts, then recognized the rectified images with an attentionbased sequence recognition network. However, in complicated (e.g. arbitrarily-oriented or serious curved) natural scenes, it is hard to optimize the STN-based method without human-labeled geometric ground truth. Besides, training STN needs sophisticated skills. For example, the thin-plate-spline (TPS) <ref type="bibr" target="#b4">[5]</ref>-based STN <ref type="bibr" target="#b31">[32]</ref> should be given some initialization pattern for the fiducial points, and is not quite effective for arbitrarily-oriented scene texts. Yang et al. <ref type="bibr" target="#b38">[39]</ref> introduced an auxiliary dense character detection task for encouraging the learning of visual representations with a fully convolutional network. Though the method showed better performance on irregular texts, it was carried out with an exhausting multi-task learning (MTL) strategy and relied on character-level bounding box annotations. Note that, though the attention-based model has the potential to perform 2D feature selection <ref type="bibr" target="#b37">[38]</ref>, we found in experiments that directly training attention-based model on irregular texts is difficult due to irregular character placements. This situation motivates us to explore new and more effective methods to recognize irregular scene texts. − → H: lef t → right, ← − H:right → lef t, − → V :top → bottom and ← − V :bottom → top and four character placement clues c1, c2, c3 and c4. Here, there are three squares connected with dashed lines. The innermost square represents the four 1D sequences of features, each comes along with an arrowed line. The middle square refers to the placement clues used for weighting the corresponding sequences of features. The outermost square stands for the weighted feature sequence by conducting Hadamard product ⊙ with character placement clues and horizontal/vertical features. For the character 'a' in the image, we can represent it by the four weighted sequences of features.</p><p>From the above analysis, we can see that most existing methods directly encode a text image as a 1D sequence of features and then decode them to the predicted text, which implies that any text in an image is treated in the same direction such as from left to right by default. However, this is not true in the wild. After carefully analyzing the typical character placement styles of natural text images, we suggest that the visual representation of an arbitrarilyoriented character in a 2D image can be described in four directions: lef t → right, right → lef t, top → bottom and bottom → top. Concretely, we can encode the input image to four feature sequences of four directions: horizontal features ( − → H), reversed horizontal features ( ← − H), vertical features ( − → V ) and reversed vertical features ( ← − V ), as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, and the length of each sequence is equal. The horizontal/vertical features can be extracted by downsampling the height/width of feature maps to 1. In order to represent an arbitrarily-oriented character, a weighting mechanism can be used to combine the four feature sequences of different directions. We call the weights character placement clues, which are denoted as c 1 , c 2 , c 3 and c 4 in <ref type="figure" target="#fig_0">Fig. 1</ref>. The character placement clues can be learned from the input images with a convolutional-based network, which guides to effectively integrate the four sequences of features, and then a filter gate (FG) generates the integrated feature sequence as the character's visual representation. Therefore, an arbitrarily-oriented character in a 2D image can be represented as the combination of horizontal and vertical features by conducting the Hadamard product with the sequences of features and the corresponding placement clues. In <ref type="figure" target="#fig_0">Fig. 1</ref>, c 1 and c 2 play the dominant role in determining the visual representation of character 'a'. In this paper, we call the four-direction feature extraction network and the clues extraction network arbitrary orientation network (AON), which means that it can effectively handle arbitrarily-oriented texts.</p><p>In this paper, we develop a novel method for robustly recognizing both regular and irregular natural texts by employing the proposed arbitrary orientation network (AON).</p><p>Major contributions of this paper are as follows:</p><p>1. We propose the arbitrary orientation network (AON) to extract scene text features in four directions and the character placement clues.</p><p>2. We design a filter gate (FG) for fusing four-direction features with the learned placement clues. That is, FG is responsible for generating the integrated feature sequence.</p><p>3. We integrate AON, FG and an attention-based decoder into the character recognition framework. The whole network can be directly trained end-to-end without any character-level bounding box annotations.</p><p>4. We conduct extensive experiments on several public irregular and regular text benchmarks, which show that our method obtains state-of-the-art performance in irregular benchmarks, and is comparable to major existing methods in regular benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>In recent years, several methods have been proposed for scene text recognition. For the general information of text recognition, readers can refer to Ye and Doermann's recent survey <ref type="bibr" target="#b40">[41]</ref>. Basically, there are two types of scene text recognition approaches: bottom-up and top-down.</p><p>Traditional methods mostly follow the bottom-up pipeline: first extracting low-level features for individual character detection and recognition one by one, then integrating these characters into words based on a set of heuristic rules or a language model. For example, <ref type="bibr" target="#b26">[27]</ref> defined a set of handcrafted features such as aspect ratio, hole area ratio etc. to train a Support Vector Machine (SVM) classifier. <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> first fetched each character in the cropped word image by sliding window, then recognized it with a character classifier trained by the extracted HOG descriptors <ref type="bibr" target="#b39">[40]</ref>. However, the performance of these methods is limited due to the low representation capability of handcrafted features. With the advancement of neural-networkbased methods, many researchers developed deep neural architectures and achieved better results. <ref type="bibr" target="#b3">[4]</ref> adopted a fully connected network of 5 hidden layers for character feature representation, then used an n-gram language model to recognize characters. <ref type="bibr" target="#b36">[37]</ref> developed a CNN-based feature extraction framework for character recognition, and applied a non-maximum suppression method for final word predictions. <ref type="bibr" target="#b15">[16]</ref> also proposed a CNN-based method with structured output layer for unconstrained recognition. These above methods require the segmentation of each character, which can be very challenging because of the complicated background clutter and the inadequate distance between consecutive characters. Besides, segmentation annotations require additional resource consuming.</p><p>The other approaches work in a top-down style: directly predicting the entire text from the original image without detecting the characters. <ref type="bibr" target="#b16">[17]</ref> conducted a 90k-class classification task with a CNN, in which each class represents an English word. Consequently, the model can not recognize out-of-vocabulary words. Recent works solve this problem as a sequence recognition problem, where images and texts are separately encoded as patch sequences and character sequences, respectively. <ref type="bibr" target="#b33">[34]</ref> extracted sequences of HOG features to represent images, and generated the character sequence with the recurrent neural network (RNN). <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b30">[31]</ref> proposed the end-to-end neural networks that combines CNN and RNN for visual feature representation, then the CTC <ref type="bibr" target="#b9">[10]</ref> Loss was combined with the RNN outputs for calculating the conditional probability between the predicted and the target sequences. <ref type="bibr" target="#b21">[22]</ref> used a recursive C-NN to learn broader contextual information, and applied the attention-based decoder for sequence generation. <ref type="bibr" target="#b5">[6]</ref> proposed a focus mechanism to eliminate the attention drift to improve the regular text recognition performance. Howev- <ref type="figure">Figure 3</ref>. The network architecture of our method, which consists of four components: 1) the basal convolutional neural network (BCNN) module for low-level visual representation; 2) the arbitrary orientation network (AON) for capturing the horizontal, vertical and character placement features; 3) the filter gate (FG) for combing four feature sequences with the character placement clues; 4) the attention-based decoder (Decoder) for predicting character sequence. The above four modules are shown in the blue, golden, dull-red and brown dashed boxes, respectively. Meanwhile, all convolution or shared convolution blocks have the following format: name, c[, /(s h , sw, p h , pw)]. The bilstm and filter gate blocks are represented as name, c. The flatten, fc (fully-connected) and softmax operations have the format: name, o(c, l). Here, c, s h , sw, p h , pw, l, / and o represent the number of channels, stride height, stride width, pad height, pad width, length of feature maps, pooling operation and output shape, respectively. The whole network can be trained end-to-end.</p><p>er, since a text image is encoded into a 1D-based sequence of features, these methods can not effectively handle the irregular texts such as the arbitrarily-oriented texts. In order to recognize irregular texts, <ref type="bibr" target="#b31">[32]</ref> applied the spatial transformer network (STN) <ref type="bibr" target="#b17">[18]</ref> for text rectification, then rec-ognized the rectified text images with the sequence recognition network. <ref type="bibr" target="#b38">[39]</ref> introduced an auxiliary dense character detection task for encouraging the learning of visual representations with a fully convolutional network (FCN) <ref type="bibr" target="#b22">[23]</ref>. In practice, training STN-based methods is extremely difficult without human-labeled geometric ground truth, especially for texts in complicated (e.g. curved, arbitrarilyoriented or perspective etc.) environments. Besides, sophisticated tricks are also required. For example, to train the thin-plate-spline (TPS) <ref type="bibr" target="#b4">[5]</ref>-based STN <ref type="bibr" target="#b31">[32]</ref>-based method, the initialization pattern should be given for the fiducial points. Though <ref type="bibr" target="#b38">[39]</ref> can recognize characters in a 2D image, the method relies on the multi-task learning framework (including 3 task branches and 2 tunable super-parameters) and character-level bounding box annotations, which results in large amount of resource consuming. While obtaining better performance on irregular texts, it performs worse on regular texts.</p><p>Different from existing approaches, in this paper we first extract deep feature representations of images by using an arbitrary orientation network (AON), then use a filter gate (FG) to generate the integrated sequence of features, which are fed to an attention-based decoder for generating predicted sequences. Furthermore, we can once for all train the whole network end-to-end with only word-level annotations.</p><p>Note that in the OCR field, natural text reading systems often consist of two steps: 1) detecting each word's location in natural images and 2) recognizing text from the cropped image. In general, robust detection is helpful in recognizing texts. Therefore, several methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b42">43]</ref> have been proposed for multi-oriented text detection. Though this work focuses on the recognition task, our AON-based method can directly recognize arbitrarily-oriented texts, which alleviates the pressure of text detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Framework</head><p>The framework of whole network is shown in <ref type="figure">Fig. 3</ref>, which consists of four major components: 1) The basal convolutional neural network (a nomenclature for initial layer, denoted by BCNN) for extracting low-level visual features; 2) The arbitrary orientation network (AON) for generating four-direction sequences of features and the character placement clues; 3) The filter gate (FG) for combining the four sequences of features with the learned placement clues to generate the integrated feature sequence, and 4) the attention-based decoder for predicting character sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Basal Convolutional Neural Network (BCNN)</head><p>The BCNN module is responsible for capturing the foundational visual representation of text images, and outputs a group of feature maps. BCNN can help reduce the computational cost and graphic memory. As shown in <ref type="figure">Fig. 3</ref>, we use four convolution blocks as the foundational feature extractor. The outputs of BCNN must be square feature maps. We empirically found that higher-level feature representation as the initial state of AON can yield better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Direction Feature Extraction Module</head><p>This module includes the arbitrary orientation network (AON) and the filter gate (FG), which constitute the core of the proposed method. With the extracted foundational features, we devise AON for capturing arbitrarily-oriented text features and the corresponding character placement clues. We also design FG for integrating multi-direction features by using the character placement clues. The details of AON and FG will be described in next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Attention-based Decoder</head><p>An attention-based decoder is a recurrent neural network (RNN) that directly generates the target sequence (y 1 , ..., y M ) from an input feature sequence (ĥ 1 , ...,ĥ L ). Bahdanau et al. <ref type="bibr" target="#b2">[3]</ref> first proposed the architecture of attention-based decoder. At the t-th step, the attention module generates an output y t as follows:</p><formula xml:id="formula_0">y t = sof tmax(W T s t ),<label>(1)</label></formula><p>where W T is a learnable parameter, and s t is the RNN hidden state at time t, computed by</p><formula xml:id="formula_1">s t = RN N (y t−1 , g t , s t−1 ),<label>(2)</label></formula><p>where g t is the weighted sum of sequential feature vectorŝ</p><formula xml:id="formula_2">H : (ĥ 1 , ...,ĥ L ), that is, g t = L ∑ j=1 α t,jĥj ,<label>(3)</label></formula><p>where α t ∈ R L is a vector of the attention weights, also called alignment factors <ref type="bibr" target="#b2">[3]</ref>. In the computation of attention weights, α t is often evaluated by scoring each element inĤ separately and normalizing the scores as follows:</p><formula xml:id="formula_3">α t = Attend(s t−1 ,Ĥ),<label>(4)</label></formula><p>where Attend describes the attending process <ref type="bibr" target="#b6">[7]</ref>. Above, the RN N function in Eq. <ref type="formula" target="#formula_1">(2)</ref> represents an LST-M recurrent network. Note that the decoder is capable of generating sequences of variable lengths. Following <ref type="bibr" target="#b33">[34]</ref>, a special end-of-sequence (EOS) token is added to the target set, so that the decoder completes the generation of characters when EOS is emitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Network Training</head><p>We integrate the BCNN, AON, FG and attention decoder into one network, as shown in <ref type="figure">Fig. 3</ref>. Therefore, given an input image I, the loss function of the network is as follows:</p><formula xml:id="formula_4">L = − ∑ t lnP (ŷ t |I, θ),<label>(5)</label></formula><p>whereŷ t is the ground truth of the t-th character and θ is a vector that combines all the network parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Character Sequence Decoding</head><p>Decoding is the final process to generate the predicted characters. Following the decoding conventions, two processing modes are given: unconstrained (lexicon-free) mode and constrained mode. We execute unconstrained text recognition by directly selecting the most probable character. While in constrained text recognition, with respect to different types of lexicons (their sizes are denoted by "50", "1k" and "full" respectively), we calculate the conditional probability distributions for all lexicon words, and take the one with the highest probability as the output result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Technical Details of AON and FG</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Arbitrary Orientation Network (AON)</head><p>We develop an arbitrary orientation network consisting of the horizontal network (HN), the vertical network (VN) and the character placement clue network (CN) for extracting horizontal, vertical and placement features respectively.</p><p>The HN encodes the foundational feature maps into a sequence of horizontal feature vectors H ∈ R L×D by first performing downsampling on height directly by 5 shared convolutional blocks (described bellow) with the corresponding pooling strategy (shown in <ref type="figure">Fig. 3</ref>) to 1, and using the bidirectional LSTM to further encode the feature sequence, then generating the reversed feature sequence by conducting reverse operation (described in Eq. (6) and <ref type="formula" target="#formula_5">(7)</ref>), where L and D represent the length ofĤ and the channel number, respectively. Symmetrically, VN first rotates the square feature maps by 90 degrees, then generates the vertical feature vectors V ∈ R L×D with the same procedure as HN. Here, reversion can accelerate training convergence, thus indirectly impacts the training of CN.</p><p>Since we describe each character sequence in four directions: lef t → right, right → lef t, top → bottom and bottom → top, H and V can be represented as follows:</p><formula xml:id="formula_5">H = { − → H : (h 1 , ..., h L ) T , lef t → right ← − H : (h L , ..., h 1 ) T , right → lef t (6) V = { − → V : (v 1 , ..., v L ) T , top → bottom ← − V : (v L , ..., v 1 ) T . bottom → top<label>(7)</label></formula><p>For each text image, the CN outputs the corresponding character placement clues C ∈ R 4×L as:</p><formula xml:id="formula_6">C = (c 1 , ..., c L ) T .<label>(8)</label></formula><p>Here, for any c i ∈ R 4 , we have ∑ 4 j=1 c ij = 1, where c ij refers to the j-th direction's weight. The extraction process of clues is depicted as the green blocks in <ref type="figure">Fig. 3</ref>.</p><p>In practice, we find that it is hard to train the HN and VN respectively and simultaneously. The state of each branch is easy to corrupted on orientation distribution unbalanced training datasets. Therefore, we design a shared convolution mechanism that performs the same convolutional filter operations for both horizontal and vertical process, and the shared convolution block is shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. With the shared convolutional mechanism, the network is robust and easy to learn on orientation unbalanced training datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Filter Gate (FG)</head><p>With the captured four feature sequences and character placement clues, we design a filter gate to neglect the irrelevant features. Formally, given the i-th features</p><formula xml:id="formula_7">( − → H i , ← − H i , − → V i , ← − V i ),</formula><p>we use the corresponding placement clue c i to attend the appropriate features:</p><formula xml:id="formula_8">h ′ i = [ − → H i ← − H i − → V i ← − V i ]c i .<label>(9)</label></formula><p>Then an activation operation is performed as follows:</p><formula xml:id="formula_9">h i = tanh(ĥ ′ i ).<label>(10)</label></formula><p>Above,ĥ i indicates the i-th element ofĤ : (ĥ 1 , ...,ĥ L ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Performance Evaluation</head><p>We conduct extensive experiments to validate the proposed method on both irregular and regular recognition benchmarks. To be fair, we train the HN used in AON as the baseline model (denoted by Naive base), which is similar to the previous works focusing on regular text recognition. We also combine HN with the TPS-based STN used in <ref type="bibr" target="#b31">[32]</ref> as the STN-based control model (denoted by STN base). All control experiments are conducted with similar training data and in similar running environment. We compare our model with not only the major existing methods (including the-state-of-the-art ones), but also the above two baseline models: Naive base and STN base. Furthermore, we explore the roles of HN, VN, CN and FG in AON.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>The regular and irregular benchmarks are as follows: SVT-Perspective <ref type="bibr" target="#b27">[28]</ref> contains 639 cropped images for testing. Images are picked from side-view angle snapshots in Google Street View, therefore one may observe severe perspective distortions. Each image is associated with a 50-word lexicon and a full lexicon.</p><p>CUTE80 (CT80 in short) <ref type="bibr" target="#b28">[29]</ref> is collected for evaluating curved text recognition. It contains 288 cropped natural images for testing. No lexicon is associated.</p><p>ICDAR 2015 (IC15 in short) <ref type="bibr" target="#b20">[21]</ref> contains 2077 cropped images where more than 200 irregular (arbitrarily-oriented, perspective or curved). No lexicon is associated.</p><p>IIIT5K-Words (IIIT5K in short) <ref type="bibr" target="#b25">[26]</ref> is collected from the Internet, containing 3000 cropped word images in its test set. Each image specifies a 50-word lexicon and a 1k-word lexicon, both of which contain the ground truth words as well as other randomly picked words.</p><p>Street View Text (SVT in short) <ref type="bibr" target="#b34">[35]</ref> is collected from the Google Street View, consists of 647 word images in its test set. Many images are severely corrupted by noise and blur, or have very low resolutions. Each image is associated with a 50-word lexicon.</p><p>ICDAR 2003 (IC03 in short) <ref type="bibr">[24]</ref> contains 251 scene images, labeled with text bounding boxes. Each image is associated with a 50-word lexicon defined by Wang et al. <ref type="bibr" target="#b34">[35]</ref>. For fair comparison, we discard images that contain non-alphanumeric characters or have less than three characters, following <ref type="bibr" target="#b34">[35]</ref>. The resulting dataset contains 867 cropped images. The lexicons include the 50-word lexicons and the full lexicon that combines all lexicon words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>Network details: The deep neural network has been detailed in <ref type="figure">Fig. 3</ref>. In our network, all images are resized to 100 × 100. As for the convolutional strategy, all convolutional blocks have 3 × 3 size of kernels, 1 × 1 size of pads and 1 × 1 size of strides, and all pooling (max) blocks have 2 × 2 size of kernels. We adopt batch normalization (BN) <ref type="bibr" target="#b13">[14]</ref> and ReLU activation right after each convolution. For the character generation task, the attention is designed with an LSTM (256 memory blocks) and 37 output units (26 letters, 10 digits, and 1 EOS symbol).</p><p>Implementation and Running Environment: We train our model on 8-million synthetic data released by Jaderberg et al. <ref type="bibr" target="#b14">[15]</ref> and 4-million synthetic instances (excluding the images that contain non-alphanumeric characters) cropped from 80-thousand images <ref type="bibr" target="#b11">[12]</ref> by the ADADELTA <ref type="bibr" target="#b41">[42]</ref> optimization method. Meanwhile, we conduct data augmentation by randomly rotating each image range from 0</p><p>• to 360</p><p>• once. Our method is implemented under the Caffe framework <ref type="bibr" target="#b18">[19]</ref>. The CUDA 8.0 and CUDNN v7 backend are extensively used in our implementation, so that most modules in our method are GPU-accelerated. Our method can handle about 190/630 samples per second in the training/testing phase. The experiments are carried out on a workstation with one Intel Xeon(R) E5-2650 2.30GHz CPU, one N-VIDIA Tesla P40 GPU, and 128GB RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Performance on Irregular Datasets</head><p>Method SVT-Perspective CT80 IC15 50 Full None None None ABBYY <ref type="bibr" target="#b34">[35]</ref> 40.5 26.1 − − − Mishra et al. <ref type="bibr" target="#b10">[11]</ref> 45.7 24.7 − − − Wang et al. <ref type="bibr" target="#b36">[37]</ref> 40.2 32.4 − − − Phan et al. <ref type="bibr" target="#b27">[28]</ref> 75.6 67.0 − − − Shi et al. <ref type="bibr" target="#b30">[31]</ref> 92.6 72.6 66.8 54.9 − Shi et al. <ref type="bibr" target="#b31">[32]</ref> 91.2 77.4 71.8 59.2 − Yang et al. <ref type="bibr" target="#b38">[39]</ref> 93.0 80.2 75.8 69.3 − Cheng et al. <ref type="bibr" target="#b5">[6]</ref> 92.6 81.6 71.  <ref type="table">Table 1</ref>. Results on irregular benchmarks. "50" is lexicon size and "Full" indicates the combined lexicon of all images in the benchmarks. "None" means lexicon-free.</p><p>Recently, Cheng et al. <ref type="bibr" target="#b5">[6]</ref> proposed FAN to improve text recognition performance, which must be trained with additional character-level bounding box annotations. Here, we also compare our method with FAN on the irregular datasets. Tab. 1 summarizes the recognition results on three irregular text datasets: SVT-Perspective, CUTE80 and ICDAR15. Comparing with the existing methods' performance results released in the literature, we find that our method outperforms the existing methods on almost all benchmarks, except for SVT-Perspective with lexicon-free released by Yang et al. <ref type="bibr" target="#b38">[39]</ref>. However, it is worthy of pointing out that Yang's method <ref type="bibr" target="#b38">[39]</ref> implicates its text-reading system with both word-level and character-level bounding box annotations, which is resource consuming, while our method can be easily carried out with only word-level annotations. Tab. 1 also gives the results of the two baseline models. We can see that Naive base does not recognize irregular texts well. Though theoretically TPS-based STN can handle any irregular texts, it seems not able to satisfactorily rectify arbitrary-oriented or seriously curved texts in practice.  <ref type="figure" target="#fig_0">Fig. 1</ref>. We can see that except for the first and the third images, the other four images are not desirably rectified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>IIIT5k SVT IC03 50 1k None 50 None 50 Full None ABBYY <ref type="bibr" target="#b34">[35]</ref> 24.3 − − 35.0 − 56.0 55.0 − Wang et al. <ref type="bibr" target="#b34">[35]</ref> − − − 57.0 − 76.0 62.0 − Mishra et al. <ref type="bibr" target="#b10">[11]</ref> 64.1 57.  <ref type="table">Table 2</ref>. Results on regular benchmarks. "50" and "1k" are lexicon sizes. "Full" indicates the combined lexicon of all images in the benchmarks. "None" means lexicon-free.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Performance on Regular Datasets</head><p>AON is designed for recognizing both irregular and regular texts. Therefore, we test our method on some regular text benchmarks, the results are shown in Tab. 2. In the constrained cases, our method achieves comparable performance to the existing methods. In the unconstrained cases, our method only falls behind Cheng et al. <ref type="bibr" target="#b5">[6]</ref> on the three benchmarks, and Jaderberg et al. <ref type="bibr" target="#b16">[17]</ref> on IC03. For <ref type="bibr" target="#b5">[6]</ref>, two major factors lead to its high performance: a) using extra geometric annotations (location of each character) in training the attention decoder, and b) exploiting a ResNetbased feature extractor for obtaining robust feature representation. However, labelling the location of each character is extremely expensive, so it is not feasible for real applications. For fair comparison, we also gave the results of Cheng's baseline (without the FocusNet branch) in Tab. 2, and found that our method outperforms Cheng's baseline in most cases, which validates the superiority of our method. Though Jaderberg et al. <ref type="bibr" target="#b16">[17]</ref> achieves an amazing results on IC03, their model cannot recognize out-of-vocabulary words, which limits its applicability in real world. Note that our model is trained without any character geometric information, and it performs better than the other existing methods. As a whole, our method performs effectively in recognizing regular texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Deep insight into AON</head><p>Here, to further clarify the working mechanism of AON, we elaborate the roles of the major components HN, VN, CN and FG in AON, and show the placement trends of texts in some real images. These trends are generated by AON. The roles of HN,VN, CN and FG in AON. We use both horizontal sequence of features and vertical sequence of features to represent arbitrarily-oriented texts. Concretely, for horizontal/vertical texts, horizontal/vertical features are enough to represent the texts; For perspective/slant or arbitrarily-oriented text, we generate the final feature sequence by combining horizontal and vertical features. HN and VN are responsible for generating horizontal and vertical features respectively. CN plays an important role in learning the weights (i.e., character placement clues) that are used to guide the generation of final feature sequences. FG is just to perform the weight-sum operation with the horizontal/vertial feature sequence and the learnt placement weights. <ref type="figure" target="#fig_5">Fig. 6</ref> shows some examples of generated placement clues. We can see that the generated clues conform to our visual observations in the images, which validates the effectiveness of CN in AON.</p><p>Text placement trends generated with AON. Here we verify that the learned character placement clues can be used to generate placement trends of character sequences by positioning each character and drawing text orientations in the original images. Bellow is the computation process of text placement trends.</p><p>We know that the alignment factors α t produced by the attention module indicate the probability distributions over the input sequence of features for generating the glimpse vector g t . And the four character placement clues C = [c 1 , c 2 , c 3 , c 4 ] imply the importance of four extracted feature sequences for representing characters. With C and α t , we roughly divide the input image into L × L patches and calculate the character position distribution dis by dis = C ⊙ α t , where</p><formula xml:id="formula_10">dis = (d 1 , d 2 , d 3 , d 4 ) ∈ R 4×L . We further normalize each element by norm(d ij ) = dij ∑ 2 i=1 ∑ L j=1 dij</formula><p>for i∈ (1, 2), and by norm(</p><formula xml:id="formula_11">d ij ) = dij ∑ 4 i=3 ∑ L j=1 dij for i ∈ (3, 4).</formula><p>Here, norm indicates the normalization operation. For a character at position (x, y), we first compute the horizontal coordinate x with</p><formula xml:id="formula_12">[d 1 , d 2 ] by x = ∑ L j=1 ∑ 2 i=1 j × norm(d ij )</formula><p>, where i ∈ (1, 2) and j ∈ (1, 2, ..., L). Similarity, we compute the vertical coordinate y with</p><formula xml:id="formula_13">[d 3 , d 4 ] by y = ∑ L j=1 ∑ 4 i=3 j × norm(d ij ).</formula><p>To visualize the placement trends of texts in the input images, we mark the coordinate (x, y) on each input image as the corresponding character's position, and consecutively connect the last character's position and the current character's position with an arrow to describe the text's placement trend. <ref type="figure">Fig. 7</ref> shows some examples of generated text placement trends of real images. We can see that the trends formed by the connected arrows basically conform to our visual observations, which again shows that our method is effective in estimating the orientations of texts in images.</p><p>Perspective Curved Oriented <ref type="figure">Figure 7</ref>. The visualization of generated placement trends for perspective, curved and oriented images, shown in the 1st, 2nd and 3rd row, respectively. The curves formed by connected red arrows indicate text placement trends. All texts in the images are correctly recognized by our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussions</head><p>The necessity of CN in AON. As the attention-based decoder is able to select features for generating characters. It is natural to suspect whether CN is necessary. To answer this, we have two experiments without CN: 1) Concatenating horizontal and vertical feature sequence along the channel axis. We find the model converges slowly and cannot achieve state-of-the-art performance, because three quarters' information in the final feature sequence is superfluous. 2) Concatenating horizontal and vertical along the temporal axis. We get results of averagely about 4% lower than that of AON on all benchmarks. The above experiments show that CN is important in AON.</p><p>Impact of aspect ratio. We studied the impact of aspect ratio by experiments, but did not observed obvious negative impact for images with a large aspect ratio. Compared to the previous works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, the enlarging/shrinking operation in height does not obviously affect recognition results of horizontal texts with a large aspect ratio.</p><p>Integrating with only two directional feature sequence. It is not reasonable to integrate only two directional sequences of features. For example, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, if we integrate the right-left and down-top directional sequences of features to generate the final feature sequence, the visual features of 'p' and 'd' will be frame-wisely mixed up due to the weighting mechanism of FG.</p><p>The computational cost of AON. Computational cost and the number of parameters are major concerns in resource-constrained scenarios such as embedded computer systems. Comparing to the Naive base model, the introducing of AON increases parameters and computational cost (twice of Naive base). However, the STN base needs triple parameters and computational cost of Naive base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this work, we propose a novel method to recognize arbitrarily oriented texts by 1) devising an arbitrary orientation network to extract visual features of characters in four directions and the character placement clues, 2) using a filter gate mechanism to combine the four-direction sequences of features, and 3) employing an attention-based decoder for generating character sequence. Different from most existing methods, our method can effectively recognize both irregular and regular texts from images. Experiments over both regular and irregular benchmarks validate the superiority of the proposed method. In the future, we plan to extend the proposed idea to other related tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Examples of irregular (slant/perspective, curved and oriented etc.) texts in natural images. Subfigures (a) -(b), (c) -(d) and (e) -(f) are slant/perspective, curved and oriented images respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Illustration of character visual representation in four directions: − → H: lef t → right, ← − H:right → lef t, − → V :top → bottom and ← − V :bottom → top and four character placement clues c1, c2, c3 and c4. Here, there are three squares connected with dashed lines. The innermost square represents the four 1D sequences of features, each comes along with an arrowed line. The middle square refers to the placement clues used for weighting the corresponding sequences of features. The outermost square stands for the weighted feature sequence by conducting Hadamard product ⊙ with character placement clues and horizontal/vertical features. For the character 'a' in the image, we can represent it by the four weighted sequences of features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The shared convolution block in the dashed box provides a mechanism that multi-groups of feature maps share the same convolutional filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Some images rectified by TPS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 shows some rectified examples by TPS, their original images are shown in Fig. 1. We can see that except for the first and the third images, the other four images are not desirably rectified.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Illustration of learned character placement clues by AON. Each image is surrounded with four changing-gray bars with arrows of different directions. Deeper gray in the bars indicates larger weight for the corresponding directional feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fan</head><label></label><figDesc>Bai and Shuigeng Zhou were partially supported by the Program of Science and Technology Innovation Action of Science and Technology Commission of Shanghai Mu- nicipality (STCSM) under grant No. 17511105204.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Word Spotting and Recognition with Embedded Attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fornés</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2552" to="2566" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end text recognition with hybrid hmm maxout models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PhotoOCR: Reading Text in Uncontrolled Conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Principal warps: Thin-plate splines and the decomposition of deformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">L</forename><surname>Bookstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Focusing Attention: Towards Accurate Text Recognition in Natural Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5076" to="5084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention-Based Models for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Whole is Greater than Sum of Parts: Recognizing Scene Text Words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="398" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Supervised mid-level features for word image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2956" to="2964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Connectionist Temporal Classification : Labelling Unsegmented Sequence Data with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Synthetic Data for Text Localisation in Natural Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reading Scene Text in Deep Convolutional Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3501" to="3508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2227</idno>
		<title level="m">Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Structured Output Learning for Unconstrained Text Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reading Text in the Wild with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Spatial Transformer Networks. NIP-S</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional Architecture for Fast Feature Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-MM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">R2CNN: Rotational Region CNN for Orientation Robust Scene Text Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<idno>arX- iv:1706.09579</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
		<title level="m">ICDAR 2015 competition on Robust Reading. In ICDAR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recursive Recurrent Nets with Attention Modeling for OCR in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Panaretos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Young</surname></persName>
		</author>
		<title level="m">ICDAR 2003 robust reading competitions. In ICDAR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="682" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01086</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">Arbitrary-Oriented Scene Text Detection via Rotation Proposals. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scene Text Recognition using Higher Order Language Priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-time scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recognizing text with perspective distortion in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lim Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A robust arbitrary text detection system for natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Risnumawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="8027" to="8048" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Detecting Oriented Text in Natural Images by Linking Segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>arX- iv:1703.06520</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust Scene Text Recognition with Automatic Rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4168" to="4176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Accurate Scene Text Recognition Based on Recurrent Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="35" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Word Spotting in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">End-to-end text recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Show</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to Read Irregular Text with Attention Mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3280" to="3286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Strokelets: A Learned Multi-scale Representation for Scene Text Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Text Detection and Recognition in Imagery: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1480" to="1500" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">ADADELTA: An Adaptive Learning Rate Method. CoRR, abs/1212</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5701</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-oriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4159" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
