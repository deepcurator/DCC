<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Hand: How to Train a CNN on 1 Million Hand Images When Your Data Is Continuous and Weakly Labelled</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Koller</surname></persName>
							<email>koller@cs.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision Speech &amp; Signal Processing</orgName>
								<orgName type="institution">Human Language Technology &amp; Pattern Recog. RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
							<email>ney@cs.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision Speech &amp; Signal Processing</orgName>
								<orgName type="institution">Human Language Technology &amp; Pattern Recog. RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bowden</surname></persName>
							<email>r.bowden@surrey.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Hand: How to Train a CNN on 1 Million Hand Images When Your Data Is Continuous and Weakly Labelled</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional Neural Networks (CNNs) have been demonstrated to provide superior performance in many tasks. But to achieve this they require large amounts of labelled training data which in many areas is a limiting factor. Pose-independent hand shape recognition, crucial to gesture and sign language recognition, suffers from large visual intra-class ambiguity and therefore places further burden on the acquisition of training data. Typically, only small and quite specific labelled data sets exist ( <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b26">26]</ref>) which usually do not provide sufficiently fine-grained hand shape classes suitable for sign language recognition. Recent advances in sign language research have given rise to many publicly available sign language lexicons that allow searching of the videos by the index of hand shapes. These resources constitute noisy but valuable data sources. In this work, we exploit the modelling capabilities of a pre-trained 22 layer deep convolutional neural network and integrate it into a force-aligning algorithm that converts noisy video level annotations into a strong frame level classifier. As such, this manuscript provides the following contributions:</p><p>• formulation of an EM-based algorithm integrating</p><p>CNNs with Hidden-Markov-Models (HMMs) for weak supervision and overcoming the temporal alignment problem in continuous video processing tasks using the strong discriminative capabilities of CNN architectures</p><p>• robust fine grained single frame hand shape recognition based on a CNN-model, trained on over 1 million hand shapes and shown to generalise across data sets without retraining</p><p>• making an articulated sign language hand shape data set publicly available comprising 3361 manual labelled frames in 45 classes <ref type="bibr" target="#b0">1</ref> • and integration of pose-independent hand shape subunits into a continuous sign language recognition pipeline This paper is organised as follows: after introducing the related literature in Section 2 we give a precise problem formulation and the solution in Section 3. Section 4 introduces the employed data sources. Subsequently, we evaluate the approach in Section 5 in two parts: firstly classifying single frames and secondly in a continuous sign language recognition pipeline. The paper closes with a conclusion in Section <ref type="bibr" target="#b6">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">State-of-the-Art</head><p>This work deals with the problem of weakly supervised learning from sequence labels applied to the problem of hand shape recognition. We therefore look at the state-ofthe-art in both areas related to the domains of gesture and sign language.</p><p>Hand shape recognition from a single image may be understood as the hand pose configuration specified by joint positions and angles which to date are mostly estimated based on depth images and pixel-wise hand segmentation <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b21">21]</ref>. However, in the scope of this work, hand shape recognition is seen as a classification task of a specific number of defined hand shapes. Known approaches fall into three categories: (i) template matching against a large data set of often synthetic gallery images <ref type="bibr" target="#b25">[25]</ref> or contour shapes <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>; (ii) generative model fitting approaches <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b29">28]</ref>; and (iii) discriminative modelling approaches such as Cooper et al. <ref type="bibr" target="#b6">[6]</ref>. Cooper uses random forests trained on HOG features to distinguish 12 hand shapes, each trained on 1000 training samples. However, they restricted the classifier to work on hands not in motion and applied it only to isolated sign language recognition. There seems to be no previous work exploiting CNNs for hand shape classification other than <ref type="bibr" target="#b41">[40]</ref> which only distinguishes 6 classes trained with 7500 images per class. A few recent publications apply CNNs to finger and joint regression based on depth data <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b24">24]</ref>. Tompson et al. <ref type="bibr" target="#b35">[34]</ref> present a CNNbased hand pose estimation based on depth data. They generate computationally heavy heat maps for 2D joint locations and infer the 3D hand pose by the depth channel and inverse kinematics.</p><p>There are many approaches to learning from ambiguous labels or weakly supervised learning (see <ref type="bibr" target="#b43">[42]</ref> for an overview). A common approach is to employ multiple instance learning (MIL), treating a video sequence as a bag which is only labelled positive if it contains at least one true positive instance. MIL iteratively estimates the instance labels measuring a predefined loss. Buehler et al. <ref type="bibr" target="#b3">[4]</ref> and similarly Kelly et al. <ref type="bibr" target="#b17">[17]</ref> apply MIL to learning sign categories from TV subtitles, circumventing the translation problem by performing sign spotting. However, Farhadi and Forsyth <ref type="bibr" target="#b9">[9]</ref> were the first to approach the subtitle-signalignment problem. They used a HMM to find sign boundaries. Cooper and Bowden <ref type="bibr" target="#b5">[5]</ref> solved the same problem by applying efficient data mining methods, an idea that was introduced to the vision community by Quack et al. <ref type="bibr" target="#b28">[27]</ref>. Another approach uses Expectation Maximisation (EM) <ref type="bibr" target="#b7">[7]</ref> to fit a model to data observations. Koller et al. <ref type="bibr" target="#b20">[20]</ref> used EM to fit a Gaussian Mixture Model (GMM) to Active Appearance Model (AAM) mouth features in order to find and model mouth shape sequences in sign language. Other works use EM to link text and image regions <ref type="bibr" target="#b38">[37]</ref>. Wu et al. <ref type="bibr" target="#b40">[39]</ref> introduced a non-linear kernel discriminant analysis step in between the expectation and maximisation step to map the features to a lower dimensional space which could help the subsequent generative model to better separate the classes. In the field of Automatic Speech Recognition (ASR) we encounter the use of a discriminative classifier with EM <ref type="bibr" target="#b31">[30]</ref>. Closely related is also the clustering of spatio-temporal motion patterns for action recognition <ref type="bibr" target="#b42">[41]</ref> and Nayak's work on iterated conditional modes <ref type="bibr" target="#b23">[23]</ref> to extract signs from continuous sentences. Learning frame labels from video annotations is an underexploited approach in the vision community and the previous literature has several shortcomings that we address with this work:</p><p>1. The discriminative capabilities of CNNs have not yet been integrated into a weakly supervised learning scheme able to exploit large ambiguously labelled data sets.</p><p>2. No previous work has explicitly worked on posture and pose-independent hand shape classification, which is crucial in real-life sign language footage, as hand shape and posture have been determined as independent information sources by sign linguists.</p><p>3. To our knowledge no previous work has exploited the classification power of CNNs with application to sign language hand shape classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>No previous work has trained a classifier on over a million hand shapes of real sign language data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>No previous work has dealt with data set independent hand shape classification.</p><p>However, there is much to be gained from addressing these shortcomings. If CNNs can be trained using weak video annotation, then we can leverage the power of CNNs to generalise over large data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Weakly Supervised CNN Training</head><p>The proposed algorithm constitutes a successful solution to the problem of weakly supervised learning from noisy sequence labels to correct frame labels. An overview of the approach is given in <ref type="figure" target="#fig_0">Figure 1</ref>, which shows the overall pipeline specific to the task of hand shape classification. However, the algorithm could be easily applied to other tasks. The input images are cropped around the tracked hands, which forms the input to our weakly supervised CNN training. The iterative learning algorithm needs an initialisation, which is referred to as 'flat start'. This involves linearly partitioning the input frames to an available initial annotation, usually a single hand shape class preceded and followed by instances of the garbage class (as the hand shape is expected to happen in the middle of the sequence). The algorithm iteratively refines the temporal class boundaries and trains a CNN that performs single image hand shape recognition. While refining the boundaries, it may drop the label sequence or exchange it for one that better fits the data. The iterative process is similar to a forced alignment procedure, however, rather than using Gaussian mixtures as the probabilistic component we use the outputs of the CNN directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>Given a sequence of images x T 1 = x 1 , . . . , x T and an ambiguous class labell for the whole sequence, we want to jointly find the true label l for each frame and train a model such that the class symbol posterior probability p(k|x) over all images and classes is maximised. We assume that a lexicon ψ of possible mappings froml → l exists, where l can be interpreted as a sequence of up to L class symbols k,</p><formula xml:id="formula_0">ψ = l : l L 1 | l ∈ {k 1 , . . . , k N , ∅}<label>(1)</label></formula><p>Optionally, l may be an empty symbol corresponding to a garbage class. Eachl can map to multiple symbol sequences (which is important asl is ambiguous and a one-toone mapping would not be sufficient). In terms of sequence constraints, we only require each symbol to span an arbitrary length of subsequent images as we assume that symbols (in our application: hand shapes) are somewhat stationary and do not instantly disappear or appear.</p><p>Due to the promising discriminatory capabilities of CNNs, we solve the problem in an iterative fashion with the EM algorithm <ref type="bibr" target="#b7">[7]</ref> in a HMM setting and use the CNN for modelling p(k|x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sequential Time-Decoding</head><p>The basic idea of EM is to start with a random model initialisation and then iteratively (i) update the assignment of class labels to images (E-Step) and then (i) re-estimate the model parameters to adapt to the change (M-Step).</p><p>The E-Step consists of the forward-backward algorithm, which identifies the sequence of class symbols aligned to the images that best fits the learnt model. Using Bayes' decision rule, we maximise the posterior probability over all possible true labels l, corresponding to casting the class symbol model P r(x t |k t ) given by the CNN as the marginal over all possible HMM temporal state sequences s T 1 = s 1 , . . . , s T defined by the symbol sequences in ψ. For an efficient implementation, following <ref type="bibr" target="#b11">[11]</ref>, we assume a first order Markov dependency and maximum approximation:</p><formula xml:id="formula_1">x T 1 → [k T 1 ] opt = argmax k N 1 P r(l) max s T 1 P r(x t |k N 1 ) · P r(s t |s t−1 )<label>(2)</label></formula><p>where P r(l) denotes the symbol sequence prior probability and P r(x t |k N 1 ) is modelled by the CNN. To add robustness, we employ a pooled state transition model P r(s t |s t−1 ) with globally set transition probabilities. Those form a HMM in bakis structure (left-to-right structure; forward, loops and skips across at most one state are allowed, where two subsequent states share the same class probabilities). The garbage class is modelled as an ergodic state with separate transition probabilities to add flexibility, such that it can always be inserted between sequences of symbols.</p><p>Usually, this approach is used jointly with GMMs, which model directly p(x|k) as generative models. However, the CNN models the posterior probability p(k|x). Inspired by the hybrid approach <ref type="bibr" target="#b1">[2]</ref> known from ASR we convert the CNN's posterior output to likelihoods given the class counts in our data (p(k)) using the Bayes' rule as follows:</p><formula xml:id="formula_2">p(x t |k) ∝ p(k|x t )/p(k) α (3)</formula><p>This allows us to add symbol sequence prior knowledge from the lexicon ψ. Equation 2 then becomes:</p><formula xml:id="formula_3">argmax k N 1 p(l) max s T 1 p(k t |x t ) p(k) α · p(s t |s t−1 ) ,<label>(4)</label></formula><p>where the scaling factor α is a hyperparameter allowing us to control the impact of the class prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Convolutional Neural Network Architecture</head><p>Knowing the weakly supervised characteristics of our problem, we would like to incorporate as much prior knowledge as possible to guide the search for the true symbol class labels. Pre-trained CNN models constitute such a source of knowledge, which seems reasonable as the pretrained convolutional filters in the lower layers may capture simple edges and corners, applicable to a wide range of image recognition tasks. We opt for a model previously trained in a supervised fashion for the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) 2014 . We choose a 22 layer deep network architecture following <ref type="bibr" target="#b33">[32]</ref> which achieves a top-1 accuracy of 68.7% and a top-5 accuracy 88.9% in the ILSVRC. The network involves an inception architecture, which helps to reduce the numbers of free parameters while allowing for a very deep structure. Our model has about 6 million free parameters. All convolutional layers and the last fully connected layer use rectified linear units as non-linearity. Additionally, a dropout layer with 70% ratio of dropouts is used to prevent over-fitting. We base our CNN implementation on <ref type="bibr" target="#b15">[15]</ref>, which is an efficient C++ implementation using the NVIDIA CUDA Deep Neural Network GPU-accelerated library.</p><p>We replace the last pre-trained fully connected layers before the output layers with those matching the number of classes in our problem (plus one garbage class), which we initialise with zeros.</p><p>As a preprocessing step, we apply a per pixel mean normalisation to the images prior to fine-tuning the CNN model with Stochastic Gradient Descent (SGD) and a softmax based cross-entropy classification loss</p><formula xml:id="formula_4">E E = − 1 N N n=1 log p(k|x n ).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Data Sets</head><p>We employ three different data sets for training the hand shape classifier. All data sets feature sign language footage. Two represent video based publicly available sign language lexicons with isolated signs from Danish sign language <ref type="bibr" target="#b14">[14]</ref> and New Zealand sign language <ref type="bibr" target="#b22">[22]</ref>. The third source represents the training set of RWTH-PHOENIX-Weather 2014 <ref type="bibr" target="#b12">[12]</ref>, a publicly available continuous sign language data set. <ref type="figure" target="#fig_1">Figure 2</ref> shows sample sequences from all three data sets, where it can be seen that the lexicons have single sign data, whereas PHOENIX provides full signed sentences. The Danish data contains hardly any motion blur, whereas there is some motion blur present in the New Zealand data and a large portion of the PHOENIX video frames contain heavy motion blur. The sign language lexica provide linguistic hand shape labels for each of the sign videos that enable a search by hand shape on the lexicon web sites. As for the danish data, we obtained a consolidated version of hand shape annotations directly from the maintainer of the lexicon. However, from a pattern recognition point of view these annotations are extremely ambiguous and noisy. They consist of a single hand shape, sometimes a sequence of two hand shapes, for a whole signed video. As can be seen in <ref type="figure" target="#fig_1">Figure 2</ref>, the hand shape can be more or less static throughout the video (top example in <ref type="figure" target="#fig_1">Figure 2)</ref>, or it reflects only one temporary portion of a changing hand configuration (middle example in <ref type="figure" target="#fig_1">Figure 2</ref>). In any case, the signer brings his hands from a neutral position to the place of sign execution, while transitioning from a neutral hand shape to the target hand shape composing the sign and to possible subsequent hand shapes. While the sign is performed, it may involve a hand movement, a rotation of the hand and changes in hand shape. The annotation may represent any of these hand shapes or an intermediate configuration that was considered linguistically dominant during the annotation. As there are no hand shape annotations for the RWTH-PHOENIX-Weather data available, we employ a publicly available sign language lexicon called SignWriting <ref type="bibr" target="#b32">[31]</ref>. It constitutes an open online resource, where people can create entries translating from written language to sign language using a pictorial notation form called SignWriting (which contains hand shape information). The German SignWriting lexicon currently comprises 24.293 entries. Inspired by <ref type="bibr" target="#b19">[19]</ref>, we parsed all entries to create the mapping ψ from sign annotations to possible hand shape sequences, where we remove all hand pose related information (such as rotations) of the hand annotations. This mapping will be made available, in order to make our results reproducible. Throughout this work we follow the hand shape taxonomy by the danish sign language lexicon team, which amounts to over 60 different hand shapes, often with very subtle differences such as a flexed versus straight thumb.  Statistics of all three data sets are given in <ref type="table">Table 1</ref>. Garbage and hand shape frames are estimated automatically by our algorithm. All three data sets total to over one million hand shape images produced by 23 individuals. Some resources have been manually created in the scope of this work. Among them a mapping from the New Zealand and the SignWriting hand shape taxonomy to the employed Danish taxonomy. Some hand shape classes were ambiguous between the two annotation schemes, yielding a one-to-many mapping that could be integrated into ψ, which will also be made available. For evaluating the 1-Million-Hands CNN classifier, we manually labelled 3361 images from the RWTH-PHOENIX-Weather 2014 Development set <ref type="bibr" target="#b1">2</ref> . Some of the 45 encountered pose-independent hand shape classes are depicted in <ref type="figure" target="#fig_2">Figure 3</ref>. They show the large intra-class variance and the strong similarity between several classes. The hand shapes occur with different frequency in the data. The distribution of counts per class can be verified in <ref type="figure">Figure 4</ref> showing that the top 14 hand shapes explain 90% of the annotated samples.</p><p>Finally, we evaluate on two publicly available continuous sign language data set benchmarks: (i) RWTH-PHOENIX-Weather 2014 Multisigner corpus <ref type="bibr" target="#b12">[12]</ref> , which is a challenging real-life continuous sign language corpus that can be considered to be one of the largest published <ref type="bibr" target="#b1">2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label Count</head><p>Hand Shape Class <ref type="figure">Figure 4</ref>. Ground truth hand shape label count of all 3361 annotations. 45 out of 60 classes have been found in the data and could be labelled. If several hand shapes appear close to one label counting bar, each hand shape alone amounts to the mentioned fraction of labels. Hand-Icons from <ref type="bibr" target="#b22">[22]</ref>.</p><p>continuous sign language corpora. It covers unconstrained sign language of 9 different signers with a vocabulary of 1081 different signs.</p><p>(ii) SIGNUM <ref type="bibr" target="#b37">[36]</ref> signer-dependent subset, which has been well established as a benchmark for a significant amount of sign language research. Both data sets are presented in detail in <ref type="bibr" target="#b18">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section we describe the experimental validation of the proposed algorithm with application to learning a robust pose-independent hand shape classifier based on a CNN. In the first two subsections we describe the training parameters and discuss evaluation on the frame-level. Subsection 5.3 applies the learnt 1-Million-Hands model to the challenging problem of continuous sign language recognition, where it outperforms current state-of-the-art by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Hand Shape Model Training</head><p>Data preparation. The data is downloaded and prepared by tracking the hands using a model-free dynamic programming tracker <ref type="bibr" target="#b8">[8]</ref>. Being based on dynamic programming, the tracker optimises the tracking decisions over time and traces back the best sequence of tracking decisions at the end of the video. The size of the hand patch is roughly chosen so that it is two to three times the size of a hand. However, the appearance of the hand changes as the signer moves it towards the camera.</p><p>Construction of Lexicon. The next step is to construct the lexicon ψ, given the hand shape annotations. If a sequence of more than one hand shape annotation is available for a given video, we add the whole sequence and each of the hand shapes on its own to the lexicon ψ. As described in Section 4, the annotation taxonomy of the New Zealand data does not match the employed Danish taxonomy one to one. This partly results in multiple hand shape annotations per video, all of which we add to the lexicon ψ. Within the lexicon definition, we also allow the garbage class to be able to account for frames before and after any hand shape.</p><p>Initialise algorithm. The input videos are linearly partitioned based on a random hand shape label sequence from the lexicon ψ, considering the beginning and end of each video as garbage class.</p><p>HMM settings. We base the HMM part of this work on the freely available state-of-the-art open source speech recognition system RASR <ref type="bibr" target="#b30">[29]</ref>. All 60 hand shape classes are represented by a double state, whereas the garbage class just has a single state. We use fixed, non-optimised transition penalties being '2-0-2' for 'loop-forward-skip' for all hand shape classes and '0-2' for the garbage 'loop-forward'. The scaling factor α is set to 0.3 in our experiments. As already pointed out by <ref type="bibr" target="#b6">[6]</ref>, we also observe a strong bias in the distribution of hand shape classes in our data, but we decided to maintain it. To speed up CNN training time we randomly sample from the observation sequences of the garbage class. In this way we decrease the amount of garbage frames and match it to the most frequently observed hand shape class.</p><p>CNN training. We replace the pre-trained output layers with a 61 dimensional fully connected layer, accounting for 60 hand shape classes and a garbage class. We have empirically noticed that training all layers with an equal learning rate outperforms training just the output layer or weighting the output layer's learning rate. For all experiments we use a fixed learning rate lr = 0.0005 for 3 epochs and finish a last epoch with lr = 0.00025. We select the best training based on the manually annotated evaluation data presented in Section 4, but, as shown in the evaluation, the automatic development data behaves comparably (see <ref type="figure" target="#fig_3">Fig. 5</ref>).</p><p>In <ref type="figure" target="#fig_3">Figure 5</ref> we show the evolving accuracy during one epoch of CNN training measured 16 times per iteration. Given is both the accuracy on the manually annotated hand shape set, as well the accuracy on a randomly split development set representing the automatic alignment generated by the HMM. It is good to see that both measures converge in a similar fashion, which indicates that using the automatic data for training may be sufficient. To obtain a strong classifier it is good to start with data providing stronger supervision while subsequently adding the remainder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Frame-Level Evaluation</head><p>In terms of run time, the CNN requires 8.24ms in the forward-pass to classify a single image (when supplied in batches of 32 images) on a single GeForce GTX 980 GPU with 4095 MiB. The algorithm can therefore run with over 100fps in a recognition system.</p><p>In <ref type="table">Table 2</ref> we display the training accuracy of the CNN measured on the manually annotated PHOENIX images across five iterations of the proposed EM algorithm. Three different setups are presented, showing the effect of increased training data. We deploy a system using solely the Danish data, one using the Danish and the New Zealand data and one using all three resources. Note, in the first two cases the CNN successfully classifies handshapes of an unseen data set and is thus independent of the data set (no samples of the evaluation corpus are used for training), as we are measuring the evaluation on the RWTH-PHOENIXWeather hand shape annotations. We see that the training accuracy increases with each iteration in the first two cases and then slowly converges. Due to the lower amount of hand shape samples in the Danish case, a single training iteration has less impact on the CNN's weights which results in slower convergence (measured per epoch). We further note that adding PHOENIX data to the train set does not seem to converge to a stable maximum (at least not after a few iterations), but improves to 62.8% top-1 accuracy and then decreases again. This is likely to be due to the fact that the PHOENIX data set covers continuously signed sentences that contain sequences of many different hand shapes. However, the SignWriting annotations used to construct the lexicon ψ are user based, not quality checked and not specifically matching the PHOENIX data set. Therefore the annotations are very noisy, yielding a high variability of the frame alignment produced by the HMM. The best training set yielding 62.8% top-1 and 85.6 top-5 accuracy is used for all subsequent evaluations and henceforth referred to as 1-Million-Hands classifier. <ref type="table">Table 3</ref> shows the per class confusion of the classifier of all 13 classes that were detected. We note that there are six classes with a precision of over 90%, two classes that reach a reasonable 60% or more, three classes that are in the 40% range and the remaining classes achieve a low precision or are not detected at all. This is a very strong result given the fact that the classifier is trained with weak annotations on the video level only and that the hand shape taxonomy understands minor finger angles as different classes. Still, the question remains, why doesn't the approach recognise all hand shapes equally well? Some possible reasons include: (i) Hand shapes in the training set are not equally distributed across the classes. (ii) Hand shapes in the evaluation set are also not equally distributed, leading to a recognition bias. (iii) There may be too few samples for the seldom occurring hand shapes. (iv) There are differences with respect to the hand shape taxonomies used for creating the hand shape labels of the different data sets. We tried to account for these differences when creating a mapping from one taxonomy to another, but there may be errors in this mapping, as we were just looking at the taxonomy description when creating the mapping, not at the data itself.  <ref type="table">Table 3</ref>. Class confusion of detected classes in <ref type="bibr">[%]</ref>. Showing per class precision on the diagonal, true classes on the y-axis and predicted classes on the x-axis of the 62.8% top-1 accuracy. HandIcons from <ref type="bibr" target="#b22">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wrong</head><p>Correct Hyp Ref <ref type="figure">Figure 6</ref>. Some examples of correct and wrong classification on the independent evaluation set. "Hyp" refers to the hypothesised class, whereas "Ref" is the reference. Hand-Icons from <ref type="bibr" target="#b22">[22]</ref>. <ref type="figure">Figure 6</ref> shows examples of correct classification as well as failure cases. The figure helps to understand that in several cases (e.g. the first four images from the left in <ref type="figure">Figure 6</ref>) the classification is not completely wrong, but does not seem to be able to distinguish minor differences in similar hand shapes (e.g. in the first row the index and thumb are recognised as touching, but they are in fact slightly separated). These errors could also happen to untrained humans. The examples in the fifth and sixth column show confusions of visually similar, but for the human clearly distinguishable handshapes (e.g. the flat hand seen from the side looks similar to an index finger). However, the examples of correct classification in <ref type="figure">Figure 6</ref> show us that the 1-Million-Hands model correctly classifies hand shapes independent of the pose and orientation. It also copes well with occlusions as can be seen in column three.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Continuous Sign Language Recognition</head><p>Sign language recognition (SLR) is very suitable to evaluate hand shape classification as it is a difficult but well defined problem offering real-life difficulties (w.r.t. occlusion, motion blur, variety of hand shapes) hard to find in simple per frame evaluation tasks of current hand shape evaluation data sets. We use the same system as <ref type="bibr" target="#b18">[18]</ref> to ensure comparability to previously published results and base the SLR recognition pipeline on <ref type="bibr" target="#b30">[29]</ref>. We use the 1024 dimensional feature maps of the last convolutional layer of our CNN, normalise its variance to unity and use PCA to reduce the dimensionality to 200. We evaluate on two publicly available data sets: RWTH-PHOENIX-Weather 2014 Multisigner data set and SIGNUM signer-dependent set presented in Section 4 and measure the error in word error rate (WER): WER = #deletions + #insertions + #substitutions #number of reference observations <ref type="bibr" target="#b6">(6)</ref> We compare the classifier against HoG-3D features, which are succesfully employed as hand shape feature in many state-of-the-art automatic SLR systems (c.f . <ref type="table" target="#tab_3">Table 4</ref>). On RWTH-PHOENIX-Weather, we see that the 1-MillionHands model outperforms the standard HoG-3D features by 9.3% absolute WER, being a relative improvement of over 15% from 60.9% down to 51.6%. On SIGNUM the 1-Million-Hands model outperforms the standard HoG-3D features by 0.5% absolute WER, from 12.5% down to 12.0%. On this data set the performance is less as it is more controlled and the tracking is better. This means that the HoG-3D is able to perform better on this easier data than it being a deficiency in the CNN.</p><p>We further compare our classifier in a multi-modal setup against the best published recognition results on the employed data sets and perform a stacked fusion with the features proposed by <ref type="bibr" target="#b18">[18]</ref> (comprising HoG-3D, right to left hand distance, movement, place of articulation and facial features). Different to <ref type="bibr" target="#b18">[18]</ref> we do not perform any sort of speaker or feature adaptation. <ref type="table" target="#tab_4">Table 5</ref> presents the recognition results competing the current state-of-theart. On RWTH-PHOENIX-Weather, the 1-Million-Hands model adds significant complementary information to the complex state-of-the-art feature vector used by <ref type="bibr" target="#b18">[18]</ref> and reduces the WER by 10.2% absolute from 57.3% to 47.1%, being a relative reduction of over 17%. On SIGNUM it reduces the WER by 2.4% absolute from 10.0% to 7.6%, being a relative reduction of 24%. It is suprising that the 1-Million-Hands model generalises so well to the completely unseen SIGNUM data set, particularly w.r.t. large visual differences in background and motion blur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In the course of this work we presented a new approach to learning a frame-based classifier using weakly labelled sequence data by embedding a CNN within an iterative EM algorithm. This allows the labeling of vast amounts of data at the frame level given only noisy video annotation. The   iterative EM algorithm leverages the discriminative ability of the CNN to iteratively refine the frame level annotation and subsequent training of the CNN. Using this approach, we trained a fine grained hand shape classifier on over 1 million weakly labelled hand shapes that distinguishes 60 classes and generalises over both individuals and datasets. The classifier achieves 62.8 % recognition accuracy on over 3000 manually labelled hand shape images which will be released to the community. When integrated into a continuous sign language recognition pipeline and evaluated on two standard benchmark corpora, the classifier achieves an absolute improvement of up to 10% word error rate and a relative improvement of over 17% compared to the state-ofthe-art. To our knowledge, no previous work has explicitely worked on posture and pose-independent hand shape classification. Moreover, we believe no previous work has exploited the discriminitive power of CNNs with application to hand shape classification in the scope of sign language. Although we demonstrate this in the context of hand shape recognition, the approach has wider application to any video recognition task where frame level labelling is not available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Overview of presented Algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Showing employed data sets for training: Top to bottom, Danish sign language dictionary, New Zealand sign language dictionary and a sentence from RWTH-PHOENIX-Weather corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. 12 exemplary manually annotated hand shape classes are shown. Three labelled frames per class demonstrate intra-class variance and inter-class similarities. Hand-Icons from [22].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Showing the top-1 and top-5 CNN accuracies for every 16 th training epoch measured on the manual annotations ('top-1' 'top-5') and on a development split of the automatically labelled training data ('auto-top-1' 'auto-top-5'). Given is the last iteration of the EM-algorithm yielding a 62.8% top-1 accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>PHOENIX</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Available at: http://www.hltpr.rwth-aachen.de/˜koller/1miohands</figDesc><table>200 
400 

0.0% 

0.1% 

0.2% 

0.3% 

0.4% 

0.5% 

0.6% 

0.7% 

0.9% 

1.1% 

1.4% 

2.6% 

3.1% 

3.4% 

4.5% 

5.9% 

9.2% 

9.6% 

9.9% 

10.4% 

12.0% 

15.7% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 .</head><label>4</label><figDesc>2014 SIGNUM Dev Test Test del/ins WER del/ins WER del/ins WER HoG-3D 25.8/4.2 60.9 23.2/4.1 58.1 2.8/2.4 12.5 1-Mio-H. 19.1/4.1 51.6 17.5/4.5 50.2 1.5/2.5 12.0Hand-only continuous sign language recognition results on RWTH-PHOENIX-Weather 2014 Multisigner and SIGNUM. 1-Mio-H. stands for the presented 1-Million-Hands classifier.Mio-H.+[18] 16.3/4.6 47.1 15.2/4.6 45.1 0.9/1.6 7.6</figDesc><table>PHOENIX 2014 
SIGNUM 
Dev 
Test 
Test 
del/ins WER del/ins WER del/ins WER 
[36] 
-
-
-
-
-
12.7 
[13] 
-
-
-
-
-
11.9 
[11] 
-
-
-
-
-
10.7 
[18] 
23.6/4.0 57.3 23.1/4.4 55.6 1.7/1.7 10.0 
[18] CMLLR 21.8/3.9 55.0 20.3/4.5 53.0 
-
-
1-</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 .</head><label>5</label><figDesc>Multi-modal continuous sign language recognition results on RWTH-PHOENIX-Weather 2014 Multisigner and SIGNUM. 1-Mio-H. stands for the presented 1-Million-Hands classifier.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Available at: http://www.hltpr.rwth-aachen.de/˜koller/1miohands</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledegments:</head><p>Special thanks to Thomas Troelsgård and Jette H. Kristoffersen, Center for Tegnsprog, Denmark (http://www.tegnsprog.dk) for providing linguistic sign language annotations and videos. We also thank the creators of the online Dictionary of New Zealand Sign Language (http://nzsl.vuw.ac.nz) for sharing their work under CC-license, which allowed us to use the hand shape icons, sign language videos and annotations. This work has been supported by EPRSC grant EP/I011811/1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Estimating 3D hand pose from a cluttered image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<idno>II-432. IEEE</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2003. Proceedings. 2003 IEEE Computer Society Conference on</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Connectionist speech recognition: a hybrid approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">247</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Linguistic Feature Vector for the Visual Interpretation of Sign Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Windridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2004</title>
		<meeting><address><addrLine>Czech Republic, Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="390" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning sign language by watching TV (using weakly aligned subtitles)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2961" to="2968" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning signs from subtitles: A weakly supervised approach to sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Miami, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="2568" to="2574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reading the signs: A video based sign dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pugeault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCV Workshops)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="914" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of the royal statistical society. Series B (methodological)</title>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tracking Using Dynamic Programming for AppearanceBased Sign Language Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dreuw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference Automatic Face and Gesture Recognition</title>
		<meeting><address><addrLine>Southampton, UK</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006-04" />
			<biblScope unit="page" from="293" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Aligning ASL for statistical translation using a discriminative word model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1471" to="1476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extraction of 3D hand shape and posture from image sequences for sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fillbrandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akyol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-F</forename><surname>Kraiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Analysis and Modeling of Faces and Gestures</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="181" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modality Combination Techniques for Continuous Sign Language Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Oberdörfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Iberian Conference on Pattern Recognition and Image Analysis</title>
		<meeting><address><addrLine>Madeira, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013-06" />
			<biblScope unit="volume">7887</biblScope>
			<biblScope unit="page" from="89" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Extensions of the Sign Language Recognition and Translation Corpus RWTH-PHOENIX-Weather</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellgardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Resources and Evaluation</title>
		<meeting><address><addrLine>Reykjavik, Island</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Enhanced Continuous Sign Language Recognition using PCA and Neural Network Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gweth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2012 Workshop on Gesture Recognition</title>
		<meeting><address><addrLine>Providence, Rhode Island, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kristoffersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><forename type="middle">Skov</forename><surname>Troelsgård</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Hardell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hardell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jørgen</forename><surname>Boye Niemelä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Sandholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toft</surname></persName>
		</author>
		<ptr target="http://www.tegnsprog.dk/" />
	</analytic>
	<monogr>
		<title level="j">Ordbog over Dansk Tegnsprog</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional Architecture for Fast Feature Embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast propagation-based skin regions segmentation in color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kawulok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weakly Supervised Training of a Sign Language Recognition System Using Multiple Instance Learning Density Matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="526" to="541" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Part B: Cybernetics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Continuous sign language recognition: Towards large vocabulary statistical recognition systems handling multiple signers. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="108" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">May the Force be with you: Force-Aligned SignWriting for Automatic Subunit Annotation of Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<editor>Shanghai, PRC</editor>
		<imprint>
			<date type="published" when="2013-04" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Read My Lips: Continuous Signer Independent Weakly Supervised Viseme Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Conference on Computer Vision</title>
		<meeting>the 13th European Conference on Computer Vision<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="281" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Combining discriminative and model based approaches for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krejov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>Automatic Face and Gesture Recognition (FG)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The Online Dictionary of New Zealand Sign Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mckee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mckee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pivac</surname></persName>
		</author>
		<ptr target="http://nzsl.vuw.ac.nz/" />
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automated extraction of signs from continuous sign language sentences using iterated conditional modes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Loeding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2583" to="2590" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.06807</idno>
		<title level="m">Hands Deep in Deep Learning for Hand Pose Estimation</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nearest neighbor search methods for handshape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Potamias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st international conference on PErvasive Technologies Related to Assistive Environments, PETRA &apos;08</title>
		<meeting>the 1st international conference on PErvasive Technologies Related to Assistive Environments, PETRA &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spelling It Out: Real-Time ASL Fingerspelling Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pugeault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Consumer Depth Cameras for Computer Vision</title>
	</analytic>
	<monogr>
		<title level="m">Proc ICCV</title>
		<meeting>ICCV<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient mining of frequent and distinctive feature configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Quack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic affine-invariant shape-appearance handshape features and classification in sign language videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Theodorakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pitsikalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1627" to="1663" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">RASR -The RWTH Aachen University Open Source Speech Recognition Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lehnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nolden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tüske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wiesler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition and Understanding Workshop</title>
		<meeting><address><addrLine>Waikoloa, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GMMfree DNN training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="5639" to="5643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A C F S</forename><surname>Writing</surname></persName>
		</author>
		<title level="m">Sign writing. Deaf Action Committee (DAC)</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Going Deeper with Convolutions</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="3224" to="3231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Real-time continuous pose recovery of human hands using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">169</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Classification of hand postures against complex backgrounds using elastic graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Triesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malsburgb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="937" to="943" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The significance of facial features for automatic sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Agris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Knorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-F</forename><surname>Kraiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition, 2008. FG&apos;08. 8th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simultaneous image classification and annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1903" to="1910" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Rule Of Thumb: Deep derotation for improved fingertip detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wetzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Slossberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05726</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Self-supervised learning for object recognition based on kernel discriminant-EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth IEEE International Conference on Computer Vision, 2001. ICCV 2001. Proceedings</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="275" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hand posture recognition based on bottom-up structured deep convolutional neural network with curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Watasue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="853" to="857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Discovering Motion Primitives for Unsupervised Grouping and One-Shot Learning of Human Actions, Gestures, and Expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1635" to="1648" />
			<date type="published" when="2013-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Semi-Supervised Learning Literature Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno>1530</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Sciences</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin -Madison</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
