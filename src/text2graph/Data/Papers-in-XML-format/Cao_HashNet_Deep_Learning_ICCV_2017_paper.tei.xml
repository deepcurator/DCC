<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HashNet: Deep Learning to Hash by Continuation *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
							<email>caozhangjie14@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
							<email>jimwang@tsinghua.edu.cnpsyu@uic.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Kliss</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moe</forename><forename type="middle">;</forename><surname>Nel-Bds</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Tnlist</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HashNet: Deep Learning to Hash by Continuation *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the big data era, large-scale and high-dimensional media data has been pervasive in search engines and social networks. To guarantee retrieval quality and computation efficiency, approximate nearest neighbors (ANN) search has attracted increasing attention. Parallel to the traditional indexing methods <ref type="bibr" target="#b20">[21]</ref>, another advantageous solution is hashing methods <ref type="bibr" target="#b37">[38]</ref>, which transform high-dimensional media data into compact binary codes and generate similar binary codes for similar data items. In this paper, we will focus on learning to hash methods <ref type="bibr" target="#b37">[38]</ref> that build data-dependent * Corresponding author: M. Long (mingsheng@tsinghua.edu.cn).</p><p>hash encoding schemes for efficient image retrieval, which have shown better performance than data-independent hashing methods, e.g. Locality-Sensitive Hashing (LSH) <ref type="bibr" target="#b9">[10]</ref>.</p><p>Many learning to hash methods have been proposed to enable efficient ANN search by Hamming ranking of compact binary hash codes <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>. Recently, deep learning to hash methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref> have shown that end-to-end learning of feature representation and hash coding can be more effective using deep neural networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b1">2]</ref>, which can naturally encode any nonlinear hash functions. These deep learning to hash methods have shown state-of-the-art performance on many benchmarks. In particular, it proves crucial to jointly learn similarity-preserving representations and control quantization error of binarizing continuous representations to binary codes <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b23">24]</ref>. However, a key disadvantage of these deep learning to hash methods is that they need to first learn continuous deep representations, which are binarized into hash codes in a separated post-step of sign thresholding. By continuous relaxation, i.e. solving the discrete optimization of hash codes with continuous optimization, all these methods essentially solve an optimization problem that deviates significantly from the hashing objective as they cannot learn exactly binary hash codes in their optimization procedure. Hence, existing deep hashing methods may fail to generate compact binary hash codes for efficient similarity retrieval.</p><p>There are two key challenges to enabling deep learning to hash truly end-to-end. First, converting deep representations, which are continuous in nature, to exactly binary hash codes, we need to adopt the sign function h = sgn (z) as activation function when generating binary hash codes using similarity-preserving learning in deep neural networks. However, the gradient of the sign function is zero for all nonzero inputs, which will make standard back-propagation infeasible. This is known as the ill-posed gradient problem, which is the key difficulty in training deep neural networks via back-propagation <ref type="bibr" target="#b13">[14]</ref>. Second, the similarity information is usually very sparse in real retrieval systems, i.e., the number of similar pairs is much smaller than the number of dissimilar pairs. This will result in the data imbalance problem, making similarity-preserving learning ineffective. Optimizing deep networks with sign activation remains an open problem and a key challenge for deep learning to hash.</p><p>This work presents HashNet, a new architecture for deep learning to hash by continuation with convergence guarantees, which addresses the ill-posed gradient and data imbalance problems in an end-to-end framework of deep feature learning and binary hash encoding. Specifically, we attack the ill-posed gradient problem in the non-convex optimization of the deep networks with non-smooth sign activation by the continuation methods <ref type="bibr" target="#b0">[1]</ref>, which address a complex optimization problem by smoothing the original function, turning it into a different problem that is easier to optimize. By gradually reducing the amount of smoothing during the training, it results in a sequence of optimization problems converging to the original optimization problem. A novel weighted pairwise cross-entropy loss function is designed for similarity-preserving learning from imbalanced similarity relationships. Comprehensive experiments testify that HashNet can generate exactly binary hash codes and yield state-of-the-art retrieval performance on standard datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Existing learning to hash methods can be organized into two categories: unsupervised hashing and supervised hashing. We refer readers to <ref type="bibr" target="#b37">[38]</ref> for a comprehensive survey.</p><p>Unsupervised hashing methods learn hash functions that encode data points to binary codes by training from unlabeled data. Typical learning criteria include reconstruction error minimization <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref> and graph learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b25">26]</ref>. While unsupervised methods are more general and can be trained without semantic labels or relevance information, they are subject to the semantic gap dilemma <ref type="bibr" target="#b34">[35]</ref> that highlevel semantic description of an object differs from lowlevel feature descriptors. Supervised methods can incorporate semantic labels or relevance information to mitigate the semantic gap and improve the hashing quality significantly. Typical supervised methods include Binary Reconstruction Embedding (BRE) <ref type="bibr" target="#b18">[19]</ref>, Minimal Loss Hashing (MLH) <ref type="bibr" target="#b29">[30]</ref> and Hamming Distance Metric Learning <ref type="bibr" target="#b30">[31]</ref>. Supervised Hashing with Kernels (KSH) <ref type="bibr" target="#b24">[25]</ref> generates hash codes by minimizing the Hamming distances across similar pairs and maximizing the Hamming distances across dissimilar pairs.</p><p>As deep convolutional neural network (CNN) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13]</ref> yield breakthrough performance on many computer vision tasks, deep learning to hash has attracted attention recently. CNNH <ref type="bibr" target="#b39">[40]</ref> adopts a two-stage strategy in which the first stage learns hash codes and the second stage learns a deep network to map input images to the hash codes. DNNH <ref type="bibr" target="#b19">[20]</ref> improved the two-stage CNNH with a simultaneous feature learning and hash coding pipeline such that representations and hash codes can be optimized in a joint learning process. DHN <ref type="bibr" target="#b43">[44]</ref> further improves DNNH by a cross-entropy loss and a quantization loss which preserve the pairwise similarity and control the quantization error simultaneously. DHN obtains state-of-the-art performance on several benchmarks.</p><p>However, existing deep learning to hash methods only learn continuous codes g and need a binarization post-step to generate binary codes h. By continuous relaxation, these methods essentially solve an optimization problem L(g) that deviates significantly from the hashing objective L(h), because they cannot keep the codes exactly binary after convergence. Denote by Q(g, h) the quantization error function by binarizing continuous codes g into binary codes h. Prior methods control the quantization error in two ways: (a) min L(g) + Q(g, h) through continuous optimization <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b21">22]</ref>; (b) min L(h)+Q(g, h) through discrete optimization on L(h) but continuous optimization on Q(g, h) (the continuous optimization is used for out-of-sample extension as discrete optimization cannot be extended to the test data) <ref type="bibr" target="#b23">[24]</ref>. However, since Q(g, h) cannot be minimized to zero, there is a large gap between continuous codes and binary codes. To directly optimize min L(h), we must adopt sign as the activation function within deep networks, which enables generation of exactly binary codes but introduces the ill-posed gradient problem. This work is the first effort to learn sign-activated deep networks by continuation method, which can directly optimize L(h) for deep learning to hash.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">HashNet</head><p>In similarity retrieval systems, we are given a training set of N points {x i } N i=1 , each represented by a D-dimensional feature vector x i ∈ R D . Some pairs of points x i and x j are provided with similarity labels s ij , where s ij = 1 if x i and x j are similar while s ij = 0 if x i and x j are dissimilar. The goal of deep learning to hash is to learn nonlinear hash function f : x → h ∈ {−1, 1} K from input space R D to Hamming space {−1, 1} K using deep neural networks, which encodes each point x into compact K-bit binary hash code h = f (x) such that the similarity information between the given pairs S can be preserved in the compact hash codes. In supervised hashing, the similarity set S = {s ij } can be constructed from semantic labels of data points or relevance feedback from click-through data in real retrieval systems.</p><p>To address the data imbalance and ill-posed gradient problems in an end-to-end learning framework, this paper presents HashNet, a novel architecture for deep learning to hash by continuation, shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The architecture accepts pairwise input images {(x i , x j , s ij )} and processes them through an end-to-end pipeline of deep representation learning and binary hash coding: (1) a convolutional network (CNN) for learning deep representation of each image x i , (2) a fully-connected hash layer (f ch) for transforming the deep representation into K-dimensional representation binary hash code h i ∈ {−1, 1} K , and (4) a novel weighted cross-entropy loss for similarity-preserving learning from imbalanced data. We attack the ill-posed gradient problem of the non-smooth activation function h = sgn (z) by continuation, which starts with a smoothed activation function y = tanh (βx) and becomes more non-smooth by increasing β as the training proceeds, until eventually goes back to the original, difficult to optimize, sign activation function.</p><formula xml:id="formula_0">z i ∈ R K , (3) a sign activation function h = sgn (z) for binarizing the K-dimensional representation z i into K-bit</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Formulation</head><p>To perform deep learning to hash from imbalanced data, we jointly preserve similarity information of pairwise images and generate binary hash codes by weighted maximum likelihood <ref type="bibr" target="#b5">[6]</ref>. For a pair of binary hash codes h i and h j , there exists a nice relationship between their Hamming distance dist H (·, ·) and inner product ·, · : dist</p><formula xml:id="formula_1">H (h i , h j ) = 1 2 (K − h i , h j ).</formula><p>Hence, the Hamming distance and inner product can be used interchangeably for binary hash codes, and we adopt inner product to quantify pairwise similarity. Given the set of pairwise similarity labels S = {s ij }, the Weighted Maximum Likelihood (WML) estimation of the hash codes H = [h 1 , . . . , h N ] for all N training points is</p><formula xml:id="formula_2">log P (S|H) = sij ∈S w ij log P (s ij |h i , h j ),<label>(1)</label></formula><p>where P (S|H) is the weighted likelihood function, and w ij is the weight for each training pair (x i , x j , s ij ), which is used to tackle the data imbalance problem by weighting the training pairs according to the importance of misclassifying that pair <ref type="bibr" target="#b5">[6]</ref>. Since each similarity label in S can only be s ij = 1 (similar) or s ij = 0 (dissimilar), to account for the data imbalance between similar and dissimilar pairs, we set</p><formula xml:id="formula_3">w ij = c ij · |S| / |S 1 | , s ij = 1 |S| / |S 0 | , s ij = 0<label>(2)</label></formula><p>where S 1 = {s ij ∈ S : s ij = 1} is the set of similar pairs and S 0 = {s ij ∈ S : s ij = 0} is the set of dissimilar pairs; c ij is continuous similarity, i.e. c ij = yi∩yj yi∪yj if labels y i and y j of x i and x j are given, c ij = 1 if only s ij is given. For each pair, P (s ij |h i , h j ) is the conditional probability of similarity label s ij given a pair of hash codes h i and h j , which can be naturally defined as pairwise logistic function,</p><formula xml:id="formula_4">P (s ij |h i , h j ) = σ ( h i , h j ) , s ij = 1 1 − σ ( h i , h j ) , s ij = 0 = σ( h i , h j ) sij (1 − σ ( h i , h j )) 1−sij (3) where σ (x) = 1/(1 + e −αx )</formula><p>is the adaptive sigmoid function with hyper-parameter α to control its bandwidth. Note that the sigmoid function with larger α will have larger saturation zone where its gradient is zero. To perform more effective back-propagation, we usually require α &lt; 1, which is more effective than the typical setting of α = 1. Similar to logistic regression, we can see in pairwise logistic regression that the smaller the Hamming distance dist H (h i , h j ) is, the larger the inner product h i , h j as well as the conditional probability P (1|h i , h j ) will be, implying that pair h i and h j should be classified as similar; otherwise, the larger the conditional probability P (0|h i , h j ) will be, implying that pair h i and h j should be classified as dissimilar. Hence, Equation <ref type="formula">(3)</ref> is a reasonable extension of the logistic regression classifier to the pairwise classification scenario, which is optimal for binary similarity labels s ij ∈ {0, 1}.</p><p>By taking Equation <ref type="formula">(3)</ref> into WML estimation in Equation (1), we achieve the optimization problem of HashNet,</p><formula xml:id="formula_5">min Θ sij ∈S w ij (log (1 + exp (α h i , h j )) − αs ij h i , h j ),<label>(4)</label></formula><p>where Θ denotes the set of all parameters in deep networks. Note that, HashNet directly uses the sign activation function h i = sgn (z i ) which converts the K-dimensional representation to exactly binary hash codes, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. By optimizing the WML estimation in Equation <ref type="formula" target="#formula_5">(4)</ref>, we can enable deep learning to hash from imbalanced data under a statistically optimal framework. It is noteworthy that our work is the first attempt that extends the WML estimation from pointwise scenario to pairwise scenario. The HashNet can jointly preserve similarity information of pairwise images and generate exactly binary hash codes. Different from HashNet, previous deep-hashing methods need to first learn continuous embeddings, which are binarized in a separated step using the sign function. This will result in substantial quantization errors and significant losses of retrieval quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning by Continuation</head><p>HashNet learns exactly binary hash codes by converting the K-dimensional representation z of the hash layer f ch, which is continuous in nature, to binary hash code h taking values of either +1 or −1. This binarization process can only be performed by taking the sign function h = sgn (z) as activation function on top of hash layer f ch in HashNet,</p><formula xml:id="formula_6">h = sgn (z) = +1, if z 0 −1, otherwise<label>(5)</label></formula><p>Unfortunately, as the sign function is non-smooth and nonconvex, its gradient is zero for all nonzero inputs, and is illdefined at zero, which makes the standard back-propagation infeasible for training deep networks. This is known as the vanishing gradient problem, which has been a key difficulty in training deep neural networks via back-propagation <ref type="bibr" target="#b13">[14]</ref>. Many optimization methods have been proposed to circumvent the vanishing gradient problem and enable effective network training with back-propagation, including unsupervised pre-training <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3]</ref>, dropout <ref type="bibr" target="#b35">[36]</ref>, batch normalization <ref type="bibr">[15]</ref>, and deep residual learning <ref type="bibr" target="#b12">[13]</ref>. In particular, Rectifier Linear Unit (ReLU) <ref type="bibr" target="#b28">[29]</ref> activation function makes deep networks much easier to train and enables end-to-end learning algorithms. However, the sign activation function is so ill-defined that all the above optimization methods will fail. A very recent work, BinaryNet <ref type="bibr" target="#b4">[5]</ref>, focuses on training deep networks with activations constrained to +1 or −1. However, the training algorithm may be hard to converge as the feed-forward pass uses the sign activation (sgn) but the back-propagation pass uses a hard tanh (Htanh) activation. Optimizing deep networks with sign activation remains an open problem and a key challenge for deep learning to hash.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Optimizing HashNet by Continuation</head><p>Input: A sequence 1 = β 0 &lt; β 1 &lt; . . . &lt; β m = ∞ for stage t = 0 to m do Train HashNet (4) with tanh(β t z) as activation Set converged HashNet as next stage initialization end Output: HashNet with sgn(z) as activation, β m → ∞ This paper attacks the problem of non-convex optimization of deep networks with non-smooth sign activation by starting with a smoothed objective function which becomes more non-smooth as the training proceeds. It is inspired by recent studies in continuation methods <ref type="bibr" target="#b0">[1]</ref>, which address a complex optimization problem by smoothing the original function, turning it into a different problem that is easier to optimize. By gradually reducing the amount of smoothing during the training, it results in a sequence of optimization problems converging to the original optimization problem. Motivated by the continuation methods, we notice there exists a key relationship between the sign function and the scaled tanh function in the concept of limit in mathematics,</p><formula xml:id="formula_7">lim β→∞ tanh (βz) = sgn (z) ,<label>(6)</label></formula><p>where β &gt; 0 is a scaling parameter. Increasing β, the scaled tanh function tanh(βz) will become more non-smooth and more saturated so that the deep networks using tanh(βz) as the activation function will be more difficult to optimize, as in <ref type="figure" target="#fig_0">Figure 1</ref> (right). But fortunately, as β → ∞, the optimization problem will converge to the original deep learning to hash problem in (4) with sgn(z) activation function.</p><p>Using the continuation methods, we design an optimization method for HashNet in Algorithm 1. As deep network with tanh(z) as the activation function can be successfully trained, we start training HashNet with tanh(β t z) as the activation function, where β 0 = 1. For each stage t, after HashNet converges, we increase β t and train (i.e. fine-tune) HashNet by setting the converged network parameters as the initialization for training the HashNet in the next stage. By evolving tanh(β t z) with β t → ∞, the network will converge to HashNet with sgn(z) as activation function, which can generate exactly binary hash codes as we desire. The efficacy of continuation in Algorithm 1 can be understood as multi-stage pre-training, i.e., pre-training HashNet with tanh(β t z) activation function is used to initialize HashNet with tanh(β t+1 z) activation function, which enables easier progressive training of HashNet as the network is becoming non-smooth in later stages by β t → ∞. Using m = 10 we can already achieve fast convergence for training HashNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Convergence Analysis</head><p>We analyze that the continuation method in Algorithm 1 decreases HashNet loss (4) in each stage and each iteration.</p><formula xml:id="formula_8">Let L ij = w ij (log (1 + exp (α h i , h j )) − αs ij h i , h j ) and L = sij ∈S L ij , where h i ∈ {−1, +1}</formula><p>K are binary hash codes. Note that when optimizing HashNet by continuation in Algorithm 1, the network activation in each stage t is g = tanh(β t z), which is continuous in nature and will only become binary after convergence β t → ∞. Denote by J ij = w ij (log (1 + exp (α g i , g j )) − αs ij g i , g j ) and J = sij ∈S J ij the true loss we optimize in Algorithm 1, where g i ∈ R</p><p>K and h i = sgn(g i ).</p><p>Our results are two theorems, with proofs provided in the supplemental materials. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct extensive experiments to evaluate HashNet against several state-of-the-art hashing methods on three standard benchmarks. Datasets and implementations are available at http://github.com/thuml/HashNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>The evaluation is conducted on three benchmark image retrieval datasets: ImageNet, NUS-WIDE and MS COCO.</p><p>ImageNet is a benchmark image dataset for Large Scale Visual Recognition Challenge (ILSVRC 2015) <ref type="bibr" target="#b31">[32]</ref>. It contains over 1.2M images in the training set and 50K images in the validation set, where each image is single-labeled by one of the 1,000 categories. We randomly select 100 categories, use all the images of these categories in the training set as the database, and use all the images in the validation set as the queries; furthermore, we randomly select 100 images per category from the database as the training points.</p><p>NUS-WIDE 1 <ref type="bibr" target="#b3">[4]</ref> is a public Web image dataset which contains 269,648 images downloaded from Flickr.com. Each image is manually annotated by some of the 81 ground truth concepts (categories) for evaluating retrieval models. We follow similar experimental protocols as DHN <ref type="bibr" target="#b43">[44]</ref> and randomly sample 5,000 images as queries, with the remaining images used as the database; furthermore, we randomly sample 10,000 images from the database as training points.</p><p>MS COCO 2 <ref type="bibr" target="#b22">[23]</ref> is an image recognition, segmentation, and captioning dataset. The current release contains 82,783 training images and 40,504 validation images, where each image is labeled by some of the 80 categories. After pruning images with no category information, we obtain 12,2218 images by combining the training and validation images. We randomly sample 5,000 images as queries, with the rest images used as the database; furthermore, we randomly sample 10,000 images from the database as training points.</p><p>Following standard evaluation protocol as previous work <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b43">44]</ref>, the similarity information for hash function learning and for ground-truth evaluation is constructed from image labels: if two images i and j share at least one label, they are similar and s ij = 1; otherwise, they are dissimilar and s ij = 0. Note that, although we use the image labels to construct the similarity information, our proposed HashNet can learn hash codes when only the similarity information is available. By constructing the training data in this way, the ratio between the number of dissimilar pairs and the number of similar pairs is roughly 100, 5, and 1 for ImageNet, NUS-WIDE, and MS COCO, respectively. These datasets exhibit the data imbalance phenomenon and can be used to evaluate different hashing methods under data imbalance scenario.</p><p>We compare retrieval performance of HashNet with ten classical or state-of-the-art hashing methods: unsupervised methods LSH <ref type="bibr" target="#b9">[10]</ref>, SH <ref type="bibr" target="#b38">[39]</ref>, ITQ <ref type="bibr" target="#b11">[12]</ref>, supervised shallow methods BRE <ref type="bibr" target="#b18">[19]</ref>, KSH <ref type="bibr" target="#b24">[25]</ref>, ITQ-CCA <ref type="bibr" target="#b11">[12]</ref>, SDH <ref type="bibr" target="#b33">[34]</ref>, and supervised deep methods CNNH <ref type="bibr" target="#b39">[40]</ref>, DNNH <ref type="bibr" target="#b19">[20]</ref>, DHN <ref type="bibr" target="#b43">[44]</ref>. We evaluate retrieval quality based on five standard evaluation metrics: Mean Average Precision (MAP), Precision-Recall curves (PR), Precision curves within Hamming distance 2 (P@H=2), Precision curves with respect to different numbers of top returned samples (P@N), and Histogram of learned codes without binarization. For fair comparison, all methods use identical training and test sets. We adopt MAP@1000 for ImageNet as each category has 1,300 images, and adopt MAP@5000 for the other datasets <ref type="bibr" target="#b43">[44]</ref>.</p><p>For shallow hashing methods, we use DeCAF 7 features <ref type="bibr" target="#b6">[7]</ref> as input. For deep hashing methods, we use raw images as input. We adopt the AlexNet architecture <ref type="bibr" target="#b17">[18]</ref> for all deep hashing methods, and implement HashNet based on the Caffe framework <ref type="bibr" target="#b16">[17]</ref>. We fine-tune convolutional layers conv1-conv5 and fully-connected layers f c6-f c7 copied from the AlexNet model pre-trained on ImageNet 2012 and train the hash layer f ch, all through back-propagation. As the f ch layer is trained from scratch, we set its learning rate to be 10 times that of the lower layers. We use mini-batch stochastic gradient descent (SGD) with 0.9 momentum and the learning rate annealing strategy implemented in Caffe, and cross-validate the learning rate from 10 −5 to 10 −3 with a multiplicative step-size 10 1 2 . We fix the mini-batch size of images as 256 and the weight decay parameter as 0.0005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>The Mean Average Precision (MAP) results are shown in <ref type="table" target="#tab_0">Table 1</ref>. HashNet substantially outperforms all comparison methods. Specifically, compared to the best shallow hashing method using deep features as input, ITQ/ITQ-CCA, we achieve absolute boosts of 15.7%, 15.5%, and 9.1% in average MAP for different bits on ImageNet, NUS-WIDE, and MS COCO, respectively. Compared to the state-of-the-art deep hashing method, DHN, we achieve absolute boosts of 14.6%, 3.7%, 2.9% in average MAP for different bits on the three datasets, respectively. An interesting phenomenon is that the performance boost of HashNet over DHN is significantly different across the three datasets. Specifically, the performance boost on ImageNet is much larger than that on NUS-WIDE and MS COCO by about 10%, which is very impressive. Recall that the ratio between the number of dis-  <ref type="bibr" target="#b19">[20]</ref>    similar pairs and the number of similar pairs is roughly 100, 5, and 1 for ImageNet, NUS-WIDE and MS COCO, respectively. This data imbalance problem substantially deteriorates the performance of hashing methods trained from pairwise data, including all the deep hashing methods. HashNet enhances deep learning to hash from imbalanced dataset by Weighted Maximum Likelihood (WML), which is a principled solution to tackling the data imbalance problem. This lends it the superior performance on imbalanced datasets.</p><p>The performance in terms of Precision within Hamming radius 2 (P@H=2) is very important for efficient retrieval with binary hash codes since such Hamming ranking only requires O(1) time for each query. As shown in <ref type="figure">Figures  2(a)</ref>, 3(a) and 4(a), HashNet achieves the highest P@H=2 results on all three datasets. In particular, P@H=2 of HashNet with 32 bits is better than that of DHN with any bits. This validates that HashNet can learn more compact binary codes than DHN. When using longer codes, the Hamming space will become sparse and few data points fall within the Hamming ball with radius 2 <ref type="bibr" target="#b8">[9]</ref>. This is why most hashing methods achieve best accuracy with moderate code lengths.</p><p>The retrieval performance on the three datasets in terms of Precision-Recall curves (PR) and Precision curves with respect to different numbers of top returned samples (P@N) are shown in <ref type="figure" target="#fig_2">Figures 2(b)∼4(b) and Figures 2(c)∼4(c)</ref>, respectively. HashNet outperforms comparison methods by large margins. In particular, HashNet achieves much higher precision at lower recall levels or when the number of top results is small. This is desirable for precision-first retrieval, which is widely implemented in practical systems. As an intuitive illustration, <ref type="figure" target="#fig_3">Figure 5</ref> shows that HashNet can yield much more relevant and user-desired retrieval results.</p><p>Recent work <ref type="bibr" target="#b27">[28]</ref> studies two evaluation protocols for supervised hashing: (1) supervised retrieval protocol where queries and database have identical classes and (2) zero-shot retrieval protocol where queries and database have different classes. Some supervised hashing methods perform well in  one protocol but poorly in another protocol. <ref type="table" target="#tab_2">Table 2</ref> shows the MAP results on ImageNet dataset under the zero-shot retrieval protocol, where HashNet substantially outperforms DHN. Thus, HashNet works well under different protocols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Empirical Analysis</head><p>Visualization of Hash Codes: We visualize the t-SNE <ref type="bibr" target="#b6">[7]</ref> of hash codes generated by HashNet and DHN on ImageNet in <ref type="figure" target="#fig_4">Figure 6</ref> (for ease of visualization, we sample 10 categories). We observe that the hash codes generated by HashNet show clear discriminative structures in that different categories are well separated, while the hash codes generated by DHN do not show such discriminative structures. This suggests that HashNet can learn more discriminative hash codes than DHN for more effective similarity retrieval.</p><p>Ablation Study: We go deeper with the efficacy of the weighted maximum likelihood and continuation methods. We investigate three variants of HashNet: (1) HashNet+C, variant using continuous similarity c ij = yi∩yj yi∪yj when image labels are given; (2) HashNet-W, variant using maximum likelihood instead of weighted maximum likelihood, i.e. w ij = 1; (3) HashNet-sgn, variant using tanh() instead of sgn() as activation function to generate continuous codes and requiring a separated binarization step to generate hash codes. We compare results of these variants in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>By weighted maximum likelihood estimation, HashNet outperforms HashNet-W by substantially large margins of 12.4%, 2.8% and 0.1% in average MAP for different bits on ImageNet, NUS-WIDE and MS COCO, respectively. The standard maximum likelihood estimation has been widely adopted in previous work <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b43">44]</ref>. However, this estimation does not account for the data imbalance, and may suffer from performance drop when training data is highly imbalanced (e.g. ImageNet). In contrast, the proposed weighted maximum likelihood estimation (1) is a principled solution to tackling the data imbalance problem by weighting the training pairs according to the importance of misclassifying that pair. Recall that MS COCO is a balanced dataset, hence HashNet and HashNet-W may yield similar MAP results.  By further considering continuous similarity (c ij = yi∩yj yi∪yj ), HashNet+C achieves even better accuracy than HashNet.</p><p>By training HashNet with continuation, HashNet outperforms HashNet-sgn by substantial margins of 8.1%, 1.4% and 3.0% in average MAP on ImageNet, NUS-WIDE, and MS COCO, respectively. Due to the ill-posed gradient problem, existing deep hashing methods cannot learn exactly binary hash codes using sgn() as activation function. Instead, they need to use surrogate functions of sgn(), e.g. tanh(), as the activation function and learn continuous codes, which require a separated binarization step to generate hash codes. The proposed continuation method is a principled solution to deep learning to hash with sgn() as activation function, which learn lossless binary hash codes for accurate retrieval.</p><p>Loss Value Through Training Process: We compare the change of loss values of HashNet and DHN through the training process on ImageNet, NUS-WIDE and MSCOCO. We display the loss values before (-sign) and after (+sign) binarization, i.e. J(g) and L(h). <ref type="figure" target="#fig_5">Figure 7</ref> reveals three important observations: (a) Both methods converge in terms of the loss values before and after binarization, which validates the convergence analysis in Section 3.3. (b) HashNet converges with a much smaller training loss than DHN both before and after binarization, which implies that HashNet can preserve the similarity relationship in Hamming space much better than DHN. (c) The two loss curves of HashNet before and after binarization become close to each other and overlap completely when convergence. This shows that the continuation method enables HashNet to approach the true loss defined on the exactly binary codes without continuous relaxation. But there is a large gap between two loss curves of DHN, implying that DHN and similar methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref> cannot learn exactly binary codes by minimizing quantization error of codes before and after binarization.</p><p>Histogram of Codes Without Binarization: As discussed previously, the proposed HashNet can learn exactly binary hash codes while previous deep hashing methods can only learn continuous codes and generate binary hash codes by post-step sign thresholding. To verify this key property, we plot the histograms of codes learned by HashNet and DHN on the three datasets without post-step binarization. The histograms can be plotted by evenly dividing [0, 1] into 100 bins, and calculating the frequency of codes falling into each bin. To make the histograms more readable, we show absolute code values (x-axis) and squared root of frequency (y-axis). Histograms in <ref type="figure" target="#fig_6">Figure 8</ref> show that DHN can only generate continuous codes spanning across the whole range of <ref type="bibr">[0,</ref><ref type="bibr" target="#b0">1]</ref>. This implies that if we quantize these continuous codes into binary hash codes (taking values in {−1, 1}) in a post-step, we may suffer from large quantization error especially for the codes near zero. On the contrary, the codes of HashNet without binarization are already exactly binary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper addressed deep learning to hash from imbalanced similarity data by the continuation method. The proposed HashNet can learn exactly binary hash codes by optimizing a novel weighted pairwise cross-entropy loss function in deep convolutional neural networks. HashNet can be effectively trained by the proposed multi-stage pre-training algorithm carefully crafted from the continuation method. Comprehensive empirical evidence shows that HashNet can generate exactly binary hash codes and yield state-of-the-art multimedia retrieval performance on standard benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head><p>This work was supported by the National Key R&amp;D Program of China (No. 2016YFB1000701), the National Natural Science Foundation of China (No. 61502265, 61325008, and 71690231), the National Sci.&amp;Tech. Supporting Program (2015BAF32B01), and the Tsinghua TNList Projects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (left) The proposed HashNet for deep learning to hash by continuation, which is comprised of four key components: (1) Standard convolutional neural network (CNN), e.g. AlexNet and ResNet, for learning deep image representations, (2) a fully-connected hash layer (f ch) for transforming the deep representation into K-dimensional representation, (3) a sign activation function (sgn) for binarizing the K-dimensional representation into K-bit binary hash code, and (4) a novel weighted cross-entropy loss for similarity-preserving learning from sparse data. (right) Plot of smoothed responses of the sign function h = sgn (z): Red is the sign function, and blue, green and orange show functions h = tanh (βz) with bandwidths βb &lt; βg &lt; βo. The key property is lim β→∞ tanh (βz) = sgn (z). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Figure 2. The experimental results of HashNet and comparison methods on the ImageNet dataset under three evaluation metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The experimental results of HashNet and comparison methods on the MS COCO dataset under three evaluation metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Examples of top 10 retrieved images and precision@10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The t-SNE of hash codes learned by HashNet and DHN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Losses of HashNet and DHN through training process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Histogram of non-binarized codes of HashNet and DHN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Theorem 1 .</head><label>1</label><figDesc>The HashNet loss L will not change across stages t and t+1 with bandwidths switched from β t to β t+1 . Theorem 2. Loss L decreases when optimizing loss J(g) by the stochastic gradient descent (SGD) within each stage.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Mean Average Precision (MAP) of Hamming Ranking for Different Number of Bits on the Three Image Datasets</figDesc><table>Method 
ImageNet 
NUS-WIDE 
MS COCO 
16 bits 32 bits 48 bits 64 bits 16 bits 32 bits 48 bits 64 bits 16 bits 32 bits 48 bits 64 bits 
HashNet 
0.5059 0.6306 0.6633 0.6835 0.6623 0.6988 0.7114 0.7163 0.6873 0.7184 0.7301 0.7362 
DHN [44] 
0.3106 0.4717 0.5419 0.5732 0.6374 0.6637 0.6692 0.6714 0.6774 0.7013 0.6948 0.6944 
DNNH </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc>MAP on ImageNet with Zero-Shot Retrieval Protocol [28]</figDesc><table>Method 
16 bits 
32 bits 
48 bits 
64 bits 
HashNet 
0.4411 
0.5274 
0.5651 
0.5756 
DHN [44] 
0.2891 
0.4421 
0.5123 
0.5342 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Mean Average Precision (MAP) Results of HashNet and Its Variants, HashNet+C, HashNet-W, and HashNet-sgn on Three Datasets Method ImageNet NUS-WIDE MS COCO 16 bits 32 bits 48 bits 64 bits 16 bits 32 bits 48 bits 64 bits 16 bits 32 bits 48 bits 64 bits</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm 2 http://mscoco.org</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Numerical continuation methods: an introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Allgower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Georg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<editor>B. Schölkopf, J. C. Platt, and T. Hoffman</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Nus-wide: A real-world web image database from national university of singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Zheng</surname></persName>
		</author>
		<editor>ICMR. ACM</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maximum likelihood in cost-sensitive learning: Model specification, approximations, and upper bounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Dmochowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Parra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010-12" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep hashing for compact binary codes learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2475" to="2483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast search in hamming space with multi-index hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Punjani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning binary codes for high-dimensional data using bilinear projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="484" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to hash with binary reconstructive embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simultaneous feature learning and hash coding with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contentbased multimedia information retrieval: State of the art and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Djeraba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature learning based deep supervised hashing with pairwise labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep supervised hashing for fast image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2064" to="2072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Supervised hashing with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<editor>CVPR. IEEE</editor>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hashing with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. ACM</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hash bit selection: a unified solution for selection problems in hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1570" to="1577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Partial hash update via hamming subspace learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1939" to="1951" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<editor>J. Fürnkranz and T. Joachims</editor>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Minimal loss hashing for compact binary codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hamming distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1061" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning a nonlinear embedding by preserving class neighbourhood structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="412" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Supervised discrete hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H. Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Content-based image retrieval at the end of the early years</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2004" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semi-supervised hashing for large-scale search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2393" to="2406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hashing for similarity search: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spectral hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Supervised hashing for image retrieval via image representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Circulant binary embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Supervised hashing with latent factor models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep semantic ranking based hashing for multi-label image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep hashing network for efficient similarity retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI. AAAI</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
