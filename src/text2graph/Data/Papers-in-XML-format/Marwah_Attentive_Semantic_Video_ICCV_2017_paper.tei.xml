<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attentive Semantic Video Generation using Captions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanya</forename><surname>Marwah</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IIT Hyderabad</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Mittal</surname></persName>
							<email>gaurav.mittal.191013@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IIT Hyderabad</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Attentive Semantic Video Generation using Captions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Vineeth N. Balasubramanian IIT Hyderabad vineethnb@iith.ac.in</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>What does the mention of a video evoke in a listener's mind? A long sequence of images (frames) with multiple changing scenes which are temporally and spatially linked to each other. Making a machine generate such an entity is a highly involved task as it requires the machine to learn how to coordinate between the long-term and short-term correlations existing between the various elements of a video. This is the primary motivation behind our work where we take the first steps towards semantic video generation with captions generating variable length videos one frame at a time.</p><p>There have been efforts in the recent past which attempt to perform unsupervised video generation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b15">16]</ref> in general, without any specific conditioning on captions. However, from an application perspective, it may not be very useful as there doesn't exist any semantic control over what will be generated at run time. In this work, we propose an approach that attempts to provide this control and streamlines video generation by using captions.</p><p>Since a video can be arbitrarily long, generation of such videos necessitates a step-by-step generation mechanism. * Equal Contribution Therefore, our model approaches video generation iteratively by creating one frame at a time, and conditioning the generation of the subsequent frames by the frames generated so far. Also, every frame is itself an amalgamation of several objects moving and interacting with each other. In order to generate such frames, we follow a recurrent attentive approach similar to <ref type="bibr" target="#b2">[3]</ref>, which focuses on one part of the frame at each time step for generation, and completes the frame generation over multiple time steps. By iteratively generating the frame over a number of time-steps in response to a given caption, our network adds captiondriven semantics to the generated video. A key advantage of following such an approach is the possibility to generate videos with multiple captions and thus change the contents of the video midway according to the new caption.</p><p>In order to achieve the aforementioned objectives, the proposed network is required to not only account for the local correlations between any two consecutively generated frames but also has to ensure that the long-term spatiotemporal nature of the video is preserved so that the generated video is not just a loosely coupled set of frames but exhibits a holistic portrayal of the given caption. In this work, we adopt a fresh perspective in this direction, by devising a network that learns these long-term and short-term video semantics separately but simultaneously. To create a frame based on a caption, instead of trivially conditioning the generation on the caption text, we introduce a soft-attention over the captions separately for the long-term and shortterm contexts. This attention mechanism serves the crucial role of allowing the network to selectively combine the various parts of the caption with these two contexts and hence, significantly improves the quality of the generated frame. Experiments show that the network, when given multiple captions, is even able to transition between scenes transferring the spatio-temporal semantics it has learned thus far, dynamically during generation. This paper makes the following contributions: (1) A novel methodology that can perform variable length semantic video generation with captions by separately and simultaneously learning the long-term and short-term context of the video; (2) A methodology for selectively combining in-formation for conditioning at various levels of the architecture using appropriate attention mechanisms; and (3) A network architecture which learns a robust latent representation of videos and is able to perform competently on tasks such as unsupervised video generation and action recognition. The results obtained on standard datasets that earlier efforts have used for such efforts are very promising, and support the claims of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recent years have witnessed the emergence of generative models like Variational Autoencoders (VAEs) <ref type="bibr" target="#b7">[8]</ref>, Generative Adversarial Networks (GANs) <ref type="bibr" target="#b0">[1]</ref> and other methods such as using autoregression <ref type="bibr" target="#b24">[25]</ref>. Earlier generative models based on methods such as Boltzmann Machines (RBMs) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b16">17]</ref> and Deep Belief Nets <ref type="bibr" target="#b3">[4]</ref> were always constrained by issues such as intractability of sampling. Recent methods have enabled learning a much more robust latent representation of the given data <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b29">30]</ref> and are helping improving the performance of several supervised learning tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Previous approaches for image generation have extended VAEs and GANs to generate images based on captions by conditioning them with textual information <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>. Our approach also constitutes a variant of Conditional VAE <ref type="bibr" target="#b19">[20]</ref> but differs in that in our paper, we use it to generate videos (earlier efforts only generated images) from captions with the capability to generate videos of arbitrary lengths. Further, our approach is distinct in that it incorporates captions by learning an attention-based embedding over them by leveraging long-term and short-term dependencies within the video to generate coherent videos based on captions.</p><p>The proposed methodology draws some similarity with past methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b15">16]</ref> that perform unsupervised video generation (but without captions), with all of them using GANs. Vondrick et al. <ref type="bibr" target="#b25">[26]</ref> use a convolutional network with a fractional stride as the generator and a spatio-temporal convolutional network as a discriminator in a two-stream approach where the background and the foreground of a scene are processed separately. Saito et al. <ref type="bibr" target="#b15">[16]</ref> introduced a network called Temporal GAN where they use a 1D deconvolutional network to output a set of latent variables, each corresponding to a separate frame and an image generator which translates them to a 2D frame of the video. However, in addition to not incorporating caption-based video generation, these approaches suffer from the drawback of not being scalable in generating arbitrarily long videos. Our approach, by approaching video generation frame-by-frame and utilizing the long-term and short-term context separately, effectively counters these limitations of earlier work. Besides, our approach learns to focus on separate objects/glimpses in a frame unlike <ref type="bibr" target="#b5">[6]</ref> where the focus is on individual pixels, and <ref type="bibr" target="#b25">[26]</ref> where network's attention is divided into just background and foreground.</p><p>In addition to generative modeling of videos, learning their underlying representations has been used to assist several supervised learning tasks such as future prediction and action recognition. <ref type="bibr" target="#b21">[22]</ref> is one of the earliest efforts in learning deep representations for videos and utilizes Long ShortTerm Memory units (LSTMs) <ref type="bibr" target="#b4">[5]</ref> to predict future frames given a set of input frames. More recent efforts on video prediction have attempted to factorize the joint likelihood of the video to predict the future frames <ref type="bibr" target="#b5">[6]</ref> or use the first image of the video with Conditional VAE to predict the motion trajectories and use them to generate subsequent frames <ref type="bibr" target="#b26">[27]</ref>. We later demonstrate that in addition to the primary application of semantically generating videos with attentive captioning, it is possible to make small changes to our network which enable it to even predict future frames given an input video sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The primary focus of our work is to allow video generation to take place semantically via captions. Generating a video with variable number of frames given a caption is very different from generating a single image for a caption, due to the need to model the temporal context in terms of the actions and interactions among the objects. Consider, for example, the caption -"A man is walking on the grass." Generating a single image for this caption, ideally, will simply show a man in a walking stance on top of a grassy ground. However, if we want to generate a sequence of frames corresponding to this caption, it necessitates the network to understand how the structure of man on the grass will transition from one frame to the other in relation to the motion of walking. In other words, it means that the network should be able to decouple the fundamental building blocks of any video, i.e., objects, actions and interactions, and have the ability to combine them semantically given a stimulus, which in this case, is a caption. We now explain how we model our approach to generate variable length videos conditioned on captions and later discuss the network architecture.  <ref type="figure">Figure 1</ref>. Illustration of the proposed model.</p><formula xml:id="formula_0">Long-Term Context Y k-1 ,Y k-2 ,...,Y 1 Caption X X 1 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model</head><p>Let the random variable Y = {Y 1 , Y 2 , · · · , Y n } denote the distribution over videos with Y 1 , Y 2 , · · · , Y n being the ordered set of n frames comprising the videos. Let X = {X 1 , X 2 , · · · , X m } be the random variable for the distribution over text captions with X 1 , X 2 , · · · , X m being the ordered set of m words constituting the caption. P (Y |X) then captures the conditional distribution of generating some video in Y for a given caption in X. Our objective is to maximize the likelihood of generating an appropriate video for a given caption. Since we would like to generate a video for a caption one frame at a time, we redefine P (Y |X) as:</p><formula xml:id="formula_1">P (Y |X) = n i=1 P (Y i |Y i−1 , · · · · · · , Y 1 , X)<label>(1)</label></formula><p>where n is the total number of frames in the video. Generation of the k th frame, Y k , can therefore be expressed as:</p><formula xml:id="formula_2">P (Y k |X) = P (Y k |Y k−1 , · · · , Y 1 , X)<label>(2)</label></formula><p>thus allowing the generation of a given frame to depend on all the previously generated frames. The generation can hence model both short-term and long-term spatio-temporal context. Y k gathers short-term context, which consists of the local spatial and temporal correlations existing between any two consecutive frames, from Y k−1 . Y k also obtains its long-term context from all the previous frames combined to understand the overall flow of the video. This ensures that that the overall consistency of the video is maintained while generating the frame. In order to model this when generating the kth frame, we define two functions: U k and V k :</p><formula xml:id="formula_3">U k = g(Y k−1 , X) (3) V k = h(Y k−1 , · · · , Y 1 , X)<label>(4)</label></formula><p>where U and V model the short-term and long-term stimulus respectively for generating Y k . These two functions are implemented as new layers in our architecture, which is discussed in subsequent sections. Therefore, we now model</p><formula xml:id="formula_4">P (Y |X) as P (Y |U, V ) = n i=1 P (Y i |U i , V i ). P (Y |U, V )</formula><p>is a complex multi-modal distribution that models the various possibilities of generating a video given a caption. For example, the same caption "Man walking on grass" can be generated with a man differing in height, face and other physiological attributes. Even the way the man walks can exhibit a wide range of variations. In order to capture this complex distribution P (Y |U, V ) and avoid an 'averaging effect' over the various possibilities, we use a variational auto-encoder (VAE) <ref type="bibr" target="#b7">[8]</ref>. The VAE introduces a latent variable z such that the likelihood of any given possibility is governed by the likelihood of sampling a particular value of z. So, we can define P (Y |U, V ) in terms of z as (similar to <ref type="bibr" target="#b7">[8]</ref>), using normal distribution (denoted by N ):</p><formula xml:id="formula_5">P (Y |U, V ) = N (f (z, U, V ), σ 2 * I)<label>(5)</label></formula><p>Using variational inference, a distribution Q(z|Y, U, V ) is introduced that takes values for Y , U and V in the training phase, and outputs an approximate posterior distribution over z that is likely to generate the appropriate video given the long-term and short-term contexts derived from a given caption. Hence the variational lower bound is:</p><formula xml:id="formula_6">log P (Y |U, V ) = KL(Q(z|U, V, Y )||P (z|U, V, Y )) + E z∼Q(z|U,V,Y ) [log P (Y, z|U, V ) − log Q(z|U, V, Y )] ≥ E z∼Q(z|U,V,Y ) [log P (Y |U, V, z) + log P (z|U, V ) − log Q(z|U, V, Y )] = E z∼Q(z|U,V,Y ) [log P (Y |U, V, z)) − KL(Q(z|U, V, Y )||P (z|U, V )] (6)</formula><p>where Q(z|U, V, Y ) approximates the intractable posterior P (z|U, V ). z is conditionally independent of U and V with P (z|U, V ) = P (z) and P (z) is assumed to be N (0, I). One way to model P (Y |U, V ) for video generation is to introduce separate latent variables for each of the frames to be generated. Such an approach does not allow the model to scale and generate videos with arbitrary number of frames (since the number of latent variables will then be arbitrary). Therefore, we propose our model with just a single latent variable, z to model P (Y |U, V ); the same latent variable is sampled recurrently to generate every subsequent frame. <ref type="figure">Figure 1</ref> shows an illustration of the proposed model. During testing, we sample z ∼ N (0, I) to get a sample (one video frame) from the distribution P (Y |U, V ). This is then continued recurrently, allowing us to generate videos with any user-defined number of frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Latent Representation and Soft Attention for Captions</head><p>Similar to <ref type="bibr" target="#b10">[11]</ref>, we first pass the given caption through a bi-directional LST M lang , as shown in <ref type="figure" target="#fig_0">Figure 2</ref>, to generate a word-level latent representation for the caption,</p><formula xml:id="formula_7">h lang = [h lang 1 , h lang 2 , · · · , h lang m ] with h lang i</formula><p>denoting the latent representation for the i th word in the caption (m being the total number of words in the caption). Since U and V model different kinds of context for a video, it is reasonable to say that a caption can trigger U and V differently. For instance, in the example caption discussed so far, words like 'grass' and 'walking' that correspond to elements like background and motion in a video can trigger its long-term characteristics; while 'man' whose posture changes in every frame can be responsible for the video's short-term characteristics. Therefore, we propose the network architecture to have a separate soft attention mechanism for each of the contexts over h lang , the latent representation of the caption. This is described in the next few sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Modeling Long-Term Context using Attention</head><p>To model the long-term context, V k , to generate the video frame Y k , our network architecture consists of a con-  volutional LSTM, which we call LST M video as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. We feed the frames Y k−1 , · · · , Y 1 , generated so far by the network to LST M video , and compute h video which is the final latent representation learned by LST M video after processing all the previous frames. This h video is then combined with h lang from the caption via a soft-attention layer to create the long-term context representation s l as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The soft-attention mechanism receives h lang and h video as input, and works by learning a set of probabilities, A = {α 1 , α 2 , · · · , α m } corresponding to each word in the caption. The output of the soft-attention layer is then given by:</p><formula xml:id="formula_8">s l = attention(h lang , h video ) = α 1 h lang 1 + α 2 h lang 2 + · · · + α m h m lang<label>(7)</label></formula><p>where:</p><formula xml:id="formula_9">α i = exp v T tanh(uh lang i + wh video + b) m j=1 exp v T tanh(uh lang j + wh video + b)</formula><p>where v, u, w and b are the network parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Modeling Short-Term Context using Attention and Frame Generation</head><p>When a frame is generated in a single pass, even though the model might be able to preserve the overall motion in the frame, the various objects in the scene suffer from blurriness and distortion. In order to overcome this drawback of one-shot frame generation, we model U k and propose our frame generator to have a differentiable recurrent attention mechanism resembling <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b2">[3]</ref> to generate frame Y k in T timesteps (as shown in <ref type="figure" target="#fig_0">Figure 2</ref>). The attention mechanism comprises of a grid of Gaussian filters, whose grid center, variances and stride are learnt by the network (as in <ref type="bibr" target="#b2">[3]</ref>). For every timestep t, we read a glimpse, r k−1 , from the previous frame, Y k−1 , pass it through a small convolutional network (2 convolutional layers with one fully connected layer) and combine it with h lang via soft-attention (similar to Section 3.3) to create the short-term context representation s st . Simultaneously, we also read a glimpse, r t , from Y kt−1 (also passed through a similar convolutional network), denoting the frame Y k generated after t−1 timesteps. r t and s st along with s l Section 3.3 are encoded by LST M enc to learn the approximate posterior Q. z is then sampled from Q and decoded using LST M dec , whose output is sent to a similar deconvolutional network before passing to a write function to generate content in a region of interest (learned by the attention mechanism) on the current frame of interest, Y k . This is different from <ref type="bibr" target="#b2">[3]</ref>, and we found this to be important in generating videos of better quality. The use of such an LSTM autoencoder architecture in our model ensures that the region to be attended to next is conditioned on the regions that have been attended so far. The information written on different regions in the frame is accumulated over T timesteps to generate a single frame Y k . The following equations explain these steps:</p><formula xml:id="formula_10">r k−1 t = conv(read(Y k−1 )) (8) r t = conv(read(Y kt−1 ))<label>(9)</label></formula><formula xml:id="formula_11">s st = attention(h lang , [r k−1 t , h dec t−1 )] (10) h enc = LST M enc (r, s s , s l , h dec t−1 ) (11) z ∼ Q(z|h enc ) (12) h dec t = LST M dec (z, s s , s l , h dec t−1 )<label>(13)</label></formula><formula xml:id="formula_12">Y kt = Y kt−1 + write(deconv(h dec t ))<label>(14)</label></formula><p>A major advantage of such a recurrent attention mechanism, in the context of this work, is that the network learns a single distribution that can distinguish between different elements of a frame, and attend to them in each time step. This further enables the network to dynamically combine these elements during inference to effectively generate frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss Function</head><p>As mentioned in the previous sections, we take P (z|U, V ) = P (z) ∼ N (0, I). This simplifies the empirical variational bound to (note that we minimize the negative of the bound, as in VAEs <ref type="bibr" target="#b7">[8]</ref>):</p><formula xml:id="formula_13">L = − 1 S S s=1 log P (Y |U, V, z s )−KL(Q(z|U, V, Y )||P (z|U, V ))<label>(15)</label></formula><p>with</p><formula xml:id="formula_14">KL(Q(z|U, V, Y )||P (z|U, V )) = 1 2 T t=1 µ 2 t +σ 2 t −log σ 2 t −T /2<label>(16)</label></formula><p>where t denotes the time-step over which the frames are generated as before, and z s denotes the s th sample taken from the z distribution among a set of total S samples which are used to compute the likelihood term. The negative likelihood term, which is also the reconstruction loss, is computed as the binary pixel-wise cross-entropy loss between the original video frame and the generated frame. All the losses here are calculated frame-wise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>We evaluated the proposed model on datasets of increasing complexity <ref type="bibr" target="#b0">1</ref> . We first created a variant of Moving MNIST dataset (similar to <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b5">6]</ref>) with videos depicting a single digit moving across the frame. Each video has a set of 10 frames, each of size 64 × 64. We added the 28 × 28 sized images of digit from the original MNIST dataset to each of the frames and varied the initial positions to make the digit move either up-and-down or left-and-right across the frames. We then captioned each video based on the digit and its motion. For instance, a video with caption 'digit 2 is going up and down' contains a sample of 2 from MNIST moving up and down in the video. We similarly created a Two-Digit Moving MNIST dataset similar to <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b5">6]</ref> where each video contains two randomly chosen digits moving across the frames going either left-and-right or up-and-down independently, giving us a dataset containing 400 combinations. Examples of the captions in this dataset are 'digit 1 is moving up and down and digit 3 is moving left and right'.</p><p>To evaluate the proposed network's performance on a more realistic dataset, we used the KTH Human Action Database <ref type="bibr" target="#b17">[18]</ref> which consists of over 2000 video sequences comprised of 25 persons performing 6 actions (walking, running, jogging, hand-clapping, hand-waving and boxing). We used the video sequences of walking, running and jogging for our evaluation because in each of these actions, a person was going either right-to-left or left-to-right which allowed us to introduce a sense of 'direction' in the video context, and study the proposed model. We uniformly sampled from the given video sequence and generated our dataset with each video having 10 frames of size 120 × 120. For each of these videos, we manually created captions such as 'person 1 is walking left-to-right' or 'person 3 is running right-to-left'. The information on person number is obtained from the metadata accompanying the dataset. The advantage of using the KTH dataset in our context is that the same set of people perform all the actions as opposed to other action datasets (such as UCF-101) where the people performing the actions changes. This allows us to add appropriate captions to the dataset, and study and validate the performance of our model.</p><p>We further performed experiments for unsupervised video generation using our model without captions. As it is related to earlier efforts, we show results on UCF-101 dataset <ref type="bibr" target="#b20">[21]</ref> to be able to compare with earlier efforts. We uniformly sampled the videos from UCF-101 dataset to generate video sequences for our training dataset with each video having 10 frames each of size 120 × 120 × 3. Important thing to note here is that although we are training our network over videos of 10 frames, we can generate videos with any number of frames (shown in Section 4.4).</p><p>We note that in order to generate the first frame of the video for a caption, we prefixed videos from all datasets with a start-of-video frame. This frame marks the beginning of every given video. It contains all 0s resembling the startof-sentence tag used to identify the beginning of a sentence in Natural Language Processing <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results on Generation with Captions</head><p>The results on the Moving MNIST and KTH datasets are shown in <ref type="figure">Figure 3</ref>, and illustrate the rather smooth generations of the model. (The results of how one frame of the video is generated over T time-steps is included in the supplementary material due to space constraints.) In order to test that the network is not memorizing, we split the captions into a training and test set. Therefore, if the training set contains the video having caption as 'digit 5 is moving up and down', the model at test-time is provided with 'digit 5 is moving left and right'. Similarly, in the KTH dataset, if the video pertaining to the caption 'person 1 is walking from left-to-right' belongs to the training set, the caption 'person 1 is walking right-to-left' is included at test time. We thus ensure that the network makes a video sequence from a caption that it has never seen before. The results show that the network is indeed able to generate good quality videos for unseen captions. We infer that it is able to dissociate the motion information from the spatial information. In natural datasets where the background is also prominent, the network selectively attends to the object over the background information. The network achieves this without the need of externally separating the object information from the background information <ref type="bibr" target="#b25">[26]</ref> or external motion trajectories <ref type="bibr" target="#b26">[27]</ref>. Another observation is that the object consistency is maintained and the long-term fluidity of the motion is preserved. This shows that the long-term and the shortterm context is effectively captured by the network archi- <ref type="figure">Figure 3</ref>. The results of our network for different datasets to generate videos for a single caption. Number of frames generated is <ref type="bibr">15.</ref> tecture and they both work in 'coordination' to generate the frames. Also, as mentioned earlier, methodology ensures that a video can be generated with any number of frames. An example of a generation with variable length of frames is shown in <ref type="figure" target="#fig_1">Figure 4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results with Spatio-Temporal Style Transfer</head><p>We further evaluated the capability of the network architecture to generate videos where the captions are changed in the middle. Here, we propose two different settings: (1) Action Transfer, where the information of the motion that the object (i.e, digit/person) is performing is changed midway during the generation; and (2) Object Transfer, where the object information (i.e, digit/person) is changed midway during the generation. During action transfer, we expect that the object information remains intact but the motion that is being performed by the object changes; and during object transfer, we expect a change in the object information with respect to what has been generated so far. Results for action and object transfer can be seen in <ref type="figure" target="#fig_2">Figures 5(a)</ref> and 5(b) respectively. We also go a step ahead and perform both Action Transfer and Object Transfer together as shown in <ref type="figure" target="#fig_2">Figure 5(c)</ref>. To test the robustness of the network, we ensured that the second caption used in this setup was not used for training the network.</p><p>We note here that when the spatio-temporal transfer happens, the object position remains the same in all the results, and the object from the second caption continues its action from exactly the same position. This is different from the case when the video is freshly generated using a caption since then the object can begin its motion from any arbitrary position. Moreover, the network maintains the context of the video while changing the object or action. For example, in <ref type="figure" target="#fig_2">Figure 5</ref>(c), the digit 5 with a certain stroke width and orientation changes to a digit 8 with the same stroke width and orientation. Similarly, in the natural dataset the type of the background and its illumination remains the same. The preservation of motion and context as well as the position is a crucial result in showcasing the ability of the network to maintain the long-term and short-term context in generating videos even when the caption is changed in the middle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>The key contribution of this work has been to condition the generation of the videos using long-term context (s l ) and short-term context (s s ). We perform an ablation study over the importance of each of these contexts, by removing them one at a time and training the network. We ensure that all the other parameters between the two networks remain the same. The results of removing long-term context are shown in <ref type="figure" target="#fig_3">Figure 6</ref> (a), where the caption is 'digit 9 moving up and down'. Compared to <ref type="figure">Figure 3(a)</ref>, there are two distinct effects that can be observed: (1) either the object characteristics (such as the shape of the digit 9) changes over time; or (2) the object starts to oscillate (instead of following the specified motion) because it has failed to capture the long-term fluid motion over the video, thus supporting the  need for s l in maintaining coherence between the frames. The results of removing short-term context are shown in <ref type="figure" target="#fig_3">Figure 6</ref> (b). Here we notice a significant deterioration in the object quality. We infer that as the frame generation takes place over a number of time-steps, it is essential that the model, while generating a frame in a given time-step, receives a strong local temporal stimulus from glimpses of the previous frame. <ref type="table">Table 1</ref> further shows a quantitative analysis of these experiments, which corroborate that the proposed approach of using a long-term and short-term context together helps learn a better model for video generation.</p><p>In order to further study the usefulness of the attention mechanism we have on the caption representation, we compared our approach with other ways to condition on the caption information. We first trained our network where we conditioned the frame generation directly on one-hot vectors created separately from the action and object in- <ref type="figure">Figure 7</ref>. Videos generated using different approaches of conditioning over test-set caption 'digit 5 is going up and down' formation present in the captions. We also performed an experiment where we conditioned on latent vectors obtained by passing the caption through a pre-trained model of skipthought vectors <ref type="bibr" target="#b8">[9]</ref>. The results of the comparison are shown in <ref type="figure">Figure 7</ref>. It can be observed that the onehot vector approach did not respond to the caption at all and generated random video samples. The approach with pre-trained skipthought vectors did match the object information but couldn't associate it with the correct motion. In comparison, our methodology is able to perform extremely well for the unseen caption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Unsupervised Video Generation</head><p>The proposed architecture also allows videos to be generated in an unsupervised manner without captions by making some modifications. In order to do this,we remove s s and s l and the frame generation is conditioned directly on   <ref type="table">Table 2</ref>. Accuracy on action classification performed with feature vectors from original and generated videos on the KTH dataset. r k−1 t and h video changing equations 11 and 13 to:</p><formula xml:id="formula_15">h enc t = LST M enc (r, r k−1 t , h video , h dec t−1 ) (17) h dec t = LST M dec (z, r k−1 t , h video , h dec t−1 )<label>(18)</label></formula><p>We show the effectiveness of our architecture to generate unsupervised videos by training our network on Two-Digit Moving MNIST dataset and UCF-101 datasets. As mentioned earlier, we used UCF-101 in this experiment to be able to compare with earlier work. (Also, UCF-101 was not used in other experiments, since it does not have captions and it's not trivial to add captions for this dataset.) Results can be seen in <ref type="figure" target="#fig_4">Figure 8</ref> along with a few generations presented by <ref type="bibr" target="#b15">[16]</ref> in their paper for comparison. We can infer that even in the unsupervised setting, our method preserves the object information throughout the video, whereas in <ref type="bibr" target="#b15">[16]</ref>, all the generated videos seem to lose the objectness towards the later part of the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Quantitative Results</head><p>Data programming: Since our network can generate caption based videos, it can even be employed to artificially create a labeled dataset <ref type="bibr" target="#b13">[14]</ref>. So, we evaluated the real and generated videos of KTH dataset for action recognition. We first trained an SVM on features extracted from the generated videos with action labels corresponding to the captions, and tested the model on the real videos. We then performed the experiment vice versa. To ensure an independent unbiased analysis, we extracted feature vectors using a 3D spatio-temporal convolutional network <ref type="bibr" target="#b23">[24]</ref> trained on Sports 1M Dataset <ref type="bibr" target="#b6">[7]</ref>. As shown in <ref type="table">Table 2</ref>, we can observe that the accuracy for the two settings is comparable. In fact, the highest is achieved on mixing the two datasets. We infer that the generated videos are able to fill up the gaps in the manifold and thus oversampling the original set of video with them can help improve the training on a supervised learning task. (We note that these models were not finetuned to achieve the best possible accuracy on the dataset, but only used for comparison against each other.) User Study: In order to assess how people perceive our generated results, we performed a user study. 10 generated and 10 real videos having the same captions for each dataset were shown to 24 subjects who were asked to rate the videos as generated or real. The results are shown in <ref type="table" target="#tab_4">Table 3</ref>. It can be clearly observed that the percentage of generated videos considered real is very close to that of real videos. This suggests that our network is able to generate new videos highly similar to the real ones. (We also performed studies on the task of future frame prediction, but are unable to include it due to space constraints. These are included in the supplemental material.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In summary, we proposed a network architecture that enables variable length semantic video generation using captions, which is the first of its kind. Through various experiments, we conclude that our approach, which combines the use of short-term and long-term spatiotemporal context, is able to effectively to generate videos on unseen captions maintaining a strong consistency between consective frames. Moreover, the network architecture is robust in transferring spatio-temporal style across frames when generating multiple frames. By learning using visual context, the network is even able to learn a robust latest representation over parts of videos, which is useful for tasks such as video prediction and action recognition. We did observe in our experiments that our network does exhibit some averaging effect when filling up the background occasionally. We infer that since the recurrent attention mechanism focuses primarily on objects, the approach may need to be extended with a mechanism to handle the background via a separate pathway. Future efforts in this direction will assist in bridging this gap as well as extending this work to address the broader challenge of learning with limited supervised data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Proposed network architecture for attentive semantic video generation with captions. Y = {Y1, Y2, · · · , Y k } denotes the video frames generated by the architecture, while X = {X1, X2, · · · , Xm} denotes the set of words in the caption.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Variable length video generation for caption 'digit 0 is going up and down'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Spatio-Temporal style transfer. First caption generates for 7 frames. Second caption continues the generation from the 8 th frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. (a) shows videos when generated without captioning on Long-term context. (b) shows videos when generated without captioning on Short-term context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Results of unsupervised video generation and comparison with results from Temporal-GAN [16].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Conv-LSTM LSTM video Y 1 ,Y 2 ,...,Y k-1</figDesc><table>BiDirectional 
LSTM lang 

X = {X 1 ,X 2 ,...,X m} 

Read 

Y k-1 

h video 

h lang 

r k-1 t 

Soft-Attention 
(Long-Term) 

Soft-Attention 
(Short-Term) 

s l 

s s t 

Y k t 

Read 

LSTM enc 

LSTM dec 

Ŷ k 

Write 

z 

μ 
σ 

+ 
× 
ε ∼ 
N(0,I) 

Conv-LSTM 
LSTM video 

Y 1 ,Y 2 ,...,Y k-1 

BiDirectional 
LSTM lang 

X = {X 1 ,X 2 ,...,X m} 

Read 

Y k-1 

h video 

h lang 

r k-1 t 

Soft-Attention 
(Long-Term) 

Soft-Attention 
(Short-Term) 

s l 

s s t 

LSTM dec 

Ŷ k 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Table 1. Quantitative comparison of loss at the end of training for different ablation experiments on captioning.</figDesc><table>Captioning Experiment 
LL 
KL 
Total Loss 
Without Captioning on 
Long-Term Context 
65.76 17.97 
83.73 

Without Captioning on 
Short-term Context 
72.53 13.56 
86.09 

With Captioning on Long-
and Short-Term Context 
63.55 11.84 
75.39 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Datasets One Digit MNIST Two Digit MNIST KTH Dataset Video Type Gen.33% 93.75% 78.53% 89.15% 75.94% 92.45%User Study Results showing the percentage of videos considered real by people.</figDesc><table>Real 
Gen. 
Real 
Gen. 
Real 
% videos 
considered real 
83.</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">All the codes, videos and other resources are available at https: //github.com/Singularity42/cap2vid</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00527</idno>
		<title level="m">Video pixel networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 2nd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generating images from captions with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Data programming: Creating large training sets, quickly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>De Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Selsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3567" to="3575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matsumoto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06624</idno>
		<title level="m">Temporal generative adversarial nets</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recognizing human actions: A local svm approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on</title>
		<meeting>the 17th International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="32" to="36" />
		</imprint>
	</monogr>
	<note>Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Information processing in dynamical systems: Foundations of harmony theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DTIC Document</title>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>CoRR abs/1609.03499</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="613" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="835" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07539</idno>
		<title level="m">Semantic image inpainting with perceptual and contextual losses</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stackgan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03242</idno>
		<title level="m">Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
