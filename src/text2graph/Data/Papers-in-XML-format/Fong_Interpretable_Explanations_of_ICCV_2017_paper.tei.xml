<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interpretable Explanations of Black Boxes by Meaningful Perturbation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><forename type="middle">C</forename><surname>Fong</surname></persName>
							<email>ruthfong@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
							<email>vedaldi@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Interpretable Explanations of Black Boxes by Meaningful Perturbation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Given the powerful but often opaque nature of modern black box predictors such as deep neural networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, there is a considerable interest in explaining and understanding predictors a-posteriori, after they have been learned. This remains largely an open problem. One reason is that we lack a formal understanding of what it means to explain a classifier. Most of the existing approaches <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19]</ref>, etc., often produce intuitive visualizations; however, since such visualizations are primarily heuristic, their meaning remains unclear.</p><p>In this paper, we revisit the concept of "explanation" at a formal level, with the goal of developing principles and methods to explain any black box function f , e.g. a neural network object classifier. Since such a function is learned automatically from data, we would like to understand what f has learned to do and how it does it. Answering the "what" question means determining the properties of the map. The "how" question investigates the internal mechanisms that allow the map to achieve these properties. We focus mainly on the "what" question and argue that it can flute: 0.9973 flute: 0.0007 Learned Mask <ref type="figure">Figure 1</ref>. An example of a mask learned (right) by blurring an image (middle) to suppress the softmax probability of its target class (left: original image; softmax scores above images).</p><p>be answered by providing interpretable rules that describe the input-output relationship captured by f . For example, one rule could be that f is rotation invariant, in the sense that "f (x) = f (x ′ ) whenever images x and x ′ are related by a rotation".</p><p>In this paper, we make several contributions. First, we propose the general framework of explanations as metapredictors (sec. 2), extending <ref type="bibr" target="#b17">[18]</ref>'s work. Second, we identify several pitfalls in designing automatic explanation systems. We show in particular that neural network artifacts are a major attractor for explanations. While artifacts are informative since they explain part of the network behavior, characterizing other properties of the network requires careful calibration of the generality and interpretability of explanations. Third, we reinterpret network saliency in our framework. We show that this provides a natural generalization of the gradient-based saliency technique of <ref type="bibr" target="#b14">[15]</ref> by integrating information over several rounds of backpropagation in order to learn an explanation. We also compare this technique to other methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19]</ref> in terms of their meaning and obtained results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Our work builds on <ref type="bibr" target="#b14">[15]</ref>'s gradient-based method, which backpropagates the gradient for a class label to the image layer. Other backpropagation methods include DeConvNet <ref type="bibr" target="#b18">[19]</ref> and Guided Backprop <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b7">8]</ref>, which builds off of DeConvNet <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b14">[15]</ref>'s gradient method to produce sharper visualizations.</p><p>Another set of techniques incorporate network activations into their visualizations: Class Activation Mapping (CAM) <ref type="bibr" target="#b21">[22]</ref> and its relaxed generalization Grad-CAM <ref type="bibr" target="#b13">[14]</ref> visualize the linear combination of a late layer's activations and class-specific weights (or gradients for <ref type="bibr" target="#b13">[14]</ref>), while Layer-Wise Relevance Propagation (LRP) <ref type="bibr" target="#b0">[1]</ref> and Excitation Backprop <ref type="bibr" target="#b19">[20]</ref> backpropagate an class-specific error signal though a network while multiplying it with each convolutional layer's activations.</p><p>With the exception of <ref type="bibr" target="#b14">[15]</ref>'s gradient method, the above techniques introduce different backpropagation heuristics, which results in aesthetically pleasing but heuristic notions of image saliency. They also are not model-agnostic, with most being limited to neural networks (all except <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b0">1]</ref>) and many requiring architectural modifications <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22]</ref> and/or access to intermediate layers <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>A few techniques examine the relationship between inputs and outputs by editing an input image and observing its effect on the output. These include greedily graying out segments of an image until it is misclassified <ref type="bibr" target="#b20">[21]</ref> and visualizing the classification score drop when an image is occluded at fixed regions <ref type="bibr" target="#b18">[19]</ref>. However, these techniques are limited by their approximate nature; we introduce a differentiable method that allows for the effect of the joint inclusion/exclusion of different image regions to be considered.</p><p>Our research also builds on the work of <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b1">2]</ref>. The idea of explanations as predictors is inspired by the work of <ref type="bibr" target="#b17">[18]</ref>, which we generalize to new types of explanations, from classification to invariance.</p><p>The Local Intepretable Model-Agnostic Explanation (LIME) framework <ref type="bibr" target="#b11">[12]</ref> is relevant to our local explanation paradigm and saliency method (sections 3.2, 4) in that both use an function's output with respect to inputs from a neighborhood around an input x 0 that are generated by perturbing the image. However, their method takes much longer to converge (N = 5000 vs. our 300 iterations) and produces a coarse heatmap defined by fixed super-pixels.</p><p>Similar to how our paradigm aims to learn an image perturbation mask that minimizes a class score, feedback networks <ref type="bibr" target="#b1">[2]</ref> learn gating masks after every ReLU in a network to maximize a class score. However, our masks are plainly interpretable as they directly edit the image while <ref type="bibr" target="#b1">[2]</ref>'s ReLU gates are not and can not be directly used as a visual explanation; furthermore, their method requires architectural modification and may yield different results for different networks, while ours is model-agnostic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Explaining black boxes with meta-learning</head><p>A black box is a map f : X → Y from an input space X to an output space Y, typically obtained from an opaque learning process. To make the discussion more concrete, consider as input color images x : Λ → R 3 where Λ = {1, . . . , H} × {1, . . . , W } is a discrete domain. The output y ∈ Y can be a boolean {−1, +1} telling whether the image contains an object of a certain type (e.g. a robin), the probability of such an event, or some other interpretation of the image content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Explanations as meta-predictors</head><p>An explanation is a rule that predicts the response of a black box f to certain inputs. For example, we can explain a behavior of a robin classifier by the rule Q 1 (x; f ) = {x ∈ X c ⇔ f (x) = +1}, where X c ⊂ X is the subset of all the robin images. Since f is imperfect, any such rule applies only approximately. We can measure the faithfulness of the explanation as its expected prediction error:</p><formula xml:id="formula_0">L 1 = E[1 − δ Q1(x;f ) ]</formula><p>, where δ Q is the indicator function of event Q. Note that Q 1 implicitly requires a distribution p(x) over possible images X . Note also that L 1 is simply the expected prediction error of the classifier. Unless we did not know that f was trained as a robin classifier, Q 1 is not very insightful, but it is interpretable since X c is.</p><p>Explanations can also make relative statements about black box outcomes. For example, a black box f , could be rotation invariant:</p><formula xml:id="formula_1">Q 2 (x, x ′ ; f ) = {x ∼ rot x ′ ⇒ f (x) = f (x ′ )}</formula><p>, where x ∼ rot x ′ means that x and x ′ are related by a rotation. Just like before, we can measure the faithfulness of this explanation as</p><formula xml:id="formula_2">L 2 = E[1−δ Q2(x,x ′ ;f ) |x ∼ x ′ ].</formula><p>1 This rule is interpretable because the relation ∼ rot is.</p><p>Learning explanations. A significant advantage of formulating explanations as meta predictors is that their faithfulness can be measured as prediction accuracy. Furthermore, machine learning algorithms can be used to discover explanations automatically, by finding explanatory rules Q that apply to a certain classifier f out of a large pool of possible rules Q.</p><p>In particular, finding the most accurate explanation Q is similar to a traditional learning problem and can be formulated computationally as a regularized empirical risk minimization such as:</p><formula xml:id="formula_3">min Q∈Q λR(Q) + 1 n n i=1 L(Q(x i ; f ), x i , f ), x i ∼ p(x). (1)</formula><p>Here, the regularizer R(Q) has two goals: to allow the explanation Q to generalize beyond the n samples x 1 , . . . , x n considered in the optimization and to pick an explanation Q which is simple and thus, hopefully, more interpretable.</p><p>Maximally informative explanations. Simplicity and interpretability are often not sufficient to find good explanations and must be paired with informativeness. Consider the following variant of rule Q 2 :  <ref type="figure">Figure 2</ref>. Comparison with other saliency methods. From left to right: original image with ground truth bounding box, learned mask subtracted from 1 (our method), gradient-based saliency <ref type="bibr" target="#b14">[15]</ref>, guided backprop <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b7">8]</ref>, contrastive excitation backprop <ref type="bibr" target="#b19">[20]</ref>, Grad-CAM <ref type="bibr" target="#b13">[14]</ref>, and occlusion <ref type="bibr" target="#b18">[19]</ref>. <ref type="figure">Figure 3</ref>. Gradient saliency maps of <ref type="bibr" target="#b14">[15]</ref>. A red bounding box highlight the object which is meant to be recognized in the image. Note the strong response in apparently non-relevant image regions.</p><formula xml:id="formula_4">Q 3 (x, x ′ ; f, θ) = {x ∼ θ x ′ ⇒ f (x) = f (x ′ )},</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stethoscope Gradient</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soup Bowl Gradient</head><p>are related by a rotation of an angle ≤ θ. Explanations for larger angles imply the ones for smaller ones, with θ = 0 being trivially satisfied. The regularizer R(Q 3 (·; θ)) = −θ can then be used to select a maximal angle and thus find an explanation that is as informative as possible. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Local explanations</head><p>A local explanation is a rule Q(x; f, x 0 ) that predicts the response of f in a neighborhood of a certain point x 0 . If f is smooth at x 0 , it is natural to construct Q by using the first-order Taylor expansion of f :</p><formula xml:id="formula_5">f (x) ≈ Q(x; f, x 0 ) = f (x 0 ) + ∇f (x 0 ), x − x 0 . (2)</formula><p>This formulation provides an interpretation of <ref type="bibr" target="#b14">[15]</ref>'s saliency maps, which visualize the gradient S 1 (x 0 ) = ∇f (x 0 ) as an indication of salient image regions. They argue that large values of the gradient identify pixels that strongly affect the network output. However, an issue is that this interpretation breaks for a linear classifier: If f (x) = w, x + b, S 1 (x 0 ) = ∇f (x 0 ) = w is independent of the image x 0 and hence cannot be interpreted as saliency.</p><p>The reason for this failure is that eq. (2) studies the variation of f for arbitrary displacements ∆ x = x − x 0 from x 0 and, for a linear classifier, the change is the same regardless of the starting point x 0 . For a non-linear black box f such as a neural network, this problem is reduced but not eliminated, and can explain why the saliency map S 1 is rather diffuse, with strong responses even where no obvious information can be found in the image ( <ref type="figure">fig. 3)</ref>.</p><p>We argue that the meaning of explanations depends in large part on the meaning of varying the input x to the black box. For example, explanations in sec. 3.1 are based on letting x vary in image category or in rotation. For saliency, one is interested in finding image regions that impact f 's output. Thus, it is natural to consider perturbations x obtained by deleting subregions of x 0 . If we model deletion by multiplying x 0 point-wise by a mask m, this amounts to studying the function f (x 0 ⊙ m) <ref type="bibr" target="#b2">3</ref> . The</p><formula xml:id="formula_6">Taylor expansion of f at m = (1, 1, . . . , 1) is S 2 (x 0 ) = df (x 0 ⊙ m)/dm| m=(1,...,1) = ∇f (x 0 ) ⊙ x 0 .</formula><p>For a linear classifier f , this results in the saliency S 2 (x 0 ) = w ⊙ x 0 , which is large for pixels for which x 0 and w are large simultaneously. We refine this idea for non-linear classifiers in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Saliency revisited 4.1. Meaningful image perturbations</head><p>In order to define an explanatory rule for a black box f (x), one must start by specifying which variations of the input x will be used to study f . The aim of saliency is to identify which regions of an image x 0 are used by the black box to produce the output value f (x 0 ). We can do so by observing how the value of f (x) changes as x is obtained "deleting" different regions R of x 0 . For example, if f (x 0 ) = +1 denotes a robin image, we expect that f (x) = +1 as well unless the choice of R deletes the robin from the image. Given that x is a perturbation of x 0 , this is a local explanation (sec. 3.2) and we expect the explanation to characterize the relationship between f and x 0 .</p><p>While conceptually simple, there are several problems with this idea. The first one is to specify what it means "delete" information. As discussed in detail in sec. 4.3, we are generally interested in simulating naturalistic or plausible imaging effect, leading to more meaningful perturbations and hence explanations. Since we do not have access to the image generation process, we consider three obvious proxies: replacing the region R with a constant value, injecting noise, and blurring the image <ref type="figure" target="#fig_1">(fig. 4)</ref>.</p><p>Formally, let m : Λ → [0, 1] be a mask, associating each pixel u ∈ Λ with a scalar value m(u). Then the perturbation operator is defined as</p><formula xml:id="formula_7">[Φ(x 0 ; m)](u) =      m(u)x 0 (u) + (1 − m(u))µ 0 , constant, m(u)x 0 (u) + (1 − m(u))η(u), noise, g σ0m(u) (v − u)x 0 (v) dv, blur,</formula><p>where µ 0 is an average color, η(u) are i.i.d. Gaussian noise samples for each pixel and σ 0 is the maximum isotropic standard deviation of the Gaussian blur kernel g σ (we use σ 0 = 10, which yields a significantly blurred image).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Deletion and preservation</head><p>Given an image x 0 , our goal is to summarize compactly the effect of deleting image regions in order to explain the behavior of the black box. One approach to this problem is to find deletion regions that are maximally informative.</p><p>In order to simplify the discussion, in the rest of the paper we consider black boxes f (x) ∈ R C that generate a vector of scores for different hypotheses about the content of the image (e.g. as a softmax probability layer in a neural network). Then, we consider a "deletion game" where the goal is to find the smallest deletion mask m that causes the score f c <ref type="figure">(Φ(x 0 ; m)</ref></p><note type="other">) ≪ f c (x 0 ) to drop significantly, where c is the target class. Finding m can be formulated as the following learning problem:</note><formula xml:id="formula_8">m * = argmin m∈[0,1] Λ λ 1 − m 1 + f c (Φ(x 0 ; m))<label>(3)</label></formula><p>where λ encourages most of the mask to be turned off (hence deleting a small subset of x 0 ). In this manner, we can find a highly informative region for the network. One can also play an symmetric "preservation game", where the goal is to find the smallest subset of the image that must be retained to preserve the score</p><formula xml:id="formula_9">f c (Φ(x 0 ; m)) ≥ f c (x 0 ): m * = argmin m λ m 1 − f c (Φ(x 0 ; m)).</formula><p>The main difference is that the deletion game removes enough evidence to prevent the network from recognizing the object in the image, whereas the preservation game finds a minimal subset of sufficient evidence.</p><p>Iterated gradients. Both optimization problems are solved by using a local search by means of gradient descent methods. In this manner, our method extracts information from the black box f by computing its gradient, similar to the approach of <ref type="bibr" target="#b14">[15]</ref>. However, it differs in that it extracts this information progressively, over several gradient evaluations, accumulating increasingly more information over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Dealing with artifacts</head><p>By committing to finding a single representative perturbation, our approach incurs the risk of triggering artifacts of the black box. Neural networks, in particular, are known to be affected by surprising artifacts <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7]</ref>; these works demonstrate that it is possible to find particular inputs that can drive the neural network to generate nonsensical or unexpected outputs. This is not entirely surprising since neural networks are trained discriminatively on natural image statistics. While not all artifacts look "unnatural", nevertheless they form a subset of images that is sampled with negligible probability when the network is operated normally.  <ref type="figure">Figure 5</ref>. From left to right: an image correctly classified with large confidence by GoogLeNet <ref type="bibr" target="#b16">[17]</ref>; a perturbed image that is not recognized correctly anymore; the deletion mask learned with artifacts. Top: A mask learned by minimizing the top five predicted classes by jointly applying the constant, random noise, and blur perturbations. Note that the mask learns to add highly structured swirls along the rim of the cup (γ = 1, λ1 = 10 −5 , λ2 = 10 −3 , β = 3). Bottom: A minimizing-top5 mask learned by applying a constant perturbation. Notice that the mask learns to introduce sharp, unnatural artifacts in the sky instead of deleting the pole (γ = 0.1, λ1 = 10 −4 , λ2 = 10 −2 , β = 3).</p><p>Although the existence and characterization of artifacts is an interesting problem per se, we wish to characterize the behavior of black boxes under normal operating conditions. Unfortunately, as illustrated in <ref type="figure">fig. 5</ref>, objectives such as eq. (3) are strongly attracted by such artifacts, and naively learn subtly-structured deletion masks that trigger them. This is particularly true for the noise and constant perturbations as they can more easily than blur create artifacts using sharp color contrasts ( <ref type="figure">fig. 5, bottom row)</ref>.</p><p>We suggests two approaches to avoid such artifacts in generating explanations. The first one is that powerful explanations should, just like any predictor, generalize as much as possible. For the deletion game, this means not relying on the details of a singly-learned mask m. Hence, we reformulate the problem to apply the mask m stochastically, up to small random jitter.</p><p>Second, we argue that masks co-adapted with network artifacts are not representative of natural perturbations. As noted before, the meaning of an explanation depends on the meaning of the changes applied to the input x; to obtain a mask more representative of natural perturbations we can encourage it to have a simple, regular structure which cannot be co-adapted to artifacts. We do so by regularizing m in total-variation (TV) norm and upsampling it from a low resolution version.</p><p>With these two modifications, eq. (3) becomes: where</p><formula xml:id="formula_10">min m∈[0,1] Λ λ 1 1 − m 1 + λ 2 u∈Λ ∇m(u) β β + E τ [f c (Φ(x 0 (· − τ ), m))],<label>(4)</label></formula><formula xml:id="formula_11">M (v) = u g σm (v/s − u)m(u)</formula><p>. is the upsampled mask and g σm is a 2D Gaussian kernel. Equation (4) can be optimized using stochastic gradient descent.</p><p>Implementation details. Unless otherwise specified, the visualizations shown were generated using Adam <ref type="bibr" target="#b2">[3]</ref> to minimize GoogLeNet's <ref type="bibr" target="#b16">[17]</ref> softmax probability of the target class by using the blur perturbation with the following parameters: learning rate γ = 0.1, N = 300 iterations, λ 1 = 10 −4 , λ 2 = 10 −2 , β = 3, upsampling a mask (28×28 for GoogLeNet) by a factor of δ = 8, blurring the upsampled mask with g σm=5 , and jittering the mask by drawing an integer from the discrete uniform distribution on [0, τ ) where τ = 4. We initialize the mask as the smallest centered circular mask that suppresses the score of the original image by 99% when compared to that of the fully perturbed image, i.e. a fully blurred image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Interpretability</head><p>An advantage of the proposed framework is that the generated visualizations are clearly interpretable. For example, the deletion game produces a minimal mask that prevents the network from recognizing the object.</p><p>When compared to other techniques ( <ref type="figure">fig. 2</ref>), this method can pinpoint the reason why a certain object is recognized without highlighting non-essential evidence. This can be noted in <ref type="figure">fig. 2</ref> for the CD player (row 7) where other visualizations also emphasize the neighboring speakers, and similarly for the cliff (row 3), the street sign (row 4), and the sunglasses (row 8). Sometimes this shows that only a part of an object is essential: the face of the Pekenese dog (row 2), the upper half of the truck (row 6), and the spoon on the chocolate sauce plate (row 1) are all found to be minimally sufficient parts.</p><p>While contrastive excitation backprop generated heatmaps that were most similar to our masks, our method introduces a quantitative criterion (i.e., maximally suppressing a target class score), and its verifiable nature (i.e., direct edits to an image), allows us to compare differing proposed saliency explanations and demonstrate that our learned masks are better on this metric. In <ref type="figure" target="#fig_2">fig. 6</ref>, row 2, we show that applying a bounded perturbation informed by our learned mask significantly suppresses the truck softmax score, whereas a boxed perturbation on the truck's back bumper, which is highlighted by contrastive excitation backprop in <ref type="figure">fig. 2</ref>, row 6, actually increases the score from 0.717 to 0.850. The principled interpretability of our method also allows us to identify instances when an algorithm may have learned the wrong association. In the case of the chocolate sauce in <ref type="figure" target="#fig_2">fig. 6</ref>, row 1, it is surprising that the spoon is highlighted by our learned mask, as one might expect the sauce-filled jar to be more salient. However, manually perturbing the image reveals that indeed the spoon is more suppressive than the jar. One explanation is that the ImageNet "chocolate sauce" images contain more spoons than jars, which appears to be true upon examining some images. More generally, our method allows us to diagnose highly-predictive yet non-intuitive and possibly misleading correlations by identified machine learning algorithms in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Deletion region representativeness</head><p>To test that our learned masks are generalizable and robust against artifacts, we simplify our masks by further blurring them and then slicing them into binary masks by thresholding the smoothed masks by α ∈ [0 : 0.05 : 0.95] ( <ref type="figure" target="#fig_3">fig. 7</ref>, top; α ∈ [0.2, 0.6] tends to cover the salient part identified by the learned mask). We then use these simplified masks to edit a set of 5,000 ImageNet images with constant, noise, and blur perturbations. Using GoogLeNet <ref type="bibr" target="#b16">[17]</ref>, we compute normalized softmax probabilities <ref type="bibr" target="#b3">4</ref> ( <ref type="figure" target="#fig_3">fig. 7</ref>, bottom). The fact that these simplified masks quickly suppress scores as α increases for all three perturbations gives confidence that the learned masks are identifying the right regions to perturb and are generalizable to a set of extracted masks and other perturbations that they were not trained on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Minimality of deletions</head><p>In this experiments we assess the ability of our method to correctly identify a minimal region that suppresses the object. Given the output saliency map, we normalize its intensities to lie in the range [0, 1], threshold it with h ∈ [0 : 0.1 : 1], and fit the tightest bounding box around the resulting heatmap. We then blur the image in the box and compute the normalized <ref type="bibr" target="#b3">4</ref> target softmax probability from  <ref type="figure">Figure 8</ref>. On average, our method generates the smallest bounding boxes that, when used to blur the original images, highly suppress their normalized softmax probabilities (standard error included).</p><formula xml:id="formula_12">4 p ′ = p − p 0 p 0 − p b ,</formula><p>GoogLeNet <ref type="bibr" target="#b16">[17]</ref> of the partially blurred image.</p><p>From these bounding boxes and normalized scores, for a given amount of score suppression, we find the smallest bounding box that achieves that amount of suppression. <ref type="figure">Figure 8</ref> shows that, on average, our method yields the smallest minimal bounding boxes when considering suppressive effects of 80%, 90%, 95%, and 99%. These results show that our method finds a small salient area that strongly impacts the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Testing hypotheses: animal part saliency</head><p>From qualitatively examining learned masks for different animal images, we noticed that faces appeared to be more salient than appendages like feet. Because we produce dense heatmaps, we can test this hypothesis. From an annotated subset of the ImageNet dataset that identifies the keypoint locations of non-occluded eyes and feet of vertebrate animals <ref type="bibr" target="#b10">[11]</ref>, we select images from classes that have at least 10 images which each contain at least one eye and foot annotation, resulting in a set of 3558 images from 76 animal classes ( <ref type="figure">fig. 9)</ref>. For every keypoint, we calculate the average heatmap intensity of a 5 × 5 window around the <ref type="figure">Figure 9</ref>. "tiger" (left two) and "bison" (right two) images with eyes and feet annotations from <ref type="bibr" target="#b10">[11]</ref>; our learned masks are overlaid. The mean average feet:eyes intensity ratio for "tigers" (N = 25) is 3.82, while that for bisons (N = 22) is 1.07.</p><p>keypoint. For all 76 classes, the mean average intensity of eyes were lower and thus more salient than that of feet (see supplementary materials for class-specific results).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Adversarial defense</head><p>Adversarial examples <ref type="bibr" target="#b4">[5]</ref> are often generated using a complementary optimization procedure to our method that learns a imperceptible pattern of noise which causes an image to be misclassified when added to it. Using our reimplementation of the highly effective one-step iterative method (ǫ = 8) <ref type="bibr" target="#b4">[5]</ref> to generate adversarial examples, our method yielded visually distinct, abnormal masks compared to those produced on natural images ( <ref type="figure" target="#fig_4">fig. 10, left)</ref>. We train an Alexnet <ref type="bibr" target="#b3">[4]</ref> classifier (learning rate λ lr = 10 −2 , weight decay λ L1 = 10 −4 , and momentum γ = 0.9) to distinguish between clean and adversarial images by using a given heatmap visualization with respect to the top predicted class on the clean and adversarial images ( <ref type="figure" target="#fig_4">fig. 10</ref>, right); our method greatly outperforms the other methods and achieves a discriminating accuracy of 93.6%.</p><p>Lastly, when our learned masks are applied back to their corresponding adversarial images, they not only minimize the adversarial label but often allow the original, predicted label from the clean image to rise back as the top predicted class. Our method recovers the original label predicted on the clean image 40.64% of time and the ground truth label 37.32% (N = 5000). Moreover, 100% of the time the original, predicted label was recovered as one of top-5 predicted labels in the "mask+adversarial" setting. To our knowledge, this is the first work that is able to recover originally predicted labels without any modification to the training set-up and/or network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Localization and pointing</head><p>Saliency methods are often assessed in terms of weaklysupervised localization and a pointing game <ref type="bibr" target="#b19">[20]</ref>, which tests how discriminative a heatmap method is by calculating the precision with which a heatmap's maximum point lies on an instance of a given object class, for more harder datasets like COCO <ref type="bibr" target="#b5">[6]</ref>. Because the deletion game is meant to discover minimal salient part and/or spurious correlation, we do not expect it to be particularly competitive on localization and pointing but tested them for completeness.</p><p>For localization, similar to <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b1">2]</ref>, we predict a bounding box for the most dominant object in each of ∼50k For each thresholding method, we search for the optimal α value on a heldout set. Localization error was calculated as the IOU with a threshold of 0.5. <ref type="table">Table 1</ref> confirms that our method performs reasonably and shows that the three thresholding techniques affect each method differently. Non-contrastive, excitation backprop <ref type="bibr" target="#b19">[20]</ref> performs best when using energy and mean thresholding; however, our method performs best with value thresholding and is competitive when using the other methods: It beats gradient <ref type="bibr" target="#b14">[15]</ref> and guided backprop <ref type="bibr" target="#b15">[16]</ref> when using energy thresholding; beats LRP <ref type="bibr" target="#b0">[1]</ref>, CAM <ref type="bibr" target="#b21">[22]</ref>, and contrastive excitation backprop <ref type="bibr" target="#b19">[20]</ref> when using mean thresholding (recall from <ref type="figure">fig. 2</ref> that the contrastive method is visually most similar to mask); and out-performs Grad-CAM <ref type="bibr" target="#b13">[14]</ref> and occlusion <ref type="bibr" target="#b18">[19]</ref> for all thresholding methods.</p><p>For pointing, table 2 shows that our method outperforms the center baseline, gradient, and guided backprop methods and beats Grad-CAM on the set of difficult images (images for which 1) the total area of the target category is less than 25% of the image and 2) there are at least two different object classes). We noticed qualitatively that our method did not produce salient heatmaps when objects were very small. This is due to L1 and TV regularization, which yield wellformed masks for easily visible objects. We test two variants of occlusion <ref type="bibr" target="#b18">[19]</ref>, blur and variable occlusion, to interrogate if 1) the blur perturbation with smoothed masks  <ref type="table">Table 1</ref>. Optimal α thresholds and error rates from the weak localization task on the ImageNet validation set using saliency heatmaps to generate bounding boxes. † Feedback error rate are taken from <ref type="bibr" target="#b1">[2]</ref>; all others (contrastive excitation BP, LRP, and CAM) are taken from <ref type="bibr" target="#b19">[20]</ref>.</p><p>§ Using <ref type="bibr" target="#b19">[20]</ref>'s code, we recalculated these errors, which are ≤ 0.4% of the originally reported rates.</p><p>‡ Minimized top5 predicted classes' softmax scores and used λ1 = 10 −3 and β = 2.0 (examples in supplementary materials).  <ref type="table">Table 2</ref>. Pointing Game <ref type="bibr" target="#b19">[20]</ref> Precision on COCO Val Subset (N ≈ 20k). § Occluded with circles (r = 35/2) softened by gσ m =10 and used to perturb with blur (σ = 10).</p><p>† Occluded with variable-sized blur circles; from the top 10% most suppressive occlusions, the one with the smallest radius is chosen and its center is used as the point.</p><p>‡ Used min. top-5 hyper-parameters (λ1 = 10 −3 , β = 2.0).</p><p>is most effective, and 2) using the smallest, highly suppressive mask is sufficient (Occ § and V-Occ in table 2 respectively). Blur occlusion outperforms all methods except contrast excitation backprop while variable while variable occlusion outperforms all except contrast excitation backprop and the other occlusion methods, suggesting that our perturbation choice of blur and principle of identifying the smallest, highly suppressive mask is sound even if our implementation struggles on this task (see supplementary materials for examples and implementation details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We propose a comprehensive, formal framework for learning explanations as meta-predictors. We also present a novel image saliency paradigm that learns where an algorithm looks by discovering which parts of an image most affect its output score when perturbed. Unlike many saliency techniques, our method explicitly edits to the image, making it interpretable and testable. We demonstrate numerous applications of our method, and contribute new insights into the fragility of neural networks and their susceptibility to artifacts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Perturbation types. Bottom: perturbation mask; top: effect of blur, constant, and noise perturbations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Interrogating suppressive effects. Left to right: original image with the learned mask overlaid; a boxed perturbation chosen out of interest (the truck's middle bounding box was chosen based on the contrastive excitation backprop heatmap from fig. 2, row 6); another boxed perturbation based on the learned mask (target softmax probabilities of for the original and perturbed images are listed above).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. (Top) Left to right: original image, learned mask, and simplified masks for sec. 5.2 (not shown: further smoothed mask). (Bottom) Swift softmax score suppression is observed when using all three perturbations with simplified binary masks (top) derived from our learned masks, thereby showing the generality of our masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. (Left) Difference between learned masks for clean (middle) and adversarial (bottom) images (28 × 28 masks shown without bilinear upsampling). (Right) Classification accuracy for discriminating between clean vs. adversarial images using heatmap visualizations (Ntrn = 4000, N val = 1000).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>where x ∼ θ x ′ means that x and x</figDesc><table>′ chocolate sauce 

orig img + gt bb 
mask 
gradient 
guided 
contrast excitation grad-CAM 
occlusion 

Pekinese 

cliff 

street sign 

Komodo dragon 

pickup 

CD player 

sunglasses 

squirrel monkey 

impala 

unicycle 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Val-α* Err (%) Ene-α* Err Mea-α* Err</figDesc><table>Grad [15] 
0.25 
46.0 
0.10 
43.9 
5.0 
41.7  § 
Guid [16, 8] 
0.05 
50.2 
0.30 
47.0 
4.5 
42.0  § 
Exc [20] 
0.15 
46.1 
0.60 
38.7 
1.5 
39.0  § 

C Exc [20] 
-
-
-
-
0.0 
57.0  † 
Feed [2] 
-
-
0.95 38.8  † 
-
-
LRP [1] 
-
-
-
-
1.0 
57.8  † 

CAM [22] 
-
-
-
-
1.0 
48.1  † 
Grad-CAM [14] 
0.30 
48.1 
0.70 
48.0 
1.0 
47.5 
Occlusion [19] 
0.30 
51.2 
0.55 
49.4 
1.0 
48.6 
Mask  ‡ 
0.10 
44.0 
0.95 
43.1 
0.5 
43.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Ctr Grad Guid Exc CExc G-CAM Occ Occ § V-Occ † Mask ‡ All 27.93 36.40 32.68 41.78 50.95 41.10 44.50 45.41 42.31 37.49 Diff 17.86 28.21 26.16 32.73 41.99 30.59 36.45 37.45 33.87 30.64</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For rotation invariance we condition on x ∼ x ′ because the probability of independently sampling rotated x and x ′ is zero, so that, without conditioning, Q 2 would be true with probability 1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Naively, strict invariance for any θ &gt; 0 implies invariance to arbitrary rotations as small rotations compose into larger ones. However, the formulation can still be used to describe rotation insensitivity (when f varies slowly with rotation), or ∼ θ 's meaning can be changed to indicate rotation w.r.t. a canonical "upright" direction for a certain object classes, etc.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">⊙ is the Hadamard or element-wise product of vectors.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for nonlinear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">130140</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Look and think twice: Capturing top-down visual attention with feedback convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2956" to="2964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<title level="m">Adversarial examples in the physical world</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Salient deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="120" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visualizing deep convolutional neural networks using natural pre-images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="255" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">I have seen enough: Transferring parts across categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Why should i trust you?: Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02391</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: The all convolutional net</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A model explanation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS Workshop on Black Box Learning and Inference</title>
		<meeting>NIPS Workshop on Black Box Learning and Inference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Topdown neural attention by excitation backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="543" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6856</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
