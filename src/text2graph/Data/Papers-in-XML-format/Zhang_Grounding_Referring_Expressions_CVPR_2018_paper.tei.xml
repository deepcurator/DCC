<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Grounding Referring Expressions in Images by Variational Context</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
							<email>hanwangzhang@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
							<email>niu@ruc.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">† Shih-Fu</forename><surname>Chang</surname></persName>
							<email>shih.fu.chang@columbia.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Grounding Referring Expressions in Images by Variational Context</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We focus on grounding (i.e., localizing or linking)   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Grounding natural language in visual data is a hallmark of AI, since it establishes a communication channel between humans, machines, and the physical world, underpinning a variety of multimodal AI tasks such as robotic navigation <ref type="bibr" target="#b37">[38]</ref>, visual Q&amp;A <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b48">49]</ref>, and visual chatbot <ref type="bibr" target="#b5">[6]</ref>. Thanks to the rapid development in deep learningbased CV and NLP, we have witnessed promising results not only in grounding nouns (e.g., object detection <ref type="bibr" target="#b29">[30]</ref>), Input Output Score = <ref type="figure">Figure 1</ref>. The proposed Variational Context model. Given an input referring expression and an image with region proposals, we localize the referent as output. We develop a grounding score function, with the variational lower-bound composed by three cue-specific multimodal modules, indicated by the description in the dashed color boxes.</p><p>but also short phrases (e.g., noun phrases <ref type="bibr" target="#b27">[28]</ref> and relations <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b35">36]</ref>). However, the more general task: grounding referring expressions <ref type="bibr" target="#b24">[25]</ref>, is still far from resolved due to the challenges in understanding of both language and scene compositions <ref type="bibr" target="#b9">[10]</ref>. As illustrated in <ref type="figure">Figure 1</ref>, given an input referring expression "largest elephant standing behind baby elephant" and an image with region proposals, a model that can only localize "elephant" is not satisfactory as there are multiple elephants. Therefore, the key for referring expression grounding is to comprehend and model the context. Here, we refer to context as the visual objects (e.g., "elephant"), attributes (e.g., "largest" and "baby"), and relationships (e.g., "behind") mentioned in the expression that help to distinguish the referent from other objects. One straightforward way of modeling the relations between the referent and context is to: 1) use external syntactic parsers to parse the expression into entities, modifiers, and relations <ref type="bibr" target="#b33">[34]</ref>, and then 2) apply visual relation detectors to localize them <ref type="bibr" target="#b46">[47]</ref>. However, this twostage approach is not practical due to the limited generalization ability of the detectors applied in the highly unrestricted language and scene compositions. To this end, re-cent approaches use multimodal embedding networks that jointly comprehend language and model the visual relations <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b10">11]</ref>. Due to the prohibitively high cost of annotating both referent and context of referring expressions in images, multiple instance learning (MIL) <ref type="bibr" target="#b6">[7]</ref> is usually adopted in them to handle the weak supervision of the unannotated context objects, by maximizing the joint likelihood of every region pair. However, for a referent, the MIL framework essentially oversimplifies the number of context configurations of N regions from O(2 N ) to O(N ). For example, to localize the "elephant" in <ref type="figure">Figure 1</ref>, we may need to consider the other three elephants all together as a multinomial subset for modeling the context such as "largest", "behind" and "baby elephant".</p><p>In this paper, we propose a novel model called Variational Context for grounding referring expressions in images. Compared to the previous MIL-based approaches <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b10">11]</ref>, our model approximates the combinatorial context configurations with weak supervision using a variational Bayesian framework <ref type="bibr" target="#b14">[15]</ref>. Intuitively, it exploits the reciprocity between referent and context, given either of which can help to localize the other. As shown in <ref type="figure">Figure 1</ref>, for each region x, we first estimate a coarse context z, which will help to refine the true localizations of the referent. This reciprocity is formulated into the variational lower-bound of the grounding likelihood p(x|L), where L is the text expression and the context is considered as a hidden variable z (cf. Section 3). Specifically, the model consists of three multimodal modules: context posterior q(z|x, L), referent posterior p(x|z, L), and context prior p z (z|L), each of which performs a grounding task (cf. Section 4.3) that aligns image regions with a cue-specific language feature; each cue dynamically encodes different subsets of words in the expression L that help the corresponding localization (cf. Section 4.2).</p><p>Thanks to the reciprocity between referent and context, our model can not only be used in the conventional supervised setting, where there is annotation for referent , but also in the challenging unsupervised setting, where there is no instance-level annotation (e.g., bounding boxes) of both referent and context. We perform extensive experiments on four benchmark referring expression datasets: RefCLEF <ref type="bibr" target="#b13">[14]</ref>, RefCOCO <ref type="bibr" target="#b44">[45]</ref>, RefCOCO+ <ref type="bibr" target="#b44">[45]</ref>, and RefCOCOg <ref type="bibr" target="#b24">[25]</ref>. Our model consistently outperforms previous methods in both supervised and unsupervised settings. We also qualitatively show that our model can ground the context in the expression to the corresponding image regions (cf. Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Grounding Referring Expressions. Grounding referring expression is also known as referring expression comprehension, whose inverse task is called referring expression generation <ref type="bibr" target="#b24">[25]</ref>. Different from grounding phrases <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28]</ref> and descriptive sentences <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32]</ref>, the key for grounding referring expression is to use the context (or pragmatics in linguistics <ref type="bibr" target="#b36">[37]</ref>) to distinguish the referent from other objects, usually of the same category <ref type="bibr" target="#b9">[10]</ref>. However, most previous works resort to use holistic context such as the entire image <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32]</ref> or visual feature difference between regions <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>. Our model is similar to the works on explicitly modeling the referent and context region pairs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26]</ref>, however, due to the lack of context annotation, they reduce the grounding task into a multiple instance learning framework <ref type="bibr" target="#b6">[7]</ref>. As we will discuss later, this framework is not a proper approximation to the original task. There are also studies on visual relation detection that detect objects and their relationships <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b47">48]</ref>. However, they are limited to a fixed-vocabulary set of relation triplets and hence are difficult to be applied in natural language grounding. Our cue-specific language feature is similar to the language modular network <ref type="bibr" target="#b10">[11]</ref> that learns to decompose a sentence into referent/context-related words, which are different from other approaches that treat the expression as a whole <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Variational Bayesian Model vs. Multiple Instance Learning. Our proposed variational context model is in a similar vein of the deep neural network based variational autoencoder (VAE) <ref type="bibr" target="#b14">[15]</ref>, which uses neural networks to approximate the posterior distribution of the hidden value q(z|x), i.e., encoder, and the conditional distribution of the observation p(x|z), i.e., decoder. VAE shows efficient and effective end-to-end optimization for the intractable logsum likelihood log z p(x, z) that is widely used in generative processes such as image synthesis <ref type="bibr" target="#b43">[44]</ref> and video frame prediction <ref type="bibr" target="#b42">[43]</ref>. Considering the unannotated context as the hidden variable z, the referring expression grounding task can also be formulated into the above log-sum marginalization (cf. Eq. <ref type="formula" target="#formula_1">(2)</ref>). The MIL framework <ref type="bibr" target="#b6">[7]</ref> is essentially a sum-log approximation of the log-sum, i.e., z log p(x, z). To see this, the max-pooling function log max z p(x, z) used in <ref type="bibr" target="#b10">[11]</ref> can be viewed as the sum-log z log p(x|z)p(z), where p(z) = 1 if z is the correct context and 0 otherwise, indicating there is only one positive instance; maximizing the noisy-or function log(1 − z (1 − p(x, z))) used in <ref type="bibr" target="#b25">[26]</ref> is equivalent to maximize z log p(x, z), assuming there is at least one positive instance. However, due to the numerical property of the log function, this sum-log approximation will unnecessarily force every (x, z) pair to explain the data <ref type="bibr" target="#b7">[8]</ref>. Instead, we use the variational Bayesian upperbound to obtain a better sum-log approximation. Note that visual attention models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b41">42]</ref> simplify the variational lower bound by assuming p(z) = q(z|x); however, we explicitly use the KL divergence KL(q(z|x)||p(z)) in the lower bound to regularize the approximate posterior q(z|x) not being too far from the prior p(z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Variational Context</head><p>In this section, we derive the variational Bayesian formulation of the proposed variational context model and the objective function for training and test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>The task of grounding a referring expression L in an image I, represented by a set of regions x ∈ X , can be viewed as a region retrieval task with the natural language query L. Formally, we maximize the log-likelihood of the conditional distribution to localize the referent region x * ∈ X :</p><formula xml:id="formula_0">x * = arg max x∈X log p(x|L),<label>(1)</label></formula><p>where we omit the image I in p(x|I, L).</p><p>As there is usually no annotation for the context, we consider it as a hidden variable z. Therefore, Eq. <ref type="formula" target="#formula_0">(1)</ref> can be rewritten as the following maximization of the loglikelihood of the conditional marginal distribution:</p><formula xml:id="formula_1">x * = arg max x∈X log z p(x, z|L).<label>(2)</label></formula><p>Note that z is NOT necessary to be one region as assumed in recent MIL approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26]</ref>, i.e., z ∈ X . For example, the contextual objects "surrounding elephants" in "a bigger elephant than the surrounding elephants" should be composed by a multinomial subset of X , resulting in an extremely large sample space that requires O(2 |X | ) search complexity. Therefore, the marginalization in Eq (2) is intractable in general.</p><p>To this end, we use the variational lower-bound <ref type="bibr" target="#b14">[15]</ref> to approximate the marginal distribution in Eq. <ref type="formula" target="#formula_1">(2)</ref> as:</p><formula xml:id="formula_2">log p(x|L) = log z p(x, z|L) ≥ Q(x, L) = E z∼q φ (z|x,L) log p θ (x|z, L) Localization − KL (q φ (z|x, L)||p ω (z|L)) Regularization ,<label>(3)</label></formula><p>where KL(·) is the Kullback-Leibler divergence, φ, θ, and ω are independent parameter sets for the respective distributions. As shown in <ref type="figure">Figure 1</ref>, the lower bound Q(x, L) offers a new perspective for exploiting the reciprocal nature of referent and context in referring expression grounding: Localization. This term calculates the localization score for x given an estimated context z, using the referent-cue of L parameterized by θ. In particular, we design a new posterior q φ (z|x, L) that approximates the true context prior p(z|x, L), which models the context z using the context-cue of L parameterized by φ. In the view of variational autoencoder <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b34">35]</ref>, this term works in an encoding-decoding fashion: q φ is the encoder from x to z, and p θ is the decoder from z to x. Regularization. As KL is non-negative, maximizing Q(x, L) would encourage that the posterior q φ is similar to the prior p ω , i.e., the estimated context z sampled from q φ (z|x, L) should not be too far from the referring expression, which is modeled by p ω (z|L) with the generic-cue of L parameterized by ω. This term is necessary as the estimated z could be overfitted to region features that are inconsistent with the visual context described in the expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training and Test</head><p>Deterministic Context. The lower-bound Q(x, L) transforms the intractable log-sum in Eq. <ref type="formula" target="#formula_1">(2)</ref> into the efficient sum-log in Eq. <ref type="formula" target="#formula_2">(3)</ref>, which can be optimized by using Monte Carlo unbiased gradient estimator such as RE-INFORCE <ref type="bibr" target="#b39">[40]</ref>. However, due to that φ is dependent on the sampling of z over O(2 |X | ) configurations, its gradient variance is large. To this end, we implement q φ (z|x, L) as a differentiable but biased encoder:</p><formula xml:id="formula_3">z = f (x, L) = x ′ ∈X x ′ · q φ (x ′ |x, L),<label>(4)</label></formula><p>where we slightly abuse q φ as a score function such that</p><formula xml:id="formula_4">x ′ q φ (x ′ |x, L) = 1.</formula><p>Note that this deterministic context can be viewed as applying the "re-parameterization" trick as in Variational Auto-Encoder <ref type="bibr" target="#b14">[15]</ref>:</p><formula xml:id="formula_5">rewriting z ∼ q φ (z|x, L) to z = f (x, L; ǫ), ǫ ∼ p(ǫ)</formula><p>, where the stochasticity of the auxiliary random variable ǫ comes from training samples x ∈ X (ǫ). A clear example is Adversarial Autoencoder <ref type="bibr" target="#b23">[24]</ref> which shows that such stochasticity achieves similar testlikelihood compared to other distributions such as Gaussian.</p><p>Objective Function. Applying Eq. <ref type="formula" target="#formula_3">(4)</ref> to Eq. (3), we can rewrite Q(x, L) into a function of only one sample estimation, which is a common practice in SGD:</p><formula xml:id="formula_6">Q(x,L) = log p θ (x|z,L)−log q φ (z|x,L)+log p ω (z|L). (5)</formula><p>In supervised setting where the ground truth of the referent is known, to distinguish the referent from other objects, we need to train a model that outputs a high p(x|L) (i.e., Q(x, L)), while maintaining a low p(x ′ |L) (i.e., Q(x ′ , L)), whenever x ′ = x. Therefore, we use the so-called Maximum Mutual Information loss as in <ref type="bibr" target="#b24">[25</ref></p><formula xml:id="formula_7">] − log{Q(x, L)/ x ′ Q(x ′ , L)},</formula><p>where we do not need to explicitly model the distributions with normalizations; we use the following score function:</p><formula xml:id="formula_8">Q(x, L) ∝ S(x, L) = s θ (x, L)−s φ (x, L)+s ω (x, L), (6)</formula><p>where z is omitted as it is a function of x in Eq. (4). s θ , s φ , and s ω are the score functions (e.g., p θ ∝ s θ ) for p θ , q φ , and p ω , respectively. These functions will be detailed in  <ref type="figure" target="#fig_2">3)</ref>. It can be trained in an end-to-end fashion with the input of a set of image regions and a referring expression, using the supervised loss ( Eq. <ref type="formula" target="#formula_9">(7)</ref>) or the unsupervised loss (Eq. <ref type="formula" target="#formula_10">(8)</ref>). fc: fully-connected layer. concat: vector concatenation. L2Norm: L2 normalization layer. ⊙: element-wise vector multiplication. ⊕: add. Section 4.3. In this way, maximizing Eq. <ref type="formula">(5)</ref> is equivalent to minimizing the following softmax loss:</p><formula xml:id="formula_9">L s = − log softmax S(x gt , L),<label>(7)</label></formula><p>where the softmax is over x ∈ X and x gt is the ground truth referent region. Note that the reciprocity between referent and context can be extended to unsupervised learning, where neither of the referent and context has annotation. In this setting, we adopt the image-level max-pooled MIL loss functions for unsupervised referring expression grounding:</p><formula xml:id="formula_10">L u = − max x∈X log softmax S(x, L),<label>(8)</label></formula><p>where the softmax is over x ∈ X . Note that the max-pooled MIL function is reasonable since there is only one ground truth referent given an expression and image training pair. At test stage, in both supervised and unsupervised settings, we predict the referent region x * by selecting the region x ∈ X with the highest score:</p><formula xml:id="formula_11">x * = arg max x∈X S(x, L),<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Model Architecture</head><p>The overall architecture of the proposed variational context model is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Thanks to the deterministic context in Eq. (4), the five modules in our model can be integrated into an end-to-end differentiable fashion. Next, we will detail the implementation of each module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">RoI Features</head><p>Given an image with a set of Region of Interests (RoIs) X , obtained by any off-the-shelf proposal generator <ref type="bibr" target="#b49">[50]</ref> or object detectors <ref type="bibr" target="#b19">[20]</ref>, this module extracts the feature vector x i for every RoI. In particular, x i is the concatenation of visual feature v i and spatial feature p i . For v i , we can use the output of a pre-trained convolutional network (cf. Section 5). If the object category of each RoI is available, we can further utilize the comparison between the referent and other objects to capture the visual difference such as "the largest/baby elephant". Specifically, we append the visual difference feature <ref type="bibr" target="#b44">[45]</ref> </p><note type="other">δv i = 1 n j =i vi−vj ||vi−vj || to the original v i visual feature, where n is the number of objects chosen for comparison (e.g., the number of RoI in the same object category). For spatial feature, we use the 5-d spatial attributes p</note><formula xml:id="formula_12">i = [ x tl W , y tl H , x br W , y br H , w·h W ·H ]</formula><p>, where x and y are the coordinates the top left (tl) and bottom right (br) RoI of the size w × h, and the image is of the size W × H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Cue-Specific Language Features</head><p>The cue-specific language feature representation for a referring expression is inspired by the attention weighted sum of word vectors <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b2">3]</ref>, where the weights are parameterized by context-cue φ, referent-cue θ, and generic-cue ω. The context-cue language feature y c = [y c1 , y c2 ] is a concatenation of y c1 for language-vision association between single RoI and the expression, and y c2 for the association between pairwise RoIs; the referent-cue language feature y r can be represented in a similar way to y c ; the generic-cue language feature y g is only for single RoI association as it is an independent prior. The weights of each cue are calculated from the hidden state vectors of a 2-layer bidirectional LSTM (BLSTM) <ref type="bibr" target="#b32">[33]</ref>, scanning through the expression. The hidden states encode forward and backward compositional semantic meanings of the sentences, beneficial for selecting words that are useful for single and pairwise associations. Specifically, suppose h j as the 4,000-d concatenation of forward and backward hidden vectors of the j-th word, without loss of generality, the word attention weight α j and the language feature y for single/pairwise association of any cue can be calculated as:</p><formula xml:id="formula_13">m j = fc(h j ), α j = softmax j (m j ), y = j α j w j , (10)</formula><p>where w j is a 300-d vector. Note that the BLSTM module can be jointly trained with the entire model. <ref type="figure" target="#fig_2">Figure 3</ref> shows that the cue-specific language features dynamically weight words in different expressions. We can have two interesting observations. First, c1 is almost uni- form while c2 is highly skewed; although r2 is more skewed than c1, it is still less skewed than r1. This is reasonable since: 1) without ground-truth, individual score (c1) does not help much for context estimation from scratch; context is more easily found by the pairwise score (c2) induced by relationships or other objects (e.g., "left" or "frisbee"); 2) in referent grounding with ground truth, individual score (r1) is sufficient (e.g., "dog lying" and "black white dog") and pairwise score (r2) is helpful; 3) g is adaptive to the number of object categories in the expression, i.e., if the context object is of the same category as the referent, g weighs descriptive or relationship words higher (e.g., "lying, standing, left"), and nouns higher (e.g., "frisbee"), otherwise; moreover, it demonstrates that the deterministic guess of z in Eq. <ref type="formula" target="#formula_3">(4)</ref> is meaningful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Score Functions</head><p>For any image and expression pair, given the RoI feature x i , and the cue-specific language feature y c , y r , and y g , we implement the final grounding score in Eq. (6) as:</p><formula xml:id="formula_14">z i = j softmax j (s φ (x i , x j , y c )) x j , s θ (x, L) ← s θ (x i , z i , y r ), s φ (x, L) ← s φ (x i , z i , y c ), s ω (x, L) ← s ω (z i , y g ),<label>(11)</label></formula><p>where the right-hand side functions are defined as below. Context Estimation Score: s φ (x i , x j , y c ). It is a score function for modeling the context posterior q φ (z|x, L), i.e., given an RoI x i as the candidate referent, we calculate the likelihood of any RoI x j to be the context. We can also use this function to estimate the final context posterior score s φ (x i , z i , y c ). Specifically, the context estimation score is a sum of the single and pairwise vision-language association scores: x j and y c1 , [x i , x j ] and y c2 . Each associate score is an fc output from the input of a normalized feature:</p><formula xml:id="formula_15">m 1 j = y c1 ⊙ fc(x j ), m 2 j = y c2 ⊙ fc([x i , x j ]), m 1 j = L2Norm(m 1 j ), m 2 j = L2Norm(m 2 j ), s φ (x i , x j , y c ) = fc( m 1 j ) + fc( m 2 j ),<label>(12)</label></formula><p>where the element-wise multiplication ⊙ is an effective way for multimodal features <ref type="bibr" target="#b1">[2]</ref>. According to Eq. (4), we can obtain the estimated context z as z i = j β j x j , where β j = softmax j (s φ (x i , x j , y c )). Referent Grounding Score: s θ (x i , z i , y r ). After obtaining the context feature z i , we can use this score function to calculate how likely a candidate RoI x i is the referent given the context z i . This function is similar to Eq. <ref type="bibr" target="#b11">(12)</ref>.</p><p>Context Regularization Score:</p><formula xml:id="formula_16">s ω (z i , y g ) − s φ (x i , z i , y c ).</formula><p>As discussed in Eq. <ref type="formula">(6)</ref>, this function scores how likely the estimated context feature z i is consistent with the content mentioned in the expression. In particular, s ω (z i , y g ) is only dependent on single RoI:</p><formula xml:id="formula_17">m i =y g i ⊙ fc(z i ), m i =L2Norm(m i ), s ω (z i , y g i )=fc(m i ).<label>(13)</label></formula><p>5. Experiment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>We used four popular benchmarks for the referring expression grounding task.</p><p>RefCOCO <ref type="bibr" target="#b44">[45]</ref>. It has 142,210 referring expressions for 50,000 referents (e.g., object instances) in 19,994 images from MSCOCO <ref type="bibr" target="#b17">[18]</ref>. The expressions are collected in an interactive way <ref type="bibr" target="#b13">[14]</ref>. The dataset is split into train, validation, Test A, and Test B, which has 120,624, 10,834, 5,657 and 5,095 expression-referent pairs, respectively. An image contains multiple people in Test A and multiple objects in Test B.</p><p>RefCOCO+ <ref type="bibr" target="#b44">[45]</ref>. It has 141,564 expressions for 49,856 referents in 19,992 images from MSCOCO. The difference from RefCOCO is that it only allows appearances but no locations to describe the referents. The split is 120,191, 10,758, 5,726 and 4,889 expression-referent pairs for train, validation, Test A, and Test B respectively.</p><p>RefCOCOg <ref type="bibr" target="#b24">[25]</ref>. It has 95,010 referring expressions for 49,822 objects in 25,799 images from MSCOCO. Different from RefCOCO and RefCOCO+, this dataset not collected in an interactive way and contains longer sentences containing both appearance and location expressions. The split is 85,474 and 9,536 expression-referent pairs for training and validation. Note that there is no open test split for RefCOCOg, so we used the hyper-parameters cross-validated on RefCOCO and RefCOCO+.</p><p>RefCLEF <ref type="bibr" target="#b13">[14]</ref>. It contains 20,000 images with annotated image regions. It has some ambiguous (e.g. anywhere) phrases and mistakenly annotated image regions that are not described in the expressions. For fair comparison, we used the split released by <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32]</ref>, i.e., 58,838, 6,333 and 65,193 expression-referent pairs for training, validation and test, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Settings and Metrics</head><p>We used an English vocabulary of 72,704 words contained in the GloVe pre-trained word vectors <ref type="bibr" target="#b26">[27]</ref>, which <ref type="table">Table 1</ref>. Supervised grounding performances (Acc%) of comparing methods on RefCOCO, RefCOCO+, and RefCOCOg. Note that <ref type="bibr" target="#b45">[46]</ref> reports slightly higher accuracies using ensemble models of Listener and Speaker. For fair comparison, we only report their single models.  We only display top 3 context objects with the context ground probability &gt; 0.1. We can observe that VC has more reasonable context localizations than CMN, even in cases when the referent ground of VC fails.</p><p>was also used for the initialization of our word vectors. We used a "unk" symbol for the input word of the BLSTM if the word is out of the vocabulary; we set the sentence length to 20 and used "pad" symbol to pad expression sentence &lt; 20. For RoI visual features on RefCOCO, RefCOCO+, and RefCOCOg which have MSCOCO annotated regions with object categories, we used the concatenation of the 4,096-d fc7 output of a VGG-16 based Faster-RCNN network <ref type="bibr" target="#b30">[31]</ref> trained on MSCOCO and its corresponding 4,096-d visdiff feature <ref type="bibr" target="#b44">[45]</ref>; although RefCLEF regions also have object categories, for fair comparison with <ref type="bibr" target="#b31">[32]</ref>, we did not use the visdiff feature.</p><p>The model training is single-image based, with all referring expressions annotated. We applied SGD of 0.95-momentum with initial learning rate of 0.01, multiplied by 0.1 after every 120,000 iterations, up to 160,000 iterations. Parameters in BILSTM and fc-layers were initialized by Xavier <ref type="bibr" target="#b8">[9]</ref> with 0.0005 weight decay. Other settings were default in TensorFlow. Note that our model is trained without bells and whistles, therefore, other optimization tricks such as batch normalization <ref type="bibr" target="#b12">[13]</ref> and GRU <ref type="bibr" target="#b3">[4]</ref> are expected to further improve the results reported here. Besides the ground truth annotations, grounding to automatically detected objects is a more practical setting. Therefore, we also evaluated with the SSD-detected bounding boxes <ref type="bibr" target="#b19">[20]</ref> on the four datasets provided by <ref type="bibr" target="#b45">[46]</ref>. A grounding is considered as correct if the intersection-over-union (IoU) of the top-1 scored region and the ground-truth object is larger than 0.5. <ref type="figure">The grounding accuracy (a.k.a, P@1)</ref> is the fraction of correctly grounded test expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluations of Supervised Grounding</head><p>We compared our variational context model (VC) with state-of-the-art referring expression methods published in recent years, which can be categorized into: 1) generationcomprehension based such as MMI <ref type="bibr" target="#b24">[25]</ref>, Attr <ref type="bibr" target="#b18">[19]</ref>, Speaker <ref type="bibr" target="#b45">[46]</ref>, Listener <ref type="bibr" target="#b45">[46]</ref>, and SCRC <ref type="bibr" target="#b11">[12]</ref>; 2) localization based such as GroundR <ref type="bibr" target="#b31">[32]</ref>, NegBag <ref type="bibr" target="#b25">[26]</ref>, CMN <ref type="bibr" target="#b10">[11]</ref>. Note that NegBag and CMN are MIL-based models. In particular, we used the author-released code to obtain the results of CMN on RefCLEF, RefCOCO, and RefCOCO+.</p><p>From the results on RefCOCO, RefCOCO+, and RefCOCOg in <ref type="table">Table 1</ref> and that on RefCLEF in <ref type="table" target="#tab_1">Table 2</ref>, we can see that VC achieves the state-of-the-art performance. We believe that the improvement is attributed to the variational Bayesian modeling of context. First, on all datasets, except for the most recent reinforcement learning based <ref type="bibr" target="#b45">[46]</ref>, VC outperforms all the other sentence generation-comprehension methods that do not model context. Second, compared to VC without the regularization term in Eq. (3) (VC w/o reg), VC can boost the performance by around 2% on all datasets. This demonstrates the effectiveness of the KL divergence for the prevention of the overfitted context estimation.</p><p>In particular, we further demonstrate the superiority of VC over the most recent MIL-based method CMN. As illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>, VC has better context comprehension in both of the language and image regions than CMN. For example, in the top two rows where VC is correct and CMN is wrong, for the grounding in the second column, CMN unnecessarily considers the "girl" as context but the expression only describes using "elephant"; in the last column, CMN misses the key context "frisbee". Even in the failure cases where VC is wrong and CMN is correct, VC still localizes reasonable context. For example, in the fourth column, although CMN grounds the correct TV, but it is based on incorrect context of other TVs; while VC can predict the correct context "children". In addition, we observed that most of the cases that CMN is better than VC involves multiple humans. This demonstrates that VC is better at grounding objects of different categories.</p><p>VC is also effective in images with more objects. <ref type="figure" target="#fig_4">Figure 5</ref> shows the performances of VC and CMN with various </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Evaluations of Unsupervised Grounding</head><p>We follow the unsupervised setting in GroundR <ref type="bibr" target="#b31">[32]</ref>. To our best knowledge, it is the only work on unsupervised referring expression grounding. Note that it is also known as "weakly supervised" detection <ref type="bibr" target="#b47">[48]</ref> as there is still imagelevel ground truth (i.e., the referring expression). <ref type="table" target="#tab_1">Table 2</ref> reports the unsupervised results on the RefCLEF. We can see that VC outperforms the state-of-the-art GroundR, which is a generation-comprehension based method. This demonstrates that using context also helps unsupervised grounding. As there is no published unsupervised results on RefCOCO, RefCOCO+, and RefCOCOg, we only compared our baselines on them in <ref type="table">Table 3</ref>. We can have the following three key observations which highlight the challenges of unsupervised grounding:</p><p>Context Prior. VC w/o reg is the baseline without the KL divergence as a context regularization in Eq. (3). We can see that in most of the cases, VC considerably outperforms VC w/o reg by over 2%, even over 5% on RefCOCO+ (det)  and RefCOCOg (det). Note that this improvement is significantly higher than that in supervised setting (e.g., &lt; 3% as reported in <ref type="table">Table 1</ref>). The reason is that the context estimation in Eq. (4) would be easier to be stuck in image regions that are irrelevant to the expression in unsupervised setting, therefore, context prior is necessary.</p><p>Language Feature. Except on RefCOCOg, we consistently observed the ineffectiveness of the cue-specific language feature in unsupervised setting, i.e., VC w/o α outperforms VC in <ref type="table" target="#tab_1">Table 2</ref> and 3. Here α represents the cuespecific word attention. This is contrary to the observation in the supervised setting as listed in <ref type="table">Table 1</ref>, where VC w/o α is consistently lower than VC. Note that without the cuespecific word attention α in Eq. (10), the language feature is merely the average value of the word embedding vectors in the expression. In this way, VC w/o α does not encode any structural language composition as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, thus, it is better for short expressions. However, when the expression is long in RefCOCOg, discarding the language structure still degrades the performance on RefCOCOg.</p><p>Unsupervised Relation Discovery. Although we demonstrated that VC improves the unsupervised grounding by modeling context, we believe that there is still a large space for improving the quality of modeling the context. As the failure examples shown in <ref type="figure">Figure (</ref>6), 1) many context estimations are still out of the scope of the expression, e.g., we may localize the "cup" and "table" as context even though the expression is "woman with green t-shirt"; 2) we c2 r1 g Supervised Unsupervised <ref type="figure">Figure 7</ref>. Word cloud visualizations of cue-specific word attention α in Eq. 10 of context-cue (c2), referent-cue (r1), and generic-cue (g) using supervised (top row) and unsupervised training (bottom row) on RefCOCOg. Without supervision, it is difficult to discover meaningful language compositions.</p><p>may mistake due to the wrong comprehension of the relations, e.g., "right" as "left", even if the objects belong to the same category, e.g., "elephant". For further investigation, <ref type="figure">Figure 7</ref> visualizes the cue-specific word attentions in supervised and unsupervised settings. The almost identical word attentions in unsupervised setting reflect the fact that the relation modeling between referent and context is not as successful as in supervised setting. This inspires us to exploit stronger prior knowledge such as language structure <ref type="bibr" target="#b40">[41]</ref> and spatial configurations <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b38">39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We focused on the task of grounding referring expressions in images and discussed that the key problem is how to model the complex context, which is not effectively resolved by the multiple instance learning framework used in prior works. Towards this challenge, we introduced the Variational Context model, where the variational lowerbound can be interpreted by the reciprocity between the referent and context: given any of which can help to localize the other, and hence is expected to significantly reduce the context complexity in a principled way. We implemented the model using cue-specific language-vision embedding network that can be efficiently trained end-to-end. We validated the effectiveness of this reciprocity by promising supervised and unsupervised experiments on four benchmarks. Moving forward, we are going to 1) incorporate expression language generation in the variational framework, 2) use more structural features of language rather than word attentions, and 3) further investigate the potential of our model in the unsupervised referring expression grounding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The architecture of the proposed Variational Context model. It consists of a region feature extraction module (Section 4.1, and a language feature extraction module (Section 4.2), and three grounding modules (Section 4.3). It can be trained in an end-to-end fashion with the input of a set of image regions and a referring expression, using the supervised loss ( Eq. (7)) or the unsupervised loss (Eq. (8)). fc: fully-connected layer. concat: vector concatenation. L2Norm: L2 normalization layer. ⊙: element-wise vector multiplication. ⊕: add.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Two qualitative examples of the cue-specific language feature word weights. Darker color indicates higher weights. c/r+1/2: context/referent-cue + single/pairwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Qualitative results on RefCOCOg (det) showing comparisons between correct (green tick) and wrong referent grounds (red cross) by VC and CMN. The denotations of the bounding box colors are as follows. Solid red: referent ground; solid green: ground truth; dashed yellow: context ground. We only display top 3 context objects with the context ground probability &gt; 0.1. We can observe that VC has more reasonable context localizations than CMN, even in cases when the referent ground of VC fails.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Performances of VC and CMN with different number of object bounding boxes on RefCOCO Test A &amp;B, RefCOCO+ Test A &amp; B, and RefCOCOg Val. Compared to CMN, we can see that VC is more effective in context modeling when the number of objects is large.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Common failure cases in unsupervised grounding with detected bounding boxes. From left to right: RefCOCO, Ref-COCO+, and RefCOCOg. The failure is mainly to the challenging unsupervised relation modeling between referent and context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Performances (Acc%) of supervised and unsupervised methods on RefCLEF.number of bounding boxes. We can observe that VC con- siderably outperforms CMN over all bounding boxes num- bers. Recall that context is the key to distinguish objects of the same category. In particular, on the Test A sets of Ref- COCO and RefCOCO+ where the grounding is only about people, i.e., the same object category, the gap between VC and CMN is becoming larger as the box number increases. This demonstrates that MIL is ineffective in modeling con- text, especially when the number of image regions is large.</figDesc><table>Sup. Sup. (det) Unsup. (det) 

SCRC [12] 
72.74 
17.93 
-

GroundR [32] 
-
26.93 
10.70 

CMN [11] 
81.52 
28.33 
-

VC 
82.43 
31.13 
14.11 

VC w/o α 
79.60 
27.40 
14.50 

Table 3. Unsupervised grounding performances (Acc%) of com-
paring methods on RefCOCO, RefCOCO+, and RefCOCOg. 

Dataset 
Split 
VC w/o reg 
VC 
VC w/o α 

RefCOCO 
Test A 
13.59 
17.34 
33.29 

Test B 
21.65 
20.98 
30.13 

RefCOCO+ 
Test A 
18.79 
23.24 
34.60 

Test B 
24.14 
24.91 
31.58 

RefCOCOg 
Val 
25.14 
33.79 
30.26 

RefCOCO(det) 
Test A 
17.14 
20.91 
32.68 

Test B 
22.30 
21.77 
27.22 

RefCOCO+(det) 
Test A 
19.74 
25.79 
34.68 

Test B 
24.05 
25.54 
28.10 

RefCOCOg(det) 
Val 
28.14 
33.66 
29.65 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detecting visual relationships with deep relational networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning cooperative visual dialog agents with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A tutorial on variational bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence review</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAIS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A game-theoretic approach to generating spatial descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Golland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modeling relationships in referential expressions with compositional modular networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Natural language object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual question generation as dual task of visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vip-cnn: Visual phrase guided convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Referring expression generation and comprehension via attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Comprehension-guided referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling context between objects for referring expression understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Phrase localization and visual relationship detection with comprehensive linguistic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting regionto-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Grounding of textual phrases in images by reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TSP</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generating semantically precise scene graphs from textual descriptions for improved image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Vision and Language</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A domain based approach to social relation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Meaning in interaction: An introduction to pragmatics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Routledge</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Guiding interaction behaviors for multi-modal grounded language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sinapov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Language Grounding for Robotics</title>
		<meeting>the First Workshop on Language Grounding for Robotics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="20" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ming-Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Weakly-supervised visual grounding of phrases with linguistic structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attribute2image: Conditional image generation from visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A joint speaker-listenerreinforcer model for referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ppr-fcn: Weakly supervised visual relation detection via parallel pairwise r-fcn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Video question answering via hierarchical spatio-temporal attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
