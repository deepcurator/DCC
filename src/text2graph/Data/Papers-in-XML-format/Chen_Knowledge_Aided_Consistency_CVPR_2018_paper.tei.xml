<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge Aided Consistency for Weakly Supervised Phrase Grounding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ram Nevatia Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ram Nevatia Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge Aided Consistency for Weakly Supervised Phrase Grounding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Given an image and a natural language query, phrase grounding aims to localize objects mentioned by the query. It is a fundamental building block for many high-level computer vision tasks such as image retrieval <ref type="bibr" target="#b2">[3]</ref>, image QA <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> and video QA <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Traditionally, training a good phrase grounding system requires large amounts of manual annotations indicating the mapping between input queries and mentioned objects in images; these are timeconsuming to acquire and suffer from potential human errors. This motivates us to address the problem of training a grounding system by weakly supervised training data where objects of interest are mentioned in language queries but are not delineated in images.</p><p>Phrase grounding is difficult as both visual and language modalities are ambiguous and we need to reason about both to find their correspondences. To address this problem, typically a proposal generation system is applied to the input image to produce a set of candidate regions (i.e., proposals). Phrase grounding task is then treated as a retrieval problem to search the most query-related proposals. Based on this, attention mechanisms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref> are learned to adaptively attend to mentioned objects for input queries. Training a phrase grounding system with weakly supervised data brings additional challenge as no direct mappings between the two modalities are provided. Consider <ref type="figure" target="#fig_0">Fig. 1(c)</ref> where we encode the query as an embedding vector and extract visual features for a set of object proposals from the image. To find correct mappings between the query and the proposals, <ref type="bibr" target="#b33">[34]</ref> proposes to associate the query with successive proposals; once a proposal is selected, a phrase is reconstructed from it and evaluated for language consistency with the input query. <ref type="bibr" target="#b36">[37]</ref> adopts continuous attention maps and explores to reconstruct the structure of input query as well as its context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidate Proposals Location Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Reconstruction</head><p>We introduce two new concepts to overcome challenges of weakly supervised training. First is that pre-trained, fixed category detectors can provide useful knowledge in selecting the proposals that should be attended to. Second is that the detector knowledge enables us to evaluate visual consistency, in addition to language consistency. This knowledge also helps improve language consistency analysis.</p><p>We observe that if a pre-trained Convolutional Neural Network (CNN) (e.g., VGG <ref type="bibr" target="#b34">[35]</ref>) is applied to extract visual features for proposals, it can also naturally produce a probability distribution of the categories of the proposals, as this is the task that the network was trained on (e.g. MSCOCO <ref type="bibr" target="#b24">[25]</ref> classification). This free distribution can be treated as complementary external knowledge to filter out, or downweight, proposals that are unrelated to the query. For example, in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>, given a query "a man playing football", a pre-trained VGG network can provide useful hints for candidate proposals by predicting whether a proposal corresponds to a high probability "people" detection.</p><p>Use of external knowledge in language consistency is straight-forward; features for reconstruction can be modified by the detection probabilities. Task of evaluating visual consistency is more difficult; a direct analogy to language consistency would be to convert visual proposal to words and reconstruct image patches. Instead, we propose to predict object locations from query and visual features to match the goal of phrase grounding. This process would be not possible without the aid of external knowledge that helps focus on the possible related proposals for prediction.</p><p>In implementation, we construct a novel Knowledge Aided Consistency Network (KAC Net) which consists of two branches: a visual consistency branch and a language consistency branch. These two branches are joined by a shared multimodal subspace where the attention model is applied. To leverage complementary knowledge from visual feature extractor, we propose a novel Knowledge Based Pooling (KBP) gate to focus on query-related proposals for visual and language reconstruction.</p><p>We evaluate KAC Net on two grounding datasets: Flickr30K Entities <ref type="bibr" target="#b31">[32]</ref> and Referit Game <ref type="bibr" target="#b22">[23]</ref>. Flickr30K Entities contains more than 30K images and 170K query phrases, while Referit Game has 19K images referred by 130K query phrases. We ignore bounding box annotations during training in weakly supervised scenario. Experiments show KAC Net outperforms state-of-the-art methods by a large margin on both two datasets, with more than 9% increase on Flickr30K Entities and 5% increase on Referit Game in accuracy.</p><p>Our contributions are twofold: First, we leverage complementary knowledge to filter out unrelated proposals and provide direct guidance. Second, we propose a visual consistency to boost grounding performance. In the following paper, we first discuss related work in Sec. <ref type="bibr" target="#b1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Phrase grounding requires learning similarity between visual and language modalities. Karpathy et al. <ref type="bibr" target="#b21">[22]</ref> first align sentence fragments and image regions in a subspace, and later apply a bi-directional RNN for multimodal alignment in <ref type="bibr" target="#b0">[1]</ref>. Hu et al. <ref type="bibr" target="#b20">[21]</ref> employ a 2-layer LSTM to rank proposals based on encoded query and visual features. Rohrbach et al. <ref type="bibr" target="#b33">[34]</ref> employ a latent attention network conditioned on query which ranks proposals in weakly supervised scenario. Recently, Plummer et al. <ref type="bibr" target="#b31">[32]</ref> augment the CCA model <ref type="bibr" target="#b30">[31]</ref> to leverage extensive linguistic cues in the phrases. Chen et al. <ref type="bibr" target="#b3">[4]</ref> introduce regression mechanism in phrase grounding to improve proposals' quality. Xiao et al. <ref type="bibr" target="#b36">[37]</ref> leverage query's language structural information to guide the learning of phrase grounding model in weakly supervised scenario. Chen et al. <ref type="bibr" target="#b4">[5]</ref> apply reinforcement learning techniques to leverage context information. In this paper, we explore consistency in visual and language modalities and leverage complementary knowledge to further boost performance of weakly supervised grounding.</p><p>Weakly supervised learning is a method aims at learning a model without heavy manual labeling work. It is widely used in different computer vision tasks. Crandall et al. <ref type="bibr" target="#b6">[7]</ref> leverage the class labeling to learn a part-based spatial model without detailed annotation of object location and spatial relationship. Maxime et al. <ref type="bibr" target="#b28">[29]</ref> propose to learn the interaction between human and objects purely from action labeling for still images. Recently, Prest et al. <ref type="bibr" target="#b32">[33]</ref> apply a deep convolutional neural network and its score maps to address object localization with image level class labels. For phrase grounding task, Rohrbach et al. <ref type="bibr" target="#b33">[34]</ref> propose to adopt an attention model which is optimized by learning to reconstruct query's information, and avoids human labeling for object locations for each query in the training set. Based on this, Xiao et al. <ref type="bibr" target="#b36">[37]</ref> leverage a continuous attention map and explore detailed structural reconstruction of language modality. Inspired by the success of weakly supervised learning, we propose to apply another visual consistency to further boost performance.</p><p>Knowledge transfer is a technique widely used for tasks in different domains. Hinton et al. <ref type="bibr" target="#b18">[19]</ref> propose to compress knowledge learned from one model into another one which is too computationally expensive to train. Inspired by this, Aytar et al. <ref type="bibr" target="#b1">[2]</ref> apply visual knowledge to train a sound classification network. Owens et al. <ref type="bibr" target="#b29">[30]</ref> use ambient sound information to train an object detection network. Lin et al. <ref type="bibr" target="#b25">[26]</ref> leverage knowledge learned in Visual Question Answering (VQA) task in image retrieval. Zhang et al. <ref type="bibr" target="#b37">[38]</ref> apply knowledge learned in image captioning and VQA to train a network detecting visual relation in images. For phrase grounding, we propose to leverage knowledge learned from pre-trained deep neural network to filter out unrelated proposals for visual consistency. Visual consistency branch aims at predicting and aligning query-related proposals' location parameters conditioned on the input query. Language consistency branch attempts to reconstruct input query from query-related proposals. To provide guidance in training and testing, a Knowledge Based Pooling (KBP) gate is applied to filter out unrelated proposals for both branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">KAC Network</head><p>KAC Net consists of two branches: a visual consistency branch and a language consistency branch which reconstructs visual and language information respectively. The two branches are joined in a shared multimodal subspace, where an attention model is applied to attend on mentioned objects based on query's semantics. To leverage external knowledge from pre-trained CNN feature extractor, a Knowledge Based Pooling (KBP) gate is proposed to select query-related proposals. KAC Net is trained end-toend, with both visual and language consistency restriction to guide the training.</p><p>We first introduce the framework of KAC Net, followed by the details of KBP gate. Then we illustrate how KBP is applied to facilitate the optimization of visual and language consistency branches. Finally, more details of training and inference are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework</head><p>The goal of KAC Net is to localize the mentioned object y given a query phrase q and an image x. To address the problem, a set of N proposals {r i } are generated via an object proposal generation system. An attention model is then applied to attend on the proposal r q which contains the mentioned object y based on the semantics of query q.</p><p>In weakly supervised scenario, the mapping between query q and the location of mentioned object y is not provided. To learn the attention model, we adopt visual and language consistency and construct two branches respectively. For language consistency, a reconstruction model is applied to reconstruct input query q given the query-related proposals predicted by the attention model. According to the language consistency, the reconstructed query should be consistent with the input. A language consistency loss L lc is generated by comparing the reconstructed and original queries.</p><p>For visual consistency, we propose to reconstruct visual information for query-related proposals. Since the goal of phrase grounding is to predict mentioned object's location, we choose to predict candidate proposals' location parameters conditioned on the input query. Similar to language consistency, visual consistency requires that the predicted parameters should recover each proposal's location. Based on this, a visual consistency loss L vc is produced by calculating the difference between the predicted and original proposals' location parameters.</p><p>To leverage rich image features and available fixed category classifiers, we apply KBP to encode knowledge provided by CNN and weight each proposal's importance in visual and language consistency. The objective of KAC Net can be written as</p><formula xml:id="formula_0">arg min θ q (L k lc + λL k vc ) + µL reg<label>(1)</label></formula><p>where θ denotes the parameters to be optimized. L k lc is the reconstruction loss from language consistency branch and L k vc is the reconstruction loss from visual consistency branch (superscript "k" refers to KBP). L reg is a weight regularization term. λ, µ are hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Knowledge Based Pooling (KBP)</head><p>We apply a pre-trained CNN to extract visual feature v i for a proposal r i , and predict a probability distribution p i for its own task, which provides useful cues to filter out unrelated proposals.</p><p>To encode this knowledge, we first parse the language query and retrieve all the noun words via a Natural Language Processing (NLP) parser. k " # = similarity ("woman", "people") = 0.72 <ref type="figure">Figure 3</ref>. A pre-trained CNN always predicts a probability distribution for its own task. We leverage the most probable category predicted by CNN and calculate the word similarity between noun words in the query as knowledge k q i</p><p>probability. The knowledge k q i for proposal r i is then calculated as the word similarity between the name of this class and noun words in the query <ref type="figure">(Fig. 3)</ref>. If a query contains multiple noun words, we average all the calculated similarities as the knowledge k q i , which can be written as</p><formula xml:id="formula_1">k q i = 1 N q Nq j=1 sim(C * i , w q j )<label>(2)</label></formula><p>where C * i is the predicted class name for proposal r i , w q j is the j-th word of all the N q noun words in the query q. sim is a function measuring the similarity between two words.</p><p>In the training stage, knowledge k q i functions as a "pooling" gate which helps visual (Sec. 3.3) and language (Sec. 3.4) consistency branches select and reconstruct reliable candidate proposals. In the test stage, knowledge k q i filters out unrelated proposals and increases the chance of finding the proposal containing the mentioned object (Sec. 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Visual Consistency</head><p>The goal of visual consistency is to optimize the attention model via learning to predict location information contained in query-related proposals. Through predicting location information conditioned on the input query, we expect to learn a better correlation between language and visual modalities. In weakly supervised scenario, no annotations are available to indicate the identity of query-related proposal. Instead, we use KBP's knowledge k q i to provide guidance during training. We expect that knowledge k q i provides a higher score when a proposal r i is query related. Thus, KBP can be applied to adaptively weight each proposal's visual consistency loss conditioned on query q.</p><p>In implementation, we first apply a Long Short-Term Memory (LSTM) <ref type="bibr" target="#b19">[20]</ref> model to encode input query q into an embedding vector q ∈ R dq . A pre-trained CNN is employed to extract visual feature v i ∈ R dv for each proposal r i , and global visual feature v ∈ R dv for input image x. The attention model then concatenates the embedding vector q, image global feature v with each of the proposal's feature v i and projects them into an m-dimensional subspace. A multimodal feature v q i is calculated as</p><formula xml:id="formula_2">v q i = ϕ(W m (q||v||v i ) + b m )<label>(3)</label></formula><p>where W m ∈ R m×(dq+2dv) , b m ∈ R m are projection parameters. ϕ(.) is a non-linear activation function. "||" denotes a concatenation operator.</p><p>After projecting into the multimodal subspace, the attention model predicts a 5D vector s p ∈ R 5 via a fully connected (fc) layer (superscript "p" denotes prediction).</p><formula xml:id="formula_3">s p i = W s v q i + b s<label>(4)</label></formula><p>where</p><note type="other">W s ∈ R 5×m and b s ∈ R 5 are projection parameters. The first element in s p i estimates the confidence of r i being relevant to input query q, and the next four elements represent the predicted location parameters for each proposal.</note><p>We compare the predicted location parameters with original proposal's parameters t i ∈ R 4 and calculate the regression loss</p><formula xml:id="formula_4">d i = 1 4 3 j=0 f (|t i [j] − s p i [j + 1]|)<label>(5)</label></formula><p>where f (.) is the smooth L1 loss function: f (x) = 0.5x 2 (|x| &lt; 1), and f (x) = |x| − 0.5(|x| ≥ 1). The location parameters t i are in the form [x i1 /w, y i1 /h, x i2 /w, y i2 /h] − 0.5, where x i1 , x i2 is the minimum and maximum x-axis location of proposal r i , and y i1 , y i2 is the minimum and maximum y-axis location.</p><p>Aided by KBP gate, we weight each proposal's regression loss d i based on the predicted confidence s </p><formula xml:id="formula_5">L k vc = N i=1 σ(k q i )φ(s p i [0])d i<label>(6)</label></formula><p>where φ(.), σ(.) denotes a softmax function and a sigmoid function respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Language Consistency</head><p>The goal of language consistency is to optimize the attention model via learning to reconstruct input query q with a language consistency constraint.</p><p>In implementation, after the attention model predicting each proposal's confidence of being relevant to query q (s p i [0] in Eq. 4), we adopt a similar structure in <ref type="bibr" target="#b33">[34]</ref> to weight each proposal's visual feature v i and project them into a reconstruction subspace. Different from <ref type="bibr" target="#b33">[34]</ref>, we introduce KBP gate into the language consistency branch to further down-weight unrelated visual features' contribution. Thus, the knowledge conditioned reconstruction feature is calculated as</p><formula xml:id="formula_6">v k att = W a N i=1 σ(k q i )φ(s p i [0])v i + b a (7)</formula><p>where W a ∈ R dr×dv , b a ∈ R dr are projections parameters to be optimized. Other notations are the same as Eq. 6.</p><p>The reconstruction visual feature v k att is then treated as the initial state of a decoding LSTM, which predicts a sequence of probability {p t q } indicating the selection of words in each time step t of reconstructed queryq. With the ground truth of input query q (selection of words w t in each time step t), the language reconstruction loss L k lc is the average of cross entropy for the sequence {p t q }.</p><formula xml:id="formula_7">L k lc = − 1 T T t=1 log(p t q [w t ])<label>(8)</label></formula><p>where T is the length of input query q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training &amp; Inference</head><p>In training stage, the parameters to be optimized include parameters in encoding and decoding LSTM and the projection parameters in Eq. 3, 4, 7. We regularize the weights of projection parameters, which is the sum of ℓ 2 norm of these parameters (L reg ). Same as <ref type="bibr" target="#b33">[34]</ref>, we select 100 proposals produced by proposal generation systems (N = 100). The rectified linear unit (ReLU) is selected as the non-linear activation function ϕ. KAC Net is trained end-to-end using the Adam <ref type="bibr" target="#b23">[24]</ref> algorithm.</p><p>In test stage, we feed the query q into the trained KAC Net, and select the most related proposal based on the confidence {s p i [0]} generated by the attention model (Eq. 4) and external knowledge k q i . The final prediction is given as (notations are the same in Eq. 6):</p><formula xml:id="formula_8">r j * , s.t. j * = arg max i {φ(s p i [0])σ(k q i )}<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>We evaluate KAC Net on Flickr30K Entities <ref type="bibr" target="#b31">[32]</ref> and Referit Game <ref type="bibr" target="#b22">[23]</ref> datasets in weakly supervised grounding scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Flickr30K Entities <ref type="bibr" target="#b31">[32]</ref>: There are 29783, 1000, 1000 images in this dataset for training, validation and testing respectively. Each image is associated with 5 captions, with 3.52 query phrases in each caption on average (360K query phrases in total). The vocabulary size for all these queries is 17150. We ignore the bounding box annotations of these two datasets in weakly supervised scenario.</p><p>Referit Game <ref type="bibr" target="#b22">[23]</ref>: There are 19,894 images of natural scenes in this dataset, with 96,654 distinct objects in these images. Each object is referred to by 1-3 query phrases (130,525 in total). There are 8800 unique words among all the phrases, with a maximum length of 19 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment Setup</head><p>Proposal generation. We adopt Selective Search <ref type="bibr" target="#b35">[36]</ref> for Flickr30K Entities <ref type="bibr" target="#b31">[32]</ref> and EdgeBoxes <ref type="bibr" target="#b38">[39]</ref> for Referit Game <ref type="bibr" target="#b22">[23]</ref> to generate proposals as grounding candidates for fair comparison with <ref type="bibr" target="#b33">[34]</ref> on these two datasets.</p><p>Visual feature representation. Same as <ref type="bibr" target="#b33">[34]</ref>, we choose a VGG Network <ref type="bibr" target="#b34">[35]</ref> finetuned by Fast-RCNN <ref type="bibr" target="#b14">[15]</ref> on PASCAL VOC 2007 <ref type="bibr" target="#b9">[10]</ref> to extract visual features for Flickr30K Entities, which are denoted as "VGG det ". Besides, we follow <ref type="bibr" target="#b33">[34]</ref> and apply a VGG Network pretrained on ImageNet <ref type="bibr" target="#b7">[8]</ref> to extract visual features for both Flickr30K Entities and Referit Game datasets, which are denoted as "VGG cls ". Both "VGG cls " and "VGG det " features are 4096D vectors (d v = 4096).</p><p>Knowledge representation. To parse different queries, we use the Stanford NLP parser <ref type="bibr" target="#b26">[27]</ref> to extract noun words in each query. We then extract probability distributions of "VGG det " features in MSCOCO <ref type="bibr" target="#b24">[25]</ref> image classification task for all proposals (#classes=90). The similarity between noun words in queries and class names are calculated as the cosine distance via a word2vec program <ref type="bibr" target="#b27">[28]</ref>. We extract probability distributions in PASCAL VOC 2007 classification task <ref type="bibr" target="#b9">[10]</ref> (#classes=20). Results of different knowledge facilitation is provided in Sec. 4.3 and 4.4.</p><p>KBP gate. For KBP gate, we adopt a soft version and a hard version. Soft KBP applies the sigmoid function to transform external knowledge k q i into probability to directly weight each proposal, while hard KBP applies thresholding to force probability being either 0 or 1 for each proposal (i.e., k q ih = δ(k q is ≥ t), δ is an indicator function, subscripts "h", "s" denote hard KBP and soft KBP respectively).</p><p>In experiments, we set the threshold t as 0.3 for Flickr30K Entities and 0.1 for Referit Game. For hard KBP, if a query's knowledge scores are 0 for all proposals (i.e. k q ih = 0, ∀i), we set them to be all 1 for language reconstruction in Eq. 7; otherwise, reconstruction features v k att provides no information to reconstruct the input query.</p><p>Model initialization. Following same settings as in <ref type="bibr" target="#b33">[34]</ref>, input queries are encoded through an LSTM model, and the query embedding vector q is the last hidden state from LSTM (d q = 512). All fc layers are initialized by Xavier method <ref type="bibr" target="#b15">[16]</ref> and all convolutional layers are initialized by MSRA method <ref type="bibr" target="#b17">[18]</ref>. We introduce batch normalization layers after projecting visual and language features in Eq. 3.</p><p>During training, we set the batch size as 40. The dimension of multimodal features v q i is set to m = 128 (Eq. 3). Hyperparameter µ for weight regularization is 0.005 and λ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach Accuracy (%)</head><p>Compared approaches GroundeR (LC) (VGG cls ) <ref type="bibr" target="#b33">[34]</ref> 24.66 GroundeR (LC) (VGG det ) <ref type="bibr" target="#b33">[34]</ref> 28.93 for visual reconstruction loss is 10.0 in Eq. 1. Analysis of hyperparameters is provided in the supplemental file.</p><p>Metric. Same as <ref type="bibr" target="#b33">[34]</ref>, we adopt accuracy as the evaluation metric, which is defined as the ratio of phrases for which the regressed box overlaps with the mentioned object by more than 50% Intersection over Union (IoU).</p><p>Compared approach. We choose GroundeR <ref type="bibr" target="#b33">[34]</ref> as the compared approach, which achieves state-of-the-art performance on both Flickr30K Entities and Referit Game datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance on Flickr30K Entities</head><p>Comparison in accuracy. We first evaluate pure visual consistency branch's performance for weakly supervised grounding task. In <ref type="table">Table 1</ref>, with a hard KBP gate, visual consistency achieves grounding accuracy as 28.53%, which is very close to GroundeR model. Then we introduce soft KBP gate into visual consistency branch, which brings 2.03% increase in accuracy. This indicates that visual consistency, even alone, is capable of providing good performance in weakly supervised scenario. According to <ref type="bibr" target="#b33">[34]</ref>, GroundeR model is actually a basic case of language consistency branch without a KBP gate. We first introduce a hard KBP gate into language consistency branch, which brings 3.42% increase in grounding performance. We then replace the hard KBP gate with a soft KBP gate, which brings an additional 1.14% increase in performance. This further validates the effectiveness of external knowledge in weakly supervised grounding problem. Finally, we combine visual and language consistency, which is the full KAC Net. By applying a hard KBP gate, KAC Net achieves 37.41% in accuracy. We then replace the hard KBP gate with a soft KBP gate. The KAC Net reaches 38.71% in accuracy, which is a 9.78% increase over the performance of GroundeR <ref type="bibr" target="#b33">[34]</ref>. From <ref type="table">Table 1</ref>, we also find soft KBP gate achieves consistently better performance over hard KBP gate.</p><p>Detailed comparison. <ref type="table" target="#tab_4">Table 3</ref> provides detailed weakly supervised grounding results based on the phrase type information for each query in Flickr30K Entities. We can observe that KAC Net provides superior results in most categories. However, different models have different strength. Language consistency with a soft KBP gate (LC+Soft KBP) is good at localizing "people", "animal" and "vehicles", with 10.91%, 20.27% and 8.5% increase in accuracy compared to GroundeR model. Compared to language consistency, visual consistency (VC+Soft KBP) is better at localizing "clothing", "body parts" and "instruments", with 1.12%, 0.38% and 8.28% increase. However, for other categories, visual consistency branch achieves inferior performances. By incorporating both visual and language consistency, KAC Net observes consistent improvement in all categories except for the category "clothing". With a soft KBP gate, KAC Net achieves 14.10%, 23.00% and 30.89% increase in localizing "people", "vehicles" and "animals". However, KAC Net also has 1.39% drop in accuracy of localizing "clothing". This may be because "clothing" is usually on "people". In this case, there is high chance for a grounding system to classify "clothing" into "people" by mistake. Besides, "clothing" does not have corresponding categories in the external knowledge.</p><p>Knowledge representation. To validate the effectiveness of external knowledge, we also evaluate KAC Net's performance using distributions predicted by VGG Network pre-trained on PASCAL VOC 2007 <ref type="bibr" target="#b9">[10]</ref> image classification. In <ref type="table">Table 2</ref>, we observe that applying external knowledge achieves consistent improvement in grounding performance compared to GroundeR <ref type="bibr" target="#b33">[34]</ref> model. However, knowledge from MSCOCO <ref type="bibr" target="#b24">[25]</ref> image classification achieves a slight increase in accuracy compared to that from PASCAL VOC 2007 <ref type="bibr" target="#b9">[10]</ref> image classification. This may be because MSCOCO contains more categories of objects, and so may be more accurate in describing the proposal's relativeness to the query.   task. In <ref type="table">Table 5</ref>, we observe applying external learned from MSCOCO <ref type="bibr" target="#b24">[25]</ref> image classification achieves better performance than that from PASCAL VOC 2007 <ref type="bibr" target="#b9">[10]</ref>. However, both knowledge representations help achieve increase in grounding accuracy over the state-of-the-art model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Performance on Referit Game</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Discussion</head><p>To further explore KAC Net performance on different types of queries, we define queries with / without words in MSCOCO categories as "Type A" and "Type B" respectively. In <ref type="table" target="#tab_6">Tables 6, 7</ref>, we evaluate two more compared methods: soft KBP only and pre-trained GroundeR <ref type="bibr" target="#b33">[34]</ref> with soft KBP (denoted as "G + KBP") on both Flickr30K Entities <ref type="bibr" target="#b31">[32]</ref> and Referit Game <ref type="bibr" target="#b22">[23]</ref> datasets.</p><p>From <ref type="table" target="#tab_6">Tables 6, 7</ref>, pre-trained GroundeR shows a performance boost by adopting KBP. However, after end-to-end training (LC+KBP) and applying visual consistency part, KAC Net still outperforms state-of-the-art methods by a significant margin. These results also show the generalizability of KAC Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Qualitative Results</head><p>We visualize some of KAC Net's grounding results on Flickr30K Entities and Referit Game datasets for qualitative evaluation in <ref type="figure">Fig. 4</ref>. For Flickr30K Entities, we first show the image description where the query phrases come from, then show the grounding results and ground truth objects in red and green bounding boxes respectively. For Referit Game, each query is independent with no common image descriptions, we visualize two example images with two queries in the third row of <ref type="figure">Fig. 4</ref>.</p><p>We find KAC Net is strong in recognizing people ("a  <ref type="table">Table 7</ref>. Different methods on Referit Game <ref type="bibr" target="#b22">[23]</ref> for two types of queries. Accuracy is in %.</p><p>girl" in the first row) and vehicle ("cars" in the third row), and is able to ground complex queries ("water bottle second in the right" in the third row), which is also validated in <ref type="table" target="#tab_4">Table 3</ref>. However, since KAC Net takes only single query phrase as input, it is unable to make use of context, such as in the example of "a man" in the third row of <ref type="figure">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a novel Knowledge Aided Consistency Network (KAC Net) to address the weakly supervised grounding task. KAC Net applies both visual and language consistency to guide the training and leverages free complementary knowledge to boost performance. Experiments show KAC Net provides a significant improvement in performance compared to state-of-the-arts, with 9.78% and 5.13% increase in accuracy on Flickr30K Entities <ref type="bibr" target="#b31">[32]</ref> and Referit Game <ref type="bibr" target="#b22">[23]</ref> datasets respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (a) supervised grounding systems, (b) state-of-the-art weakly supervised grounding systems guided by language consistency, (c) KAC Net applies both visual and language consistency and leverages complementary knowledge from the visual feature extractor to facilitate weakly supervised grounding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Knowledge Aided Consistency Network (KAC Net) consists of a visual consistency branch and a language consistency branch. Visual consistency branch aims at predicting and aligning query-related proposals' location parameters conditioned on the input query. Language consistency branch attempts to reconstruct input query from query-related proposals. To provide guidance in training and testing, a Knowledge Based Pooling (KBP) gate is applied to filter out unrelated proposals for both branches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>. More details of KAC Net are provided in Sec. 3. Finally we analyze and compare KAC Net with other approaches in Sec. 4.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>For each proposal's distri- bution p i , we select the most probable class with the highest</figDesc><table>CNN 

category tree 
people car 
table 
… 

p i 
0.02 
0.84 
0.07 
0.01 
… 

Pre-trained CNN prediction: people 

Query: A smiling woman 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Comparison in accuracy. Following [34], we adopt EdgeBoxes [39] as a proposal generator. As shown in Ta- ble 4, by introducing KBP gate, KAC Net achieves 2.32% (Hard KBP) and 3.27% (Soft KBP) increase compared to state-of-the-art GroundeR [34] model. We observe us- ing soft KBP gate achieves a slight increase in perfor- mance than hard KBP gate. When KAC Net incorporates A girl rides a blue bike down a city sidewalkFigure 4. Some phrase grounding results in Flickr30K Entities [32] (first three rows) and Referit Game [23] (forth row). We visualize ground truth bounding box and grounding result in green and red respectively. When query is not clear without further context information, KAC Net may ground reasonably incorrect objects (e.g., image in row three, column two).</figDesc><table>Query 1: A girl 
Query 2: a blue bike 
Query 3: a city sidewalk 

A man is taking a photo of 
another man and his two dogs 
on some grassy hills 

Query 1: A man (incorrect) 
Query 2: two dogs 
Query 3: some grassy hills 

Query 1: red backpack 
Query 2: water bottle second 
in the right 

Query 1: cars 
Query 2: people standing on 
the right 

A lady in a red car is crossing 
the bridge 

Query 1: A lady 
Query 2: a red car 
Query 3: the bridge 

Phrase Type 
people clothing body parts animals vehicles instruments scene other 

GroundeR (VGG det ) [34] 44.32 
9.02 
0.96 
46.91 
46.00 
19.14 
28.23 16.98 

LC + Soft KBP 
55.23 
4.21 
2.49 
67.18 
54.50 
11.73 
37.37 13.25 
VC + Soft KBP 
51.56 
5.33 
2.87 
58.11 
51.50 
20.01 
26.86 12.63 
KAC Net (Hard KBP) 
55.14 
7.29 
2.68 
73.94 
66.75 
20.37 
43.14 17.05 
KAC Net (Soft KBP) 
58.42 
7.63 
2.97 
77.80 
69.00 
20.37 
43.53 17.05 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Phrase grounding performances for different phrase types defined in Flickr30K Entities. Accuracy is in percentage. both visual and language consistency, it achieves another 1.66% and 1.86% increase compared to language consis- tency branch with hard and soft KBP respectively. The full model achieves 15.83% grounding accuracy, with 5.13% in- crease over the GroundeR model. Knowledge representation. Similar to Flickr30K Enti- ties, we also evaluate KAC Net's performance using knowl- edge from PASCAL VOC 2007 [10] image classification</figDesc><table>Approach 

Accuracy (%) 

Compared approaches 
LRCN [9] 
8.59 
Caffe-7K [17] 
10.38 
GroundeR [34] (LC) (VGG cls ) 
10.70 

Our approaches 
LC + Hard KBP (VGG cls ) 
13.02 
LC + Soft KBP (VGG cls ) 
13.97 
KAC Net + Hard KBP (VGG cls ) 
14.68 
KAC Net + Soft KBP (VGG cls ) 
15.83 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Different models' performance on Referit Game. We leverage knowledge from MSCOCO [25] classification task.Table 5. Comparison of KAC Net using different KBP gates and external knowledge on ReferitGame. Accuracy is in %.</figDesc><table>Knowledge PASCAL VOC [10] MSCOCO [25] 
Hard KBP 
12.04 
14.68 
Soft KBP 
13.38 
15.83 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 .</head><label>6</label><figDesc>Different methods on Flickr30K Entities [32] for two types of queries. Accuracy is in %.</figDesc><table>Type A Type B 
All 
# queries 
1762 
15757 17519 

Soft KBP 
37.26 
19.77 
21.53 
GroundeR 
26.54 
29.19 
28.93 
G + KBP 
41.03 
32.17 
33.06 
LC + KBP 
42.13 
33.44 
34.31 
KAC Net 
45.66 
37.93 
38.71 

Type A Type B 
All 
# queries 
8275 
51796 60071 

Soft KBP 
12.88 
7.74 
8.45 
GroundeR 
7.29 
11.24 
10.70 
G + KBP 
14.16 
12.56 
12.78 
LC + KBP 
15.28 
13.76 
13.97 
KAC Net 
18.36 
15.43 
15.83 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This paper is based, in part, on research sponsored by the Air Force Research Laboratory and the Defense Advanced Research Projects Agency under agreement number FA8750-16-2-0204. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Air Force Research Laboratory and the Defense Advanced Research Projects Agency or the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Andrej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Soundnet: Learning sound representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">AMC: Attention guided multi-modal correlation learning for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">MSRC: Multimodal spatial regression with semantic context for phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kovvuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Query-guided regression network with context policy for phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kovvuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">ABC-CNN: An attention based convolutional neural network for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of part-based spatial models for visual object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Stylenet: Generating attractive visual captions with styles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">VQS: Linking segmentations to questions and answers for supervised attention in vqa and question-focused semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Motion-appearance co-memory networks for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">TALL: Temporal activity localization via language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aistats</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Open-vocabulary object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: science and systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Workshop</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Darrell. Natural language object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Referit game: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Leveraging visual question answering for image-caption ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (System Demonstrations)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR Workshop</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Is object localization for free?-weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ambient sound provides supervision for visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of interactions between humans and objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Grounding of textual phrases in images by reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Smeulders. Selective search for object recognition. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Weakly-supervised visual grounding of phrases with linguistic structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
