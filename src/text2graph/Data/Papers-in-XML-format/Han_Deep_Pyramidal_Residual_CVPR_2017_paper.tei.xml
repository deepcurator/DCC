<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Pyramidal Residual Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
							<email>dyhan@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">EE, KAIST</orgName>
								<orgName type="department" key="dep2">EE, KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwhan</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">EE, KAIST</orgName>
								<orgName type="department" key="dep2">EE, KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>EE, KAIST</roleName><forename type="first">Junmo</forename><surname>Kim</surname></persName>
							<email>junmo.kim@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">EE, KAIST</orgName>
								<orgName type="department" key="dep2">EE, KAIST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Pyramidal Residual Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code is available at https://github.com/jhkim89/PyramidNet</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Deep convolutional neural networks (DCNNs)   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The emergence of deep convolutional neural networks (DCNNs) has greatly contributed to advancements in solving complex tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19]</ref> in computer vision with significantly improved performance. Since the proposal of LeNet <ref type="bibr" target="#b15">[16]</ref>, which introduced the use of deep neural network architectures for computer vision tasks, the advanced architecture AlexNet <ref type="bibr" target="#b12">[13]</ref> was selected as the winner of the 2012 ImageNet competition <ref type="bibr" target="#b21">[22]</ref> by a large margin over traditional methods. Subsequently, ZF-net <ref type="bibr" target="#b34">[35]</ref>, * These two authors contributed equally.</p><p>VGG <ref type="bibr" target="#b24">[25]</ref>, GoogleNet <ref type="bibr" target="#b30">[31]</ref>, Residual Networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, and Inception Residual Networks <ref type="bibr" target="#b29">[30]</ref> were successively proposed to demonstrate advances in network architectures. In particular, Residual Networks (ResNets) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> leverage the concept of shortcut connections <ref type="bibr" target="#b28">[29]</ref> inside a proposed residual unit for residual learning, to make it possible to train much deeper network architectures. Deeper network architectures are known for their superior performance, and these network architectures commonly have deeply stacked convolutional filters with nonlinearity <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>With respect to feature map dimension, the conventional method of stacking several convolutional filters is to increase the dimension while decreasing the size of feature maps by increasing the strides of the filters or poolings. This is the widely adopted method of controlling the size of feature maps, because extracting the diversified highlevel attributes with the increased feature map dimension is very effective for classification tasks. Architectures such as those of AlexNet <ref type="bibr" target="#b12">[13]</ref> and VGG <ref type="bibr" target="#b24">[25]</ref> utilize this method of increasing the feature map dimension to construct their network architectures. The most successful deep neural network, ResNets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, which was introduced by He et al. <ref type="bibr" target="#b6">[7]</ref>, also follows this approach for filter stacking.</p><p>According to the research of Veit et al. <ref type="bibr" target="#b32">[33]</ref>, ResNets are considered to behave as ensembles of relatively shallow networks. These researchers showed that the deletion of an individual residual unit from ResNets, i.e., such that only a shortcut connection remains, does not significantly affect the overall performance, proving that deleting a residual unit is equivalent to deleting some shallow networks in the ensemble networks. Contrary to this, deleting a single layer in plain network architectures such as a VGG-network <ref type="bibr" target="#b24">[25]</ref> damages the network by causing additional severe errors.</p><p>However, in the case of ResNets, it was found that deleting the building blocks in a residual unit with downsampling, where the feature map dimension is doubled, still increases the classification error by a significant margin. Interestingly, when the residual net is trained using a stochastic depth <ref type="bibr" target="#b9">[10]</ref>, it was found that deleting the blocks with downsampling does not degrade the classification performance, as shown in <ref type="figure">Figure 8</ref> in <ref type="bibr" target="#b32">[33]</ref>. One may think that <ref type="figure">Figure 1</ref>. Schematic illustration of (a) basic residual units <ref type="bibr" target="#b6">[7]</ref>, (b) bottleneck residual units <ref type="bibr" target="#b6">[7]</ref>, (c) wide residual units <ref type="bibr" target="#b33">[34]</ref>, (d) our pyramidal residual units, and (e) our pyramidal bottleneck residual units.</p><p>this phenomenon is related to the overall improvement in the classification performance enabled by stochastic depth.</p><p>Motivated by the ensemble interpretation of residual networks in Veit et al. <ref type="bibr" target="#b32">[33]</ref> and the results with stochastic depth <ref type="bibr" target="#b9">[10]</ref>, we devised another method to handle the phenomenon associated with deleting the downsampling unit. In the proposed method, the feature map dimensions are increased at all layers to distribute the burden concentrated at locations of residual units affected by downsampling, such that it is equally distributed across all units. It was found that using the proposed new network architecture, deleting the units with downsampling does not degrade the performance significantly. In our paper, we refer to this network architecture as a deep "pyramidal" network and a "pyramidal" residual network with a residual-type network architecture. This reflects the fact that the shape of the network architecture can be compared to that of a pyramid. That is, the number of channels gradually increases as a function of the depth at which the layer occurs, which is similar to a pyramid structure of which the shape gradually widens from the top downwards. This structure is illustrated in comparison to other network architectures in <ref type="figure">Figure 1</ref>. The key contributions are summarized as follows:</p><p>• A deep pyramidal residual network (PyramidNet) is introduced. The key idea is to concentrate on the feature map dimension by increasing it gradually instead of by increasing it sharply at each residual unit with downsampling. In addition, our network architecture works as a mixture of both plain and residual networks by using zero-padded identity-mapping shortcut connections when increasing the feature map dimension.</p><p>• A novel residual unit is also proposed, which can further improve the performance of ResNet-based architectures (compared with state-of-the-art network architectures).</p><p>The remainder of this paper is organized as follows. Section 2 presents our PyramidNets and introduces a novel residual unit that can further improve ResNet. Section 3 closely analyzes our PyramidNets via several discussions. Section 4 presents experimental results and comparisons with several state-of-the-art deep network architectures. Section 5 concludes our paper with suggestions for future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Network Architecture</head><p>In this section, we introduce the network architectures of our PyramidNets. The major difference between PyramidNets and other network architectures is that the dimension of channels gradually increases, instead of maintaining the dimension until a residual unit with downsampling appears. A schematic illustration is shown in <ref type="figure">Figure 1</ref> (d) to facilitate understanding of our network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Feature Map Dimension Configuration</head><p>Most deep CNN architectures <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref> utilize an approach whereby feature map dimensions are increased by a large margin when the size of the feature map decreases, and feature map dimensions are not increased until they encounter a layer with downsampling. In the case of the original ResNet for CIFAR datasets <ref type="bibr" target="#b11">[12]</ref>, the number of feature map dimensions D k of the k-th residual unit that belongs to the n-th group can be described as follows:</p><formula xml:id="formula_0">D k = 16, if n(k) = 1, 16 · 2 n(k)−2 , if n(k) ≥ 2,<label>(1)</label></formula><p>in which n(k) ∈ {1, 2, 3, 4} denotes the index of the group to which the k-th residual unit belongs. The residual units that belong to the same group have an equal feature map size, and the n-th group contains N n residual units. In the first group, there is only one convolutional layer that converts an RGB image into multiple feature maps. For the n-th group, after N n residual units have passed, the feature size is downsampled by half and the number of dimensions is doubled. We propose a method of increasing the feature map dimension as follows:</p><formula xml:id="formula_1">D k = 16, if k = 1, ⌊D k−1 + α/N ⌋, if 2 ≤ k ≤ N + 1,<label>(2)</label></formula><p>in which N denotes the total number of residual units, defined as N = 4 n=2 N n . The dimension is increased by a step factor of α/N , and the output dimension of the final unit of each group becomes 16 + (n − 1)α/3 with same number of residual units in each group. The details of our network architecture are presented in <ref type="table">Table 1</ref>.</p><p>The above equations are based on an addition-based widening step factor α for increasing dimensions. However, of course, multiplication-based widening (i.e., the process of multiplying by a factor to increase the channel dimension geometrically) presents another possibility for creating a pyramid-like structure. Then, eq.(2) can be transformed as follows:</p><formula xml:id="formula_2">D k = 16, if k = 1, ⌊D k−1 · α 1 N ⌋, if 2 ≤ k ≤ N + 1. (3)</formula><p>The main difference between additive and multiplicative PyramidNets is that the feature map dimension of an additive network gradually increases linearly, whereas the dimension of a multiplicative network increases geometrically. That is, the dimension slowly increases in input-side layers and sharply increases in output-side layers. This process is similar to that of the original deep network architectures such as VGG <ref type="bibr" target="#b24">[25]</ref> and ResNet <ref type="bibr" target="#b6">[7]</ref>. The visual illustrations of additive and multiplicative PyramidNets are shown in <ref type="figure" target="#fig_0">Figure 2</ref>. In this paper, we compare the performance of both of these dimension-increasing approaches by comparing an additive PyramidNet (eq. <ref type="formula" target="#formula_1">(2)</ref>) and a multiplicative PyramidNet (eq. (3)) in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Building Block</head><p>The building block (i.e., the convolutional filter stacks with ReLUs and BN layers) in a residual unit is the core of ResNet-based architectures. It is obvious that in order to maximize the capability of the network architecture, designing a good building block is essential. As shown in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head><p>Output size Building Block conv 1 32×32 <ref type="table">Table 1</ref>. Structure of our PyramidNet for benchmarking with CIFAR-10 and CIFAR-100 datasets. α denotes the widening factor, and Nn signifies the number of blocks in a group. Downsampling is performed at conv3 1 and conv4 1 with a stride of 2. <ref type="figure" target="#fig_4">Figure 6</ref>, the layers can be stacked in various manners to construct a single building block. We found the building block shown in <ref type="figure" target="#fig_4">Figure 6</ref> (d) to be the most promising, and therefore we included this structure as building block in our PyramidNets. The discussion of this matter is continued in the following section.</p><formula xml:id="formula_3">[3 × 3, 16] conv 2 32×32 3 × 3, ⌊16 + α(k − 1)/N ⌋ 3 × 3, ⌊16 + α(k − 1)/N ⌋ × N 2 conv 3 16×16 3 × 3, ⌊16 + α(k − 1)/N ⌋ 3 × 3, ⌊16 + α(k − 1)/N ⌋ × N 3 conv 4 8×8 3 × 3, ⌊16 + α(k − 1)/N ⌋ 3 × 3, ⌊16 + α(k − 1)/N ⌋ × N 4 avg pool 1×1 [8 × 8, 16 + α]</formula><p>In terms of shortcut connections, many researchers either use those based on identity mapping, or those employing convolution-based projection. However, as the feature map dimension of PyramidNet is increased at every unit, we can only consider two options: zero-padded identitymapping shortcuts, and projection shortcuts conducted by 1×1 convolutions. However, as mentioned in the work of He et al. <ref type="bibr" target="#b7">[8]</ref>, the 1×1 convolutional shortcut produces a poor result when there are too many residual units, i.e., this shortcut is unsuitable for very deep network architectures. Therefore, we select zero-padded identity-mapping shortcuts for all residual units. Further discussions about the zero-padded shortcut are provided in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Discussions</head><p>In this section, we present an in-depth study of the architecture of our PyramidNet, together with the proposed novel residual units. The experiments we include here support the study and confirm that insights obtained from our network architecture can further improve the performance of existing ResNet-based architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Effect of PyramidNet</head><p>According to the work of Veit et al. <ref type="bibr" target="#b32">[33]</ref>, ResNets can be viewed as ensembles of relatively shallow networks, supported by the observation that deleting an individual building block in a residual unit of ResNets incurs minor classification loss, whereas removing layers from plain networks such as VGG <ref type="bibr" target="#b24">[25]</ref> severely reduces the classification rate. However, in both original and pre-activation ResNets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, another noteworthy aspect is that deleting the units with downsampling (and doubling the feature dimension) still degrades performance by a large margin <ref type="bibr" target="#b32">[33]</ref>. Meanwhile, when a stochastic depth <ref type="bibr" target="#b9">[10]</ref> is applied, this phenomenon is not observed, and the performance is also improved, according to the experiment of Veit et al. <ref type="bibr" target="#b32">[33]</ref>. The objective of our PyramidNet is to resolve this phenomenon differently, by attempting to gradually increase the feature map dimension instead of doubling it at one of the residual units and to evenly distribute the burden of increasing the feature maps. We observed that our PyramidNet indeed resolves this phenomenon and at the same time improves overall performance. We further analyze the effect of our PyramidNet by comparing it against the pre-activation ResNet, with the following experimental results. First, we compare the training and test error curves of our PyramidNet with those of the pre-activation ResNet <ref type="bibr" target="#b7">[8]</ref> in <ref type="figure" target="#fig_1">Figure 3</ref>. The standard preactivation ResNet with 110 layers is used for comparison. For our PyramidNet, we used a depth of 110 layers with a widening factor of α = 48; it had the same number of parameters (1.7M) as the pre-activation ResNet to allow for a fair comparison. The results indicate that our PyramidNet has superior test accuracy, thereby confirming its greater ability to generalize compared to existing deep networks.</p><p>Second, we verify the ensemble effect of our PyramidNets by evaluating the performance after deleting individual units, similar to the experiment of Veit et al. <ref type="bibr" target="#b32">[33]</ref>. The results are shown in <ref type="figure" target="#fig_2">Figure 4</ref>. As mentioned by Veit et al. <ref type="bibr" target="#b32">[33]</ref>, removing individual units only causes a slight performance loss, compared with a plain network such as the VGG <ref type="bibr" target="#b24">[25]</ref>. However, in the case of the pre-activation ResNet, removing the blocks subjected to downsampling tends to affect the classification accuracy by a relatively large margin, whereas this does not occur with our PyramidNets. Furthermore, the mean average error differences between the baseline result and the result obtained when individual units were deleted from both the pre-activation ResNet and our PyramidNet were 0.72% and 0.54%, re- spectively. This result shows that the ensemble effect of our PyramidNet becomes stronger than the original ResNet, such that generalization ability is improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Zero-padded Shortcut Connection</head><p>ResNets and pre-activation ResNets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> were studied several types of shortcuts, such as an identity-mapping shortcut or projection shortcut. The experimental results in <ref type="bibr" target="#b7">[8]</ref> showed that the identity-mapping shortcut is a much more appropriate choice than other shortcuts. Because an identity-mapping shortcut does not have parameters, it has a lower possibility of overfitting compared to the other types of shortcuts; this ensures improved generalization ability. Moreover, it can purely pass through the gradient according to the identity mapping, and therefore it provides more stability in the training stage.</p><p>In the case of our PyramidNet, identity mapping alone cannot be used for a shortcut because the feature map dimension differs among individual residual units. Therefore, only a zero-padded shortcut or projection shortcut can be used for all the residual units. However, as discussed in <ref type="bibr" target="#b7">[8]</ref>, a projection shortcut can hamper information propagation and lead to optimization problems, especially for very deep networks. On the other hand, we found that the zero-padded shortcut does not lead to the overfitting problem because no additional parameters exist, and surprisingly, it shows significant generalization ability compared to other shortcuts.</p><p>We now examine the effect of the zero-padded identitymapping shortcut on the k-th residual unit that belongs to the n-th group with the reshaped vector x l k of the l-th feature map:</p><formula xml:id="formula_4">x l k = F (k,l) (x l k−1 ) + x l k−1 , if 1 ≤ l ≤ D k−1 F (k,l) (x l k−1 ), if D k−1 &lt; l ≤ D k<label>(4)</label></formula><p>where F (k,l) (·) denotes the l-th residual function of the kth residual unit and D k represents the pre-defined channel dimensions of the k-th residual unit. From eq. <ref type="formula" target="#formula_4">(4)</ref>, zeropadded elements of the identity-mapping shortcut for increasing dimension let x l k contain the outputs of both residual networks and plain networks. Therefore, we could conjecture that each zero-padded identity-mapping shortcut can provide a mixture of the residual network and plain network, as shown in <ref type="figure" target="#fig_3">Figure 5</ref>. Furthermore, our PyramidNet increases the channel dimension at every residual unit, and the mixture effect of the residual network and plain network increases markedly. <ref type="figure" target="#fig_2">Figure 4</ref> supports the conclusion that the test error of PyramidNet does not oscillate as much as that of the pre-activation ResNet. Finally, we investigate several types of shortcuts including proposed zero-padded identity-mapping shortcut in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">A New Building Block</head><p>To maximize the capability of the network, it is natural to ask the following question: "Can we design a better building block by altering the stacked elements inside the building block in more principled way?". The first building block types were proposed in the original paper on ResNets <ref type="bibr" target="#b6">[7]</ref>, and another type of building block was subsequently proposed in the paper on pre-activation ResNets <ref type="bibr" target="#b7">[8]</ref>, to answer the question. Moreover, pre-activation ResNets attempted to solve the backward gradient flowing problem <ref type="bibr" target="#b7">[8]</ref> by redesigning residual modules; this proved to be successful in trials. However, although the pre-activation residual unit was discovered with empirically improved performance, further investigation over the possible combinations is not yet performed, leaving a potential room for improvement. We next attempt to answer the question from two points of view by considering Rectified Linear Units (ReLUs) <ref type="bibr" target="#b19">[20]</ref> and Batch Normalization (BN) <ref type="bibr" target="#b10">[11]</ref> layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">ReLUs in a Building Block</head><p>Including ReLUs <ref type="bibr" target="#b19">[20]</ref> in the building blocks of residual units is essential for nonlinearity; however, we found empirically that the performance can vary depending on the loca-  <ref type="table">Table 2</ref>. Top-1 errors (%) on CIFAR datasets using our PyramidNet with several combinations of shortcut connections.</p><p>tions and the number of ReLUs. This could be discussed with original ResNets <ref type="bibr" target="#b6">[7]</ref>, for which it was shown that the performance increases as the network becomes deeper; however, if the depth exceeds 1,000 layers, overfitting still occurs and the result is less accurate than that generated by shallower ResNets. First, we note that using ReLUs after the addition of residual units adversely affects performance:</p><formula xml:id="formula_5">x l k = ReLU (F (k ,l) (x l k −1 ) + x l k −1 ),<label>(5)</label></formula><p>where the ReLUs seem to have the function of filtering nonnegative elements. Gross and Wilber <ref type="bibr" target="#b4">[5]</ref> found that simply removing ReLUs from the original ResNet <ref type="bibr" target="#b6">[7]</ref> after each addition with the shortcut connection leads to small performance improvements. This could be understood by considering that, after addition, ReLUs provide non-negative input to the subsequent residual units, and therefore the shortcut connection is always non-negative and the convolutional layers would take responsibility for producing negative output before addition; this may decrease the overall capability of the network architecture as analyzed in <ref type="bibr" target="#b7">[8]</ref>. The preactivation ResNets proposed by He et al. <ref type="bibr" target="#b7">[8]</ref> also overcame this issue with pre-activated residual units that place BN layers and ReLUs before (instead of after) the convolutional layers:</p><formula xml:id="formula_6">x l k = F (k,l) (x l k−1 ) + x l k−1 ,<label>(6)</label></formula><p>where ReLUs are removed after addition to create an identity path. Consequently, the overall performance has increased by a large margin without overfitting, even at depths exceeding 1,000 layers. Furthermore, Shen et al. <ref type="bibr" target="#b23">[24]</ref> proposed a weighted residual network architecture, which locates a ReLU inside a residual unit (instead of locating ReLU after addition) to create an identity path, and showed that this structure also does not overfit even at depths of more than 1,000 layers. Second, we found that the use of a large number of ReLUs in the blocks of each residual unit may negatively affect performance. Removing the first ReLU in the blocks of each residual unit, as shown in <ref type="figure" target="#fig_4">Figure 6</ref> (b) and (d), was found to enhance performance compared with the blocks shown in <ref type="figure" target="#fig_4">Figure 6</ref> (a) and (c). Experimentally, we found that removal of the first ReLU in the stack is preferable and that the other ReLU should remain to ensure nonlinearity. Removing the second ReLU in <ref type="figure" target="#fig_4">Figure 6</ref>  blocks to BN-ReLU-conv-BN-conv, and it is clear that, in these blocks, the convolutional layers are successively located without ReLUs to weaken their representation powers of each other. However, when we remove the first ReLU, the blocks are changed to BN-conv-BN-ReLU-conv, in which case the two convolutional layers are separated by the second ReLU, thereby guaranteeing nonlinearity. The results in <ref type="table">Table 3</ref> confirm that removing the first ReLU as in (b) and (d) in <ref type="figure" target="#fig_4">Figure 6</ref>, enhances the performance. Consequently, provided that an appropriate number of ReLUs are used to guarantee the nonlinearity of the feature space manifold, the remaining ReLUs could be removed to improve network performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">BN Layers in a Building Block</head><p>The main role of a BN layer is to normalize the activations for fast convergence and to improve performance. The experimental results of the four structures provided in <ref type="table">Table 3</ref> show that the BN layer can be used to maximize the capability of a single residual unit. A BN layer conducts an affine transformation with the following equation:</p><formula xml:id="formula_7">y = γx + β,<label>(7)</label></formula><p>where γ and β are learned for every activation in feature maps. We experimentally found that the learned γ and β could closely approximate 0. This implies that if the learned γ and β are both close to 0, then the corresponding activation is considered not to be useful. Weighted ResNets <ref type="bibr" target="#b23">[24]</ref>, in which the learnable weights occur at the end of their building blocks, are also similarly learned to determine whether the corresponding residual unit is useful. Thus, the BN layers at the end of each residual unit are a generalized version including <ref type="bibr" target="#b23">[24]</ref> to enable decisions to be made as to whether each residual unit is helpful. Therefore, the degrees ResNet Architecture CIFAR-10 CIFAR-100 (a) Pre-activation <ref type="bibr" target="#b7">[8]</ref> 5. of freedom obtained by involving γ and β from the BN layers could improve the capability of the network architecture. The results in <ref type="table">Table 3</ref> support the conclusion that adding a BN layer at the end of each building block, as in type (c) and (d) in <ref type="figure" target="#fig_4">Figure 6</ref>, improves the performance. Note that the aforementioned network removing the first ReLU is also improved by adding a BN layer after the final convolutional layer. Furthermore, the results in <ref type="table">Table 3</ref> show that both PyramidNet and a new building block improve the performance significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We evaluate and compare the performance of our algorithm with that of existing algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34]</ref> using representative benchmark datasets: CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b11">[12]</ref>. CIFAR-10 and CIFAR-100 each contain 32×32-pixel color images, consists of 50,000 training images and 10,000 testing images. But in case of CIFAR-10, it includes  <ref type="table">Table 4</ref>. Top-1 error rates (%) on CIFAR datasets. All the results of PyramidNets are produced with additive PyramidNets, and α denotes the widening factor. "Output Feat. Dim." denotes the feature dimension of just before the last softmax classifier.</p><p>10 classes, and CIFAR-100 includes 100 classes. The standard data augmentation, horizontal flipping, and translation by 4 pixels are adopted in our experiments, following the common practice <ref type="bibr" target="#b17">[18]</ref>. The results achieved by PyramidNets are based on the proposed residual unit: placing a BN layer after the final convolutional layer, and removing the first ReLU as in <ref type="figure" target="#fig_4">Figure 6</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training Settings</head><p>Our PyramidNets are trained using backpropagation <ref type="bibr" target="#b14">[15]</ref> by Stochastic Gradient Descent (SGD) with Nesterov momentum for 300 epochs using CIFAR-10 and CIFAR-100 datasets. The initial learning rate is set to 0.1 for CIFAR-10 and 0.5 for CIFAR-100, and is decayed by a factor of 0.1 at 150 and 225 epochs, respectively. The filter parameters are initialized by "msra" <ref type="bibr" target="#b5">[6]</ref>. We use a weight decay of 0.0001, a dampening of 0, a momentum of 0.9, and a batch size of 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Evaluation</head><p>In our work, we mainly use the top-1 error rate for evaluating our network architecture. Additive PyramidNets with both basic and pyramidal bottleneck residual units are used. The error rates are provided in <ref type="table">Table 4</ref> for ours and the stateof-the-art models. The experimental results show that our network has superior generalization ability, in terms of the number of parameters, showing the best results compared with other models. <ref type="figure" target="#fig_7">Figure 7</ref> compares additive and multiplicative PyramidNets. When the number of parameters is low, both additive and multiplicative PyramidNets show similar performance, because these two network architectures do not have significant structural differences. As the number of parameters increases, they start to show a more marked difference in terms of the feature map dimension configuration. Because the feature map dimension increases linearly in the case of additive PyramidNets, the feature map dimensions of the input-side layers tend to be larger, and those of the outputside layers tend to be smaller, compared with multiplicative PyramidNets as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref> typically set multiplicative scaling of feature map dimension for downsampling modules, which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output-side layers. However, for our PyramidNet, the results in <ref type="figure" target="#fig_7">Figure 7</ref> implies that increasing the model capacity of the input-side layers would lead to a better performance improvement than using a conventional way of multiplicative scaling of feature map dimension.</p><p>We also note that, although the use of regularization methods such as dropout <ref type="bibr" target="#b27">[28]</ref> or stochastic depth <ref type="bibr" target="#b9">[10]</ref> could further improve the performance of our model, we did not involve those methods to ensure a fair comparison with other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">ImageNet</head><p>1,000-class ImageNet dataset <ref type="bibr" target="#b21">[22]</ref> used for ILSVRC contains more than one million training images and 50,000 validation images. We use additive PyramidNets with the pyramidal bottleneck residual units, deleting the first ReLU and adding a BN layer at the last layer as described in Section 3.3 and shown in <ref type="figure" target="#fig_4">Figure 6</ref> (d) for further performance improvement.</p><p>We train our models for 120 epochs with a batch size of 128, and the initial learning rate is set to 0.05, divided by 10 at 60, 90 and 105 epochs. We use the same weight decay, momentum, and initialization settings as those of CIFAR datasets. We train our model by using a standard data augmentation with scale jittering and aspect ratio as suggested in Szegedy et al. <ref type="bibr" target="#b30">[31]</ref>. <ref type="table">Table 5</ref> shows the results of our PyramidNets in ImageNet dataset compared with the stateof-the-art models. The experimental results show that our PyramidNet with α = 300 has a top-1 error rate of 20.5%, which is 1.2% lower than the pre-activation ResNet-200 <ref type="bibr" target="#b7">[8]</ref> which has a similar number of parameters but higher output feature dimension than our model. We also notice that increasing α with an appropriate regularization method can further improve the performance.</p><p>For comparison with the Inception-ResNet <ref type="bibr" target="#b29">[30]</ref> that uses a testing crop with 299 × 299 size, we test our model on a 320 × 320 crop, by the same reason with the work of He et al. <ref type="bibr" target="#b7">[8]</ref>. Our PyramidNet with α = 300 shows a top-1 error rate of 19.6%, which outperforms both the pre-activation ResNet <ref type="bibr" target="#b7">[8]</ref> and the Inception-ResNet-v2 <ref type="bibr" target="#b29">[30]</ref> models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>The main idea of the novel deep network architecture described in this paper involves increasing the feature map dimension gradually, in order to construct so-called PyramidNets along with the concept of ResNets. We also developed a novel residual unit, which includes a new building block for a residual unit with a zero-padded shortcut; this design leads to significantly improved generalization ability. In tests using CIFAR-10, CIFAR-100, and ImageNet1k datasets, our PyramidNets outperform all previous stateof-the-art deep network architectures. Furthermore, the insights in this paper could be utilized by any network architecture, to improve their capacity for better performance. In future work, we will develop methods of optimizing parameters such as feature map dimensions in more principled ways with proper cost functions that give insight into the nature of residual networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Visual illustrations of (a) additive PyramidNet, (b) multiplicative PyramidNet, and (c) a comparison of (a) and (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Performance comparison between the pre-activation ResNet [8] and our PyramidNet, using CIFAR datasets. Dashed and solid lines denote the training loss and test error, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Test error curves to study the extent to which residual units contribute to the performance in different network architectures by deleting their individual units. The dashed and solid lines denote the test errors that occur when no units are deleted, and when an individual unit is deleted, respectively. Bold vertical lines denote the location of residual units through downsampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Structure of residual unit (a) with zero-padded identitymapping shortcut, (b) unraveled view of (a) showing that the zeropadded identity-mapping shortcut constitutes a mixture of a residual network with a shortcut connection and a plain network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Various types of basic and bottleneck residual units. "BatchNorm" denotes a Batch Normalization (BN) layer. (a) original pre-activation ResNets [8], (b) pre-activation ResNets removing the first ReLU, (c) pre-activation ResNets with a BN layer after the final convolutional layer, and (d) pre-activation ResNets removing the fist ReLU with a BN layer after the final convolutional layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Table 3 .</head><label>3</label><figDesc>Top-1 errors (%) on CIFAR datasets for several build- ing block combinations of ReLUs and BN layers shown in Fig- ure 6 (a)-(d), using ResNet [8] (with original feature map dimen- sion configuration) and our PyramidNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(d). Our code is built on the Torch open source deep learning framework [1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Comparison of test error curves with error bars of additive PyramidNet and multiplicative PyramidNet, according to the different number of parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Network # of Params Output Feat. Dim. Augmentation Train Crop Test Crop Top-1 Top-5Table 5. Comparisons of single-model, single-crop error (%) on the ILSVRC 2012 validation set. All the results of PyramidNets are produced with additive PyramidNets. "asp ratio" means the aspect ratio applied for data augmention, and "Output feat. dim." denotes the feature dimension of just after the last global pooling layer. * denotes the models which applied dropout method, and † denotes the results obtained from https://github.com/facebook/fb.resnet.torch.</figDesc><table>ResNet-152 [7] 
60.0M 
2,048 
scale 
224×224 
224×224 
23.0 
6.7 
Pre-ResNet-152 
 † [8] 
60.0M 
2,048 
scale+asp ratio 
224×224 
224×224 
22.2 
6.2 
Pre-ResNet-200 
 † [8] 
64.5M 
2,048 
scale+asp ratio 
224×224 
224×224 
21.7 
5.8 
WRN-50-2-bottleneck [34] 
68.9M 
2,048 
scale+asp ratio 
224×224 
224×224 
21.9 
6.0 
PyramidNet-200 (α = 300) 
62.1M 
1,456 
scale+asp ratio 
224×224 
224×224 
20.5 
5.3 
PyramidNet-200 (α = 300) 

 *  

62.1M 
1,456 
scale+asp ratio 
224×224 
224×224 
20.5 
5.4 
PyramidNet-200 (α = 450) 

 *  

116.4M 
2,056 
scale+asp ratio 
224×224 
224×224 
20.1 
5.4 
ResNet-200 [7] 
64.5M 
2,048 
scale 
224×224 
320×320 
21.8 
6.0 
Pre-ResNet-200 [8] 
64.5M 
2,048 
scale+asp ratio 
224×224 
320×320 
20.1 
4.8 
Inception-v3 [32] 
-
2,048 
scale+asp ratio 
299×299 
299×299 
21.2 
5.6 
Inception-ResNet-v1 [30] 
-
1,792 
scale+asp ratio 
299×299 
299×299 
21.3 
5.5 
Inception-v4 [30] 
-
1,536 
scale+asp ratio 
299×299 
299×299 
20.0 
5.0 
Inception-ResNet-v2 [30] 
-
1,792 
scale+asp ratio 
299×299 
299×299 
19.9 
4.9 
PyramidNet-200 (α = 300) 
62.1M 
1,456 
scale+asp ratio 
224×224 
320×320 
19.6 
4.8 
PyramidNet-200 (α = 300) 

 *  

62.1M 
1,456 
scale+asp ratio 
224×224 
320×320 
19.5 
4.8 
PyramidNet-200 (α = 450) 

 *  

116.4M 
2,056 
scale+asp ratio 
224×224 
320×320 
19.2 
4.7 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6071</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Fractional max-pooling. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Training and investigating residual nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wilber</surname></persName>
		</author>
		<ptr target="http://torch.ch/blog/2016/02/04/resnets.html.5" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<title level="m">Densely connected convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">Tech Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fractalnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07648</idno>
		<title level="m">Ultra-deep neural networks without residuals</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Weighted residuals for very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08831</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Swapout: Learning an ensemble of deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Residual networks behave like ensembles of relatively shallow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
