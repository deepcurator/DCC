<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly Supervised Dense Video Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Su</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjun</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
							<email>yurong.chen@intel.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
							<email>xyxue@fudan.edu.cn‡jianguo.li</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Shanghai</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly Supervised Dense Video Captioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Automatically describing images or videos with natural language sentences has recently received significant attention in the computer vision community. For images, researchers have investigated image captioning with one sentence <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b46">47]</ref> or multiple sentences <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b32">33]</ref>. For videos, most of the works focused on gener-  ating only one caption for a short video clip using methods based on mean pooling of features over frames <ref type="bibr" target="#b48">[49]</ref>, the soft-attention scheme <ref type="bibr" target="#b52">[53]</ref>, or visual-semantic embedding between visual feature and language <ref type="bibr" target="#b29">[30]</ref>. Some recent works further considered the video temporal structure, such as the sequence-to-sequence learning (S2VT) <ref type="bibr" target="#b47">[48]</ref> and hierarchical recurrent neural encoder <ref type="bibr" target="#b28">[29]</ref>.</p><p>However, using a single sentence cannot well describe the rich contents within images/videos. The task of dense image captioning is therefore proposed, which aims to generate multiple sentences for different detected object locations in images <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref>. However, this setting requires region-level caption annotations for supervised training purpose. As is well-known, videos are much more complex than images since the additional temporal dimension could provide informative contents such as different viewpoints of objects, object motions, procedural events, etc. It is fairly expensive to provide region-sequence level sentence annotations for dense video captioning. The lack of such annotations has largely limited the much-needed progress of dense video captioning. Our work in this paper is motivated by the following two questions. First, most existing datasets have multiple video-level sentence annotations, which usu- ally describe very diverse aspects (regions/segments) of the video clip. However, existing video captioning methods simply represented all sentence descriptions with one global visual representation. This one-to-many mapping is far from accurate. It is thus very interesting to investigate if there is an automatic way to (even weakly) associate sentence to region-sequence. Second, is it possible to perform dense video captioning with those weakly associations (without strong 1-to-1 mapping between sentences and region-sequence) in a weakly supervised fashion?</p><p>In this paper, we propose an approach to generate multiple diverse and informative captions by weakly supervised learning from only the video-level sentence annotations. <ref type="figure" target="#fig_2">Figure 2</ref> illustrates the architecture of the proposed approach, which consists of three major components: visual sub-model, region-sequence sub-model and language sub-model. The visual sub-model is a lexical-FCN trained with weakly supervised multi-instance multi-label learning, which builds the weak mapping between sentence lexical words and grid regions. The second component solves the region-sequence generation problem. We propose submodular maximization scheme to automatically generate informative and diverse region-sequences based on Lexical-FCN outputs. A winner-takes-all scheme is proposed to weakly associate sentences to region-sequences in the training phase. The third component generates sentence output for each region-sequence with a sequence-to-sequence learning based language model <ref type="bibr" target="#b47">[48]</ref>. The main contributions are summarized as follows:</p><p>(1) To the best of our knowledge, this is the first work for dense video captioning with only video-level sentence annotations. (2) We propose a novel dense video captioning approach, which models visual cues with Lexical-FCN, discovers region-sequence with submodular maximization, and decodes language outputs with sequence-tosequence learning. Although the approach is trained with weakly supervised signal, we show that informative and diverse captions can be produced. (3) We evaluate dense captioning results by measuring the performance gap to oracle results, and diversity of the dense captions. The results clearly verify the advantages of the proposed approach. Especially, the best single caption by the proposed approach outperforms the state-of-the-art results on the MSR-VTT challenge by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Multi-sentence description for videos has been explored in various works recently <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18]</ref>. Most of these works <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b36">37]</ref> focused on generating a long caption (story-like), which first temporally segmented the video with action localization <ref type="bibr" target="#b40">[41]</ref> or different levels of details <ref type="bibr" target="#b36">[37]</ref>, and then generated multiple captions for those segments and connected them with natural language processing techniques. However, these methods simply considered the temporally segmentation, and ignored the framelevel region attention and the motion-sequence of regionlevel objects. Yu et al. <ref type="bibr" target="#b53">[54]</ref> considered both the temporal and spatial attention, but still ignored the association or alignment of the sentences and visual locations. In contrast, this paper tries to exploit both the temporal and spatial region information and further explores the correspondence between sentences and region-sequences for more accurate modeling.</p><p>Lexical based CNN model is of great advantages over the ImageNet based CNN model <ref type="bibr" target="#b38">[39]</ref> in image/video captioning, since the ImageNet based CNN model only captures a limited number of object concepts, while the lexical based CNN model is able to capture all kinds of semantic concepts (nouns for objects and scenes, adjective for shape and attributes, verb for actions, etc). It is non-trivial to adopt/fine-tune the existing ImageNet CNN models with lexical output. Previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b18">19]</ref> have proposed several ways for this purpose. For instance, <ref type="bibr" target="#b6">[7]</ref> adopted a weakly supervised multiple instance learning (MIL) approach <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b55">56]</ref> to train a CNN based word detector without the annotations of image-region to words correspondence; and <ref type="bibr" target="#b0">[1]</ref> applied a multiple label learning (MLL) method to learn the CNN based mapping between visual inputs and multiple concept tags.</p><p>Sequence to sequence learning with long short-term memory (LSTM) <ref type="bibr" target="#b12">[13]</ref> was initially proposed in the field of machine translation <ref type="bibr" target="#b42">[43]</ref>. <ref type="bibr">Venugopalan et al. (S2VT)</ref>  <ref type="bibr" target="#b47">[48]</ref> generalized it to video captioning. Compared with contemporaneous works <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b29">30]</ref> which require additional temporal features from 3D ConvNets <ref type="bibr" target="#b44">[45]</ref>, S2VT can directly encode the temporal information by using LSTM on the frame sequence, and no longer needs the frame-level soft-attention mechanism <ref type="bibr" target="#b52">[53]</ref>. This paper adopts the S2VT model <ref type="bibr" target="#b47">[48]</ref> with a bi-directional formulation to improve the encoder quality, which shows better performance than the vanilla S2VT model in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our ultimate goal is to build a system that describes input videos with dense caption sentences. The challenges are two folds. First, we do not have fine-grained trainingdata annotations which link sentence captions to regionsequences. Second, we must ensure the generated sentences being informative and diverse. As discussed earlier, the proposed approach consists of three components (see <ref type="figure" target="#fig_2">Figure 2</ref>): lexical-FCN based visual model, region-sequence generation and language model. We elaborate each of them in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Lexical FCN Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Multi-instance Multi-label Lexical Model</head><p>We adopt multi-instance multi-label learning (MIMLL) to train our lexical model, which could be viewed as a combination of word detection <ref type="bibr" target="#b6">[7]</ref> (MIL) and deep lexical classification <ref type="bibr" target="#b0">[1]</ref> (MLL). <ref type="figure" target="#fig_3">Figure 3</ref> illustrates the comparison of the three methods.</p><p>Multi-instance learning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b6">7]</ref> assumes that the word label y w i is assigned to a bag of instances X i = {x i1 ,...,x ij } where i is the bag index, x ij 2 R d is a ddimensional feature vector for the j-th instance. The word detection method <ref type="bibr" target="#b6">[7]</ref> used fc7 features of VGG-16 as the instance representations. The bag is positive with a word label y w i =1if at least one of the instances in X i contains the word w, although it is not exactly known which one contains the word. The bag is negative with label y w i =0if no instance contains the word w.</p><p>Multi-label learning assumes that each instance x i has multiple word labels:</p><formula xml:id="formula_0">y i = {y 1 i ,...,y k i }</formula><p>where k is the number of labels. For this purpose, we usually train a deep neural network with a sigmoid cross-entropy loss <ref type="bibr" target="#b0">[1]</ref>.</p><p>Multi-instance multi-label learning <ref type="bibr" target="#b56">[57]</ref> is a natural generalization of MIL. It takes as input pairs {X i , y i }, where each X i is a bag of instances labeled with a set of words y i = {y</p><formula xml:id="formula_1">1 i ,...,y k i }.</formula><p>In MIMLL, each instance usually has one or multiple word labels. For instance, we can use "woman", "people", "human" or other synonyms in the lexicon to describe a female (see <ref type="figure" target="#fig_3">Figure 3</ref> for one example). Now we define the loss function for a bag of instances. As each bag has multiple word labels, we adopt the crossentropy loss to measure the multi-label errors:</p><formula xml:id="formula_2">L(X, y; θ)=− 1 N N X i=1 [y i · logp i +(1− y i ) · log(1 −p i )],<label>(1)</label></formula><p>where ✓ is the model parameters, N is the number of bags, y i is the label vector for bag X i , andp i is the corresponding probability vector. We weakly label the bag as negative when all instances in the bag are negative, and thus use a noisy-OR formulation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27]</ref> to combine the probabilities that the individual instances in the bag are negative:</p><formula xml:id="formula_3">p w i = P (y w i =1|X i ; θ)=1− Y x ij 2X i (1 − P (y w i =1|x ij ; θ)),<label>(2)</label></formula><p>wherep w i is the probability when word w in the i-th bag is positive. We define a sigmoid function to model the individual word probability:</p><formula xml:id="formula_4">P (y w i =1|x ij ; θ)=σ(wwx ij + bw),<label>(3)</label></formula><p>where w w is the weight matrices, b w is the bias vector, and σ(x)=1 /(1 + exp(−x)) is the logistic function. In our Lexical-FCN model, we use the last pooling layer (pool5 for ResNet-50) as the representation of instance x ij , which will be elaborated in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Details of Lexical-FCN</head><p>Lexical-FCN model builds the mapping between frame regions and lexical labels. The first step of Lexical-FCN is to build a lexical vocabulary from the video caption training set. We extract the part-of-speech <ref type="bibr" target="#b43">[44]</ref> of each word in the entire training dataset. These words may belong to any part of sentences, including nouns, verbs, adjectives and pronouns. We treat some of the most frequent functional words 1 as stop words, and remove them from the lexical vocabulary. We keep those remaining words appearing at least five times in the MSR-VTT training set, and finally obtain a vocabulary V with 6,690 words.</p><p>The second step of Lexical-FCN is to train the CNN models with MIMLL loss described above. Instead of training from scratch, we start from some state-of-the-art ImageNet models like VGG-16 <ref type="bibr" target="#b41">[42]</ref> or ResNet-50 <ref type="bibr" target="#b10">[11]</ref>, and fine-tune them with the MIMLL loss on the MS-VTT training set. For VGG-16, we re-cast the fully connected layers to convolutions layers to obtain a FCN. For ResNet-50, we remove final softmax layer and keep the last mean pooling layer to obtain a FCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Regions from Convolutional Anchors</head><p>In order to obtain the dense captions, we need grounding the sentences to sequences of ROI (regions of interest). Early solutions in object detection adopt region proposal algorithms to generate region candidates, and train a CNN model with an additional ROI pooling layer <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b35">36]</ref>. This cannot be adopted in our case, since we do not have the bounding box ground-truth for any words or concepts required in the training procedure. Instead, we borrow the idea from YOLO <ref type="bibr" target="#b34">[35]</ref>, and generate coarse region candidates from anchor points of the last FCN layer <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b6">7]</ref>. In both training and inference phases, we sample the video frames and resize both dimensions to 320 pixels. After feeding forward through the FCN, we get a 4⇥4 response map (4096 channels for VGG-16 and 2048 channels for ResNet-50). Each anchor point in the response map represents a region in the original frame. Unlike object detection approaches, the bounding-box regression process is not performed here since we do not have the ground-truth bounding boxes. We consider the informative region-sequence generation problem directly starting with these 16 verycoarse grid regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Region-Sequence Generation</head><p>Regions between different frames are matched and connected sequentially to produce region-sequences. As each frame has 16 coarse regions, even if each video clip is downsampled to 30 frames, we have to face a search space of size 16</p><p>30 for region-sequence generation. This is intractable for common methods even for the training case that has video-level sentence annotations. However, our Lexical-FCN model provides the lexical descriptions for each region <ref type="bibr" target="#b0">1</ref> Functional words are 'is', 'are', 'at', 'on', 'in', 'with', 'and' and 'to <ref type="bibr">'.</ref> at every frame, so that we can consider the problem from a different perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Problem Formulation</head><p>We formulate the region-sequence generation task as a sub-set selection problem <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9]</ref>, in which we start from an empty set, and sequentially add one most informative and coherent region at each frame into the subset, and in the meantime ensure the diversity among different regionsequences. Let S v denote the set of all possible region sequences of video v, A is a region-sequence sub-set, i.e., A✓S v . Our goal is to select a region-sequence A * , which optimizes an objective R:</p><formula xml:id="formula_5">A ⇤ =arg max A✓Sv R(xv, A),<label>(4)</label></formula><p>where x v are all region feature representations of video v. We define R(x v , A) as linear combination objectives</p><formula xml:id="formula_6">R(xv, A)=wv T f (xv, A),<label>(5)</label></formula><p>where</p><formula xml:id="formula_7">f =[f inf ,f div ,f coh ]</formula><p>T , which describe three aspects of the region-sequence, i.e., informative, diverse and coherent. The optimization problem of Eq-4 quickly becomes intractable when S v grows exponentially with the video length. We restrict the objectives f to be monotone submodular function and w v to be non-negative. This allows us to find a near optimal solution in an efficient way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Submodular Maximization</head><p>We briefly introduce submodular maximization and show how to learn the weights w v . A set function is called submodular if it fulfills the diminishing returns property. That means, given a function f and arbitrary sets A✓B✓ S v \ r, f is submodular if it satisfies:</p><formula xml:id="formula_8">f (A[{r}) − f (A) ≥ f (B[{r}) − f (B).<label>(6)</label></formula><p>Linear combination of submodular functions is still submodular for non-negative weights. For more details, please refer to <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b21">22]</ref>. Submodular functions have many properties that are similar to convex or concave functions, which are desirable for optimization. Previous works <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b8">9]</ref> have shown that maximizing a submodular function with a greedy algorithm yields a good approximation to the optimal solution. In this paper, we apply a commonly used cost-effective lazy forward (CELF) method <ref type="bibr" target="#b21">[22]</ref> for our purpose. We defined a marginal gain function as</p><formula xml:id="formula_9">L(wv; r)=R(A t−1 [{r}) − R(A t−1 ) = wv T f (xv, A t−1 [{r}) − wv T f (xv, A t−1 ).<label>(7)</label></formula><p>The CELF algorithm starts with an empty sequence A 0 = ;, and adds the region r t at step t into region-sequence which can maximize the marginal gain: where S t means region sets in frame-t.</p><formula xml:id="formula_10">A t = A t−1 [{rt}; rt =argmax r2S t L(wv; r),<label>(8)</label></formula><p>Given N pairs of known correspondences {(r, s)}, we optimize w v with the following objective:</p><formula xml:id="formula_11">min wv ≥0 1 N N X i=1 max r2r i L i (wv; r)+ λ 2 kwvk 2 ,<label>(9)</label></formula><p>where the max-term is a generalized hinge loss, which means ground-truth or oracle selected region r should have a higher score than any other regions by some margin. Our training data do not have (r, s) pairs, i.e., the sentence to region-sequence correspondence. We solve this problem in a way that is similar to the alternative directional optimization: (1) we initialize w v = 1 (all elements equals to 1); (2) we obtain a region-sequence with submodular maximization with that w v ; (3) we weakly associate sentence to region-sequence with a winner-takes-all (WTA) scheme (described later); (4) we refine w v with the obtained sentence to region-sequence correspondence; (5) we repeat step-2⇠4 until w v is converged.</p><p>The WTA scheme works in four steps when giving a ground-truth sentence s. First, we extract the lexical labels from s based on the vocabulary V, and form a lexical subset V s . Second, we obtain probability of word w 2V s for the i-th region-sequence by p is the probability of word w in the j-th frame, which is in fact from the Lexical-FCN output for each region. Third, we threshold p w i with a threshold ✓, i.e., redefining p w i =0 if p w i &lt;✓(✓ = 0.1 in our studies). Last, we compute the matching score by</p><formula xml:id="formula_12">f i = X w2Vs; p w i ≥θ p w i ,<label>(10)</label></formula><p>and obtain the best region-sequence by i * = arg max i f i . This objective suggests that we should generate regionsequences having high-scored words in the sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Submodular Functions</head><p>Based on the properties of submodular function <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28]</ref>, we describe how to define the three components as follows.</p><p>Informativeness of a region-sequence is defined as the sum of each region's informativeness:</p><formula xml:id="formula_13">f inf (xv, At)= X w p w ; p w =m a x i2A t p w i .<label>(11)</label></formula><p>If video-level sentence annotations are known either in the training case or by an oracle, we replace the definition with Eq-10, which limits words by the sentence vocabulary V s . Coherence aims to ensure the temporal coherence of the region-sequence, since significant changes of region contents may confuse the language model. Similar to some works in visual tracking <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref>, we try to select regions with the smallest changes temporally, and define the coherence component as</p><formula xml:id="formula_14">f coh = X rs2A t−1 hxr t , xr s i,<label>(12)</label></formula><p>where x rt is the feature of region r t at t-th step, x rs is one of the region feature in the previous (t − 1) steps, and h, i means dot-production operation between two normalized feature vectors. In practice, we also limit the search space of region r t within the 9 neighborhood positions of the region from the previous step. Diversity measures the degree of difference between a candidate region-sequence and all the existing regionsequences. Suppose {p</p><formula xml:id="formula_15">w i } N i=1</formula><p>are the probability distribution of the existing N region-sequences and q w is the probability distribution of a candidate region-sequence, the diversity is defined with the Kullback-Leibler divergence as</p><formula xml:id="formula_16">f div = N X i=1 Z w p w i log p w i q w dw.<label>(13)</label></formula><p>We initially pick the most informative region-sequence, and feed it to a language model (LM) for sentence output. Then we iteratively pick a region-sequence which maximizes diversity to generate multiple sentence outputs. <ref type="figure" target="#fig_4">Figure 4</ref> illustrates our region-sequence generation method. The detailed algorithm is given in the supplementary file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Language Models</head><p>We model the weakly associated temporal structure between region-sequence and sentence with the sequence-tosequence learning framework (S2VT) <ref type="bibr" target="#b47">[48]</ref>, which is an encoder-decoder structure. S2VT encodes visual feature of region-sequencesṼ =( v 1 , ··· , v T ) with LSTM, and decodes the visual representation into a sequence of output wordsũ =( u 1 , ··· ,u S ). LSTM is used to model a sequence in both the encoder part and the decoder part. As a variant of RNN, LSTM is able to learn long-term temporal information and dependencies that traditional RNN is difficult to capture <ref type="bibr" target="#b12">[13]</ref>. Our LSTM implementation is based on <ref type="bibr" target="#b54">[55]</ref> with dropout regularization on all LSTM units (dropout ratio 0.9).</p><p>We extend the original S2VT with bi-directional encoder, so that the S2VT learning in <ref type="figure" target="#fig_2">Figure 2</ref> stacks three LSTM models. The first LSTM encodes forward visual feature sequence {Ṽ }, and the second encodes the reverse visual feature sequence { Ṽ }. These two LSTM networks form the encoder part. We will show the benefit of bidirection LSTM encoding later. The third LSTM decodes visual codes from both the forward pass and backward pass into sequences of words (sentences).</p><p>To further improve accuracy, we propose a category-wise language model extension. Videos may belong to different categories, such as news, sports, etc. Different video category has very different visual patterns and sentence styles. The category-wise language model is defined as</p><formula xml:id="formula_17">s ⇤ =argmaxs P (s|c, v)P (c|v),<label>(14)</label></formula><p>where c is the category label, v is the video feature representation, and s is the predicted sentence. P (s|c, v) is the probability conditional on category c and video v, and P (c|v) is prior confidence of video v belongs to a category c, which can be obtained from a general video categorization model. The category-wise language model can be viewed as max-a-posterior estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Implementation Details</head><p>We conduct experiments on the MSR-VTT dataset <ref type="bibr" target="#b50">[51]</ref>, which is a recently released large-scale video caption benchmark. This dataset contains 10,000 video clips (6,513 for training, 497 for validation and 2,990 for testing) from 20 categories, including news, sports, etc. Each video clip is manually annotated with 20 natural sentences. This is currently the largest video captioning dataset in terms of the amounts of sentences and the size of the vocabulary. Although this dataset was mainly used for evaluating single sentence captioning results, we assume that the 20 sentences for each clip contain very diversified annotations and can be used in the task of dense captioning (with some redundancy as will be discussed later).</p><p>For the evaluation of single captioning, the authors of this benchmark proposed machine translation based metrics like METEOR <ref type="bibr" target="#b20">[21]</ref>, BLEU@1-4 <ref type="bibr" target="#b31">[32]</ref>, ROUGE-L <ref type="bibr" target="#b22">[23]</ref> and CIDEr <ref type="bibr" target="#b45">[46]</ref>. For dense video captioning results, we propose our own evaluation protocol to justify the results.</p><p>All the training and testing are done on an Nvidia TitanX GPU with 12GB memory. Our model is efficient during the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Studies on Single Sentence Captioning</head><p>We first evaluate the effect of several design components through single sentence captioning experiments, which produce a caption with the maximal informative score defined by Eq-11 (i.e.,ŝ 0 in <ref type="figure" target="#fig_4">Figure 4</ref>). Effectiveness of Network Structure. We compare VGG-16 and ResNet-50 for the Lexical-FCN model. Due to the GPU memory limitation, we do not try a deeper network like ResNet-152. <ref type="figure" target="#fig_6">Figure 5</ref> shows that ResNet-50 achieves better training loss than VGG-16, which is consistent with their results on ImageNet. <ref type="table">Table 1</ref> summarizes the single sentence captioning results on the MSR-VTT validation set by two networks. As can be seen, in all the cases, ResNet-50 performs better than VGG-16. Based on these results, we choose ResNet-50 as our network structure in the following studies when there is no explicit statement.</p><p>Effectiveness of Bi-directional Encoder. Next we compare the performances of bi-directional and unidirectional S2VT models for language modeling. Results are also shown in <ref type="table">Table 1</ref>. It is obvious that bi-directional model outperforms unidirectional model on all the evaluated metrics. The benefit of bi-directional model is not that significant. We conjecture that this is due to the fact that the region-sequences already include enough temporal and local information. Nevertheless, for better accuracy, all the following studies adopt the bi-directional model.</p><p>Effectiveness of MIMLL. Our Lexical-FCN model is trained on video frames. Compared with image-level lexical learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b0">1]</ref>, our setting is much more challenging since the sentences are on the video-level, and it is hard to determine which words correspond to which frames. Here we show the effectiveness of the MIMLL in two aspects. First, we compare the single captioning results by MIMLL and MIL in <ref type="table" target="#tab_1">Table 2</ref>. We can see that MIMLL achieves better accuracy than MIL on all the four metrics. Second, we compare the word detection accuracy of MIMLL and MIL. We first compute the max-probability of each word within the region-sequence. If the max-probability of a word is greater than a threshold (0.5), we claim that the word is detected. We observe that MIMLL is better in detecting accuracy than MIL in this study (43.1% vs 41.3%). Both results demonstrate the effectiveness of the proposed MIMLL for the Lexical-FCN model.</p><p>Effectiveness of Category-wise Language Model. All the previous studies are based on language model without using video category information. Here, we study the benefit of the category-wise language model, as defined in Eq-14. Results are shown in the 2nd last and the 3rd last rows in <ref type="table" target="#tab_3">Table 3</ref>. We observe that the category-wise language model achieves much better accuracy than that without categorywise modeling. The benefit is due to that category information provides a strong prior about video content.</p><p>Comparison with State-of-the-arts. We also compare our single sentence captioning results with the state-of-theart methods in MSR-VTT benchmark. For better accuracy, this experiment adopts data augmentation during the training procedure, similar to the compared methods. We preprocess each video clip to 30-frames with different sampling strategies (random, uniform, etc), and obtain multiple instances for each video clip.</p><p>We first compare our method with mean-pooling <ref type="bibr" target="#b48">[49]</ref>, soft-attention <ref type="bibr" target="#b52">[53]</ref> and S2VT <ref type="bibr" target="#b47">[48]</ref> on the validation set of MSR-VTT. All these alternative methods have source codes available for easy evaluation. Results are summarized in <ref type="table" target="#tab_3">Table 3</ref>. Our baseline approach (the 2nd last row) is significantly better than these 3 methods. We also compare with the top-4 results from the MSR-VTT challenge in the table, including v2t navigator <ref type="bibr" target="#b14">[15]</ref>, Aalto <ref type="bibr" target="#b39">[40]</ref>, VideoLAB <ref type="bibr" target="#b33">[34]</ref> and ruc uva <ref type="bibr" target="#b5">[6]</ref> 2 , which are all based on features from multiple cues such as action features like C3D and audio features like Bag-of-Audio-Words (BoAW) <ref type="bibr" target="#b30">[31]</ref>. Our baseline has on-par accuracy to the state-of-the-art methods. For fair comparison, we integrate C3D action features and audio features together with our lexical features and feed them into the language model. Clearly better results are observed.</p><p>In <ref type="table" target="#tab_4">Table 4</ref>, we compare our results on the test set of MSR-VTT with the top-4 submissions in the challenge leaderboard, where we can see that similar or better results are obtained in all the four evaluated metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation of Dense Captioning Results</head><p>The proposed approach can produce a set of regionsequences with corresponding multiple captions for an input video clip. Besides qualitative results in <ref type="figure" target="#fig_1">Figure 1</ref> and the supplementary file, we evaluate the results quantitatively in two aspects: 1) performance gap between automatic results and oracle results, and 2) diversity of the dense captions.</p><p>2 http://ms-multimedia-challenge.com/.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Performance Gap with Oracle Results</head><p>We measure the quality of dense captioning results by the performance gap between our automatic results and oracle results. Oracle leverages information from ground-truth sentences to produce the caption results. Oracle information could be incorporated in two settings. First, similar to the training phase, during inference oracle uses the groundtruth information to guide sentence to region-sequence association. Second, oracle uses the ground-truth sentences to measure the goodness of each caption sentence using metrics like METEOR and CIDEr, and re-ranks the sentences according to their evaluation scores. It is obvious that the oracle results are the upper bound of the automatic method. Inspired by the evaluation of dense image captioning <ref type="bibr" target="#b15">[16]</ref>, we use averaged precision (AP) to measure the accuracy of dense video captioning. We compute the precision in terms of all the four metrics (METEOR, BLEU@4, ROUGE-L and CIDEr) for every predicted sentence, and obtain average values of the top-5 and top-10 predicted sentences. The gap of AP values between oracle results and our automatic results will directly reflect the effectiveness of the automatic method.</p><p>For our automatic method, the output sentences need to be ranked to obtain the top-5 or top-10 sentences. Similar to <ref type="bibr" target="#b39">[40]</ref>, we train an evaluator network in a supervised way for this purpose, since submodular maximization does not ensure that sentences are generated in quality decreasing order. <ref type="table">Table 5</ref> lists the comparative results on the validation set of MSR-VTT using three strategies: (1) oracle for both sentence to region-sequence association and sentence re-ranking (OSR + ORE in short); (2) dense video captioning + oracle re-ranking (Dense + ORE in short); (3) fully automatic dense video captioning method (DenseVidCap).</p><p>Results indicate that the dense video captioning + ora-  <ref type="table">Table 5</ref>: Averaged precision of the top-5/10 sentences generated on the validation set of MSR-VTT. OSR means oracle for sentence to region-sequence association, and ORE means oracle for sentence re-ranking. The values in the parenthesis indicate the relative percentage (%) to the fully oracle results (OSR+ORE).</p><p>cle re-ranking could reach ≥95% relative accuracy of the "fully" oracle results (OSR+ORE) on all the metrics for the top-5 sentences, and ≥93% relative accuracy to the fully oracle results for the top-10 sentences. The fully automatic method (our DenseVidCap) can consistently achieve more than 82% relative accuracy of the oracle results on both top-5 and top-10 settings. This is very encouraging as the performance gap is not very large, especially considering that our model is trained with weakly annotated data. One important reason that causes the gap is that the evaluator network is not strong enough when compared with oracle re-ranking, which is a direction for further performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Diversity of Dense Captions</head><p>The diversity of the generated captions is critical for dense video captioning. We evaluate diversity from its oppositethe similarity of the captions. A common solution is to determine the similarity between pairs of captions, or between one caption to a set of other captions. Here we consider similarity from the apparent semantic relatedness of the sentences. We use the Latent semantic analysis (LSA) <ref type="bibr" target="#b3">[4]</ref> which first generates sentence bag-of-words (BoW) representation, and then maps it to LSA space to represent a sentence. This method has demonstrated its effectiveness in measuring document distance <ref type="bibr" target="#b19">[20]</ref>. Based on the representation, we compute cosine similarity between two LSA vectors of sentences. Finally, the diversity is calculated as:</p><formula xml:id="formula_18">D div = 1 n X s i ,s j 2S; i6 =j (1 −hs i , s j i),<label>(15)</label></formula><p>where S is the sentence set with cardinality n, and hs i , s j i denotes the cosine similarity between s i and s j . As aforementioned, we assume that the multiple videolevel captions cover diversified aspects of the video content with some redundancy. The diversity metric can be applied in two aspects: evaluating the diversity degree of (1) our dense captioning results and (2) the manually generated captions in the ground-truth. Some of the manually annotated ground-truth sentences on MSR-VTT are redun- dant. For instance, the captions "a woman is surfing" and "a woman surfing in the ocean" are more or less the same. We remove the redundant captions by clustering on each video caption set with the LSA based representation. Different clustering numbers can lead to different diversity scores. As shown in <ref type="figure" target="#fig_7">Figure 6</ref>(a), five clusters give the highest diversity score on this dataset. We compare the diversity score of our automatic results with that of the ground-truth sentences in <ref type="figure" target="#fig_7">Figure 6</ref>(b). As can be seen, our DenseVidCap achieves better diversity score (0.501) than that of the original 20 ground-truth sentences (0.463), but is slightly worse than that of the best of the clustered ground-truth sentences (0.569). Please refer to <ref type="figure" target="#fig_1">Figure 1</ref> and the supplementary file for some qualitative dense video captioning results. Both the diversity score and the qualitative results confirm that our proposed approach could produce diversified captioning output.</p><p>Through the comparison with the oracle results and the diversity evaluation in this subsection, we have demonstrated that our method can indeed produce good dense captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a weakly supervised dense video captioning approach, which is able to generate multiple diversified captions for a video clip with only video-level sentence annotations during the training procedure. Experiments have demonstrated that our approach can produce multiple informative and diversified captions. Our best single caption output outperforms the state-of-the-art methods on the MSR-VTT challenge with a significant margin. Future work may consider leveraging the context among the dense captions to produce a consistent story for the input video clips.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>⇤</head><label></label><figDesc>This work was done when Zhiqiang Shen was an intern at Intel Labs China. Jianguo Li and Yu-Gang Jiang are the corresponding authors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of dense video captioning (DenseVidCap). Each region-sequence is highlighted in white bounding boxes along with corresponding predicted sentence in its bottom. The ground-truth sentences are presented on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of our Dense Video Captioning framework. In the language model, &lt;BOS&gt; denotes the begin-of-sentence tag and &lt;EOS&gt; denotes the end-of-sentence tag. We use zeros as &lt;pad&gt; when there is no input at the time step. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Three paradigms of learning a lexical model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of region-sequence generation. r j i is the j-th region-sequence in i-th frame and 'LM' denotes language model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The lexical training loss on the MSR-VTT dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: (a) Diversity score of clustered ground-truth captions under different cluster numbers; (b) Diversity score comparison of our automatic method (middle) and the ground-truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Table 1: Single sentence captioning accuracy by bi-/uni- directional encoder on the validation set of MSR-VTT.</figDesc><table>Model 
METEOR BLEU@4 ROUGE-L CIDEr 
Unidirectional (VGG-16) 
25.2 
32.7 
56.0 
31.1 
Bi-directional (VGG-16) 
25.4 
32.8 
56.5 
32.9 
Unidirectional (ResNet-50) 
25.7 
32.1 
56.4 
32.5 
Bi-directional (ResNet-50) 
25.9 
33.7 
56.9 
32.6 

Model 
METEOR BLEU@4 ROUGE-L CIDEr 
MIL (bi-directional) 
23.3 
28.7 
53.1 
24.4 
MIMLL (bi-directional) 
25.9 
33.7 
56.9 
32.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Single sentence captioning accuracy by MIL and MIMLL on the validation set of MSR-VTT.</figDesc><table>testing stage. It can process a 30-frame video clip in about 
840ms on the TitanX GPU, including 570ms for CNN fea-
ture extraction, 90ms for region-sequence generation, and 
180ms for language model. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Comparison with state of the arts on the validation set of MSR-VTT dataset. See texts for more explanations.</figDesc><table>Model 
METEOR BLEU@4 ROUGE-L CIDEr 
ruc-uva [6] 
26.9 
38.7 
58.7 
45.9 
VideoLAB [34] 
27.7 
39.1 
60.6 
44.1 
Aalto [40] 
26.9 
39.8 
59.8 
45.7 
v2t navigator [15] 
28.2 
40.8 
60.9 
44.8 
Ours 
28.3 
41.4 
61.1 
48.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Comparison with state of the arts on the test set of MSR- VTT dataset. See texts for more explanations.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>Dense + ORE 28.0 (95.6) 40.8 (96.5) 62.8 (97.9) 41.9 (96.5) DenseVidCap 26.5 (90.4) 34.8 (82.3) 57.7 (90.0) 37.3 (85.9) Averaged Precision of Top-10 Sentences</figDesc><table>Model 

METEOR BLEU@4 ROUGE-L 
CIDEr 
Averaged Precision of Top-5 Sentences 
OSR + ORE 
29.3 (100) 
42.3 (100) 
64.1 (100) 
43.4 (100) 
OSR + ORE 
27.9 (100) 
38.8 (100) 
61.4 (100) 
39.1 (100) 
Dense + ORE 26.4 (94.6) 36.6 (94.3) 59.5 (96.9) 36.6 (93.6) 
DenseVidCap 26.1 (93.5) 33.6 (86.6) 57.1 (93.0) 35.3 (90.3) 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Yu-Gang Jiang and Xiangyang Xue were supported in part by three NSFC projects (#U1611461, #61622204 and #61572138) and a grant from STCSM, Shanghai, China (#16JC1420401).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep compositional captioning: Describing novel object categories without paired training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visual tracking with online multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">391</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Anne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Early embedding and late reranking for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia Grand Challenge</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video summarization by learning submodular mixtures of objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A tractable inference algorithm for diagnosing multiple diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1304.1511</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust online appearance models for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Jepson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>El-Maraghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T PAMI</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Describing videos using multi-modal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia Grand Challenge</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human focused video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">U G</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gotoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Understanding and generating simple image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IEEE T PAMI</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">From word embeddings to document distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Meteor universal: language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cost-effective outbreak detection in networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-04 workshop on Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Submodular functions and convexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lovász</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Programming The State of the Art</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1983" />
			<biblScope unit="page" from="235" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6632</idno>
		<title level="m">Deep captioning with multimodal recurrent neural networks (m-rnn)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A framework for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An analysis of approximations for maximizing submodular set functionsi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Nemhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Wolsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural encoder for video representation with application to captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Jointly modeling embedding and translation to bridge video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Softening quantization in bag-ofaudio-words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pancoast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Akbacak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICASSP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multimodal video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanishka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia Grand Challenge</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">You only look once: Unified, realtime object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Coherent multi-sentence video description with variable level of detail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The long-short story of movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Frame-and segment-level features and candidate pool evaluation for video caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04959</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Beyond caption to narrative: Video captioning with multiple sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ohnishi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05440</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07770</idno>
		<title level="m">Captioning images with diverse objects</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Translating videos to natural language using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<title level="m">Neural image caption generation with visual attention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Video paragraph captioning using hierarchical recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multiple instance boosting for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning with application to scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
