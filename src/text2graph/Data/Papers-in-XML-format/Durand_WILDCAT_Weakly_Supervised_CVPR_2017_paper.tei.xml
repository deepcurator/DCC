<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WILDCAT: Weakly Supervised Learning of Deep ConvNets for Image Classification, Pointwise Localization and Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Durand</surname></persName>
							<email>thibaut.durand@lip6.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 7606</orgName>
								<orgName type="institution" key="instit1">Sorbonne Universités</orgName>
								<orgName type="institution" key="instit2">UPMC Univ Paris 06</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<addrLine>4 place Jussieu</addrLine>
									<postCode>LIP6, 75005</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">)</forename><forename type="middle">⋆</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Mordan</surname></persName>
							<email>taylor.mordan@lip6.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 7606</orgName>
								<orgName type="institution" key="instit1">Sorbonne Universités</orgName>
								<orgName type="institution" key="instit2">UPMC Univ Paris 06</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<addrLine>4 place Jussieu</addrLine>
									<postCode>LIP6, 75005</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Thales Optronique S.A.S</orgName>
								<address>
									<addrLine>2 Avenue Gay Lussac, 78990Élancourt</addrLine>
									<country>France (</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
							<email>nicolas.thome@lip6.fr</email>
							<affiliation key="aff2">
								<orgName type="institution">CEDRIC -Conservatoire National des Arts et Métiers</orgName>
								<address>
									<addrLine>292 rue St Martin</addrLine>
									<postCode>75003</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
							<email>matthieu.cord@lip6.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 7606</orgName>
								<orgName type="institution" key="instit1">Sorbonne Universités</orgName>
								<orgName type="institution" key="instit2">UPMC Univ Paris 06</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<addrLine>4 place Jussieu</addrLine>
									<postCode>LIP6, 75005</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WILDCAT: Weakly Supervised Learning of Deep ConvNets for Image Classification, Pointwise Localization and Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over the last few years, deep learning and Convolutional Neural Networks (CNNs) have become state-of-theart methods for visual recognition, including image classification <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b27">28]</ref>, object detection <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b9">10]</ref> or semantic segmentation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b8">9]</ref>. CNNs often require a huge number of training examples: a common practice is to use models pre-trained on large scale datasets, e.g. ImageNet <ref type="bibr" target="#b52">[53]</ref>, and to fine tune them on the target domain.</p><p>Regarding spatial information, there is however a large shift between ImageNet, which essentially contains centered objects, and other common datasets, e.g. VOC or MS COCO, containing several objects and strong scale and translation variations. To optimally perform domain adaptation in this context, it becomes necessary to align informative image regions, e.g. by detecting objects <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b28">29]</ref>, parts <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b34">35]</ref> or context <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b12">13]</ref>. Although some works incorporate more precise annotations during training, e.g. bounding boxes <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b20">21]</ref>, the increased annotation cost prevents its widespread use, especially for large datasets and pixel-wise labeling, i.e. segmentation masks <ref type="bibr" target="#b2">[3]</ref>. In this paper, we propose WILDCAT (Weakly supervIsed Learning of Deep Convolutional neurAl neTworks), a method to learn localized visual features related to class modalities, e.g. heads or legs for a dog -see <ref type="figure" target="#fig_0">Figure 1</ref>(c) and 1(d). The proposed model can be used to perform image classification as well as weakly supervised pointwise object localization and segmentation <ref type="figure" target="#fig_0">(Figure 1(b)</ref>).</p><p>The overall architecture of WILDCAT ( <ref type="figure">Figure 2</ref>) improves existing deep Weakly Supervised Learning (WSL) models at three major levels. Firstly, we make use of the latest Fully Convolutional Networks (FCNs) as back-end module, e.g. ResNet <ref type="bibr" target="#b27">[28]</ref> (left of <ref type="figure">Figure 2</ref>). FCNs have recently shown outstanding preformances for fully super- <ref type="figure">Figure 2</ref>. WILDCAT architecture. It is based on FCN ResNet-101 to extract local features from whole images with good spatial resolution (Section 3.1). All regions are encoded into multiple class modalities with a WSL multi-map transfer layer (Section 3.2). Feature maps are then combined separately to yield class-specific heatmaps that can be globally pooled to get a single probability for each class, using a new spatial aggregation module (Section 3.3). WILDCAT is trained with image-level labels in a WSL way and is applied to complex scene understanding, WSL object detection and semantic segmentation (Section 3.4).</p><p>vised object detection <ref type="bibr" target="#b9">[10]</ref> and semantic segmentation <ref type="bibr" target="#b8">[9]</ref>, and we adapt their ability to preserve spatial information in our WSL context. Secondly, we incorporate a new multi-map WSL transfer layer (middle of <ref type="figure">Figure 2</ref>), which explicitly learns multiple localized features related to complementary class modalities, e.g. head and legs for dogs in <ref type="figure" target="#fig_0">Figure 1</ref>. Our multimap strategy is not specifically designed for any particular kind of feature, e.g. part or view-based features, as some approaches are <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>Finally, we address the problem of aggregating spatial scores into a global prediction, which is a crucial issue for WSL training. We propose a new pooling strategy (right of <ref type="figure">Figure 2</ref>) which generalizes several approaches in the literature, including (top) max pooling <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b38">39]</ref>, global average pooling <ref type="bibr" target="#b69">[70]</ref> or negative evidence models <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>We also present a thorough evaluation of the WILDCAT model on six datasets, reporting outstanding performances on classification, WSL pointwise detection and segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Despite excellent performances, deep ConvNets <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b27">28]</ref> carry limited invariance properties, i.e. small shift invariance through pooling layers <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b6">7]</ref>. This is questionable for object or scene databases with strong scale and translation variations. One option to detect informative image regions is to revisit the Bag of Words (BoW) model <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b1">2]</ref>, by using deep features as local region activations <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24]</ref> or by designing specific BoW layers, e.g. NetVLAD <ref type="bibr" target="#b0">[1]</ref>.</p><p>Another option to gain strong invariance is to consider a Weakly Supervised Learning framework (WSL), where we can explicitly align image regions. An important paradigm for WSL is Multiple Instance Learning (MIL) <ref type="bibr" target="#b10">[11]</ref>, which considers an image as a bag of instances (regions). The main issue concerns the aggregation function to pool instance scores into a global prediction. Different strategies have been explored to combine deep models and MIL. Max pooling <ref type="bibr" target="#b43">[44]</ref> only selects the most informative region for the MIL prediction. Recent alternatives include Global Average Pooling (GAP) <ref type="bibr" target="#b69">[70]</ref>, soft max in LSE pooling <ref type="bibr" target="#b57">[58]</ref>, Learning from Label Proportion (LLP) <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b35">36]</ref>, and top max scoring <ref type="bibr" target="#b38">[39]</ref>. Negative evidence models <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> explicitly select regions accounting for the absence of the class. In WILDCAT, we propose to incorporate negative evidence insights, but with a differentiate positive and negative contribution process.</p><p>Concerning the WSL localization task, <ref type="bibr" target="#b4">[5]</ref> uses label co-occurrence information and a coarse-to-fine strategy based on deep feature maps to predict object locations. ProNet <ref type="bibr" target="#b57">[58]</ref> uses a cascade of two networks: the first generates bounding boxes and the second classifies them. Similarly, <ref type="bibr" target="#b5">[6]</ref> proposes an specific architecture with two branches dedicated to classification and detection. Another important WSL application is segmentation. Many methods are based on MIL framework: MIL-FCN <ref type="bibr" target="#b48">[49]</ref> extends MIL to multi-class segmentation, MIL-Base <ref type="bibr" target="#b49">[50]</ref> introduces a soft extension of MIL, EM-Adapt <ref type="bibr" target="#b44">[45]</ref> includes an adaptive bias into the MIL framework, and Constrained CNN (CCNN) <ref type="bibr" target="#b47">[48]</ref> uses a loss function optimized for any set of linear constraints on the output space of a CNN.</p><p>Similarly to WSL, the attention-based models <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b63">64]</ref> select relevant regions to support decisions. However the WSL methods usually include some structure on the selection process while it is implicit in attention-based approaches.</p><p>Different semantic categories are often characterized by multiple localized attributes corresponding to different class modalities (see for example head and legs for the dog class in <ref type="figure" target="#fig_0">Figure 1)</ref>. The seminal DPM model <ref type="bibr" target="#b15">[16]</ref> including several template regions for decision has been extensively studied <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b45">46]</ref>, optionally incorporating priors, e.g. sparsity or diversity, in order to learn sensible models <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b58">59]</ref>. While <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b59">60]</ref> are direct generalizations of DPM to CNN, R-FCN <ref type="bibr" target="#b9">[10]</ref> improves performances by explicitly learning several part models and using a part-based pooling of features designed for accurate spatial localization and directly inserted at the top of the network. MR-CNN <ref type="bibr" target="#b18">[19]</ref> exploits several modalities by modeling objects with a fixed set of few local features (e.g. parts, context) and incorporating segmentation cues. Combining different regions has also recently been addressed through explicit context modeling <ref type="bibr" target="#b22">[23]</ref>, or by modeling region correlations as in RRSVM <ref type="bibr" target="#b60">[61]</ref>. For fine-grained recognition, multi-feature detection has been tackled in the fully supervised setting <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b67">68]</ref>, and in WSL <ref type="bibr" target="#b32">[33]</ref>.</p><p>When computing local features with deep models, the most naive approach is to rescale each region into a fixedsize vector adapted to the CNN architecture, as done in early works for detection, e.g. R-CNN <ref type="bibr" target="#b20">[21]</ref>, or scene understanding <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b11">12]</ref>. Since this approach is highly inefficient, there have been extensive attempts for using convolutional layers to share feature computation, for image classification <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b69">70]</ref>, object detection <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b51">52]</ref> or image segmentation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42]</ref>. However, fully connected layers are beneficial in standard deep architectures, e.g. AlexNet <ref type="bibr" target="#b33">[34]</ref> or VGG <ref type="bibr" target="#b55">[56]</ref>. Recently, the huge success of Fully Convolutionnal Networks (FCNs) for image classification, e.g. ResNet <ref type="bibr" target="#b27">[28]</ref>, has been driving successful approaches using FCN for fully supervised object detection <ref type="bibr" target="#b9">[10]</ref> and image segmentation <ref type="bibr" target="#b8">[9]</ref>, which enable complete feature sharing and state-of-the-art performances. Our approach adapts these insights from these latest FCNs to the WSL setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">WILDCAT Model</head><p>The overall WIDLCAT architecture ( <ref type="figure">Figure 2</ref>) is based on a FCN which is suitable for spatial predictions <ref type="bibr" target="#b41">[42]</ref>, a multi-map WSL transfer layer encoding modalities associated with classes, and a global pooling for WSL that learns accurate localization. We now delve into each of the three parts of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fully convolutional architecture</head><p>The selection of relevant information within feature maps is a major issue in WSL. It impacts the localization of the learned representation and the precision of the results (e.g. semantic segmentation or object detection). We thus expect the resolution of the feature maps to be a key component for WILDCAT: finer maps keep more spatial resolution and lead to more specific regions (e.g. objects, parts).</p><p>To this end we exploit the recently introduced FCN ResNet-101 <ref type="bibr" target="#b27">[28]</ref> (left of <ref type="figure">Figure 2</ref>) that naturally preserves spatial information throughout the network. It also computes local features from all the regions in a single forward pass, without resizing them. Besides, ResNet architectures are effective at image classification while being parameterand time-efficient <ref type="bibr" target="#b27">[28]</ref>. This kind of architecture has been exploited to speed up computation and to produce accurate spatial predictions in fully supervised setups, e.g. in object detection <ref type="bibr" target="#b9">[10]</ref> and semantic segmentation <ref type="bibr" target="#b8">[9]</ref>.</p><p>We use the publicly released model pre-trained on ImageNet dataset <ref type="bibr" target="#b52">[53]</ref> and remove the last layers (global average pooling and fully connected) to replace them with WSL transfer and wildcat pooling layers ( <ref type="figure" target="#fig_1">Figure 3</ref>) described in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-map transfer layer</head><p>We introduce a multi-map WSL transfer layer that learns multiple class-related modalities, encoded into M feature maps per class through 1 × 1 convolutions (middle of <ref type="figure">Figure 2)</ref>. The modalities are learned in a WSL fashion with only the image-level labels and the transfer layer keeps spatial resolution, key in WSL. We note w × h × d the size of conv5 maps of ResNet-101, which is The M modalities aim at specializing to different classspecific features, e.g. parts <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> (head and legs of dog in <ref type="figure" target="#fig_0">Figure 1</ref>) or views <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref>. We highlight differences with some specific encoding approaches: position-sensitive RoI pooling in R-FCN <ref type="bibr" target="#b9">[10]</ref> forces position-based specialization (relative to the object) while our method can also learn other kind of features, e.g. semantic parts <ref type="figure" target="#fig_0">(Figure 1</ref>). In the same way DPM <ref type="bibr" target="#b15">[16]</ref> learns only discriminating parts where our multi-map transfer model can find more general features, e.g. context. Furthermore, contrarily to the DPM where a different model is learned for each view, we share most of the computation within the FCN, which is more efficient. We note that when M = 1 this reduces to a standard classification layer, i.e. into C classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Wildcat pooling</head><p>WILDCAT learns from image-level labels so we need a way to summarize all information contained in the feature maps for each class (right of <ref type="figure">Figure 2</ref>). We note that there are no more learned parameters in this pooling layers, which means we can directly interpret and visualize feature maps at this level <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>We perform this in two steps ( <ref type="figure" target="#fig_1">Figure 3</ref>): a class-wise pooling (Equation <ref type="formula" target="#formula_0">(1)</ref>) that combines the M maps from the multi-map transfer layer, then a spatial pooling module (Equation <ref type="formula" target="#formula_1">(2)</ref>) that selects relevant regions within the maps to support predictions. This leads to wildcat pooling, a two-stage pooling operation to compute the score s c of class c:</p><formula xml:id="formula_0">    z c i,j = Cl. Pool m∈{1,...,M } z c,m i,j s c = Sp. Pool (i,j)∈{1,...,w}×{1,...,h}z c i,j<label>(1)</label></formula><p>where z is the output of the transfer layer, Cl. Pool is the chosen class-wise pooling function and Sp. Pool is the spatial aggregation process.</p><p>Class-wise pooling. The first step consists in combining the M maps for all classes independently, and is described in Equation <ref type="formula" target="#formula_0">(1)</ref> with a generic pooling function Cl. Pool. We use average pooling in the following. The maps are transformed from w × h × M C to w × h × C <ref type="figure" target="#fig_1">(Figure 3</ref>). When M = 1 this operation is not needed as each class is already represented by a single map. We note that even if a multi-map followed by an average pooling is functionally equivalent to a single convolution (i.e. M = 1), the explicit structure it brings with M modalities has important practical advantages making training easier. We empirically show that M &gt; 1 yields better results than regular M = 1.</p><p>Spatial pooling. We now introduce our new spatial aggregation method implementing the second, spatial pooling step in Equation <ref type="formula" target="#formula_1">(2)</ref> for each map c: s c = max <ref type="table">Table 1</ref>. Generalization of wildcat spatial pooling to other existing MIL approaches with corresponding parameters. n is the total number of regions, ρ is the proportion of positive labels in LLP, k is an arbitrary number of regions to choose.</p><formula xml:id="formula_2">h∈H k + 1 k + i,j h i,jz c i,j +α   min h∈H k − 1 k − i,j h i,jz c i,j   (3) k + k − α Pooling 1 0 0 Maximum [44] k / ρn 0 0 Top instances [39] / LLP [65] n 0 0 Average [70] k k 1 WELDON [13]</formula><p>where H k is such that h ∈ H k satisfies h i,j ∈ {0, 1} and</p><formula xml:id="formula_3">i,j h i,j = k. It</formula><note type="other">consists in selecting for each class c the k + (resp. k − ) regions with the highest (resp. lowest) activations from inputz c . The output s c for class c of this layer is the weighted average of scores of all the selected regions. We only consider regions defined by single neurons in the convolutional feature maps.</note><p>Several similar MIL approaches have been used but our proposed model generalizes them in numerous of ways. The corresponding parameters are described in <ref type="table">Table 1</ref>. The standard max-pooling MIL approach <ref type="bibr" target="#b43">[44]</ref> is obtained with only one element, and both top instance model <ref type="bibr" target="#b38">[39]</ref>, Learning with Label Proportion <ref type="bibr" target="#b64">[65]</ref> and global average pooling <ref type="bibr" target="#b69">[70]</ref> can be obtained with more. Drawing from negative evidence <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> we can incorporate minimum scoring regions to support classification and our spatial pooling function can reduce to the kMax+kMin layer of <ref type="bibr" target="#b12">[13]</ref>.</p><p>Maximum and minimum scoring regions both are important for good results <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, but do not bring the same kind of information. We explore relative weighting of both types of regions by introducing a factor α which trades off relative importance between both terms. We hypothesize that maximum scoring regions are more useful for classification as they directly support the decision, while minimum scoring regions essentially act as regularization. With α &lt; 1 WILDCAT should focus more on discriminating regions and then better localize features than with α = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>WILDCAT architecture is composed of a transfer layer followed by pooling. Since there are no parameters to learn in the pooling module, the transfer layer performs classification and it is easy to visualize heatmaps with direct localization of discriminating regions. We note that this kind of architecture is reversed in <ref type="bibr" target="#b69">[70]</ref> where pooling is performed before the last fully connected layer, as in the original ResNet architecture <ref type="bibr" target="#b27">[28]</ref> for example. However this order requires an unnatural way of visualizing class-specific heatmaps <ref type="bibr" target="#b69">[70]</ref>.</p><p>It is shown in <ref type="bibr" target="#b69">[70]</ref> that if the spatial aggregation method is linear, e.g. global average pooling, then the order of both layers is not important, but the two configurations can behave differently with a non linear pooling function such as wildcat spatial pooling. The difference is more significant when k + + k − is low, i.e. when wildcat spatial pooling really differs from global average pooling. We evaluate the impact of this design choice and of the chosen pooling function in the experiments and show that our architecture yields better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">WILDCAT applications</head><p>Training phase. Our WILDCAT model is based on the backbone architecture ResNet-101 <ref type="bibr" target="#b27">[28]</ref>. We initialize it from a model pre-trained on ImageNet <ref type="bibr" target="#b52">[53]</ref> and train it with Stochastic Gradient Descent (SGD) with momentum with image-level labels only. All the layers of the network are fine tuned. The input images are warped to a square size at a given scale. We use a multi-scale setup where a different model is learned for each scale and they are combined with Object Bank <ref type="bibr" target="#b37">[38]</ref> strategy.</p><p>WILDCAT is designed to learn from image-level supervision only: the same training procedure is used for image classification, weakly supervised pointwise object detection and weakly supervised semantic segmentation. When learning WILDCAT, the gradients are backpropagated through the wildcat layer only within the k + + k − selected regions, all other gradients being discarded <ref type="bibr" target="#b12">[13]</ref>. The selection of right regions for backpropagation is key to learn precisely localized features without any spatial supervision <ref type="bibr" target="#b57">[58]</ref>.</p><p>Inference phase. Predictions differ according to the task at hand. For image classification, prediction simply takes the single-value output of the network (like in training). Object detection and semantic segmentation require spatial predictions so we extract the class-specific maps before spatial pooling to keep spatial resolution. They are at resolution 1 32 with respect to the input image for ResNet-101 architecture <ref type="bibr" target="#b27">[28]</ref>. For weakly supervised pointwise object detection, we extract the region (i.e. neuron in the feature map) with maximum score for each class and use it for point-wise localization, as it is done in <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b4">5]</ref>. For weakly supervised semantic segmentation we compute the final segmentation mask either by taking the class with maximum score at each spatial position independently or by applying a CRF for spatial prediction as is common practice <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b47">48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Classification Experiments</head><p>We evaluate WILDCAT for classification tasks. Our model is implemented with Torch7 (http://torch.ch/). To show the robustness of our method in very different recognition contexts, we evaluate it on six datasets: object recognition (VOC 2007 <ref type="bibr" target="#b13">[14]</ref>, VOC 2012 <ref type="bibr" target="#b14">[15]</ref>), scene categorization (MIT67 <ref type="bibr" target="#b50">[51]</ref> and 15 Scene <ref type="bibr" target="#b36">[37]</ref>), and visual recognition where the context plays an important role (MS COCO <ref type="bibr" target="#b40">[41]</ref>, VOC 2012 Action <ref type="bibr" target="#b14">[15]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with state-of-the-art methods</head><p>We compare WILDCAT with several state-of-the-art object classification models. The parameters of our model are fixed at M = 4 and α = 0.7. The results for object classifications <ref type="table" target="#tab_1">(Table 2)</ref> show that WILDCAT outperforms all recent methods by a large margin. We can point out a large improvement compared to deep features computed on the whole image with ResNet-101 <ref type="bibr" target="#b27">[28]</ref>: 5.2 pt on VOC 2007 and 4.2 pt on VOC 2012. Note that these differences directly measure the relevance of the proposed WSL method, because WILDCAT is based on ResNet-101. We also compare our model to region selection approaches: DeepMIL <ref type="bibr" target="#b43">[44]</ref>, WELDON <ref type="bibr" target="#b12">[13]</ref> and RRSVM <ref type="bibr" target="#b60">[61]</ref>. Although using multiple regions as in <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b60">61]</ref> is important, we show here that we can further significantly improve performances by learning multiple modalities per category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>VOC 2007 VOC 2012 VGG16 <ref type="bibr" target="#b55">[56]</ref> 89.3 89.0 DeepMIL <ref type="bibr" target="#b43">[44]</ref> -86.3 WELDON <ref type="bibr" target="#b12">[13]</ref> 90.2 -ResNet-101 (*) <ref type="bibr" target="#b27">[28]</ref> 89.8 89.2 ProNet <ref type="bibr" target="#b57">[58]</ref> -89.3 RRSVM <ref type="bibr" target="#b60">[61]</ref> 92.9 -SPLeaP <ref type="bibr" target="#b34">[35]</ref> 88.0 -WILDCAT 95.0 93.4 In <ref type="table">Table 3</ref>, we compare WILDCAT results for scene categorization with recent global image representations used for image classification: deep features <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b27">28]</ref>, and global image representation with deep features computed on image regions: MOP CNN <ref type="bibr" target="#b24">[25]</ref> and Compact Bilinear Pooling <ref type="bibr" target="#b17">[18]</ref>. Again, WILDCAT gets the best results, showing the capacity of our model to seek discriminative part regions, whereas background and non-informative parts are incorporated into image representation with other approaches. We also compare WILDCAT to existing part-based models including negative evidence during training <ref type="bibr" target="#b46">[47]</ref> and nonlinear part classifiers combined with part-dependent soft pooling <ref type="bibr" target="#b34">[35]</ref>. WILDCAT also outperforms recent WSL models with different spatial pooling strategies: 17 pt with respect to GAP GoogLeNet <ref type="bibr" target="#b69">[70]</ref> which uses a global average pooling and 6 pt with respect to WELDON <ref type="bibr" target="#b12">[13]</ref> which uses a kMax+kMin pooling. This validates the relevance of our spatial pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>15 Scene MIT67 CaffeNet Places <ref type="bibr" target="#b70">[71]</ref> 90.2 68.2 MOP CNN <ref type="bibr" target="#b24">[25]</ref> -68.9 Negative parts <ref type="bibr" target="#b46">[47]</ref> -77.1 GAP GoogLeNet <ref type="bibr" target="#b69">[70]</ref> 88.3 66.6 WELDON <ref type="bibr" target="#b12">[13]</ref> 94.3 78.0 Compact Bilinear Pooling <ref type="bibr" target="#b17">[18]</ref> -76.2 ResNet-101 (*) <ref type="bibr" target="#b27">[28]</ref> 91.9 78.0 SPLeaP <ref type="bibr" target="#b34">[35]</ref> -73.5 WILDCAT 94.4 84.0 <ref type="table">Table 3</ref>. Classification performances (multi-class accuracy) on scene datasets.</p><p>Finally, we report the excellent performances of WILD-CAT on context datasets in <ref type="table">Table 4</ref>. We compare our model to ResNet-101 deep features <ref type="bibr" target="#b27">[28]</ref> computed on the whole image and recent WSL models for image classification: DeepMIL <ref type="bibr" target="#b43">[44]</ref>, WELDON <ref type="bibr" target="#b12">[13]</ref> and ProNet <ref type="bibr" target="#b57">[58]</ref>. WILD-CAT outperforms ResNet-101 by 8 pt on both datasets, again validating our WSL model in this context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>VOC 2012 Action MS COCO DeepMIL <ref type="bibr" target="#b43">[44]</ref> -62.8 WELDON <ref type="bibr" target="#b12">[13]</ref> 75.0 68.8 ResNet-101 (*) <ref type="bibr" target="#b27">[28]</ref> 77.9 72.5 ProNet <ref type="bibr" target="#b57">[58]</ref> -70.9 WILDCAT 86.0 80.7 <ref type="table">Table 4</ref>. Classification performances (MAP) on context datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Further analysis</head><p>We detail the impact of our contributions on three datasets: VOC 2007, VOC 2012 Action and MIT67. We present results for an input image of size 448 × 448 and k + = k − = 1, but similar behaviors are observed for other scales and larger k + and k − . By default, our model parameters α and M are fixed to 1.</p><p>Deep structure. Firstly, to validate the design choice of the proposed WILDCAT architecture, we evaluate two different configurations (see discussion before Section 3.4): (a) conv5 + conv + pooling (our architecture); (b) conv5 + pooling + conv (architecture proposed in <ref type="bibr" target="#b69">[70]</ref>).These two configurations are different for the nonlinear WILDCAT pooling scheme described in Section 3.3, and their comparison is reported in <ref type="table">Table 5</ref>. We can see that our architecture (a) leads to a consistent improvement over architecture (b) used in GAP <ref type="bibr" target="#b69">[70]</ref> on all three datasets, e.g. 1.7 pt on VOC07.  <ref type="table">Table 5</ref>. Classification performances for architectures (a) and (b).</p><p>Note that the strategy of architecture (a) has a very different interpretation from (b): (a) classifies each region independently and then pools the region scores, whereas (b) pools the output of the convolution maps and then performs image classification on the pooled space.</p><p>Impact of parameter α. We investigate the effect of the parameter α on classification performances. From the results in <ref type="figure" target="#fig_3">Figure 4</ref>, it is clear that incorporating negative evidence, i.e. α &gt; 0, is beneficial for classification, compared to standard max pooling, i.e. α = 0. We further note that using different weights for maximum and minimum scores, i.e. α = 1, yields better results than with α = 1 from <ref type="bibr" target="#b12">[13]</ref>, with best improvement of 1.6 pt (resp. 2 and 1.8) with α = 0.6 (resp. 0.7 and 0.8) on VOC 2007 (resp. VOC 2012 Action and MIT67). This confirms the relevance of using a relative weighting for negative evidence. Moreover our model is robust with respect to the value of α.  <ref type="table">Table 6</ref>. Explicitly learning multiple modalities, i.e. M &gt; 1, yields large gains with respect to a standard classification layer, i.e. M = 1 <ref type="bibr" target="#b12">[13]</ref>. However encoding more modalities than necessary (e.g. M = 16) might lead to overfitting since the performances decrease. The best improvement is 3.5 pt (resp. 4.3 and 3.5) with M = 8 (resp. 8 and 12) on VOC 2007 (resp. VOC 2012 Action and MIT 67). Examples of heatmaps for the same category are shown in <ref type="figure" target="#fig_6">Figure 6</ref>. Ablation study. We perform an ablation study to illustrate the effect of each contribution. Our baseline is a WSL transfer with M = 1 and the spatial pooling with α = 1. The   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Weakly Supervised Experiments</head><p>In this section, we show that our model can be applied to various tasks, while being trained from global image labels only. We evaluate WILDCAT for two challenging weakly supervised applications: pointwise localization and segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Weakly supervised pointwise localization</head><p>We evaluate the localization performances of our model on PASCAL VOC 2012 validation set <ref type="bibr" target="#b14">[15]</ref> and MS COCO validation set <ref type="bibr" target="#b40">[41]</ref>. The performances are evaluated with the point-based object localization metric introduced by <ref type="bibr" target="#b43">[44]</ref>. This metric measures the quality of the detection, while being less sensitive to misalignments compared to other metrics such as IoU <ref type="bibr" target="#b14">[15]</ref>, which requires the use of additional steps (e.g. bounding box regression).</p><p>WILDCAT localization performances are reported in Table 8. Our model significantly outperforms existing weakly supervised methods. We can notice an important improvement between WILDCAT and MIL-based architecture DeepMIL <ref type="bibr" target="#b43">[44]</ref>, which confirms the relevance of our spatial pooling function. In spite of its simple and multipurpose architecture, our model outperforms by a large margin the complex cascaded architecture of ProNet <ref type="bibr" target="#b57">[58]</ref>. It also outperforms the recent weakly supervised model <ref type="bibr" target="#b4">[5]</ref> by 3.2 pt (resp. 4.2 pt) on VOC 2012 (resp. MS COCO), which use a more complex strategy than our model, based on searchtrees to predict locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>VOC 2012 MS COCO DeepMIL <ref type="bibr" target="#b43">[44]</ref> 74  Note that since the localization prediction is based on classification scores, good classification performance is important for robust object localization. In <ref type="figure" target="#fig_5">Figure 5</ref>, we evaluate the classification and localization performances with respect to α on VOC 2012. Both classification and localization curves are very similar. The best localization performances are obtained for α ∈ [0.6, 0.7], and the improvement between α = 1 and α = 0.7 is 1.6 pt. We can note that the worst performance is obtained for α = 0, which confirms that the contextual information brought by the minimum is useful for both classification and localization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Weakly supervised segmentation</head><p>We evaluate our model on the PASCAL VOC 2012 image segmentation dataset <ref type="bibr" target="#b14">[15]</ref>, consisting of 20 foreground object classes and one background class. We train our model with the train set (1,464 images) and the extra annotations provided by <ref type="bibr" target="#b25">[26]</ref> (resulting in an augmented set of 10,582 images), and test it on the validation set (1,449 images). The performance is measured in terms of pixel Intersection-over-Union (IoU) averaged across the 21 categories. As in existing methods, we add a fully connected CRF (FC-CRF) <ref type="bibr" target="#b31">[32]</ref> to post-process the final output labeling.</p><p>Segmentation results. The result of our method is presented in <ref type="table" target="#tab_7">Table 9</ref>. We compare it to weakly supervised methods that use only image labels during training. We can see that WILDCAT without CRF outperforms existing weakly supervised models by a large margin. We note a large gain with respect to MIL models based on (soft-)max pooling <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref>, which validates the relevance of our pooling for segmentation. The improvement between WILD-CAT with CRF and the best model is 7.1 pt. This confirms the ability of our model to learn discriminative and accurately localized features. We can note that all the methods evaluated in <ref type="table" target="#tab_7">Table 9</ref> have comparable complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Mean IoU MIL-FCN <ref type="bibr" target="#b48">[49]</ref> 24.9 MIL-Base+ILP+SP-sppxl <ref type="bibr" target="#b49">[50]</ref> 36.6 EM-Adapt +FC-CRF <ref type="bibr" target="#b44">[45]</ref> 33.8 CCNN + FC-CRF <ref type="bibr" target="#b47">[48]</ref> 35.3 WILDCAT 39.2 WILDCAT + FC-CRF 43.7 With a quite more complex strategy, the very recent paper <ref type="bibr" target="#b30">[31]</ref> presents impressive results (50.7 MIoU). The training scheme in <ref type="bibr" target="#b30">[31]</ref> incorporates different terms, which are specifically tailored to segmentation: one enforces the segmentation mask to match low-level image boundaries, another one incorporates prior knowledge to support predicted classes to occupy a certain image proportion. In contrast, WILDCAT uses a single model which is trained in the same manner for the three tasks, i.e. classification, localization and segmentation.</p><p>Qualitative Results. In <ref type="figure" target="#fig_6">Figure 6</ref>, we show predicted segmentation masks for four images. Compared to ground truth ((b) column), we can see that our predicted segmentation masks ((e) column) are always relevant, except for the last example where the rails and the train are glued together. The heatmaps from the same class (columns (c) and (d)) show different modalities learned by our model. When successful, they focus on different parts of the objects. For example, on the first row, the heatmap (c) focuses on the head of the bird whereas the heatmap (d) focuses on the legs and the tail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose WILDCAT, a new weakly supervised learning dedicated to learn discriminative localized visual features by using only image-level labels during training. Extensive experiments have shown the effectiveness of WILD-CAT on three main visual recognition tasks: image classification, for which we report outstanding performances on six challenging datasets, and WSL localization and segmentation, using a single and generic training scheme for all tasks.</p><p>Future works include adapting WILDCAT for semantic applications where localized features are crucial, e.g. Visual Question Answering <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b3">4]</ref> or Visual Grounding <ref type="bibr" target="#b16">[17]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>⋆Figure 1 .</head><label>1</label><figDesc>Figure 1. WILDCAT example performing localization and segmentation (b), based on different class-specific modalities, here head (c) and legs (d) for the dog class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. WILDCAT local feature encoding and pooling. Class modalities are encoded with a multi-map WSL transfer layer and pooled separately for all classes. Local features are then aggregated with a global spatial pooling to yield a single score per class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>2048 for an original image of size W × H × 3 [28]. The transfer output is then of size w × h × M C (Figure 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Analysis of parameter α.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>M</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Classification and localization performances with respect to α on VOC 2012.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Segmentation examples on VOC 2012. Our prediction is correct except for the train (last row) where our model aggregated rails and train regions. For objects as bird or plane, one can see how two heatmaps (heatmap1 (c) and heatmap2 (d) representing the same class: respectively bird, aeroplane, dog and train) succeed to focus on different but relevant parts of the objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>). The performances on MIT67, 15 Scene, VOC 2007 and 2012 are evaluated following the standard protocol. On MS COCO dataset (resp. VOC 2012 Action), we follow the protocol of [44] (resp. [13]). De- tailed information is available in section 1 of Supplemen- tary. We first compare our model to state-of-the-art meth- ods, then we analyze our contributions.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Classification performances (MAP) on object recogni- tion datasets. We used VOC evaluation server to evaluate on VOC 2012. (*) means that results are obtained with online code https://github.com/facebook/fb.resnet.torch.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 7 .</head><label>7</label><figDesc></figDesc><table>Ablation study on VOC 2007, VOC 2012 Action (VO-
CAc) and MIT67. The results are different from results of section 
4.1 because only one scale is used for this analysis. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 8 .</head><label>8</label><figDesc>Pointwise object localization performances (MAP) on PASCAL VOC 2012 and MS COCO.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 9 .</head><label>9</label><figDesc>Comparison of weakly supervised semantic segmentation methods on VOC 2012.</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Pooling in image representation: the visual codeword point of view. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What&apos;s the Point: Semantic Segmentation with Point Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">MU-TAN: Multimodal Tucker Fusion for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Ben</forename><surname>Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly supervised localization using deep feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Bency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Max-min convolutional neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">R-FCN: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MANTRA: Minimum Maximum Latent Structural SVM for Image Classification and Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weldon</surname></persName>
		</author>
		<title level="m">Weakly Supervised Learning of Deep Convolutional Neural Networks. In CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object Detection with Discriminatively Trained Part Based Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAMI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<title level="m">Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Compact Bilinear Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Object detection via a multiregion and semantic segmentation-aware cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deformable part models are convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Contextual action recognition with r* cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning Deep Hierarchical Visual Feature Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatial Transformer Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Blocks that shout: Distinctive parts for scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Juneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning features and parts for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spleap: Soft pooling of learned parts for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zepeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chevallier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Video event detection by inferring temporal instance labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Object Bank: A High-Level Image Representation for Scene Classification &amp; Semantic Feature Sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multiple Instance Learning for Soft Bags via Top Instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep lac: Deep localization, alignment and classification for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Is object localization for free? Weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a DCNN for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Reconfigurable models for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Parizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Oberlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Automatic discovery and optimization of parts for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Parizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Constrained Convolutional Neural Networks for Weakly Supervised Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fully Convolutional Multi-Class Multiple Instance Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Workshop)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">From image-level to pixellevel labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Recognizing indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Latent pyramidal regions for recognizing scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Understanding and improving convolutional neural networks via concatenated rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">ProNet: Learning to Propose Object-Specific Boxes for Cascaded Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning discriminative part detectors for image classification and cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">End-to-end integration of a convolution network, deformable parts model and nonmaximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Region Ranking SVM for Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Cresceptron: a selforganizing neural network which grows adaptively</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zemel</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ask</surname></persName>
		</author>
		<editor>B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">∝svm for learning with label proportions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jebara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Top-down neural attention by excitation backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sclaroff</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">SPDA-CNN: Unifying Semantic Part Detection and Abstraction for Fine-grained Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Partbased r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">PANDA: Pose Aligned Networks for Deep Attribute Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Discriminative Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Scene Recognition using Places Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
