<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Action Sets: Weakly Supervised Action Segmentation without Ordering Constraints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
							<email>richard@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
							<email>kuehne@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
							<email>gall@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Action Sets: Weakly Supervised Action Segmentation without Ordering Constraints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Due to the huge amount of publicly available video data, there is an increasing interest in methods to analyze these data. In the field of human action recognition, considerable advances have been made in recent years. A lot of research has been published on action recognition, i.e. action classification on pre-segmented video clips <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b10">11]</ref>. While current methods already achieve high accuracies on large datasets such as UCF-101 <ref type="bibr" target="#b31">[32]</ref> and HMDB-51 <ref type="bibr" target="#b13">[14]</ref>, the assumption of having pre-segmented action clips does not apply for most realistic tasks. Therefore, there is a growing interest in efficient methods for finding actions in temporally untrimmed videos. With the availability of large scale datasets such as Thumos <ref type="bibr" target="#b8">[9]</ref>, Activity Net <ref type="bibr" target="#b4">[5]</ref>, or Breakfast <ref type="bibr" target="#b11">[12]</ref>, many new approaches to temporally locate and classify actions in untrimmed videos emerged <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b17">18]</ref>. However, these approaches usually rely on fully supervised data, i.e. the exact temporal location of each action occurring in the training videos is known. Creation of such training data requires manual annotation on video frame level which is very expensive as well as impractical for large datasets. Thus, there is a need for methods that can learn temporal action segmentation and labeling with less supervision. A commonly made assumption is that instead of full supervision, only an ordered sequence of the actions occurring in the video is provided <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25]</ref>. Although this kind of weak supervision is already much easier to obtain, e.g. from movie scripts or subtitles, for a vast amount of real world tasks, such information still can not be assumed to be available. Instead, weak labels often arise in form of meta tags or unordered lists from document indexing.</p><p>To address this problem, we propose a weakly supervised method that can learn temporal action segmentation and labeling from unordered action labels, which we refer to as action sets. In contrast to the above mentioned methods (cf . <ref type="figure" target="#fig_0">Figure 1a)</ref>, we assume that neither ordering nor number of occurrences of actions is provided during training. Instead, only a set of actions occurring within the video is given (cf . <ref type="figure" target="#fig_0">Figure 1b)</ref>. This task is much more difficult than the case where ordered action transcripts are given. Consider, for instance, a video with T frames and a transcript of C ordered actions. Then, there are (C+T )! C!T ! possible labelings for the video. If the actions are not ordered, there are already C T possible labelings. For a very short video of 100 frames and C = 5, this means that using unordered actions sets as supervision already allows for about 10 60 times more possible labelings than when provided ordered action transcripts.</p><p>In order to deal with such an enormously large search space, we propose three model components that aim at decomposing the search space on three different levels of granularity. The coarsest level is addressed by a context model that restricts the space of possible action sequences. On a finer level, a length model restricts the durations of actions to a reasonable length. On the lowest, most fine- grained level, a frame model provides class probabilities for each video frame.</p><p>Note that context models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25]</ref> and length models <ref type="bibr" target="#b23">[24]</ref> have been used before. However, in these works either ordered action transcripts for the context model or framewise annotations for the length models are provided. To the best of our knowledge, we are the first to use these models without being provided any training data that allows to directly infer such models from the video annotation.</p><p>In an extensive evaluation, we investigate the impact of each component within the system. Moreover, temporal segmentation and action labeling quality is evaluated on unseen videos alone and on videos with action sets given at inference time as additional supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Strong feature extractors developed in classical action recognition such as Fisher vectors of improved dense trajectories <ref type="bibr" target="#b34">[35]</ref> or a variety of sophisticated CNN methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b3">4]</ref> have also pushed the advances in untrimmed action segmentation.</p><p>When processing untrimmed videos, actions can either be localized in the temporal domain only <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26]</ref>, or in the spatio-temporal domain <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38]</ref>. For the latter, videos are usually constrained to contain only few action instances. While most approaches in this area are fully supervised, <ref type="bibr" target="#b37">[38]</ref> propose a weakly supervised method for actor-action segmentation that is based on a multi-task ranking model.</p><p>In this work, we focus on localizing actions in the temporal domain only. In this setting, videos either contain multiple actions of several classes occurring densely throughout the whole video <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27]</ref>, or sparsely <ref type="bibr" target="#b8">[9]</ref>, i.e. most of the video is background and all instances of a single class or a small set of classes need to be detected in the video. Well studied methods from classical action recognition are frequently used as framewise feature extractors <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b23">24]</ref>. Although CNN features are successful in some action detection methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b28">29]</ref>, they usually require to be retrained using full supervision. Improved dense trajectories, on the contrary, are extracted in an unsupervised manner, making them the features of choice for most weakly supervised approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>In the context of fully supervised action detection, most approaches use a sliding window to efficiently segment a video <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref> and rely on CNNs or recurrent networks <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b28">29]</ref> that can not be used if only weak supervision is available. The same holds for <ref type="bibr" target="#b23">[24]</ref>, who model context and length information, which is also done in our approach. They show that length and context information significantly improve action segmentation systems, using a Poisson distribution to model action lengths and a language model to incorporate action context information. Other fully supervised methods guided by grammars have been proposed in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b12">13]</ref>. Note that in contrast to our task, their length and context model can be easily estimated from the frame-level training annotations. The challenge for our problem formulation, however, is that no annotations that allow a direct estimation of a context or length model are provided.</p><p>When working with weak supervision, existing methods use ordered action sequences as annotation. Early works suggest to get action sequences from movie scripts <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b2">3]</ref>. Alayrac et al. <ref type="bibr" target="#b0">[1]</ref> propose to localize specific actions in a video from narrated instructions. In <ref type="bibr" target="#b18">[19]</ref>, it is proposed to use automatic speech recognition and align textual descriptions, in their cases recipes, to the recognized spoken sequence. Bojanowski et al. <ref type="bibr" target="#b1">[2]</ref> address the task of aligning actions to frames. In their work, ordered action sequences are assumed to be provided during training and testing and only an alignment between the frames and the action sequence is learned. Kuehne et al. <ref type="bibr" target="#b14">[15]</ref> extend their approach from <ref type="bibr" target="#b12">[13]</ref> to weak supervision by inferring a linear segmentation from ordered action sequences and training a classical GMM+HMM speech recognition system on iteratively refined segmentations. A further extension of this idea has been proposed by Richard et al. <ref type="bibr" target="#b24">[25]</ref>, where the GMM is replaced by a recurrent neural network. Recently, Huang et al. <ref type="bibr" target="#b5">[6]</ref> proposed to use connectionist temporal classification (CTC) to learn temporal action segmentation from weakly supervised videos. In order to avoid degenerate alignments between video frames and provided action transcripts, they propose to use a visual similarity measure as an extension to the classical CTC approach.</p><p>In contrast to the approaches of <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25]</ref>, our approach only uses action sets, i.e. a much weaker supervision. Consequently, the way our model is learned is also different from the above mentioned approaches.</p><p>Another recently published and related method by Wang et al. <ref type="bibr" target="#b35">[36]</ref> addresses the task of detecting an action in a video with sparse action occurrences. More precisely, for a given action class, they generate action proposals and train a neural network to distinguish instances of this action from background in the video. Being designed to distinguish actions from background in a video, their method is not suited for densely labeled videos containing many difference actions followed by one another, as it is the case in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Temporal Action Labeling</head><p>Task Definition. Let (x 1 , . . . , x T ) be a video with T frames and x t are the framewise feature vectors. The task is to assign an action label c from a predefined set of possible labels C to each frame of the video. Following the notation of <ref type="bibr" target="#b23">[24]</ref>, connected frames of the same label can be interpreted as an action segment of class c and length l. With this notation, the goal is to cut the video into an unknown number of N action segments, i.e. to define N segments with lengths (l 1 , . . . , l N ) and action labels (c 1 , . . . , c N ). To simplify notation, we abbreviate sequences of video frames, lengths, and classes by x Model Definition. In order to solve this task, we propose a probabilistic model and aim to find the most likely segmentation and segment labeling of a given video,</p><formula xml:id="formula_0">(l N 1 ,ĉ N 1 ) = arg max N,l N 1 ,c N 1 p(c N 1 , l N 1 |x T 1 ) ,<label>(1)</label></formula><p>where l n is the length of the n-th segment and c n is the corresponding action label. We use a background class for all parts of the video in which no action (or no action of interest) occurs. So, all video frames belong to one particular action class and segment. Hence, l In order to build a probabilistic model, we first decompose Equation (1) using Bayes rule,</p><formula xml:id="formula_1">(l N 1 ,ĉ N 1 ) = arg max N,l N 1 ,c N 1 p(c N 1 )p(l N 1 |c N 1 )p(x T 1 |c N 1 , l N 1 ) .<label>(2)</label></formula><p>The first factor, p(c</p><formula xml:id="formula_2">N 1</formula><p>) is the coarsest model, controlling the likelihood of action sequences. The second factor, on a finer level, is a length model that controls the action durations, and the third factor finally provides a likelihood of the video frames for a specific segmentation and labeling. The same factorization has also been proposed in <ref type="bibr" target="#b23">[24]</ref> for fully supervised action detection. We would like to emphasize that our model only shares the factorization with the work of <ref type="bibr" target="#b23">[24]</ref>. Due to weak supervision, the actual models we use and the way they are trained are highly different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Weak Supervision</head><p>While most works on weakly supervised temporal action segmentation use ordered action sequences as supervision <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b1">2]</ref>, in our task, only unordered sets of actions occurring in the video are provided, cf . <ref type="figure" target="#fig_0">Figure 1b</ref>. Notably, neither the order of the actions nor the number of occurrences per action is known. Assuming the training set consists of I videos, then the supervision available for the i-th video is a set A i ⊆ C of actions occurring in the video.</p><p>During inference, no action sets are provided for the video and the model has to infer an action labeling from the video frames only. As an additional task, we also discuss the case where action sets are given for inference, see Section 4.6.</p><p>In the following, the models for the three factors p(c</p><formula xml:id="formula_3">N 1 ), p(l N 1 |c N 1 ), and p(x T 1 |c N 1 , l N 1 ) from Equation (2) are intro- duced.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Context Modeling with Context-free Grammars</head><p>Our first step to handle the huge search space is to restrict the possible action orderings using a context-free grammar G in order to model the context prior p(c N 1 ). Once the grammar is generated, define</p><formula xml:id="formula_4">p(c N 1 ) = const, if c N 1 ∈ G, 0, otherwise.<label>(3)</label></formula><p>Concerning the maximization in Equation <ref type="formula" target="#formula_1">(2)</ref>, this means that each action sequence generated by G has the same probability and all other sequences have zero probability, i.e. they can not be inferred. We propose the following strategies to obtain a grammar: Naive Grammar. All action sequences that can be created using elements from each action set from the training data are possible. Formally, this means</p><formula xml:id="formula_5">G naive = I i=1 A * i ,<label>(4)</label></formula><p>where i indicates the i-th training sample and A * i is the Kleene closure of A i .</p><p>Monte-Carlo Grammar. We randomly generate a large amount of k action sequences. Each sequence is generated by randomly choosing a training sample i ∈ {1, . . . , I}. Then, actions are uniformly drawn from the corresponding action set A i until the accumulated estimated means λ c of all drawn actions exceed the video length T i . The mean lengths λ c are estimated action class durations, see Section 3.3.</p><p>Text-Based Grammar. Frequently, it is possible to obtain a grammar from external text sources, e.g. from web recipes or books. Given some natural language texts, we enhance the monte-carlo grammar by mining frequent word combinations related to the action classes. Consider two action classes v and w, for instance butter pan and crack egg. If either of the words butter or pan is preceding crack or egg in the textual source, we increase the count N (v, w) by one. This way, word conditional probabilities</p><formula xml:id="formula_6">p(w|v) = N (v, w) w N (v,w)<label>(5)</label></formula><p>are obtained that have a high value if v precedes w frequently and a low value otherwise. The actual construction of the grammar follows the same protocol as the montecarlo grammar with the only difference that the actions are not drawn uniformly from the action set but according to the distribution p(w|v), where v is the previously drawn action class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Length Model from Action Sets</head><p>While a grammar already introduces some ordering constraints, the search space is still tremendously large, considering that actions can be of arbitrary and even practically unreasonable durations. Therefore, as a second step, we estimate a length model out of the scarce information we get from the training data. In order to model the length factor p(l</p><formula xml:id="formula_7">N 1 |c N 1 )</formula><p>, we assume conditional independence of each segment length and further drop the dependence of all class labels but the one of the current segment, i.e.</p><formula xml:id="formula_8">p(l N 1 |c N 1 ) = N n=1 p(l n |c n ).<label>(6)</label></formula><p>Each class-conditional p(l|c) is modeled with a Poisson distribution for class c. For the estimation of the class-wise Poisson distributions, only the action sets A i provided in the training data can be used. Ideally, the free parameter of a Poisson distribution, λ c , should be set to the mean length of action class c. Since this can not be estimated from the action sets, we propose two strategies to approximate the mean duration of each action class.</p><p>Naive Approach. In the naive approach, the frames of each training video are assumed to be uniformly distributed among the actions in the respective action set. The average length per class can then be computed as</p><formula xml:id="formula_9">λ c = 1 |I c | i∈Ic T i |A i | ,<label>(7)</label></formula><p>where I c = {i : c ∈ A i } and T i is the length of the i-th video.</p><p>Loss-based. The drawback of the naive approach is that actions that are usually short are assumed to be longer if the video is long. Instead, we propose to estimate the mean of all classes together. This can be accomplished by minimizing a quadratic loss function,</p><formula xml:id="formula_10">I i=1 c∈Ai (λ c − T i ) 2 subject to λ c &gt; l min ,<label>(8)</label></formula><p>where l min is a minimal action length. For minimization, we use constrained optimization by linear approximation (COBYLA) <ref type="bibr" target="#b22">[23]</ref>.</p><p>Note that the true mean length of action c is likely to be smaller than λ c since actions may occur multiple times in a video. However, this can not be included into the length model since the action sets do not provide such information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Multi-task Learning of Action Frames</head><p>Given the grammar and the length model that already strongly restrict the search space, the last missing factor is the actual framewise model providing a likelihood for each class to be present in a given frame.</p><p>In order to model this last factor from Equation (2), we train a network with |C| many binary softmax output layers. Each layer predicts if for a given frame x t label c is present, i.e. if c ∈ A i or not. Since an action c usually occurs in different context, all frames belonging to class c are always labeled with its true class c and some varying other classes. Thus, a classifier can learn a strong response on the presence of the correct class and weaker responses on the presence of other falsely assigned classes. As loss of our network, we therefore use the accumulated cross-entropy loss of each binary classification task.</p><p>In order to use the output probabilities of the multi-task network during inference, they need to be transformed to model the last factor from Equation (2), p(x</p><formula xml:id="formula_11">T 1 |c N 1 , l N 1 )</formula><p>. We therefore define the class-posterior probabilities</p><formula xml:id="formula_12">p(c|x t ) := p(c present|x t ) c p(c present|x t )<label>(9)</label></formula><p>and transform them into class-conditional probabilities</p><formula xml:id="formula_13">p(x t |c) ∝ p(c|x t ) p(c) .<label>(10)</label></formula><p>Since the network is a framewise model, p(c) is also a framewise prior. More specifically, if count(c) is the total number of frames labeled with c present, then p(c) is the relative frequency count(c)/ c count(c).</p><p>Assuming conditional independence of the video frames, the probability of an action segment ranging from frame t s to t e can then be modeled as</p><formula xml:id="formula_14">p(x te ts |c) = te t=ts p(x t |c).<label>(11)</label></formula><p>Framewise conditional independence is a commonly made assumption in multiple action detection and temporal segmentation methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b12">13]</ref>. Note that t s and t e are implicitly given by the segment lengths l <ref type="formula" target="#formula_1">(2)</ref> is now modeled using the previously defined segment probabilities,</p><note type="other">N 1 . For the n-th segment in the video, t (n) s = 1 + i&lt;n l i and t (n) e = i≤n l i . The third factor of Equation</note><formula xml:id="formula_15">p(x T 1 |c N 1 , l N 1 ) := N n=1 p(x t (n) e t (n) s |c n ).<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Inference</head><p>With the explicit models for each factor, the optimization problem from Equation (2) reduces to</p><formula xml:id="formula_16">(l N 1 ,ĉ N 1 ) = arg max N,l N 1 ,c N 1 ∈G N n=1 p(l n |c n ) · p(x t (n) e t (n) s |c n ) .<label>(13)</label></formula><p>Note that the arg max is only taken over action sequences that can be generated by the grammar. Since the same probability has been assigned to all those sequences, the factor p(c N 1 ) from Equation <ref type="formula" target="#formula_1">(2)</ref> is a constant. Moreover, the length model p(l n |c n ) strongly penalizes unlikely action durations and allows for an efficient pruning of unlikely segmentations. Both together lead to a significant reduction of the search space.</p><p>The solution to Equation (13) can now be efficiently computed using a Viterbi algorithm over context-free grammars, as widely used in automatic speech recognition, see for example <ref type="bibr" target="#b9">[10]</ref>. The algorithm is linear in the number of frames and therefore allows for efficient processing of videos with arbitrary length. The authors of <ref type="bibr" target="#b23">[24]</ref> have shown that adding a length model increases the complexity from O(T ) to O(T L), where L is the maximal action length that can occur. In theory, there is no limitation on the duration of actions, so inference would be quadratic in the number of frames. In practice, however, it is usually possible to limit the maximal allowed action length L to some reasonable constant, maintaining linear runtime. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we analyze the components of our approach, starting with the grammar (Section 4.2) and the length model (Section 4.3), before we compare our system to existing methods that use more supervision (Section 4.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Datasets. We evaluate our approach on three datasets for weakly supervised temporal action segmentation and labeling, namely the Breakfast dataset <ref type="bibr" target="#b11">[12]</ref>, MPII Cooking 2 <ref type="bibr" target="#b27">[28]</ref>, and Hollywood Extended <ref type="bibr" target="#b1">[2]</ref>.</p><p>The Breakfast dataset is a large scale dataset comprising 1, 712 videos, corresponding to roughly 67 hours of video and 3.6 million frames. Each video is labeled by one of the 10 coarse breakfast related activities like coffee or fried eggs. Additionally, a finer action segmentation into 48 classes is provided which is usually used for action detection and segmentation. Overall, there are nearly 12, 000 instances of these fine grained action classes with durations between a few seconds and several minutes, making the dataset very challenging. The actions are densely annotated and only 7% of the frames are background frames. We use four splits as suggested in <ref type="bibr" target="#b11">[12]</ref> and provide frame accuracy as evaluation metric.</p><p>MPII Cooking 2 consists of 273 videos with 2.8 million frames. We use the 67 action classes without object annotations. Overall, around 14, 000 action segments are annotated in the dataset. The dataset provides a fixed split into a train and test set, separating 220 videos for training. With 29%, the background portion in this dataset is at a medium level. For evaluation, we use the midpoint hit criterion as proposed in <ref type="bibr" target="#b26">[27]</ref>.</p><p>Hollywood Extended is a smaller dataset comprising 937 videos with roughly 800, 000 frames. There are about 2, 400 non-background action instances from 16 different classes. With 61% of the frames, the background portion within this dataset is comparably large. We follow the suggestion of <ref type="bibr" target="#b1">[2]</ref> and use a 10-fold cross-validation. The originally proposed evaluation metric is a variant of the Jaccard index, intersection over detection, which is only reasonable for a transcript-to-video alignment task where the transcripts and thus the action orderings are known for the test sequences as in <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b5">[6]</ref>. For temporal action segmentation, only a video is given during inference and the number of predicted segments can differ from the number of annotated segments. In this case, the metric can not be used. Thus, we stick to the Jaccard index (intersection over union), which is widely used in the domain of action detection <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b8">9]</ref> and has also been used on this dataset by <ref type="bibr" target="#b14">[15]</ref>.</p><p>Feature extraction. For a fair comparison, we use the same features as <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b5">[6]</ref>. Fisher vectors of improved dense trajectories <ref type="bibr" target="#b34">[35]</ref> are extracted for each frame and the result is projected to a 64-dimensional subspace using PCA as proposed by Kuehne et al. <ref type="bibr" target="#b12">[13]</ref>. Then, the features are normalized to have zero mean and unit variance along each dimension. If not mentioned otherwise, we use the monte-  <ref type="table">Table 1</ref>. Evaluation of our method on Breakfast using different context-free grammars. As length model, the loss-based approach is used.</p><p>carlo grammar and the loss-based length model. The indepth evaluation of our approach is conducted on Breakfast, final results on other datasets are reported in Section 4.5.</p><p>Model. For the neural network in the framewise model we use a simple feed forward network with a single hidden layer of 256 rectified linear units. Experiments with deeper models could not generalize to the test data (VGG-16: accuracy of 0.031 on Breakfast). We also evaluated the neural network based multiple instance learning approach of <ref type="bibr" target="#b36">[37]</ref>, which also was not able to make reliable predictions (accuracy 0.089 on Breakfast). We therefore found the multi-task network as proposed in Section 3.4 to be a simple yet effective model.</p><p>Efficient inference. During inference, we allow to hypothesize new segments only every 30 frames. This allows for inference roughly in realtime without affecting the performance of the system compared to a more fine-grained segment hypothesis generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Effect of the Grammar</head><p>The main contribution of the grammar is to limit the search space and remove unrealistic action sequences. We compare different kinds of grammars and report the frame accuracy on both, test and train set. Recall that due to weak supervision, our method does not necessarily provide good results on the training videos, making it interesting to investigate both sets. As shown in <ref type="table">Table 1</ref>, the use of a sophisticated grammar is crucial for good performance. The naive grammar is only slightly better than the system without any grammar. The monte-carlo grammar boosts the frame accuracy by 10% on the test set. Note that we found the number of k monte-carlo samples for the grammar not to be critical and chose 1, 000 randomly generated sequences for all experiments. Using a ground truth grammar, i.e. a grammar learned from ordered action transcripts (which are not provided in our setting) gives an upper bound on the performance that can be reached by improving the grammar only. Notably, the monte-carlo grammar is only 6% below this upper bound.</p><p>For a further comparison, we gave all action sets from the training data to an annotator who was asked to manually create an ordered action sequence for each set. This manually created grammar serves as a comparison of the purely   <ref type="table">Table 3</ref>. Evaluation of our method on Breakfast using different length models. As grammar, the monte-carlo approach is used.</p><p>data driven monte-carlo grammar to human knowledge. Although the manual grammar is better, the frame accuracy only differs by 3.6%. Since the annotator on average only needed one minute per action set, a manual grammar is also a cheap opportunity to add human knowledge without the need to actually annotate videos. As proposed in Section 3.2, textual sources can be used to enhance the monte-carlo grammar by restricting the transition between action classes to only the likely ones. We evaluate such a text-based grammar for all three datasets. For Breakfast, we used a webcrawler to download more than 1, 200 breakfast related recipes, for Hollywood Extended, 10 movie scripts of IMDB top-ranked movies have been downloaded, and for Cooking 2, we used the scripts provided by the authors of the dataset. These scripts were obtained by asking annotators to write sequential instructions on how to execute the respective kitchen task. Consequently, the text sources used for Breakfast and Hollywood Extended are only loosely connected to the datasets, whereas the textual source for Cooking 2 covers exactly the same domain as the videos. Not surprisingly, we find that only for this case, the text-based grammar leads to an improvement over the monte-carlo grammar, cf . <ref type="table" target="#tab_2">Table 2</ref>. For the other datasets, neither an improvement nor a degradation is observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Effect of the Length Model</head><p>Besides the choice of the context-free grammar, the length model is a crucial component of our system. The estimated mean action lengths influence the performance in two ways: first, they define the Poisson distribution that contributes to the actual length of hypothesized action segments. Secondly, they have a huge impact on the number of action instances that are generated for each action sequence in the monte-carlo grammar.</p><p>Mean Length Approximation. We compare the two proposed mean approximation strategies, naive and lossbased mean approximation, with a ground truth model, i.e. the true action means estimated on a frame-level ground truth annotation of the training data. The results are shown in <ref type="table">Table 3</ref>. The naive mean approximation suffers from some conceptual drawbacks. Due to the uniform distribution of video frames among all actions occurring in the video, short actions may be assigned a reasonable length as long as the video is also short. If the video is long, however, short actions get the same share of frames as long actions, resulting in an over-estimation of the mean for short actions and an under-estimation of the mean for long actions. The loss-based mean approximation, on the contrary, can provide more realistic estimates by minimizing Equation <ref type="bibr" target="#b7">(8)</ref>. Note that the solution of the problem in principle would allow for negative action means. Hence, setting the minimal action length l min &gt; 0 is crucial. In practice, we want to ensure a reasonable minimum length and set l min = 50 frames, corresponding to roughly two seconds of video. The loss-based mean approximation performs significantly better than the naive approximation, increasing the frame accuracy by 3%.</p><p>Comparing these numbers to the ground truth length model reveals that particularly on the train set, on which the ground truth lengths have been estimated, there is still room for improvement. Considering the small amount of supervision that we can utilize to estimate mean lengths, i.e. actions sets only, and the small gap between the lossbased approach and the ground truth model on the test set, on the other hand, we find that our loss-based method already yields a good approximation.</p><p>Evaluating Different Length Models. So far we modeled the length with a Poisson distribution. There is a variety of other possible length models. In <ref type="figure" target="#fig_4">Figure 2</ref>, three additional models are evaluated, a Gaussian, a box-, and a triangle model. Box and triangle model are zero outside [µ − σ, µ + σ]. The standard deviation σ of each action class is heuristically estimated by mapping actions according to their mean length onto the possible segmentations generated by the monte-carlo grammar. The Gaussian model decays too fast around the mean lengths and leads to low accuracies. Although the other models perform well, the Poisson distribution still yields the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Impact of Model Components</head><p>All three components, the grammar, the length model, and the framewise model, contribute their share to restrict-  <ref type="table">Table 4</ref>. The first four rows are a comparison of the impact of the grammar and the length model on the Breakfast dataset; the last is our system trained on fully supervised, i.e. framewise annotated, data. It is an upper bound for the weakly supervised setup. <ref type="figure">Figure 3</ref>. Example segmentation on a test video from Breakfast. Row one to four correspond to row one to four from <ref type="table">Table 4</ref>. The last row is the ground truth segmentation.</p><formula xml:id="formula_17">✗, ✗ ✗, ✓ ✓, ✗ ✓, ✓ GT</formula><p>ing the search space to reasonable segmentations. In this section, we evaluate the impact of the grammar and length model on their own and in combination with each other. We use the best-working grammar and length approximation, i.e. the monte-carlo grammar with loss-based mean approximation, and analyze the effect of omitting the grammar and/or the length model from Equation <ref type="formula" target="#formula_0">(13)</ref> during inference. The results are reported in <ref type="table">Table 4</ref>. Not surprisingly, the performance without a grammar is poor, as the model easily hypothesizes unreasonable action sequences. Adding a grammar alone already boosts the performance, restricting the search space to more reasonable sequences. In order to also get action segments of reasonable length, however, the combination of grammar and length model is crucial. This effect can also be observed in a qualitative segmentation result, see <ref type="figure">Figure 3</ref>. Note the strong over-segmentation if neither grammar nor length model is used. Introducing the length model partially improves the result but still the grammar is crucial for a reasonable segmentation in terms of correct segment labeling and segment lengths. The fully supervised model (last row of <ref type="table">Table 4</ref>) is trained by assigning the ground truth action label to each video frame. Apart from the labeling, the multi-task network architecture remains unchanged. The full supervision defines an upper bound for our weakly supervised method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison to State of the Art</head><p>The task of weakly supervised learning of a model for temporal action segmentation given only action sets has not been addressed before. Still, there are some works on temporal action segmentation given ordered action sequences. In this section, we compare our approach to these methods on the three datasets. Kuehne et al. <ref type="bibr" target="#b14">[15]</ref>   <ref type="bibr" target="#b5">[6]</ref> 0.218 − − ECTC <ref type="bibr" target="#b5">[6]</ref> 0.277 − − HMM+RNN <ref type="bibr" target="#b24">[25]</ref> 0.333 − 0.119 <ref type="table">Table 5</ref>. Performance of our method compared to state of the art methods for weakly supervised temporal segmentation. Note that our method uses action sets as weak supervision, whereas <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25]</ref> have a stronger supervision with ordered action sequences.  <ref type="table">Table 6</ref>. Different levels of video trimming for Cooking 2. More videos and less actions per video result in better performance. models and Richard et al. <ref type="bibr" target="#b24">[25]</ref> extend their approach using recurrent neural networks. Huang et al. <ref type="bibr" target="#b5">[6]</ref>, in contrast, rely on connectionist temporal classification (CTC) with LSTMs and extend it by downweighting degenerated alignments and incorporating visual similarity of frames into the decoding algorithm. They call their approach extended CTC (ECTC). All of these approaches use ordered action sequences, and thus a much stronger supervision than our method. Keeping the tremendously large search space for our problem compared to <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25]</ref> in mind (cf . Section 1), our model achieves remarkable results on Breakfast and Hollywood Extended, cf . <ref type="table">Table 5</ref>. Note that training the HMM approach of <ref type="bibr" target="#b14">[15]</ref> with monte-carlo sampled action transcripts (i.e. with the same amount of supervision as in this paper) only yields an accuracy of 0.145 on Breakfast, which is far less than our approach. An example segmentation of our approach is shown in <ref type="figure">Figure 4</ref>. Falsely recognized actions are frequently those that only occur jointly, such as spoon powder and pour milk. In these cases, the model typically fails to predict the correct ordering. Actions per Video. While our approach works well on Breakfast and Hollywood Extended, the results on Cooking 2 show its limitations. The dataset has many classes (67) but only a small amount of training videos (220), which are very long and contain a huge amount of different ac-  <ref type="table">Table 7</ref>. Results of our method when the action sets are provided for inference.</p><p>tions. These characteristics make it difficult for the multitask learning to distinguish different classes, as many of them occur jointly in most training videos. We show the importance of having enough videos by cutting each video of Cooking 2 into two/four parts <ref type="table">(Table 6</ref>). This increases the number of videos and reduces the number of actions per video. The more videos and the less actions per video on average, the better are the results of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Inference given Action Sets</head><p>So far, it has always been assumed that no weak supervision in form of action sets is provided for inference. If the action sets for the videos are, for example, generated using meta-tags of Youtube videos, however, they may as well be available during inference. In this section, we evaluate our method under this assumption.</p><p>Let A be the given action set for a video. During inference, only action sequences that are consistent with A need to be considered, i.e. for a grammar G, only sequences c  <ref type="table">Table 7</ref>. The above mentioned limitations on Cooking 2 again prevent our method from generating a better segmentation. On Breakfast and Hollywood Extended, a clear improvement of 5% and 15% compared to the inference without given action sets <ref type="table">(Table 5</ref>) can be observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have introduced a system for weakly supervised temporal action segmentation given only unordered action sets. In contrast to ordered action sequences that have been proposed as weak supervision by previous works, action sets are often publicly available in form of meta-tags for videos and do not need to be annotated. Although action sets provide by far less supervision than ordered action sequences and lead to a tremendously large search space, our method still achieves good results. Providing the possibility to incorporate data-driven grammars as well as text-based information or human knowledge, our method can be adapted to specific requirements in different video analysis tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (a) Weak supervision with ordered action sequences [2, 15, 6]. The number of actions and their ordering is known. (b) Weak supervision with action sets (our setup). Note that neither action orderings nor the number of occurrences per action are provided.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where the subscript is the start index of the sequence and the superscript the ending index.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>labeling of the complete video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Evaluation of different length models on Breakfast.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>are possible. If G ∩ A * is empty, we consider all sequences c N 1 ∈ A * . The results are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Evaluation of the text-based grammar. For Cooking 2, 
where the text sources are closely related to the content of the 
videos, an improvement can be observed. 

frame accuracy 

Length model 
train 
test 

naive 
0.254 
0.201 
loss-based 
0.282 
0.233 
ground truth 
0.341 
0.257 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>approach the problem with hidden Markov models and Gaussian mixture Breakfast Cooking 2 Holl. Ext. frame acc. midpoint hit jacc. idx</figDesc><table>Weak supervision: unordered action sets 
monte-carlo 
0.233 
0.098 
0.093 
text-based 
0.232 
0.106 
0.092 

Stronger supervision: ordered action transcripts 
HMM [15] 
0.259 
0.200 
0.086 
CTC </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>Figure 4. Example segmentation. All relevant ground truth actions are present. Note that spoon powder always occurs jointly with pour milk, so it is hard for our model to distinguish them.</figDesc><table>ours 

GT 

spoo pour milk spoon pour m 
stir milk 
spoon powder pour milk 
stir milk 

cuts per video 
4 
2 
-

avg. #actions per video 
12.5 
25 
50 
midpoint hit 
0.174 
0.121 
0.098 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>Breakfast Cooking 2 Holl. Ext. frame acc. midpoint hit jacc. idx</figDesc><table>monte-carlo 
0.284 
0.102 
0.230 
text-based 
0.280 
0.106 
0.242 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Source code and details on the dynamic programming equations can be found on https://alexanderrichard.github.io</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised learning from narrated instruction videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised action labeling in videos under ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="628" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic annotation of human actions in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Duchenne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Connectionist temporal modeling for weakly supervised action labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="137" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Action localization with tubelets from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What do 15,000 object categories tell us about classifying and localizing actions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="46" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using a stochastic contextfree grammar as a language model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wooters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fosler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tajchaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="189" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goal-directed human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="780" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An end-to-end generative framework for video segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Weakly supervised learning of actions from transcripts. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Segmental spatiotemporal CNNs for fine-grained action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What&apos;s cookin&apos;? Interpreting cooking videos using text, speech and vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malmaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spot on: Action localization from pointly-supervised proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The LEAR submission at Thumos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>Inria</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parsing videos of actions with segmental grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="612" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A direct search optimization method that models the objective and constraint functions by linear interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in optimization and numerical analysis</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="51" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with RNN based fine-to-coarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neuralnetwork-viterbi: A framework for weakly supervised video learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1194" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recognizing finegrained and composite activities using hand-centric features and script data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="373" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A multi-stream bi-directional recurrent neural network for finegrained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">APT: Action localization proposals from dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">From stochastic grammar to bayes network: Probabilistic parsing of complex activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2641" to="2648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep multiple instance learning for image classification and auto-annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Weakly supervised actor-action segmentation via robust multi-task ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Endto-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
