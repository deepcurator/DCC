<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RPAN: An End-to-End Recurrent Pose-Attention Network for Action Recognition in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen College of Advanced Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RPAN: An End-to-End Recurrent Pose-Attention Network for Action Recognition in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Recent studies demonstrate the effectiveness of Recurrent Neural Networks (RNNs) for action recognition in videos. However, previous works mainly utilize video-level category as supervision to train RNNs, which may prohibit RNNs to learn complex motion structures along time. In this paper, we propose a recurrent pose-attention network (RPAN) to address this challenge, where we introduce a novel pose-attention mechanism to adaptively learn pose-related features at every time-step action prediction of RNNs. More specifically, we make three main contributions in this paper. Firstly, unlike previous works on pose-related action recognition, our RPAN is an end-toend recurrent network which can exploit important spatialtemporal evolutions of human pose to assist action recognition in a unified framework. Secondly, instead of learning individual human-joint features separately, our poseattention mechanism learns robust human-part features by sharing attention parameters partially on the semanticallyrelated human joints. These human-part features are then fed into the human-part pooling layer to construct a highlydiscriminative pose-related representation for temporal action modeling. Thirdly, one important byproduct of our RPAN is pose estimation in videos, which can be used for coarse pose annotation in action videos. We evaluate the proposed RPAN quantitatively and qualitatively on two popular benchmarks, i.e., Sub-JHMDB and PennAction. Experimental results show that RPAN outperforms the recent state-of-the-art methods on these challenging datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action recognition in videos has been intensely investigated in computer vision areas, due to its wide applications * Equally-contributed first authors ({wb.du, yl.wang}@siat.ac.cn) † Corresponding author (yu.qiao@siat.ac.cn) in video retrieval, human-computer interaction, etc <ref type="bibr" target="#b26">[27]</ref>. The challenges of classifying actions in the wild videos mainly come from high dimension of video data, complex motion styles, large inter-category variations, and confused background clutters. With tremendous successes of deep models in image classification, there is a growing interest in developing deep neural networks for action recognition <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>Recurrent Neural Networks (RNNs) show the power as sequential models for action videos <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31]</ref>. In most of these works, the inputs to RNN are high-level features extracted from the fully-connected layer of CNNs, which may be limited in describing fine details about action. To alleviate this issue, attention-based models have been proposed <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28]</ref>. However, most existing attention approaches only utilize video-level category as supervision to train RNNs, which may lack a detailed and dynamical guidance (such as human movement over time), and consequently restrict their capacity of modeling complex motions in videos. Alternatively, human poses have proven useful for action recognition <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b39">40]</ref>. As shown in Subplots (a-c) of <ref type="figure" target="#fig_0">Fig. 1</ref>, human poses of different actors are closely related to the saliency regions in the average of convolutional feature maps estimated by CNN, and different joints of human pose can also be highly activated in certain individual feature maps. More importantly, spatial-temporal evolution of human poses in Subplot (d) of <ref type="figure" target="#fig_0">Fig. 1</ref> yields a dynamical attention cue, which can guide RNNs to efficiently learn complex motions for action recognition in videos.</p><p>Inspired by this analysis, this paper proposes a novel recurrent pose-attention network (RPAN) for action recognition in videos, which can adaptively learn a highlydiscriminative pose-related feature for every-step action prediction of LSTM. Specifically, we make three main contributions as follows. Firstly, unlike the previous works on pose-related action recognition, our RPAN is an end-to-end recurrent network, which allows to take advantage of dy- We generate convolutional cube from the 5a layer (9 × 15 × 1024) in the spatial-stream of temporal segment net <ref type="bibr" target="#b46">[47]</ref>, and then sum the convolutional cube over feature channels to obtain this averaged feature map. (c1) The highest-activated feature map for different human joints (Ankle, Elbow, and Wrist). First, the video frame is reshaped to be the same size as the feature map in the convolutional cube. Then, we find the location of each human joint on all the feature maps. Finally, the feature map with the highest-activated value at the joint location is selected as the highest-activated feature map for the corresponding joint. (c2) Image patch at the highest-activated location. The highest-activated feature map is firstly reshaped to be the same size as the video frame. Then we find the image patch (80 × 80) from the video frame, according to the location of the highest-activated value in the resized feature map. (d) The pose-attention-related heat maps and estimated poses of sampled video frames by our recurrent pose attention network (RPAN). One can see that, human pose is a discriminative cue for action recognition (Subplots a-c). More importantly, spatial-temporal evolution of human pose can provide a dynamical guidance to assist recurrent network learning (Subplot d).</p><p>namical human pose cues to improve action recognition in a unified framework. Secondly, our novel pose-attention mechanism can learn a number of robust human-part features, with guidance of human body joints in videos. By sharing attention parameters partially on the semanticallyrelated joints, human-part features not only represent the distinct joint characteristics, but also preserve rich humanbody-structure information which is robust to recognize complex actions. Subsequently, these features are fed into a human-part pooling layer to construct a discriminative pose feature for temporal action modeling. Thirdly, one important byproduct of our RPAN is pose estimation in videos, which can be applied to coarse pose annotation in action videos. To show the effectiveness of our RPAN, we conduct extensive experiments on two popular benchmarks (sub-JHMDB and PennAction) in pose-related action recognition. The empirical results show that, the classification accuracy of RPAN outperforms the recent state-of-the-art approaches on these challenging datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Action Recognition. Early approaches for action recognition are mainly based on hand-crafted features <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>, which represent videos with a number of local descriptors. However, hand-crafted approaches may only capture the local contents and thus lack the discriminative power to recognize complex actions <ref type="bibr" target="#b44">[45]</ref>. With significant successes of CNNs in image recognition <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref>, several works proposed to design effective CNNs for action recognition in videos <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46]</ref>. One of the most popular approaches is two-stream CNNs <ref type="bibr" target="#b28">[29]</ref>, where <ref type="figure">Figure 2</ref>. Our End-to-End Recurrent Pose-Attention Network (RPAN). At the t-th step, the video frame is fed into CNN to generate the convolutional feature cube Ct. Then, with guidance of the previous hidden state ht−1 of LSTM, our pose attention mechanism learns several human-part-related features F P t from Ct. As attention parameters are partially shared on the semantic-related human joints belonging to the same body part, our human-part-related features encode robust body-structure-information to discriminate complex actions. Finally, these features are fed into the human-part pooling layer to produce a highly-discriminative pose-related feature St, which is the input to LSTM for action recognition. The whole RPAN can be efficiently trained in an end-to-end fashion, by considering the action loss (predictionŷt vs. action label) and the pose loss (attention heat maps α spatial and temporal CNNs were designed to process RGB images and optical flows separately. One limitation in this approach is that the stacked optical flows can only capture motion information in short temporal scale. To improve the performance, several extensions have been proposed by designing trajectory-pooled deep descriptors <ref type="bibr" target="#b44">[45]</ref>, mining key volume of videos <ref type="bibr" target="#b53">[54]</ref>, fusing two streams <ref type="bibr" target="#b10">[11]</ref>, introducing temporal segments <ref type="bibr" target="#b46">[47]</ref>. Furthermore, the sequential nature of video inspires researchers to learn video representations by RNNs, especially LSTM <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31]</ref>. However, the inputs to these LSTMs are high-level features obtained from the fully-connected (FC) layer of CNNs, which are limited to represent fine action details in videos <ref type="bibr" target="#b0">[1]</ref>. Recently, attention has been incorporated into LSTMs to learn detailed spatial or temporal action cues <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b50">51]</ref>, motivated by its efficiency for image understanding <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b49">50]</ref>. However, these attention methods only utilize video-level category as supervision, and thus lack the temporal guidance (such as human-pose dynamics) to train LSTMs. This may restrict their capacity of modeling complex motions in the real-world action videos.</p><p>Pose-related Action Recognition. Human pose has proven highly-discriminative to recognize complex actions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b39">40]</ref>. One well-known pose-based representation is poselet <ref type="bibr" target="#b1">[2]</ref> which has been applied to action recognition and detection in videos <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b51">52]</ref>. However, the hand-crafted features in these approaches may lack the discriminative power to represent pose-related complex actions. To improve the performance, several latent structures were proposed by learning meaningful hierarchical pose representations for action recognition <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b47">48]</ref>. Furthermore, with the recent development of deep models in action recognition <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38]</ref> and pose estimation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b48">49]</ref>, pose-related deep approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10]</ref> have been recently introduced to boost recognition accuracy. However, these approaches are not in an end-toend learning procedure, since human poses are either given or estimated before action recognition. As a result, spatialtemporal pose evolutions may not effectively apply to action recognition in a unified framework.</p><p>Different from the works above, we propose a novel endto-end recurrent pose-attention network (RPAN) for action recognition in videos. At each time step, our pose attention learns a highly-discriminative pose feature for key action regions, with the guidance of human joints. Subsequently, the resulting pose feature is fed into LSTM for action recognition. In this case, our RPAN naturally takes advantage of human-pose evolutions as a dynamical assistant task for action recognition, and thus it can alleviate the complexity of hand-crafted designs in the previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Recurrent Pose-Attention Network (RPAN)</head><p>In this section, we describe the proposed Recurrent PoseAttention Network (RPAN), which can dynamically identify the important pose-related feature to enhance every time-step action prediction of LSTM. First, the current video frame is fed into CNN to generate a convolutional feature cube. Then, our pose attention mechanism takes the previous hidden state of LSTM as a guidance to estimate a number of human-part-related features from the current convolutional cube. Our attention parameters are partially shared on semantic-related human joints, hence the learnt human-part features encode rich and robust body-structureinformation. Next, these features are fed into a humanpart pooling layer to produce a highly-discriminative poserelated feature for temporal action modeling within LSTM. The whole framework is shown in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Convolutional Feature Cube from CNN</head><p>In this work, we use the well-known deep architecture in action recognition, two-steam CNNs <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>, to generate the convolutional cubes from spatial (RGB) and temporal (Optical Flow) stream CNNs. Since we follow <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref> to process two streams separately, we henceforth describe the <ref type="figure">Figure 3</ref>. Our Pose-Attention Mechanism. We firstly group the semantically-related human joints into a number of body parts. For each body part P , we take the previous hidden state ht−1 of LSTM as guidance to generate attention heat maps α J t (k) (Eq. 2-3) for each joint J ∈ P . Since attention parameters are partially shared for the joints in P , their attention maps not only represent their joint characteristics, but also preserve the important body structure information. Subsequently, we use these attention maps α J t (k) in the human part P to learn the human-part feature F P t from the convolutional cube Ct (Eq. 4). In this case, F P t can contain the robust human-partinformation. Finally, we fuse all the human-part features with a human-part-pooling layer, to generate a discriminative pose feature for temporal modeling. More details can be found in Section 3.2.</p><p>convolutional feature cube in general to reduce notation redundancy. More details can be found in our experiments.</p><p>For the t-th video frame (t = 1, ..., T ), we denote the convolutional cube from CNN as C t ∈ R K1×K2×dc , which consists of d c feature maps with size of K 1 × K 2 . Furthermore, we denote C t as a set of feature vectors at different spatial locations,</p><formula xml:id="formula_0">C t = {C t (1), ..., C t (K 1 × K 2 )},<label>(1)</label></formula><p>where the feature vector at the k-th location is C t (k) ∈ R dc and k = 1, ..., K 1 × K 2 . Based on the convolutional cube from CNN, we next propose a novel pose-attention mechanism to assist action prediction at each step of LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pose Attention Mechanism</head><p>After obtaining C t , we use it for temporal modeling with LSTM. However, LSTM with only action-category supervision often lacks the dynamical guidance (such as humanpose movements over time). This may restrict the capacity of LSTM to learn complex motion structures in the realworld action videos. Motivated by the fact that spatialtemporal evolutions of human poses provide important cues for action recognition <ref type="bibr" target="#b16">[17]</ref>, we design a novel pose-attention mechanism to learn a discriminative pose feature for LSTM. An illustration of our pose attention is shown in <ref type="figure">Fig. 3</ref>.</p><p>Pose-Attention with Human-Part-Structure. In fact, human parts (such as Torso in <ref type="figure">Fig. 3</ref>) often contain more robust action information than individual joints (such as Head, Shoulders, and Hips in <ref type="figure">Fig. 3</ref>) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b39">40]</ref>. Inspired by this, we propose a novel pose attention mechanism with human part structure.</p><p>Firstly, we group semantically-related human joints into a number of body parts in <ref type="figure">Fig. 3</ref>, where P denotes a body part, and J denotes a human joint belonging to P . For each body part P , we use the previous hidden state h t−1 of LSTM as action guidance, and estimate the importance of convolutional cube C t for each joint J ∈ P ,</p><formula xml:id="formula_1">α J t (k) = v J tanh(A P h h t−1 + A P c C t (k) + b P ),<label>(2)</label></formula><p>where C t (k) is the feature vector of C t at the k-th spa-</p><formula xml:id="formula_2">tial location (k = 1, ..., K 1 × K 2 ),α J t (k)</formula><p>is the unnormalized attention score of C t (k) for the joint J, and {v J , A </p><p>With α J t (k) of all the joints in the human part P , we can learn the human-part-related feature from C t ,</p><formula xml:id="formula_4">F P t = Σ J∈P Σ k α J t (k)C t (k).<label>(4)</label></formula><p>Due to the novel human-part-structure design in our pose attention, the learned features F P t can contain bodystructure robustness for complex actions.</p><p>Human-Part Pooling Layer. To generate a highly dynamical and discriminative pose-related feature for temporal modeling, we design a human-part pooling layer to fuse all the human-part-related features,</p><formula xml:id="formula_5">S t = P artP ool(F P t ),<label>(5)</label></formula><p>where P artP ool is investigated with the max, mean or concat operations in our experiments. Note that, our pose attention takes account of occlusions, via the proposed human-part-structure design. First, the human-part feature in Eq. <ref type="formula" target="#formula_4">(4)</ref> is the attention summarization of all joints belonging to this part. In this case, when some joints are occluded, other joints in the same part may be discriminative for action recognition. Second, the pose feature in Eq. <ref type="formula" target="#formula_5">(5)</ref> is the part-pooling of all human-part features. In this case, when some parts are occluded, other parts may still yield discriminative features for action recognition. As shown in <ref type="figure">Fig. 2-3</ref>, our approach correctly recognizes Baseball-Swing, even though the upper body of the player is self-occluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Sequential Modeling with LSTM</head><p>Finally, we feed the dynamical pose feature S t into LSTM for temporal modeling,</p><formula xml:id="formula_6">(i t , f t , o t ) = σ(U s ⋆ S t + U h ⋆ h t−1 + b ⋆ ),<label>(6)</label></formula><formula xml:id="formula_7">g t = tanh(U s g S t + U h g h t−1 + b g ),<label>(7)</label></formula><formula xml:id="formula_8">r t = f t ⊙ r t−1 + i t ⊙ g t ,<label>(8)</label></formula><formula xml:id="formula_9">h t = o t ⊙ tanh(r t ),<label>(9)</label></formula><formula xml:id="formula_10">y t = sof tmax(U h y h t + b y ),<label>(10)</label></formula><p>where ⋆ denotes i, f and o for i t , f t and o t , the sets of U and b are the parameters of LSTM, σ(·) and tanh(·) are the sigmoid and tanh functions, ⊙ is the element-wise multiplication, i t , f t and o t are the input, forget and output gates, g t , r t and h t are the candidate memory, memory state and hidden state, andŷ t is the action prediction vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">End-to-End Learning</head><p>Different from previous approaches in pose-related action recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>, the proposed RPAN can be trained in an end-to-end fashion with the total loss,</p><formula xml:id="formula_11">L total = λ action L action + λ pose L pose + λ Θ Θ 2 , (11)</formula><p>where L action and L pose are respectively the action and pose losses, Θ 2 is the weight decay regularization for all the model parameters, and λ action ,λ pose ,λ Θ are the coefficients for action, pose losses and weight decay.</p><p>Given the training action label y t (one-hot label vector), L action is the cross-entropy loss between y t and its predictionŷ t in Eq. (10),</p><formula xml:id="formula_12">L action = −Σ T t=1 Σ C c=1 y t,c logŷ t,c ,<label>(12)</label></formula><p>where C is the number of action classes, T is the number of total time steps. Furthermore, given the training pose annotation M </p><formula xml:id="formula_13">L pose = Σ J Σ T t=1 Σ K1×K2 k=1 (M J t (k) − α J t (k)) 2 ,<label>(13)</label></formula><p>where each training heat map M J t is generated by adding a fixed Gaussian centered at the corresponding joint location. Note that, the heat map loss like Eq. (13) has been widely used in deep networks for pose estimation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b35">36]</ref>, since the pixel-level supervision yields richer pose representation. With our end-to-end training procedure, spatial-temporal pose evolutions can be efficiently used as a dynamical guidance of action recognition in a unified framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate our recurrent pose-attention network (RPAN) on two popular benchmarks in poserelated action recognition, i.e., Sub-JHMDB <ref type="bibr" target="#b14">[15]</ref> and PennAction <ref type="bibr" target="#b52">[53]</ref>, where Sub-JHMDB / PennAction consists of 316/2,326 videos with 12/15 action classes, and the fullbody human joints are annotated for each video. Since videos in both datasets are collected from internet, the complex body occlusions, large appearance and motion variations make both datasets challenging for pose-related action recognition <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref>. We use the published evaluation protocol <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref> to report classification accuracy for both datasets. Note that, the joint information is only required for training in our RPAN, such information is not required for testing. For a testing frame, we use the estimated heatmaps of all joints (Eq. 2 -3) to summarize the convolutional cube as a pose feature. This feature is then fed into LSTM for action recognition. All our experiments are performed in this way, without using the joint information in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Unless stated otherwise, we perform our RPAN with the following implementation details. Firstly, for Sub-JHMDB / PennAction (size of 240 × 320 / 270 × 480), the convolutional cube is generated from the convolutional layer (the 5a layer, 8 × 10 × 1024 / 9 × 15 × 1024) of temporal segment net (TSN) <ref type="bibr" target="#b46">[47]</ref>, due to its good performance on action recognition in videos. Moreover, as TSN is a two-stream deep structure, the convolution cubes for different streams are separately generated by processing the RGB image and stacked optical flow of each video frame respectively. In this case, we perform our RPAN separately on the convolution cubes from different streams, similar to two-stream fashion of TSN. The training data sets for both benchmarks are augmented by the mirror operation. Secondly, human parts are defined as Torso, Elbow, Wrist, Knee and Ankle for both datasets, as shown in <ref type="figure">Fig. 3</ref>. For Sub-JHMDB / PennAction, the dimensions of all hidden variables in LSTM are 512/1024, and the dimensions of attention parameters {v J , A  respectively. Thirdly, for the training set, we add a fixed Gaussian (std: 5) centered at the joint location. Then, we resize the ground truth heat map of each human joint to be the same size as the attention heat map (8 × 10 / 9 × 15 for Sub-JHMDB / PennAction) and normalize it. Finally, we train our RPAN with mini-batch stochastic gradient descent. For both datasets, 16 videos are randomly chosen in each training mini-batch, where 8 frames are randomly sampled from each video with equal interval. 8 frames from each test video with equal interval are selected and the last-frame prediction is used to report test accuracy of our RPAN. The momentum is 0.9, both action and pose coefficients λ action , λ pose are 1, the weight decay coefficient λ Θ is 5 × 10 −4 , the learning rate is set to 0.1 initially, reduced to 10 −2 after 40/50 epochs for Sub-JHMDB / PennAction. The training procedure stops at 100 epochs. We implement our RPAN by Theano <ref type="bibr" target="#b34">[35]</ref>, with multi-GPU of BPTT <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Properties of Our RPAN</head><p>To investigate the properties of our RPAN, we evaluate the effectiveness of its key model components on Sub-JHMDB (split one) and PennAction. To be fair, when we explore different strategies of one component in our RPAN, all other components are with the basic strategy, where the convolutional cube is extracted from the temporal-steam of TSN, the pose-attention with human-part-structure refers to Section 3.2, the human-part-pooling layer is based on the concat strategy.</p><p>Pose Attention Mechanism. We examine our poseattention mechanism with different settings in <ref type="table" target="#tab_0">Table 1</ref>. First, the 'Share-All' attention setting of our RPAN outperforms the 'Without' setting (1024 dimension feature vector of global pool layer in TSN is fed into LSTM without any attention). It illustrates that the visual attention is important  <ref type="table" target="#tab_0">Table 1</ref>. (b) 'Our Pose Attention': our proposed approach. The values in the matrix for both cases are the number of test videos. First, the confusion matrix of 'Our Pose Attention' is much sparser than the one of 'Without Attention'. It illustrates that more test videos are correctly classified by our approach. Second, we compare two matrices on the 'tennis forehand' action class, where our approach correctly classifies 7 more videos (68-61=7) than 'Without Attention'. The main difference comes from confusion between 'tennis forehand' and 'tennis serve'. In 'Without Attention', these two action classes are largely confused (11 confused test videos). On the contrary, our approach can take spatial-temporal pose evolutions as a dynamical attention cue to reduce confusion between similar actions. for action recognition. Second, the 'Separate-Joint' attention setting outperforms the 'Share-All' setting. It shows that the human joint in each video frame is an effective dynamical guidance of attention. Finally, the 'Human-Part' attention setting, the proposed mechanism in Section 3.2, outperforms the 'Separate-Joint' setting. It demonstrates that, <ref type="table">Table 3</ref>. Classification accuracy of RPAN with different basic CNNs. We evaluate RPAN, based on two widely-used CNNs in action recognition, i.e., Good-Practice CNN <ref type="bibr" target="#b45">[46]</ref> and TSN <ref type="bibr" target="#b46">[47]</ref>.</p><p>CNNs for RPAN Sub-JHMDB PennAction Good-Practice CNN 69.5 88.6 TSN 80.0 97.1 <ref type="table">Table 4</ref>  the human-part-structure information in our pose attention is robust and discriminative for action recognition. Furthermore, we analyze classification results with confusion matrix in <ref type="figure" target="#fig_5">Fig. 4</ref>. The confusion matrix of our approach is much sparser than the one of 'Without Attention', showing that our approach recognizes more test videos correctly. Then, we compare two approaches on the 'tennis forehand' action class, where the difference between two approaches is the largest with regard to the number of correctly-classified videos (our pose attention vs. without attention: 68 vs. 61). This difference mainly comes from confusion between 'tennis forehand' and 'tennis serve'. In the without-attention setting, these two classes are largely confused (11 confused test videos), while our approach takes spatial-temporal pose evolutions as a dynamical attention cue to reduce confusion between similar actions.</p><p>Human-Part-Pooling Layer. We investigate different strategies for our human-part-pooling layer. As shown in <ref type="table" target="#tab_1">Table 2</ref>, the recognition performance is generally robust to different pooling strategies. Hence, in our experiments we use the concat strategy for comparison consistency.</p><p>Choice of Basic CNNs. We next evaluate different basic CNNs for our RPAN. Hence, we perform our RPAN, based on two widely-used CNNs in the research of action recognition, Good-Practice CNN (built on VGG16) <ref type="bibr" target="#b45">[46]</ref> and TSN (built on BN-Inception) <ref type="bibr" target="#b46">[47]</ref>. Both CNNs are pretrained on UCF101. For Good-Practice CNN <ref type="bibr" target="#b45">[46]</ref>, the convolutional feature cube is generated from the convolutional layer in the temporal-stream (the conv5 3 layer, 7 × 10 × 512 / 8 × 15 × 512 for Sub-JHMDB / PennAction). The convolutional cube from the temporal-stream TSN is the same as before. <ref type="table">Table 3</ref> shows that our RPAN achieves better performance with TSN. The main reason is that, TSN is a deeper CNN for action recognition, which can generate more powerful convolutional cubes than Good-Practice CNN.</p><p>Additionally, we evaluate our RPAN, based on the temporal-stream TSN so far. As TSN is a two-stream CNN architecture for action recognition in videos, we next investigate the performance of our RPAN with different streams of TSN (spatial-stream and temporal-stream). As shown in <ref type="table">Table 4</ref>, the temporal stream outperforms the spatial stream for both TSN and RPAN, showing the fact that the motion cue is generally more important than the appearance cue for action recognition. The spatial and temporal score fusion (S:T is 1:2) can further improve accuracy due to the complementary properties between them. Finally, RPAN outperforms TSN for all cases of different streams, showing that our recurrent pose-attention is an effective dynamical mechanism for action recognition.</p><p>Parameter Robustness. In the previous experiments, we use a fixed Gaussian (std: 5) to generate M J t in Eq. <ref type="bibr" target="#b12">(13)</ref>. We change this parameter to 1 / 10, and the accuracy of our approach is 78.3 / 79.3 on Sub-JHMDB (split 1). These results are comparable to the one with std=5 ('Human-Part' in <ref type="table" target="#tab_0">Table 1)</ref>, showing the parameter robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the State-of-the-art</head><p>We evaluate our RPAN, by comparing it with the recent state-of-the-art approaches in pose-based action recognition. In <ref type="table" target="#tab_2">Table 5</ref>, our RPAN outperforms other recent hand-crafted and deep learning approaches on both Sub-JHMDB and PennAction datasets. This is mainly credited to the fact that, our RPAN is an end-to-end recurrent framework, where spatial-temporal evolutions of human pose are exploited as a highly-discriminative attention cue to dynamically assist action recognition in a unified fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Byproduct: Pose Estimation in Videos</head><p>One important byproduct of our RPAN is pose estimation in videos, although our main objective is action recog- One can see that, maps in our pose attention are more sparsely-activated than the ones in without-attention setting, illustrating that spatial-temporal evolutions of human pose in our approach can assist ht to capture motion dynamics in videos. Additionally, as time goes on, we find that more dimensions of ht are gradually activated in our approach. This demonstrates that the hidden states at the later steps can effectively integrate important motion information from the previous ones, and improve action recognition in a recurrent manner. nition in videos. We evaluate this perspective on PennAction. For the attention heat map of each human joint, we find the location with the highest attention score as the estimated joint location. Then we follow the standard evaluation criteria for pose estimation in videos <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref>, where the threshold is 0.2 for PennAction. The pose estimation accuracy of our RPAN is 0.68, which is comparable to the recent approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref>. It illustrates that our RPAN can provide reliable coarse pose annotation in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization</head><p>In this section, we qualitatively evaluate our RPAN with the following visualizations. Firstly, we visualize poseattention heat maps (α J t (k) in Section 3.2) of our RPAN in <ref type="figure" target="#fig_0">Fig. 1(d)</ref>. We select an action video (Jumping-Jacks in PennAction) and show different heat maps (averaged, right ankle, right elbow, right wrist) for each sampled video frame. <ref type="figure" target="#fig_0">Fig. 1(d)</ref> shows that, our pose-attention mechanism can take spatial-temporal evolutions of human pose as a dynamical cue to effectively capture different movements of this action along time, and consequently assist to recognize this action in a recurrent fashion. Additionally, we visualize the important byproduct, i.e., pose estimation in videos, for JumpingJacks in <ref type="figure" target="#fig_0">Fig. 1(d)</ref>. One can see that, our RPAN can provide reliable coarse pose annotation in videos.</p><p>Secondly, we further examine whether our recurrent pose attention mechanism can provide a dynamical cue to assist the learning of LSTM. We show feature dynamics of hidden state h t in LSTM, since h t is used to make action prediction for each time step (Eq. 10). We compare our approach to the without-attention setting of <ref type="table" target="#tab_0">Table 1</ref>. As shown in <ref type="figure" target="#fig_7">Fig.  5</ref>, maps of feature dynamics in our pose attention are more sparsely-activated than the ones in without-attention setting, indicating that spatial-temporal pose evolutions in our approach can assist h t to capture motion dynamics in videos. Hence, our approach correctly classifies this Baseball-Pitch action video, while the without-attention setting recognizes it as a wrong action (i.e., Bowl). Furthermore, as time goes on, we find that more dimensions of h t are gradually activated in our approach. This shows that the hidden states at the later steps can effectively integrate important motion information from the previous ones, and consequently improve action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we design a novel recurrent pose-attention network (RPAN) for action recognition, with the dynamical guidance of human joints in videos. First, our RPAN is an end-to-end recurrent framework, which takes advantage of spatial-temporal pose evolutions as a dynamical attention cue of action recognition in a unified fashion. Second, our pose-attention can adaptively learn a discriminative pose feature to enhance action prediction at every step of LSTM. Via sharing attention parameters partially on the semantically-related joints, our pose-related representations contain rich and robust human-part-structure information. Finally, an important byproduct of our RPAN is pose estimation in videos, which can be used as a reliable tool for coarse pose annotation in videos. We evaluated our RPAN on two popular benchmarks, i.e., Sub-JHMDB and PennAction. The results demonstrated that our RPAN outperforms the recent state-of-the-art approaches on both datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Our motivations. (a) The sampled video frames of different actions in PennAction. The ground truth human poses are annotated in the video frames. (b) Averaged feature map. We generate convolutional cube from the 5a layer (9 × 15 × 1024) in the spatial-stream of temporal segment net [47], and then sum the convolutional cube over feature channels to obtain this averaged feature map. (c1) The highest-activated feature map for different human joints (Ankle, Elbow, and Wrist). First, the video frame is reshaped to be the same size as the feature map in the convolutional cube. Then, we find the location of each human joint on all the feature maps. Finally, the feature map with the highest-activated value at the joint location is selected as the highest-activated feature map for the corresponding joint. (c2) Image patch at the highest-activated location. The highest-activated feature map is firstly reshaped to be the same size as the video frame. Then we find the image patch (80 × 80) from the video frame, according to the location of the highest-activated value in the resized feature map. (d) The pose-attention-related heat maps and estimated poses of sampled video frames by our recurrent pose attention network (RPAN). One can see that, human pose is a discriminative cue for action recognition (Subplots a-c). More importantly, spatial-temporal evolution of human pose can provide a dynamical guidance to assist recurrent network learning (Subplot d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>vs. pose annotations) together.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>b P } are attention parameters. Note that, v J is distinct for each joint J ∈ P , while {A P h , A P c , b P } are shared for all the joints in the body part P . With this partial- parameter-sharing design, each joint heat mapα .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Jt</head><label></label><figDesc>(heat maps for all the joints), L pose is the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>b P } are {1 × 32, 32 × 512, 32 ×</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Confusion Matrix Comparison (PennAction). (a) 'Without Attention': the 'Without' setting in Table 1. (b) 'Our Pose Attention': our proposed approach. The values in the matrix for both cases are the number of test videos. First, the confusion matrix of 'Our Pose Attention' is much sparser than the one of 'Without Attention'. It illustrates that more test videos are correctly classified by our approach. Second, we compare two matrices on the 'tennis forehand' action class, where our approach correctly classifies 7 more videos (68-61=7) than 'Without Attention'. The main difference comes from confusion between 'tennis forehand' and 'tennis serve'. In 'Without Attention', these two action classes are largely confused (11 confused test videos). On the contrary, our approach can take spatial-temporal pose evolutions as a dynamical attention cue to reduce confusion between similar actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>. Classification accuracy of RPAN with different streams of TSN. TSN-S: Spatial TSN. TSN-T: Temporal TSN. TSN- (S+T): Prediction score fusion on TSN-S and TSN-T. RPAN-S: RPAN with spatial TSN. RPAN-T: RPAN with temporal TSN. RPAN-(S+T): Prediction score fusion on RPAN-S and RPAN-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Feature Dynamics of Hidden State ht in LSTM (Without Attention vs. Our Pose Attention). For each approach, we show two maps of feature dynamics. One refers to positive-activations of ht, while the other refers to negative-activations of ht. For each map, each row refers to 256 highest positive-activated (or negative-activated) dimensions of ht, where t is the time index of video frames. Note that, 256 highest activated dimensions are sorted in the descending order, according to the activation values in the last video frame. One can see that, maps in our pose attention are more sparsely-activated than the ones in without-attention setting, illustrating that spatial-temporal evolutions of human pose in our approach can assist ht to capture motion dynamics in videos. Additionally, as time goes on, we find that more dimensions of ht are gradually activated in our approach. This demonstrates that the hidden states at the later steps can effectively integrate important motion information from the previous ones, and improve action recognition in a recurrent manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Evaluation of the proposed pose-attention mecha- nism via classification accuracy of our RPAN. 'Without': We take the feature vector from the fully-connected layer of CNN as the input to LSTM, without any attention. 'Share-All': We perform the attention mechanism without guidance of human joints, where the attention parameters {v J , A) are changed to be independent of human joints and body parts, i.e., {v, A h , Ac, b}. 'Separate-Joint': We perform the attention mechanism with guidance of separate joints, where {v J , A</figDesc><table>P 

h , A 

P 

c , b 
P } in Eq. 
(2Attention in our RPAN Sub-JHMDB PennAction 
Without 
68.5 
95.0 
Share-All 
71.7 
96.4 
Separate-Joint 
78.3 
96.5 
Human-Part 
80.0 
97.1 

1024, 32 × 1} / {1 × 128, 128 × 1024, 128 × 1024, 128 × 1} 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Classification accuracy with different strategies of our Human-Part-Pooling Layer.</figDesc><table>Human-Part-Pooling Sub-JHMDB PennAction 
Max 
82.6 
96.6 
Mean 
81.5 
96.2 
Concat 
80.0 
97.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>Comparison with state-of-the-art on Sub-JHMDB (aver-
age over three splits) and PennAction. 

State-of-the-art 
Year Sub-JHMDB PennAction 
Dense+Pose [15] 2013 
52.9 
-
STIP [53] 
2013 
-
82.9 
Action Bank [53] 2013 
-
83.9 
MST [43] 
2014 
45.3 
74.0 
AOG [25] 
2015 
61.2 
85.5 
P-CNN [6] 
2015 
66.8 
-
Hierarchical [22] 2016 
77.5 
-
C3D [3] 
2016 
-
86.0 
JDD [3] 
2016 
77.7 
87.4 
idt-fv [14] 
2017 
60.9 
92.0 
Pose+ idt-fv [14] 2017 
74.6 
92.9 
Our RPAN 
78.6 
97.4 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">J t (k) not only represents distinct joint characteristics, but also preserves rich human-part-structure information. Secondly, we normalizeα J t (k) to the corresponding attention heat map α J t (k), α J t (k) = exp{α J t (k)} k exp{α J t (k)}</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">P h , A P c , b P } is changed for each separate joints without human-part-structure consideration, i.e., {v</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported in part by National High-Tech Research and Development Program of China (2015AA042303) and National Natural Science Foundation of China (61502470, 61502152).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Delving deeper into convolutional networks for learning video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Poselets: Body part detectors trained using 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Action recognition with joints-pooled 3d deep convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">P-cnn: Pose-based cnn features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chéron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">P-CNN: Pose-based CNN Features for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chéron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Theano-based large-scale visual recognition with multiple gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning latent structure for activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Englebienne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pose for action action for pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE FG</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3D Convolutional Neural Networks for Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual perception of biological motion and a model for its analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On space-time interest points. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">VideoLSTM convolves, attends and flows for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<editor>ArXiv</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A hierarchical pose-based approach to complex action understanding using dictionaries of actionlets and motion poselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Soto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint action recognition and pose estimation from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A survey on vision-based human action recognition. Image and Vison Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Action recognition using visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unsupervised Learning of Video Representations using LSTMs. ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Moving poselets: A discriminative and interpretable skeletal motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theano</forename><surname>Development Team</surname></persName>
		</author>
		<idno>abs/1605.02688</idno>
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
	<note>arXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An approach to posebased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cross-view action modeling, learning and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Video action detection with relational dynamic-poselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Towards good practices for very deep two-stream convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning a discriminative hidden part model for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The moving pose: An efficient 3d kinematics descriptor for low-latency action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A key volume mining deep framework for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
