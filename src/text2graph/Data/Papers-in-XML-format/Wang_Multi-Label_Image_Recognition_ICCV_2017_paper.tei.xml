<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:17+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-label Image Recognition by Recurrently Discovering Attentional Regions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
							<email>liguanbin@mail.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Engineering Research Center for Advanced Computing Engineering Software of Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Engineering Research Center for Advanced Computing Engineering Software of Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-label Image Recognition by Recurrently Discovering Attentional Regions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recognizing multiple labels of images is a fundamental yet practical problem in computer vision, as real-world images always contain rich and diverse semantic information. Besides the challenges shared with single-label image classification (e.g., large intra-class variation caused by viewpoint, scale, occlusion, illumination), multi-label image classification is much more difficult since accurately predicting the presence of multiple object categories usu- <ref type="figure">Figure 1</ref>. Multi-label image recognition with discovered attentional regions by our approach. These regions (highlighted by different colors) corresponding to the semantic labels (visualized below the images) are contextualized and discriminative in terms of classification, although they may not preserve object boundaries well.</p><p>ally needs understanding the image in depth (e.g., associating semantic labels with regions and capturing their dependencies).</p><p>Recently, convolutional neural networks (CNNs) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref> achieve great success in visual recognition/classification tasks by learning powerful feature representations from raw images, and they have been also applied to the problem of multi-label image classification by combining with some object localization techniques <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b26">27]</ref>. The resulting common pipeline usually involves two steps. A batch of hypothesis regions are first produced by either exploiting bottom-up image cues <ref type="bibr" target="#b24">[25]</ref> or casting extra detectors <ref type="bibr" target="#b3">[4]</ref>, and these regions are assumed to contain all possible foreground objects in the image. A classifier or neural network is then trained to predict the label score on these hypothesis regions, and these predictions are aggregated to achieve the multi-label classification results. Despite acknowledged successes, these methods take the redundant computational cost of extracting region proposals and usually over-simplify the contextual dependencies among foreground objects, leading to a sub-optimal performance in complex scenarios. Recently, Wang et al. <ref type="bibr" target="#b25">[26]</ref> proposed to jointly characterize the semantic label dependency and the image-label relevance by combining recurrent neural networks (RNNs) with CNNs. However, their model disregards the explicit associations between semantic labels and image contents, and lacks fully exploiting the spatial context in images. In contrast to all these mentioned methods, we introduce an end-to-end trainable framework that explicitly discovers attentional regions over image scales corresponding to multiple semantic labels and captures the contextual dependencies of these regions from a global perspective. No extra step of extracting hypothesis regions is needed in our approach. Two examples generated by our approach are illustrated in <ref type="figure">Figure 1</ref>.</p><p>To search for meaningful and discriminative regions in terms of multi-label classification, we propose a novel recurrent memorized-attention module, which is combined with convolutional neural networks in our framework. Specifically, this module consists of two components: i) a spatial transformer layer to locate attentional regions on the convolutional maps and ii) an LSTM (Long-Short Term Memory) sub-network to sequentially predict the labeling scores over the attentional regions and output the parameters of the spatial transformer layer. Notably, the global contextual dependencies among the attentional regions are naturally captured (i.e., memorized) together with the LSTM sequential encoding. And the two components are alternately performed during the recurrent learning. In this way, our approach enables to learn a contextualized and interpretable region-label relevance while improving the discriminability for multi-label classification.</p><p>The main contributions of this work are three-fold.</p><p>• We develop a proposal-free pipeline for multi-label image recognition, which is capable of automatically discovering semantic-aware regions over image scales and simultaneously capturing their long-range contextual dependencies.</p><p>• We further propose three novel constraints on the spatial transformer, which help to learn more meaningful and interpretable regions, and in turn, facilitate multi-label classification.</p><p>• We conduct extensive experiments and evaluations on large-scale benchmarks such as PASCAL VOC <ref type="bibr" target="#b5">[6]</ref> and Microsoft COCO <ref type="bibr" target="#b19">[20]</ref>, and demonstrate the superiority of our proposed model in both recognition accuracy and efficiency over other leading multi-label image classification methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>The performance of image classification has recently witnessed a rapid progress due to the establishment of largescale labeled datasets (i.e., PASCAL VOC <ref type="bibr" target="#b5">[6]</ref>, COCO <ref type="bibr" target="#b19">[20]</ref>) and the fast development of deep CNNs <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b11">12]</ref>. In recent years, many researchers have attempted to adapt the deep CNNs to multi-label image recognition problem and have achieved great success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-label image recognition</head><p>Traditional multi-label image recognition methods apply the bag-of-words (BOW) model to solve this problem <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. Although performing well on the simple benchmarks, these methods may fail in classifying images with complex scenes since BOW based models depend largely on the hand-crafted low-level features. In contrast, features learned by deep models have been confirmed to be highly versatile and far more effective than the handcrafted features. Since this paper focuses on deep learning based multi-label image recognition, we discuss the relevant works in the following context.</p><p>Recently, there have been attempts to apply deep learning to multi-label image recognition task <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26]</ref>. Razavian et al. <ref type="bibr" target="#b22">[23]</ref> applies off-the-shelf features extracted from deep network pretrained on ImageNet <ref type="bibr" target="#b21">[22]</ref> for multi-label image classification. Gong et al. <ref type="bibr" target="#b9">[10]</ref> propose to combine convolutional architectures with an approximate top-k ranking objective function for annotating multi-label images. Instead of extracting off-the-shelf deep features, Chatfield et al. <ref type="bibr" target="#b1">[2]</ref> fine tune the network with the target multi-label datasets, which can learn task-specific features and thus boost the classification performance. To better consider the correlations between labels instead of treating each label independently, traditional graphical models are widely incorporated, such as Conditional Random Field <ref type="bibr" target="#b7">[8]</ref>, Dependency Network <ref type="bibr" target="#b10">[11]</ref>, and co-occurrence matrix <ref type="bibr" target="#b28">[29]</ref>. Recently, Wang et al. <ref type="bibr" target="#b25">[26]</ref> utilize the RNNs to learn a joint image-label embedding to characterize the semantic label dependency as well as the image-label relevance.</p><p>All of the aforementioned methods consider extracting the features of the whole image with no spatial information, which on one hand were unable to explicitly perceive the corresponding image regions to the detected classification labels, and on the other hand, were extremely vulnerable to the complex background. To overcome this issue, some researchers propose to exploit object proposals to only focus on the informative regions, which effectively eliminate the influences of the non-object areas and thus demonstrate significant improvement in multi-label image recognition task <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b26">27]</ref>. More specifically, Wei et al. <ref type="bibr" target="#b26">[27]</ref> propose a Hypotheses-CNN-Pooling framework to aggregate the label scores of each specific object hypotheses to achieve the final multi-label predictions. Yang et al. <ref type="bibr" target="#b29">[30]</ref> formulate the multi-label image recognition problem as a multi-class multi-instance learning problem to incorporate local information and enhance the discriminative ability of the features by encoding the label view information. However, these object proposals based methods are generally not efficient with the preprocessing step of object proposal generation being the bottleneck. Moreover, the training stage is not perfect and can hardly be modeled as an end-to-end scheme in both training and testing. In this paper, we pro-  <ref type="figure">Figure 2</ref>. Overview of our proposed framework for multi-label image recognition. Our model iteratively locates the attentional regions corresponding to semantic labels and predicts the score for the current region.</p><p>pose to incorporate a recurrent memorized-attention module in the neural network to simultaneously locate the attentional regions and predict the labels on various located regions. Our proposed method does not resort to the extraction of object proposals and is thus very efficient and can be trained in an end-to-end mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Visual attention model</head><p>Attention model has been recently applied to various computer vision tasks, including image classification <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b0">1]</ref>, saliency detection <ref type="bibr" target="#b18">[19]</ref>, and image captioning <ref type="bibr" target="#b27">[28]</ref>. Most of these works use the recurrent neural network for sequential attentions, and optimized their models with reinforcement learning technique. Works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b0">1]</ref> formulate a recurrent attention model and apply it to the digital classification tasks for which the images are low-resolution with a clean background, using the small attention network. The model is non-differential and addressed with reinforcement learning to learn task-specific policies. Jaderberg et al. <ref type="bibr" target="#b14">[15]</ref> propose a differential spatial transformer module which could be used to extract attentional regions with any spatial transformation, including scaling, rotation, transition, and cropping. Moreover, it could be easily integrated into the neural network and optimized using the standard backpropagation algorithm without reinforcement learning. <ref type="figure">Figure 2</ref> illustrates the architecture of the proposed model. The input image I is first fed into a VGG-16 ConvNet without additional object proposals. The network first processes the whole image with several convolutional (conv) and max pooling layers to produce the conv feature maps, denoted as f I . Here, we use the conv feature maps from the last conv layer (i.e., conv5 3). The recurrent memorized-attention module, comprising a spatial transformer (ST) <ref type="bibr" target="#b14">[15]</ref> and an LSTM network <ref type="bibr" target="#b13">[14]</ref> that work collaboratively in an iterative manner, predicts the label distributions directly from the input image features. Specifically, in one iterative procedure, the ST locates an attentional region for the LSTM, and the LSTM predicts the scores regarding this region for multi-label classification and simultaneously updates the parameters of ST. Finally, the scores from several attentional regions are fused to achieve the final label distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">ST for Attentional Region Localization</head><p>We briefly introduce the spatial transformer (ST) <ref type="bibr" target="#b14">[15]</ref> for completeness before diving deep into the recurrent memorized-attention module. ST is a sample-based differential module that spatially transforms its input maps to the output maps with a given size which correspond to a subregion of the input maps. It is convenient to embed an ST layer in the neural network and train it with the standard back-propagation algorithm. In our model, the ST is incorporated in the recurrent memorized-attention module for the localization of attentional regions.</p><p>Formally, the ST layer extracts features of an attentional region, denoted as f k , from the feature maps f I of the whole input image. The computational procedure is as follows. A transformation matrix M is first estimated by a localization network (explained later). After that, the corresponding coordinate grid in f I is obtained, based on the coordinates of f k . Then the sampled feature maps f k that correspond to the attentional region are generated by bilinear interpolation. <ref type="figure" target="#fig_1">Fig. 3</ref> shows an example of coordinate mapping. As we aim to locate the attentional regions, we constrain the transformation matrix M to involve only cropping, translation and scaling, expressed as </p><formula xml:id="formula_0">M = s x 0 t x 0 s y t y ,<label>(1)</label></formula><p>where s x , s y , t x , t y are the scaling and translation parameters. In our model, we apply a standard neural network to estimate these parameters to facilitate an end-to-end learning scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Recurrent Memorized-Attention Module</head><p>The core of our proposed model is the recurrent memorized-attention module, which combines the recurrent computation process of an LSTM network and a spatial transformer. It iteratively searches the most discriminative regions, and predicts the scores of label distribution for them. In this subsection, we introduce this module in detail.</p><p>In the k-th iteration, our model searches an attentional region, and extracts the corresponding features by applying the following spatial transformer, expressed as</p><formula xml:id="formula_1">f k = st(f I , M k ), M k = s k x 0 t k x 0 s k y t k y ,<label>(2)</label></formula><p>where st(·) is the spatial transformation function, and M k is the transformation matrix estimated in the previous round by the localization network. We initialize the attentional region with the whole image at the first iteration, i.e., the initial transformation matrix is set to be</p><formula xml:id="formula_2">M 0 = 1 0 0 0 1 0 .<label>(3)</label></formula><p>Note that we apply the spatial transformer operation on the feature maps f I instead of the input image to avoid repeating the computational intensive convolutional processes. The LSTM takes the sampled feature map f k as input to compute the memory cell and hidden state. The computation process can be expressed as</p><formula xml:id="formula_3">x k = relu(W f x f k + b x ) i k = σ(W xi x k + W hi h k−1 + b i ) g k = σ(W xg x k + W hg h k−1 + b g ) o k = σ(W xo x k + W ho h k−1 + b o ) m k = tanh(W xm x k + W hm h k−1 + b m ) c k = g k ⊙ c k−1 + i k ⊙ m k h k = o k ⊙ c k<label>(4)</label></formula><p>where relu(·) is the rectified linear function, σ(·) is the sigmoid function, tanh(·) is the hyperbolic tangent function; h k−1 and c k−1 are the hidden state and memory cell of previous iteration; i k , g k , o k and m k are the outputs of the input gate, forget gate, output gate, and input modulation gate, respectively. These multiplicative gates can ensure the robust training of LSTMs as they work well in exploding and vanishing gradients <ref type="bibr" target="#b13">[14]</ref>. The memory cell c k encodes the useful information of previous (k − 1) regions, and it is possible to benefit our task in the following two aspects. First, previous works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b25">26]</ref> have shown that different categories of objects exhibit strong co-occurrence dependencies. Therefore, it helps to recognize objects within the current attentional region aided by "remembering" information of previous ones. Second, it is expected that our model can find out all relevant and useful regions for classification. Simultaneously considering the information of previous regions is a feasible approach that implicitly enhances the diversity and complementarity among the attentional regions. Update rule of M. Given the hidden state h k , the classifier and localization network can be expressed as</p><formula xml:id="formula_4">z k = relu(W hz h k + b z ) s k = W zs z k + b s , k = 0 M k+1 = W zm z k + b m (5)</formula><p>where s k is the predicted score distribution of the k-th region, and M k+1 is the transformation matrix for the next iteration. Note that at the first iteration (k = 0), we make no prediction of s and just estimate the matrix M because no attentional region is obtained initially. Category-wise max-pooling.</p><p>The iterations are repeated for K + 1 times, resulting in K score vectors {s 1 , s 2 , . . . , s K }, where s k = {s </p><p>4. Learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Loss for Classification</head><p>We employ the Euclidean loss as the objective function following <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30]</ref></p><note type="other">. Suppose there are N training samples, and each sample x i has its label vector y i = {y 1 i , y 2 i , . . . , y C i }. y c i (c = 1, 2, . . . , C) is assigned as 1 if the sample is annotated with the class label c, and 0 otherwise. The ground-truth probability vector of the i-th sample is defined asp i = y i /||y i || 1 . Given the predicted probability vector p i p c i = exp(s</note><formula xml:id="formula_6">c i ) C c ′ =1 exp(s c ′ i ) c = 1, 2, . . . , C,<label>(7)</label></formula><p>and the classification loss function is expressed as</p><formula xml:id="formula_7">L cls = 1 N N i=1 C c=1 (p c i −p c i ) 2 .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Loss for Attentional Region Constraints</head><p>As discussed above, we obtain the final result by aggregating the scores of the attentional regions. Thus, we hope that the attentional regions selected by our model contain all of the objects in the input image. If one object is left out unexpectedly, an inevitable error occurs because the LSTM network has never seen this object during the prediction procedure. We experimentally found that the proposed model can be trained with the defined classification loss, however, has notable drawbacks:</p><p>• Redundancy. The ST layer usually picks up the same region that corresponds to the most salient objects. As a result, it would be difficult to retrieve all of the objects appearing in the input image, since the set of attentional regions are redundant.</p><p>• Neglect of tiny objects. The ST layer tends to locate regions in a relatively large size and ignores the tiny objects, which hampers the classification performance.</p><p>• Spatial flipping. The selected attentional region may be mirrored vertically or horizontally.</p><p>To address these issues, we further define a loss function that consists of three constraints on the parameters of the transformation matrix M. Anchor constraint. It would be better if the attentional regions scatter over different semantic regions in the image. For the first iteration, adding no constraint helps to find the most discriminative region. After that, we push the other (K − 1) attentional regions away from the image center by an anchor constraint. We draw a circle of radius √ 2 2 centered on the image center, and pick up the anchor points on the circle uniformly, as depicted in <ref type="figure" target="#fig_3">Figure 4</ref>. We use K = 5 in the experiments, so four anchor points are generated at (0.5, 0.5), (0.5, −0.5), (−0.5, 0.5), and (−0.5, −0.5), respectively <ref type="bibr" target="#b0">1</ref> . The anchor constraint is formulated as</p><formula xml:id="formula_8">ℓ A = 1 2 {(t k x − c k x ) 2 + (t k y − c k y ) 2 },<label>(9)</label></formula><p>where (c k x , c k y ) is the location of the k-th anchor point. Scale constraint. This constraint attempts to push the scale parameters in a certain range, so that the located attentional region will not be too large in size. It can be formulated as</p><formula xml:id="formula_9">ℓ S = ℓ sx + ℓ sy ,<label>(10)</label></formula><p>in which ℓ sx = (max(|s x | − α, 0))</p><formula xml:id="formula_10">2 ℓ sy = (max(|s y | − α, 0)) 2<label>(11)</label></formula><p>where α is a threshold value, and it is set as 0.5 in our experiments. Positive constraint. The last one also constrains the scale parameters. Positive constraint prefers a transformation matrix with positive scale parameters, leading to attentional regions that are not be mirrored:</p><formula xml:id="formula_11">ℓ P = max(0, β − s x ) + max(0, β − s y ),<label>(12)</label></formula><p>where β is a threshold value, set as 0.1 in our experiments. Finally, we combine the aforementioned three types of constraints on the parameters of the transformation matrix to define a loss of localization of attentional regions. It is formulated as the weighted sum of the three components:</p><formula xml:id="formula_12">L loc = ℓ S + λ 1 ℓ A + λ 2 ℓ P ,<label>(13)</label></formula><p>where λ 1 and λ 2 are the weighted parameters, and they are set as 0.01 and 0.1, respectively. Our model is jointly trained with the classification loss and the localization loss, so the overall loss function can be expressed as L = L cls + γL loc .</p><p>We set the balance parameter γ as 0.1 since the classification task is dominated in our model. Optimization is performed using the recently proposed Adam algorithm <ref type="bibr" target="#b16">[17]</ref> and standard back-propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Settings</head><p>Implementation details. We implemented our method on the basis of Caffe <ref type="bibr" target="#b15">[16]</ref> for deep network training and testing. In the training stage, we employed a two-step training mechanism to initialize the convolutional neural network following <ref type="bibr" target="#b26">[27]</ref>. The CNN is first pre-trained on the ImageNet, a large scale single label classification dataset, and further fine-tuned on the target multi-label classification dataset. The learned parameters are used to initialize the parameters of the corresponding layers in our proposed model, while the parameters of other newly added layers in our network are initialized with Xavier algorithm <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref> rather than manual tuning. All the training images are first resized to N × N , and randomly cropped with a size of (N − 64) × (N − 64). The training samples are also augmented by horizontal flipping. In our experiments, we trained two models with N = 512 and N = 640, respectively. Both of the models are optimized using Adam with a batch size of 16, momentum of 0.9 and 0.999. The learning rate is set to 0.00001 initially and divided by 10 after 30 epochs. We trained the models for about 45 epochs for each scale, and selected the model with the lowest validation loss as the best model for testing.</p><p>In the testing phase, we follow <ref type="bibr" target="#b17">[18]</ref> to perform ten-view evaluation across different scales. Specifically, we first resized the input image to N × N (N = 512, 640), and extracted five patches (i.e., the four corner patches and the center patch) with a size of (N − 64) × (N − 64), as well as their horizontally flipped versions. Instead of repeatedly extracting features for each patch, the model feeds the N×N image to the VGG-16 ConvNet, and crops the features on the conv5 3 features maps accordingly to achieve the features of all patches. Then, for each patch, the model extracts the features for each located attentional region, and eventually aggregates the features of this patch by max-pooling. The image representation is obtained via averaging the features of all the patches. At last, we trained a one-vs-rest SVM classifier for each category using the LIBLINEAR library <ref type="bibr" target="#b6">[7]</ref>. We test our model on a single NVIDIA GeForce GTX TITAN-X, and it takes about 150ms for ten-view evaluation for scale 512, and about 200 ms for scale 640. It reduces the execution time by more than an order of magnitude, compared with previous proposal-based methods, e.g., HCP <ref type="bibr" target="#b26">[27]</ref>, which costs about 10s per image. Evaluation metrics. We use the same evaluation metrics as <ref type="bibr" target="#b25">[26]</ref>. For each image, we assign top k highest-ranked labels to the image, and compare with the ground-truth labels. We compute the overall precision, recall, F1 (OP, OR, OF1) and per-class precision, recall, F1 (CP, CR, CF1) in Eq. 15. Following <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b4">5]</ref>, we also apply average precision (AP) for each category, and the mean average precision (mAP) over all categories as well. Generally, overall F1, per-class F1, and mAP are relatively important.</p><formula xml:id="formula_14">OP = i N c i i N p i OR = i N c i i N g i OF 1 = 2 × OP × OR OP + OR CP = 1 C i N c i N p i CR = 1 C i N c i N g i CF 1 = 2 × CP × CR CP + CR ,<label>(15)</label></formula><p>where C is the number of labels, N c i is the number of images that are correctly predicted for the i-th label, N p i is the number of predicted images for the i-th label, N g i is the number of ground truth images for the i-th label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with State-of-the-art Methods</head><p>To validate the effectiveness of our model, we conduct the experiments on two benchmarks, PASCAL VOC 2007 <ref type="bibr" target="#b5">[6]</ref> and Microsoft COCO <ref type="bibr" target="#b19">[20]</ref>. VOC 2007 is the most widely used benchmark, and most works have reported the results on this dataset. We compare the performance of our proposed method against the following state-of-the-art approaches: FeV+LV-20-VD <ref type="bibr" target="#b29">[30]</ref>, HCP <ref type="bibr" target="#b26">[27]</ref>, RLSD <ref type="bibr" target="#b30">[31]</ref>, CNN-RNN <ref type="bibr" target="#b25">[26]</ref>, VeryDeep <ref type="bibr" target="#b23">[24]</ref> and CNN-SVM <ref type="bibr" target="#b22">[23]</ref> on the VOC 2007 dataset. MS-COCO is released later than VOC and more challenging. Recent works have also used this benchmark for evaluation. We compare with CNN-RNN <ref type="bibr" target="#b25">[26]</ref>, RLSD <ref type="bibr" target="#b30">[31]</ref> and WARP <ref type="bibr" target="#b9">[10]</ref> on the COCO dataset as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Performance on the VOC 2007 dataset</head><p>The PASCAL VOC 2007 dataset contains 9,963 images from 20 object categories, which is divided into train, val and test sets. We train our model on the trainval set, and evaluate the performance on the test set, following other competitors. <ref type="table">Table 1</ref> presents the experimental results. The previous best-performing methods are HCP and FeV+LV, which achieve a mAP of 90.9% and 90.6%, respectively. Both of them share a similar two-step pipeline: they first extract the object proposals of the image, and then aggregate the features of them for multi-label classification. Different from them, our method is proposal-free since the attentional regions are selected by the ST layer that works collaboratively with the LSTM network. In this way, the interaction between attentional region localization and classification is well explored, leading to improvement in performance. Our proposed method achieves a mAP of 91.9%, that outperforms previous state-of-the-art algorithms. Note that our model learned with a single scale of 512 or 640 also surpasses previous works. This better demonstrates the effectiveness of the proposed method.  <ref type="table">Table 2</ref>. Comparison of our model and state-of-the-art methods on the MS-COCO dataset. The best results and second best results are highlighted in red and blue, respectively. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Performance on the MS-COCO dataset</head><p>The MS-COCO dataset is primarily built for object detection, and it is also widely used for multi-label recognition recently. It comprises a training set of 82,081 images, and a validation set of 40,137 images. The dataset covers 80 common object categories, with about 3.5 object labels per image. The label number for each image also varies considerably, rendering MS-COCO even more challenging. As the ground truth labels of the test set are not available, we evaluate the performance of all the methods on the validation set instead. We follow <ref type="bibr" target="#b25">[26]</ref> to select the top k = 3 labels for each image, and filter out the labels with probabilities lower than a threshold 0.5, so the label number of some images would be less than 3. We compare the overall precision, recall, F1, and perclass precision, recall, F1 in <ref type="table">Table 2</ref>. Our model outperforms the existing methods by a sizable margin. Specifically, it achieves a per-class F1 score of 67.4% and an overall F1 score of 72.0, improving those of the previously best method by 5.4% and 4.2%, respectively. Similar to the results on VOC, the model learned with a single scale also beats the state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Study</head><p>In this subsection, we perform ablative studies to carefully analyze the contribution of the critical components of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Attentional regions v.s. object proposals</head><p>One of the main contributions of this work is that our model is capable of discovering the discriminative regions, which facilitates the task of multi-label image classification compared with proposal-based methods. In this subsection, we present a comparison to reveal the fact that attentional regions have significant advantages against object proposals.</p><p>Proposal-based methods are proved to be powerful for objectness detection. However, satisfactory recall rates are difficult to achieve until thousands of proposals are provided. In addition, it is extremely time-consuming to examine all of the provided proposals with a deep network. As an example, although HCP selects some representative proposals, it still needs 500 proposals to obtain desired performance. Besides, computing the object proposals also introduces additional computational overhead. In contrast, our model utilizes an efficient spatial transformation layer to find out a small number of discriminative regions, making the model runs much faster. Here we also present the visualization results of attentional regions discovered by our model, and those generated by EdgeBox <ref type="bibr" target="#b31">[32]</ref>, a representative proposal method. For our method, K is set as 5, so five attentional regions are found. For EdgeBox, we directly use the codes provided by <ref type="bibr" target="#b31">[32]</ref> to extract the proposals, and adopt non-maximum suppression (NMS) with a threshold of 0.7 on them based on their objectness scores to exclude the seriously overlapped proposals. We also visualize the top five ones for a fair comparison. <ref type="figure" target="#fig_4">Figure 5</ref> shows that the regions generated by our model better capture the discriminative regions (e.g., the head part of dogs), and most of them concentrate on the area of semantic objects. For EdgeBox, although its top-5 proposals cover most objects in the given image, most of them contain non-object areas that carry less discriminative information for classification.</p><p>In order to clearly show the advantages of attentional region localization, we conduct experiments to compare the classification performance when using attentional regions or object proposals in the same framework. To this end, we first remove the spatial transformer and replace the attentional region with the selected five object proposals, with the other components left unchanged. <ref type="table">Table 3</ref> gives the results on the VOC 2007 dataset. It is shown that attentional regions lead to better performance. In fact, proposal-based methods need hundreds of regions or even more proposals to cover most objects. Our model also achieves better performance than those using hundreds of proposals, such as HCP and FeV+LV, which use 500 and 400 proposals, respectively (see <ref type="table">Table 1</ref>). type mAP object proposals 88.6 attentional regions 90.4 <ref type="table">Table 3</ref>. Comparison of the mAPs of our model using attentional regions and object proposals, respectively, on the PASCAL VOC 2007 dataset. The results are all evaluated using single-crop at scale of 512×512</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Analysis of the attentional region constraints</head><p>We propose three types of novel constraints for attentional region localization, facilitating the task of multi-label image classification. To validate their contributions, we remove all three constraints, and retrain the model on the VOC 2007 dataset. The results, depicted in <ref type="table">Table 4</ref>, show a significant drop in mAP, well demonstrating the effectiveness of the constraints as a whole. We further remove one of three constraints, and retrain the model to evaluate the effectiveness of each constraint individually. The performance also declines when any constraint is excluded (see <ref type="table">Table 4</ref>). Therefore, it suggests that all of the three constraints are of importance for our model, they work cooperatively to facilitate the improvement of classification. We also conduct similar experiments on the MS-COCO dataset. As <ref type="table">Table 5</ref> shown, although MS-COCO is far different from VOC, similar results have been observed, again demonstrating their contributions on various scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Multi-scale multi-view evaluation</head><p>We assess the impact of fusion of multi-scale and multi-crop at the test stage. Two scales (512 × 512 and 640 × 640) are used in our experiments. For each scale, we extract ten crop features. Hence, we reported the performance of singlescale + single-crop, single-scale + multi-crop and multiscale + multi-crop, in boost the performance, and fusing the results of both of two scale shows a further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have introduced a recurrent memorizedattention module into the deep neural network architecture to solve the problem of multi-label image recognition. Specifically, our proposed recurrent memorized-attention module is composed of a spatial transformer layer for localizing attentional regions from the image and an LSTM unit to predict the labeling score based on the feature of a localized region and preserve the past information for the located regions. Experimental results on large-scale benchmarks (e.g., PASCAL VOC, COCO) demonstrate that our proposed deep model can significantly improve the state of the art in both accuracy and efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Illustration of coordinate grid mapping on (a) the feature maps and (b) the corresponding input image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>denotes the scores over C class labels. Following [27], we employ the category-wise max-pooling to fuse the scores into the final result s = {s 1 , s 2 , . . . , s C }. It simply maximizes out the scores over regions for each category s c = max(s c 1 , s c 2 , . . . , s c K ), c = 1, 2, . . . , C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Anchor selection for left: K=5 and right: K=9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Comparison of visualization of the attentional regions (indicated by green boxes) located by our method, and the object proposals (indicated by blue boxes) generated by EdgeBox.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 6 .</head><label>6</label><figDesc>The results show that ag- gregating information from multi-crop on a single-scale canTable 4. Comparison of mAP of our model learned using differ- ent constraints of attentional region localization on the PASCAL VOC 2007 dataset. The results are all evaluated using multi-crop at the scale of 512×512. We abbreviate anchor, scale and positive constraints as A, S, P for simple illustration.Table 5. Comparison of C-F1, O-F1 and mAP of our model learned with and without constraints of attentional region localization on the MS-COCO dataset. The results are all evaluated using multi- crop at the scale of 512×512. We abbreviate anchor, scale and positive constraints as A, S, P for simple illustration.Table 6. Comparison of mAP with multi-scale and multi-crop on the PASCAL VOC 2007 and MS-COCO datasets.</figDesc><table>constraints mAP 
null 
89.9 
S+P 
90.2 
A+S 
90.4 
A+P 
90.3 
A+S+P 
91.3 

constraints C-F1 O-F1 mAP 
null 
65.8 
70.9 
71.5 
A+S+P 
66.5 
71.3 
72.2 

VOC 2007 MS-COCO 
s=512 + single-crop 
90.4 
70.4 
s=640 + single-crop 
90.4 
70.5 
s=512 + ten-crop 
91.3 
72.2 
s=640 + ten-crop 
91.3 
72.3 
two scales + ten-crop 
91.9 
73.4 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The range of coordinate is rescaled to [-1, 1]</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7755</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3531</idno>
		<title level="m">Return of the devil in the details: Delving deep into convolutional nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchical matching with side information for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3426" to="3433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bing: Binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3286" to="3293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Subcategory-aware object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="827" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Collective multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghamrawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM international conference on Information and knowledge management</title>
		<meeting>the 14th ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="195" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Aistats</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4894</idno>
		<title level="m">Deep convolutional ranking for multilabel image annotation</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multi-label classification using conditional dependency networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM international conference on Multimedia</title>
		<meeting>ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Recurrent attentional networks for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03227</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Selective search for object recognition. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="154" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Cnn-rnn: A unified framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04573</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Hcp: A flexible cnn framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<title level="m">Show, attend and tell: Neural image caption generation with visual attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Correlative multi-label multi-instance image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="651" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploit bounding box annotations for multi-label object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="280" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multi-label image classification with regional latent semantic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01082</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
