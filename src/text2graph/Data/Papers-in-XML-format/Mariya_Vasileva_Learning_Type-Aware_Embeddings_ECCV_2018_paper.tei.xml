<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Type-Aware Embeddings for Fashion Compatibility</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariya</forename><forename type="middle">I</forename><surname>Vasileva</surname></persName>
							<email>mvasile2@illnois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
							<email>bplumme2@illnois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Dusad</surname></persName>
							<email>dusad2@illnois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreya</forename><surname>Rajpal</surname></persName>
							<email>srajpal2@illnois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjitha</forename><surname>Kumar</surname></persName>
							<email>ranjitha@illnois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Type-Aware Embeddings for Fashion Compatibility</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fashion</term>
					<term>embedding methods</term>
					<term>appearance representations</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Outfits in online fashion data are composed of items of many different types (e.g. top, bottom, shoes) that share some stylistic relationship with one another. A representation for building outfits requires a method that can learn both notions of similarity (for example, when two tops are interchangeable) and compatibility (items of possibly different type that can go together in an outfit). This paper presents an approach to learning an image embedding that respects item type, and jointly learns notions of item similarity and compatibility in an end-toend model. To evaluate the learned representation, we crawled 68,306 outfits created by users on the Polyvore website. Our approach obtains 3-5% improvement over the state-of-the-art on outfit compatibility prediction and fill-in-the-blank tasks using our dataset, as well as an established smaller dataset, while supporting a variety of useful queries 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Outfit composition is a difficult problem to tackle due to the complex interplay of human creativity, style expertise, and self-expression involved in the process of transforming a collection of seemingly disjoint items into a cohesive concept. Beyond selecting which pair of jeans to wear on any given day, humans battle fashion-related problems ranging from, "How can I achieve the same look as celebrity X on a vastly inferior budget?", to "How much would this scarf contribute to the versatility of my personal wardrobe?", to "How should I dress to communicate motivation and competence at a job interview?". This paper provides a step towards answering such diverse and logical queries.</p><p>To learn how to compose outfits the underlying representation must support both notions of similarity (e.g., when two tops are interchangeable) and notions of compatibility (items of possibly different type that can go together in an outfit). Current research handles both kinds of relationships with an embedding strategy: one trains a mapping, typically implemented as a convolutional neural network, <ref type="figure">Fig. 1</ref>: Left: Conventional embedding strategies embed objects of all types in one underlying space. Objects that are compatible must lie close; as a result, all shoes that match a given top are obliged to be close. Right: Our type-respecting embedding, using "top", "bottom" and "shoes" as examples. We first learn a single, shared embedding space. Then, we project from that shared embedding to subspaces identified by type. This means that all shoes that match a given top must be close in shoe-top space, but can be very different in the general embedding space. This enables us to search for two pairs of shoes that 1) match the same top, and 2) look very different from one another that takes input items to an embedding space (e.g. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34]</ref>). The training process tries to ensure that similar items are embedded nearby, and items that are different have widely separated embeddings (i.e. the left side of <ref type="figure">Figure 1</ref>).</p><p>These strategies, however, do not respect types (e.g. shoes embed into the same space hats do), which has important consequences. Failure to respect types when training an embedding compresses variation: for instance, all shoes matching a particular hat are forced to embed close to one another, thus making them appear compatible even if they are not, which severely limits the model's ability to address diverse queries. Worse, this strategy encourages improper triangles: if a pair of shoes match a hat, and that hat in turn matches a blouse, then a natural consequence of models without type-respecting embeddings is that the shoes are forced to also match the blouse. This is because the shoes must embed close to the hat to match, the hat must embed close to the shoes to match, thus ensuring the shoes embed close to the blouse as well. Instead, they should be allowed to match in one context, and not match in another. An alternative way to describe the issue is that compatibility is not naturally a transitive property, but being nearby is. Thus, an embedding that clusters items close together is not a natural way to measure compatibility without paying attention to context. By learning type-respecting spaces to measure compatibility, as in the right side of <ref type="figure">Figure 1</ref>, we avoid the issues stemming from using a single embedding.</p><p>We begin by encoding each image in a general embedding space which we use to measure item similarity. The general embedding is trained using a visualsemantic loss between the image embedding, and features representing a text description of the corresponding item. This helps ensure that semantically similar items are projected in a nearby space. In addition, we use a learned projection which maps our general embedding to a secondary embedding space that scores compatibility between two item types. We utilize a different projection for each pairwise compatibility comparison, unlike prior work which typically uses a single embedding space for compatibility scoring (e.g. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34]</ref>). For example, if an outfit contains a hat, a blouse, and a shoe, we would learn projections for hat-blouse, hat-shoe, and blouse-shoe embeddings. The embeddings are trained along with a generalized distance metric, which we use to compute compatibility scores between items. Please refer to the supplementary material for a visualization of our approach.</p><p>Since many of the current fashion datasets either do not contain outfit compatibility annotations <ref type="bibr" target="#b19">[20]</ref>, or are limited in size and the type of annotations they provide <ref type="bibr" target="#b11">[12]</ref>, we collect our own dataset which we describe in Section 3. In Section 4 we discuss our type-aware embedding model, which enables us to perform complex queries on our data. Our experiments outlined in Section 5 demonstrate the effectiveness of our approach, reporting a 4% improvement in a fill-in-the-blank outfit completion experiment, and a 5% improvement in an outfit compatibility prediction task over the prior state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Embedding methods provide the means to learn complicated relationships by simply providing samples of positive and negative pairs. These approaches tend to be trained as a siamese network <ref type="bibr" target="#b2">[3]</ref> or using triplet losses <ref type="bibr" target="#b22">[23]</ref> and have demonstrated impressive performance on challenging tasks like face verification <ref type="bibr" target="#b25">[26]</ref>. This is attractive for many fashion-related tasks which typically require the learning of hard-to-define relationships between items (e.g. <ref type="bibr">[10-12, 14, 34]</ref>). Veit et al . <ref type="bibr" target="#b33">[34]</ref> demonstrate successful similarity and compatibility matching for images of clothes on a large scale, but do not distinguish items by type (e.g. top, shoes, scarves) in their embedding. This is extended in Han et al . <ref type="bibr" target="#b11">[12]</ref> by feeding the visual representation for each item in an outfit into an LSTM in order to jointly reason about the outfit as a whole. There are several approaches which learn multiple embeddings (e.g. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b29">30]</ref>), but tend to assume that instances of distinct types are separated (i.e. comparing bags only to other bags), or use knowledge graph representation (e.g. <ref type="bibr" target="#b35">[36]</ref>). Multi-modal embeddings appear to reveal novel feature structures (e.g. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>), which we also take advantage of in Section 4.1. Training these type of embedding networks remains difficult because arbitrarily chosen triples can provide poor constraints <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Fashion Studies. Much of the recent fashion related work in the computer vision community has focused on tasks like product search and matching <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b40">41]</ref>, synthesizing garments from word descriptions <ref type="bibr" target="#b41">[42]</ref>, or using synthesis results as a search key <ref type="bibr" target="#b40">[41]</ref>. Kiapour et al . <ref type="bibr" target="#b15">[16]</ref> trained an SVM over a set of hand-crafted features to categorize items into clothing styles. Vaccaro et al . <ref type="bibr" target="#b31">[32]</ref> trained a topic model over outfits in order to learn latent fashion concepts. Others have identified clothing styles using meta-data labels <ref type="bibr" target="#b27">[28]</ref> or learning a topic model over a bag of visual attributes <ref type="bibr" target="#b14">[15]</ref>. Liu et al . <ref type="bibr" target="#b18">[19]</ref> measure compatibility between items by reasoning about clothing attributes learned as latent variables in their SVM-based recommendation system. Others have focused on attribute recognition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b36">37]</ref>, identifying relative strength of attributes between items (i.e. a shoe is more/less pointy than another shoe) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref>, or focused on predicting the popularity of clothing items <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>Polyvore Dataset: The Polyvore fashion website enables users to create outfits as compositions of clothing items, each containing rich multi-modal information such as product images, text descriptions, associated tags, popularity score, and type information. Han et al . supplied a dataset of Polyvore outfits (referred to as the Maryland Polyvore dataset <ref type="bibr" target="#b11">[12]</ref>). This dataset is relatively small, does not contain item types or detailed text descriptions (see <ref type="table" target="#tab_0">Table 1</ref>), and has some inconsistencies in the test set that make quantitative evaluation unreliable (additional details in Section 5). To resolve these issues, we collected our own dataset from Polyvore annotated with outfit and item ID, fine-grained item type, title, text descriptions, and outfit images. Outfits that contain a single item or are missing type information are discarded, resulting in a total of 68,306 outfits and 365,054 items. Statistics comparing the two datasets are provided in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Test-train splits: Splits for outfit data are quite delicate, as one must consider whether a garment in the train set should be allowed to appear in unseen test outfits, or not at all. As some garments are "friendly" and appear in many outfits, this choice has a significant effect on the dataset. We provide two different versions of our dataset with respective train and test splits. An "easier" split contains 53,306 outfits for training, 10,000 for testing, and 5,000 for validation, whereby no outfit appearing in one of the three sets is seen in the other two, but it is possible that an item participating in a train outfit is also encountered in a test outfit. A more "difficult" split is also provided whereby a graph segmentation algorithm was used to ensure that no garment appears in more than one split. Each item is a node in the graph, and an edge connects two nodes if the corresponding items appear together in an outfit. This procedure requires discarding "friendly" garments, or else the number of outfits collapses due to superconnectivity of the underlying graph. By discarding the smallest number of nodes necessary, we end up with a total of 32,140 outfits and 175,485 items, 16,995 of which are used for training and 15,145 for testing and validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Respecting Type in Embedding</head><p>For the i'th data item x i , an embedding method uses some regression procedure (currently, a multilayer convolutional neural network) to compute a nonlinear feature embedding</p><formula xml:id="formula_0">y i = f (x i ; θ) ∈ R d .</formula><p>The goal is to learn the parameters θ of the mapping f such that for a pair of items (x i , x j ), the Euclidean distance between the embedding vectors y i and y j reflects their compatibility. We would like to achieve a "well-behaved" embedding space in which that distance is small for items that are labelled as compatible, and large for incompatible pairs.</p><p>Assume we have a taxonomy of T types, and let us denote the type of an item as a superscript, such that x</p><formula xml:id="formula_1">(τ ) i</formula><p>represents the i'th item of type τ , where τ = 1, . . . , T . A triplet is defined as a set of images {x</p><formula xml:id="formula_2">(u) i , x (v) j , x (v)</formula><p>k } with the following relationship: the anchor image x i is of some type u, and both x j and x k are of a different type v. The pair (x i , x j ) is compatible, meaning that the two items appear together in an outfit, while x k is a randomly sampled item of the same type as x j that has not been seen in an outfit with x i . Let us write the standard triplet loss in the general form</p><formula xml:id="formula_3">ℓ(i, j, k) = max{0, d(i, j) − d(i, k) + µ} ,<label>(1)</label></formula><p>where µ is some margin. We will denote by M (u,v) the type-specific embedding space in which objects of types u and v are matched. Associated with this space is a projection P u→(u,v) which maps the embedding of an object of type u to M <ref type="bibr">(u,v)</ref> . Then, for a pair of data items (x</p><formula xml:id="formula_4">(u) i , x (v) j ) that are compatible, we require the distance ||P u→(u,v) (f (x (u) i ; θ)) − P v→(u,v) (f (x (v)</formula><p>j ; θ))|| to be small. This does not mean that the embedding vectors f (x (u) i ; θ) and f (x (v) j ; θ) for the two items in the general embedding space are similar -the differences just have to lie close to the kernel of P u→ <ref type="bibr">(u,v)</ref> . This general form requires the learning of 2 (d × d) matrices per pair of types for a d-dimensional general embedding. In this paper, we investigate two simplified versions: (a) assuming diagonal projection matrices such that</p><formula xml:id="formula_5">P u→(u,v) = P v→(u,v) = diag(w (u,v) ), where w (u,v) ∈ R d</formula><p>is a vector of learned weights, and (b) the same case, but with w (u,v) being a fixed binary vector chosen in advance for each pairwise type-specific space, and acting as a gating function that selects the relevant dimensions of the embedding most responsible for determining compatibility. Compatibility is then measured with</p><formula xml:id="formula_6">d uv ij = d(x (u) i , x (v) j , w (u,v) ) = ||f (x (u) i ; θ) ⊙ w (u,v) − f (x (v) j ; θ) ⊙ w (u,v) ||</formula><p>where ⊙ denotes component-wise multiplication, and learned with the modified triplet loss:</p><formula xml:id="formula_7">L comp (x (u) i , x (v) j , x (v) k , w (u,v) ; θ) = max{0, d uv ij − d uv ik + µ} ,<label>(3)</label></formula><p>where µ is some margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Constraints on the learned embedding</head><p>To regularize the learned notion of compatibility, we further make use of the text descriptions accompanying each item image and feed them as input to a text embedding network. Let the embedding vector outputted by that network for the description t</p><formula xml:id="formula_8">(u) i of image x (u) i be denoted g(t (u)</formula><p>i ; φ), and substitute g for f in ℓ as required; the loss used to learn similarity is then</p><formula xml:id="formula_9">L sim = λ 1 ℓ(x (v) j , x (v) k , x (u) i ) + λ 2 ℓ(t (v) j , t (v) k , t (u) i ) ,<label>(4)</label></formula><p>where λ 1−2 are scalar parameters.</p><p>We also train a visual-semantic embedding in the style of Han et al . <ref type="bibr" target="#b11">[12]</ref> by requiring that image x (u) i is embedded closer to its description t (u) i in visualsemantic space than the descriptions of the other two images in a triplet:</p><formula xml:id="formula_10">L vsei = ℓ(x (u) i , t (u) i , t (v) j ) + ℓ(x (u) i , t (u) i , t (v) k )<label>(5)</label></formula><p>and imposing analogical constraints on x k . To encourage sparsity in the learned weights w so that we achieve better disentanglement of the embedding dimensions contributing to pairwise type compatibility, we add an ℓ 1 penalty on the projection matrices P ·→(·,·) . We further use ℓ 2 regularization on the learned image embedding f (x; θ). The final training loss therefore becomes:</p><formula xml:id="formula_11">L(X, T, P ·→(·,·) , λ, θ, φ) = L comp + L sim + λ 3 L vse + λ 4 L ℓ2 + λ 5 L ℓ1<label>(6)</label></formula><p>where X and T denote the image embeddings and corresponding text embeddings in a batch, L vse = L vsei + L vsej + L vse k , and λ 3−5 are scalar parameters. We preserve the dependence on P ·→(·,·) in notation to emphasize the type dependence of our embedding. As Section 5 shows, this term has significant effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment Details</head><p>Following Han et al . <ref type="bibr" target="#b11">[12]</ref> we evaluate how well our approach performs on two tasks. In the fashion compatibility task, a candidate outfit is scored as to whether its constitute items are compatible with each other. Performance is evaluated using the average under a receiver operating characteristic curve (AUC). The second task is to select from a set of candidate items (four in this case) in a fillin-the-blank (FITB) fashion recommendation experiment. The goal is to select the most compatible item with the remainder of the outfit, and performance is evaluated by accuracy on the answered questions.</p><p>Datasets. For experiments on the Maryland Polyvore dataset <ref type="bibr" target="#b11">[12]</ref> </p><note type="other">in Section 5.1 we use the provided splits which separate the outfits into 17,316 for training, 3,076 for testing, and 1,407 for validation. For experiments using our dataset in Section 5.2 we use the different version splits described in Section 3. We shall refer to the "easier" split as Polyvore Outfits, and the split containing only disjoint outfits down to the item level as Polyvore Outfits-D. Implementation. We use a 18-layer Deep Residual Network [13] which was pretrained on ImageNet [7] for our image embedder with a general embedding size of 64 dimensions unless otherwise noted. Our model is trained with a learning rate of 5e −5 , batch size of 256, and a margin of 0.2. For our text representation, we use the HGLMM Fisher vector encoding [17] of word2vec [22] after having been PCA reduced down to 6000 dimensions. We set λ 3 = 5e −1 from Eq. (6) for the Maryland Polyvore dataset (λ 3 = 5e −5 for experiments on our dataset), and all other λ parameters from Eq. (4) and Eq. (6) to 5e −4 .</note><p>Sampling Testing Negatives. In the test set provided by Han et al . <ref type="bibr" target="#b11">[12]</ref>, a negative outfit for the compatibility experiment could end up containing only tops and no other items, and a fill-in-the blank question could have an earring be among the possible answers when trying to select a replacement for a shoe. This is a result of sampling negatives at random without restriction, and many of these negatives could simply be dismissed without considering item compatibility. Thus, to correct this issue and focus on outfits that cannot be filtered out in such a manner, we take into account item category when sampling negatives. In the compatibility experiments, we replace each item in a ground truth outfit by randomly selecting another item of the same category. For the fill-in-the-blank experiments, our incorrect answers are randomly selected items from the same category as the correct answer.</p><p>Comparative Evaluation. In addition to performance of the state-of-the-art methods reported in prior work, we compare the following approaches:</p><p>-SiameseNet (ours). The approach of Veit et al . <ref type="bibr" target="#b33">[34]</ref> which uses the same ResNet and general embedding size as used for our type-specific embeddings. -CSN, T1:1. Learns a pairwise type-dependent transformation using the approach of Veit et al . <ref type="bibr" target="#b32">[33]</ref> to project a general embedding to a type-specific space which measures compatibility between two item categories. -CSN, T4:1. Same as the previous approach, but where each learned pairwise type-dependent transformation is responsible for four pairwise comparisons (instead of one) which are assigned at random. For example, a single projection may be used to measure compatibility in the (shoe-top, bottom-hat, earrings-top, outwear-bottom) type-specific spaces. This approach allows us to assess the importance of having distinct learned compatibility spaces for each pair of item categories versus forcing the compatibility spaces to "share" <ref type="table">Table 2</ref>: Comparison of different methods on the Maryland Polyvore dataset <ref type="bibr" target="#b11">[12]</ref> using their unrestricted randomly sampled negatives on the fill-in-the-blank and outfit compatibility tasks. "All Negatives" refers to using their entire test split as is, while "Composition Filtering" refers to removing easily identifiable negative samples. The numbers in (a) are the results reported from Han et al . <ref type="bibr" target="#b11">[12]</ref> or run using their code, and (b) reports our results multiple pairwise comparisons, thus allowing for better scalability as we add more fine-grained item categories to the model. -VSE. Indicates that a visual-semantic embedding as described in Section 4.1 is learned jointly with the compatibility embedding. -Sim. Along with training the model to learn a visual-semantic embedding for compatibility between different categories of items as done with the VSE, the same embeddings are also used to measure similarity between items of the same category as described in Section 4.1. -Metric. In the triplet loss, rather than minimizing Euclidean distance between compatible items and maximizing the same for incompatible ones, an empirically more robust way is to optimize over the inner products instead. To generalize the distance metric, we take an element-wise product of the embedding vectors in the type-specific spaces and feed it into a fully-connected layer, the learned weights of which act as a generalized distance function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Maryland Polyvore</head><p>We report performance using the test splits of Han et al . <ref type="bibr" target="#b11">[12]</ref> in <ref type="table">Table 2</ref> where the negative samples were sampled completely at random without restriction. The first line of <ref type="table">Table 2</ref>(b) contains our replication of the approach of Veit et al . <ref type="bibr" target="#b33">[34]</ref>, using the same convolutional network as implemented in our models for a fair comparison. We see on the second line of <ref type="table">Table 2</ref>(b) that performance on both tasks using all the negative samples in the test split is actually reduced, while after removing easily identifiable negative samples it is increased. This is likely  <ref type="table">Table 4</ref>: Effect the embedding size has on performance on the fill-in-the-blank and the outfit compatibility tasks on the Maryland Polyvore dataset <ref type="bibr" target="#b11">[12]</ref> using our negative samples due to how the negatives were sampled. We only learn type specific embeddings to compare the compositions of items which occur during training. Thus, at test time, if we are asked to compare two tops, but no outfit seen during training contained two tops, we did not learn a type-specific embedding for this case and are forced to compare them using our general embedding. This also explains why our performance drops using the negative samples of Han et al . when we include the similarity constraint on the third line of <ref type="table">Table 2</ref>(b), since we explicitly try to learn similarity in our general embedding rather than compatibility. The same effect also accounts for the discrepancy when we train our learned metric shown in the last two lines of <ref type="table">Table 2</ref>(b). Although we report much better performance than prior work using all the negative samples, our performance is much closer to the LSTM-based method of Han et al . <ref type="bibr" target="#b11">[12]</ref> after removing the easy negatives. Since many of the negative samples of Han et al . <ref type="bibr" target="#b11">[12]</ref> can be easily filtered out, and thus make it difficult to evaluate our method due to their invalid outfit compositions, we report performance on the fill-in-the-blank and outfit compatibility tasks where the negatives are selected by replacing items of the same category in <ref type="table" target="#tab_2">Table 3</ref>. The first line of Table 2(b) shows that using our type-specific embeddings, we obtain a 2-3% improvement over learning a single embedding <ref type="figure">Fig. 2</ref>: Left: t-SNE of the learned general embedding space on Polyvore Outfits. We see the learned embedding respects color variations and for types where shape is a unique identifier (e.g., pants and sunglasses) items are more closely grouped together. Right: Overlapping items for each cell of the highlighted four columns in the t-SNE plot. Note that each row contains items that are very similar to each other, which suggests a well-behaved embedding. Best viewed in color at high resolution to compare all types. In the second and third lines of <ref type="table" target="#tab_2">Table 3</ref>(b), we see that including our visual semantic embedding, along with training our general embedding to explicitly learn similarity between objects of the same category, provides small improvements over simply learning our type-specific embeddings. We also see a pronounced improvement using our learned metric, resulting in a 3-4% improvement on both tasks over learning just the type-specific embeddings. The last line of <ref type="table" target="#tab_2">Table 3</ref>(b) reports the results of our approach using the same embedding size as Han et al . <ref type="bibr" target="#b11">[12]</ref>, showing that we obtain similar performance. This is particularly noteworthy since Han et al . uses a more powerful feature representation (Inception-v3 <ref type="bibr" target="#b30">[31]</ref> vs. ResNet-18), and takes into account the entire outfit when making comparisons, both of which would likely further improve our model. A full accounting of how the dimensions of the final embedding affects the performance of our approach is provided in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Polyvore Outfits</head><p>We report our results on the fill-in-the-blank and outfit compatibility experiments using our own dataset in <ref type="table" target="#tab_3">Table 5</ref>. The first line of <ref type="table" target="#tab_3">Table 5</ref>(b) shows that learning our type specific embeddings gives a consistent improvement over training a single embedding to make all comparisons. We note that our relative performance using the entire dataset is higher than our disjoint set, which we attribute to likely being due to the additional training data for learning each type-specific embedding. Analogous to the Maryland dataset, the next three lines of <ref type="table" target="#tab_3">Table 5</ref> show a consistent performance improvement as we add in the remaining pieces of our model. Note that the learned notion of similarity is more abstract than matching simply based on color or shape. For example, in the second row, the model is perfectly happy suggesting as alternatives both short-and long-leg pants as well as skirts, so long as the general light colors, flowery patterns and flowy materials are present. Similarly, in the third row, all similar shoes are black but they vastly differ in style (e.g., platform vs. sandal vs. bootie vs. loafer) and all have a unique statement element just like the straps detail of the query item: laces, a golden clasp, yellow detail, metal bridge  We note that the LSTM-based method of Han et al . <ref type="bibr" target="#b11">[12]</ref> works relatively poorly on our dataset, which suggests that the limited number of items in the outfits in the Maryland dataset (the maximum length of an outfit sequence is fixed to 8 items) may play an important role in obtaining good performance with their approach. The last line of <ref type="table" target="#tab_3">Table 5</ref>(b) reports the performance of our model using the same embedding size as the Han et al . <ref type="bibr" target="#b11">[12]</ref>. The full effect the embedding dimension has on our approach is provided in <ref type="table" target="#tab_4">Table 6</ref>. Additional results including an ablation study can be found in supplementary material.</p><p>Interestingly, the two splits of our data obtain similar performance, with the better results using the easy version of our dataset only a little better than on the version where all items in all outfits are novel. This suggests that having unseen outfits in the test set is more important than ensuring there are no shared items between the training and testing splits, and hence in reproductions of our experiments, using the larger version of our dataset is a fair approach.</p><p>Why does respecting type help? We visualize the global embedding space and some type-specific embedding spaces with t-SNE <ref type="bibr" target="#b20">[21]</ref>. <ref type="figure">Figure 2</ref> shows the <ref type="figure">Fig. 6</ref>: Examples of outfit generation by recursive item swaps. The top row represents a valid (i.e., human-curated) outfit. At each step, we replace an item from the starting outfit with one that is of the same type and equally compatible with the rest of the outfit, but different from the removed item. For full figure, refer to the supplementary. Best viewed in color global embedding space; <ref type="figure">Figure 3</ref> shows three distinct type-specific embedding spaces. Note how the global space is strongly oriented toward color matches (large areas allocated to each range of color), but for example the scarf-jewelery space in <ref type="figure">Figure 3</ref>(c) is not particularly focused on color representation, preferring to encode shape (long pendants vs. smaller pieces). As a result, local type-specific spaces can specialize in different aspects of appearance, and so force the global space to represent all aspects fairly evenly.</p><p>Geometric Queries. In light of the problems pointed out in the introduction, we show that our type-respecting embedding is able to handle the following geometric queries which previous models are unable or ill-equipped to perform. SiameseNet <ref type="bibr" target="#b33">[34]</ref> is not able to answer such queries by construction, and it is not straightforward how the approach of Han et al . <ref type="bibr" target="#b11">[12]</ref> would have to be repurposed in order to handle them. Our model is the first to demonstrate that this type of desirable query can be successfully addressed.</p><p>-Given an item x (see <ref type="figure">Figure 6</ref>).</p><p>-Given an item x   (see <ref type="figure">Figure 7)</ref>. <ref type="figure">Fig. 7</ref>: Examples of item swaps and outfit diversification. Row shaded in green represents a human-curated outfit. Highlighted in blue is a randomly selected heldout item from the outfit to be replaced. Rows shaded in gray displays alternatives that are all equally compatible with the rest of the outfit as the heldout item. The bottom row shows a random selection of alternatives of the same type as the heldout item. The suggested alternatives made by our model in the middle row, although equally compatible with the rest of the items in the outfit, are not forced to be similar to each other but differ vastly in color, style, shape and fine-grained type. Best viewed in color <ref type="bibr" target="#b5">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Our qualitative and quantitative results show that respecting type in embedding methods produces several strong and useful effects. First, on an established dataset, respecting type produces better performance at established tasks. Second, on a novel, and richer, dataset, respecting type produces strong performance improvements on established tasks over previous methods. Finally, an embedding method that respects type can represent both similarity relationships (whereby garments are interchangeable -say, two white blouses) and compatibility relationships (whereby a garment can be combined with another to form a coherent outfit). Visualizing the learned embedding spaces suggests that the reason we obtain significant improvements on the fill-in-the-blank and outfit compatibility tasks over prior state-of-the-art is that different type-specific spaces specialize in encoding different kinds of appearance variation. The resulting representation admits new and useful queries for clothing. One can search for item replacements; one can find a set of possible items to complete an outfit that has high variation; and one can swap items in garments to produce novel outfits. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :Fig. 4 :Fig. 5 :</head><label>345</label><figDesc>Fig. 3: t-SNE of the learned type-specific embedding space on our Polyvore dataset for: (a) tops and bags; (b) shoes and sunglasses; (c) scarves and jewelery. As hypothesized, respecting type allows the embedding to specialize to features that dominate compatibility relationships for each pair of types: for example, color seems to matter more in (a) than in (c), where shape is an equally important feature, with a concentration of long pendants in the lower right and smaller pieces towards the top. Best viewed in color at high resolution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>the same type that are all compatible with the rest of the outfit S \{x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Acknowledgements: This work is supported in part by ONR MURI Award N00014-16-1-2007, in part by NSF under Grant No. NSF IIS-1421521, and in part by a Google MURA Award and an Amazon Research Faculty Award.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Comparison in dataset statistics. Our dataset's variants (last two rows) contains more outfits than related datasets along with detailed descriptions and fine-grained semantic categories</figDesc><table>Dataset 
#Outfits #Items 
Max Items/ Text Available? 
Semantic 
Outfit 
Category? 
Maryland Polyvore [12] 21,889 164,379 
8 
Titles Only 
-
Polyvore Outfits-D 
32,140 175,485 
16 
Titles &amp; Descriptions 
Polyvore Outfits 
68,306 365,054 
19 
Titles &amp; Descriptions 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Comparison of different methods on the Maryland Polyvore Dataset [12]</figDesc><table>on the fill-in-the-blank and outfit compatibility tasks using our category-aware 
negative sampling method. (a) contains the results of prior work using their 
code unless otherwise noted, and (b) contains results using our approach 

Method 
FITB Compat. 
Accuracy AUC 

(a) Bi-LSTM + VSE (512-D) [12] 
64.9 
0.94 
SiameseNet (ours) 
54.4 
0.85 
(b) CSN, T1:1 
57.9 
0.87 
CSN, T1:1 + VSE 
58.1 
0.88 
CSN, T1:1 + VSE + Sim 
59.0 
0.87 
CSN, T4:1 + VSE + Sim + Metric 
59.9 
0.90 
CSN, T1:1 + VSE + Sim + Metric 
61.0 
0.90 
CSN, T1:1 + VSE + Sim + Metric (512-D) 65.0 
0.93 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison of different methods on the two versions of our dataset 
on the fill-in-the-blank and outfit compatibility tasks using our category-aware 
negative sampling method. (a) contains the results of prior work using their 
code unless otherwise noted, and (b) contains results using our approach 

Polyvore Outfits-D Polyvore Outfits 

Method 
FITB Compat. FITB Compat. 
Accuracy AUC Accuracy AUC 

(a) Bi-LSTM + VSE (512-D) [12] 
39.4 
0.62 
39.7 
0.65 
SiameseNet (ours) 
51.8 
0.81 
52.9 
0.81 
(b) CSN, T1:1 
52.5 
0.82 
54.0 
0.83 
CSN, T1:1 + VSE 
53.0 
0.82 
54.5 
0.84 
CSN, T1:1 + VSE + Sim 
53.4 
0.82 
54.7 
0.85 
CSN, T4:1 + VSE + Sim + Metric 
53.7 
0.82 
55.1 
0.85 
CSN, T1:1 + VSE + Sim + Metric 
54.1 
0.82 
55.3 
0.86 
CSN, T1:1 + VSE + Sim + Metric (512-D) 55.2 
0.84 
56.2 
0.86 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 6 :</head><label>6</label><figDesc>Effect the embedding size has on performance on the fill-in-the-blank and the outfit compatibility tasks using the two versions of our dataset</figDesc><table>Polyvore Outfits-D Polyvore Outfits 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code and data: https://github.com/mvasil/fashion-compatibility</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">, (2)</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fashion forward: Forecasting visual style in fashion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning visual similarity for product design with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>IJPRAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Describing clothing by semantic attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep domain adaptation for describing people based on fine-grained clothing attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Leveraging weakly annotated data for fashion image retrieval and label prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Corbiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ollion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Style finder: Finegrained clothing style detection and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dress like a star: Retrieving fashion products from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vogiatzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Self-supervised learning of visual features through embedding images into text topic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rusinol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Where to buy it: Matching street clothing photos in online shops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadi Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning fashion compatibility with bidirectional lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning compatibility across categories for heterogeneous item recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning the latent &quot;look&quot;: Unsupervised discovery of a style-coherent embedding from fashion images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Hipster wars: Discovering elements of fashion styles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fisher vectors derived from hybrid gaussianlaplacian mixture models for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mining fashion outfit composition using an end-toend deep learning approach on set data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1946" to="1955" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Hi, magic closet, tell me what to wear! In</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ACM MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing high-dimensional data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning a distance metric from relative comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">NIPS</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-modal embedding for main product detection in fashion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning cross-modal embeddings for cooking recipes and food images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hynes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Neuroaesthetics in fashion: Modeling the perception of fashionability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fashion style in 128 floats: Joint ranking and classification using weak data for feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">End-to-end localization and ranking for relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning unified embedding for apparel recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The elements of fashion style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vaccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karahalios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual Symposium on User Interface Software and Technology</title>
		<meeting>the 29th Annual Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Conditional similarity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karaletsos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning visual clothing style with heterogeneous dyadic co-occurrences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kovacs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Ssp: Semantic space projection for knowledge graph embedding with text descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Mix and match: Joint model for clothing and attribute recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taniguchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Fine-Grained Visual Comparisons with Local Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Just noticeable differences in visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Semantic jitter: Dense supervision for visual comparisons via synthetic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Memory-augmented attribute manipulation networks for interactive fashion search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Be your own prada: Fashion synthesis with structural coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Fast training of triplet-based deep binary embedding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
