<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Low-shot learning with large-scale diffusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Facebook</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Low-shot learning with large-scale diffusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Large, diverse collections of images are now commonplace; these often contain a "long tail" of visual concepts. Some concepts like "person" or "cat" appear in many images, but the vast majority of the visual classes do not occur frequently. Even though the total number of images may be large, it is hard to collect enough labeled data for most of the visual concepts. Thus if we want to learn them, we must do so with few labeled examples. This task is named low-shot learning in the literature.</p><p>In order to learn new classes with little supervision, a standard approach is to leverage classifiers already learned for the most frequent classes, employing a so-called transfer learning strategy. For instance, for new classes with few labels, only the few last layers of a convolutional neural network are re-trained. This limits the number of parameters that need to be learned and limits over-fitting.</p><p>In this paper, we consider the low-shot learning problem described above, where the goal is to learn to detect new visual classes with only a few annotated images per class, but we also assume that we have many unlabelled images. This is called semi-supervised learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b36">37]</ref> (con- * This work was carried out while B. Hariharan was post-doc at FAIR. test images (labels witheld) <ref type="figure">Figure 1</ref>. The diffusion setup. The arrows indicate the direction of diffusion. There is no diffusion performed from the test images. For the rest of the graph, the edges are bidirectional (i.e., the graph matrix is symmetric). Except when mentioned otherwise, the edges have no weights.</p><p>sidered, e.g., for face annotation <ref type="bibr" target="#b13">[14]</ref>). The motivation of this work is threefold. First we want to show that with modern computational tools, classical semi-supervised learning methods scale gracefully to hundreds of millions of unlabeled points. A limiting factor in previous evaluations was that constructing the similarity graph supporting the diffusion was slow. This is no longer a bottleneck: thanks to advances both in computing architectures and algorithms, one can routinely compute the similarity graph for 100 millions images in a few hours <ref type="bibr" target="#b20">[21]</ref>. Second, we want to answer the question: Does a very large number of images help for semi-supervised learning? Finally, by comparing the results of these methods on Imagenet and the YFCC100M dataset <ref type="bibr" target="#b32">[33]</ref>, we highlight how these methods exhibit some artificial aspects of Imagenet that can influence the performance of low shot learning algorithms.</p><p>In summary, the contribution of our paper is a study of semi-supervised learning in the scenario where we have a very large number of unlabeled images. Our main results are that in this setting, semi-supervised learning leads to state of the art low-shot learning performance. In more detail, we make the following contributions:</p><p>• We carry out a large-scale evaluation for diffusion methods for semi-supervised learning and compare it to recent low-shot learning papers. Our experiments are all carried out on the public benchmark Imagenet <ref type="bibr" target="#b10">[11]</ref> and the YFC100M dataset <ref type="bibr" target="#b32">[33]</ref>.</p><p>• We show that our approach is efficient and that the diffusion process scales up to hundreds of millions of images, which is order(s) of magnitude larger than what we are aware in the literature on image-based diffusion <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref>. This is made possible by leveraging the recent state of the art for efficient k-nearest neighbor graph construction <ref type="bibr" target="#b20">[21]</ref>.</p><p>• We evaluate several variants and hypotheses involved in diffusion methods, such as using class frequency priors <ref type="bibr" target="#b37">[38]</ref>. This scenario is realistic in situations where this statistic is known a priori. We propose a simple way to estimate it without this prior knowledge, and extend this assumption to a multiclass setting by introducing a probabilistic projection step derived from Sinkhorn-Knopp algorithm.</p><p>• Our experimental study shows that a simple propagation process significantly outperforms some state-ofthe-art approaches in low-shot visual learning when (i) the number of annotated images per class is small and when (ii) the number of unlabeled images is large or the unlabeled images come form the same domain as the test images.</p><p>This paper is organized as follows. Section 2 reviews related works and Section 3 describes the label propagation methods. The experimental study is presented in Section 4. Our conclusion in section 5 summarizes our findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Low-shot learning Recently there has been a renewed interest for low-shot learning, i.e., learning with few examples thanks to prior statistics on other classes. Such works include metric learning <ref type="bibr" target="#b25">[26]</ref>, learning kNN <ref type="bibr" target="#b34">[35]</ref>, regularization and feature hallucination <ref type="bibr" target="#b15">[16]</ref> or predicting parameters of the network <ref type="bibr" target="#b4">[5]</ref>. Ravi and Larochelle introduce a meta-learner to learn the optimization parameters invovled in the low-shot learning regime <ref type="bibr" target="#b29">[30]</ref>. Most of the works consider small datasets like Omniglot, CIFAR, or a small subset of Imagenet. In our paper we will focus solely on large datasets, in particular the Imagenet collection <ref type="bibr" target="#b30">[31]</ref> associated with the ILSVRC challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diffusion methods</head><p>We refer the reader to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref> for a review of diffusion processes and matrix normalization options. Such methods are an efficient way of clustering images given a matrix of input similarity, or a kNN graph, and have been successfully used in a semi-supervised discovery setup <ref type="bibr" target="#b13">[14]</ref>. They share some connections with spectral clustering <ref type="bibr" target="#b5">[6]</ref>. In <ref type="bibr" target="#b28">[29]</ref>, a kNN graph is clustered with spectral clustering, which amounts to computing the k eigenvectors associated with the k largest eigenvalues of the graph, and clustering these eigenvectors. Since the eigenvalues are obtained via Lanczos iterations <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">Chapter 10]</ref>, the basic operation is similar to a diffusion process. This is also related to power iteration clustering <ref type="bibr" target="#b24">[25]</ref>, as in the work of Cho et al. <ref type="bibr" target="#b7">[8]</ref> to find clusters.</p><p>Semi-supervised learning The kNN graph can be used for transductive and semi-supervised learning (see e.g. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">39]</ref> for an introduction). In transductive learning, a relatively small number of labels are used to augment a large set of unlabeled data and the goal is to extend the labeling to the unlabeled data (which is given at train time). Semisupervised learning is similar, except there may be a separate set of test points that are not seen at train time. In our work, we consider the simple proposal of Zhu et al. <ref type="bibr" target="#b37">[38]</ref>, where powers of the (normalized) kNN graph are used to find smooth functions on the kNN graph with desired values at the labeled points. There exist many variations on the algorithms, e.g., Zhou et al. <ref type="bibr" target="#b36">[37]</ref> weight the edges based on distances and introduce a loss trading a classification fitting constraint and a smoothness term enforcing consistency of neighboring nodes.</p><p>Label propagation is a transductive method. In order to evaluate on new data, we need to extend the smooth functions out of the training data. While deep networks have been used before for out of sample extension, e.g., in <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b19">[20]</ref>, in the speech domain, in this work, we use a weighted sum of nearest neighbors from the (perhaps unlabeled) training data <ref type="bibr" target="#b3">[4]</ref>.</p><p>Efficient kNN-graph construction The diffusion methods use a matrix as input containing the similarity between all images of the dataset. Considering N images, e.g., N = 10 8 , it is not possible to store a matrix of size N 2 . However most of the image pairs are not related and have a similarity close to 0. Therefore diffusion methods are usually implemented with sparse matrices. This means that we compute a graph connecting each image to its neighbors, as determined by the similarity metric between image representations. In particular, we consider the k-nearest neighbor graph (kNN-graph) over a set of vectors. Several approximate algorithms <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b16">17]</ref> have been proposed to efficiently produce the kNN graph used as input of iterative/diffusion methods, since this operation is of quadratic complexity in the number of images. In this paper, we employ the Faiss library,which was shown capable to construct a graph connecting up to 1 billion vectors <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Propagating labels</head><p>This section describes the initial stage of our proposal, which estimates the class of the unlabelled images with a diffusion process. It includes an image description step, the construction of a kNN graph connecting similar images, and a label diffusion algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Image description</head><p>A meaningful semantic image representation and an associated metric is required to match instances of classes that have not been seen beforehand. While early works on semisupervised labelling <ref type="bibr" target="#b13">[14]</ref> were using ad-hoc semantic global descriptors like GIST <ref type="bibr" target="#b26">[27]</ref>, we extract activation maps from a CNN trained on images from a set of base classes that are independent from the novel classes on which the evaluation is performed. See the experimental section for more details about the training process for descriptors.</p><p>The mean class classifier introduced for low-shot learning <ref type="bibr" target="#b25">[26]</ref> is another way to perform dimensionality reduction while improving accuracy thanks to a better comparison metric. We do not consider this approach since it can be seen as part of the descriptor learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Affinity matrix: approximate kNN graph</head><p>As discussed in the related work, most diffusion processes use as input the kNN graph representing the N × N sparse similarity matrix, denoted by W, which connects the N images of the collection. We build this graph using approximate k-nearest neighbor search. Thanks to recent advances in efficient similarity search <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>, trading some accuracy against efficiency drastically improves the graph construction time. As an example, with the FAISS library <ref type="bibr" target="#b20">[21]</ref>, building the graph associated with 600k images takes 2 minutes on 1 GPU. In our preliminary experiments, the approximation in the knn-graph does not induce any sub-optimality, possibly because the diffusion process compensates the artifacts induced by the approximation.</p><p>Different strategies exist to set the weights of the affinity matrix W. We choose to search the k nearest neighbors of each image, and set a 1 for each of the neighbors in the corresponding row of a sparse matrix W 0 . Then we symmetrize the matrix by adding it to its transpose. We subsequently ℓ 1 -normalize the rows to produce a sparse stochastic matrix:</p><formula xml:id="formula_0">W = D −1 (W ⊤ 0 + W 0 )</formula><p>, with D the diagonal matrix of row sums.</p><p>The handling for the test points is different: test points do not participate in label propagation because we classify each of them independently of the others. Therefore, there are no outgoing edges on test points; they only get incoming edges from their k nearest neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Label propagation</head><p>We now give details about the diffusion process itself, which is summarized in <ref type="figure">Figure 1</ref>. We build on the straightforward label propagation algorithm of <ref type="bibr" target="#b37">[38]</ref>. The set of images on which we perform diffusion is composed of n L labelled seed images and n B unlabelled background images (N = n L +n B ). Define the N ×C matrix L, where C is the number of classes for which we want to diffuse the labels, i.e., the new classes not seen in the training set. Each row l i in L is associated with a given image, and represents the probabilities of each class for that image. A given column corresponds to a given class, and gives its probabilities for each image. The method initializes l i to a one-hot vector for the seeds. Background images are initialized with 0 probabilities for all classes. Diffusing from the known labels, the method iterates as L t+1 = WL t .</p><p>We can optionally reset the L rows corresponding to seeds to their 1-hot ground-truth at each iteration. When iterating to convergence, all l i would eventually converge to the eigenvector of W with largest eigenvalue (when not resetting), or to the harmonic function with respect to W with boundary conditions given by the seeds (when resetting). Empirically, for low-shot learning, we observe that resetting is detrimental to accuracy. Early stopping performs better in both cases, so we cross-validate the number of diffusion iterations.</p><p>Classification decision &amp; combination with logistic regression We predict the class of a test example i as the the column that maximizes the score l i . Similar to Zhou et al. <ref type="bibr" target="#b36">[37]</ref>, we have also optimized a loss balancing the fitting constraint with the diffusion smoothing term. However we found that a simple late fusion (weighted mean of logprobabilities, parametrized by a single cross-validated coefficient) of the scores produced by diffusion and logistic regression achieves better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Variations</head><p>Using priors The label propagation can take into account several priors depending on the assumptions of the problem, which are integrated by defining a normalization operator η and by modifying the update equation as</p><formula xml:id="formula_1">L t+1 = η(WL t ).<label>(1)</label></formula><p>Multiclass assumption. For instance, in the ILSVRC challenge built upon the Imagenet dataset <ref type="bibr" target="#b30">[31]</ref>, there is only one label per class, therefore we can define η as a function that ℓ 1 -normalizes each row to provide a distribution over labels (by convention the normalization leaves all-0 vectors unchanged).</p><p>Class frequency priors. Additionally, we point out that labels are evenly distributed in Imagenet. If we translate this setup to our semi-unsupervised setting, it would mean that we may assume that the distribution of the unlabelled images is uniform over labels. This assumption can be taken into account by defining η as the function performing a ℓ 1 normalization of columns of L. While one could argue that this is not realistic in general, a more realistic scenario is to consider that we know the marginal distribution of the labels, as proposed by Zhu et al. <ref type="bibr" target="#b37">[38]</ref>, who show that the prior can be simply enforced (i.e., apply column-wise normalization to L and multiply each column by the prior class probability). This arises in situations such as tag prediction, if we can empirically measure the relative probabilities of tags, possibly regularized for lowest values.</p><p>Combined Multiclass assumption and class frequency priors. We propose a variant way to use both a multiclass setting and prior class probabilities by enforcing the matrix L to jointly satisfy the following properties:</p><formula xml:id="formula_2">L1 C = 1 N 1 ⊤ N L ∝ p C<label>(2)</label></formula><p>where p C is the prior distribution over labels. For this purpose, we adopt a strategy similar to that of Cuturi <ref type="bibr" target="#b8">[9]</ref> in his work on optimal transport, in which he shows that the Sinkhorn-Knopp algorithm <ref type="bibr" target="#b31">[32]</ref> provides an efficient and theoretically grounded way to project a matrix so that it satisfies such marginals. The Sinkhorn-Knopp algorithm iterates by alternately enforcing the marginal conditions, as</p><formula xml:id="formula_3">L ← L diag(L1 C ) −1 diag(p C ) (3) L ← diag(1 ⊤ N L) −1 L<label>(4)</label></formula><p>until convergence. Here we assume that the algorithm only operates on rows and columns whose sum is strictly positive. As discussed by <ref type="bibr">Knight [24]</ref>, the convergence of this algorithm is fast. Therefore we stop after 5 iterations. This projection is performed after each update by Eqn. 1. Note that Zhu et al. <ref type="bibr" target="#b37">[38]</ref> solely considered the second constraint in Eqn. 2, which can be obtained by enforcing the prior, as discussed by Bengio et al. <ref type="bibr" target="#b2">[3]</ref>. We evaluate both variants in the experimental section 4.</p><p>Non-linear updates. The Markov Clustering (MCL) <ref type="bibr" target="#b12">[13]</ref> is another diffusion algorithm with nonlinear updates originally proposed for clustering. In contrast to the previous algorithm, MCL iterates directly over the similarity matrix as</p><formula xml:id="formula_4">W ′ t ← W t · W t W t+1 ← Γ r (W ′ t ),<label>(5)</label></formula><p>where Γ r is an element-wise raising to power r of the matrix, followed by a column-wise normalization <ref type="bibr" target="#b12">[13]</ref>. The power r ∈ (1, 2] is a bandwidth parameter: when r is high, small edges quickly vanish along the iterations. A smaller r preserves the edges longer. The clustering is performed by extracting connected components from the final matrix. In Section 4 we evaluate the role of the non-linear update of MCL by introducing the Γ r non-linearity in the diffusion procedure. More precisely, we modify Equation 1 as L t+1 = Γ r (η(WL t )) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Complexity</head><p>For the complexity evaluation, we distinguish two stages. In the off-line stage, (i) the CNN is trained on the base classes, (ii) descriptors are extracted for the background images, and (iii) a knn-graph is computed for the background images. In the on-line stage, we receive training and test images from novel classes, (i) compute features for them, (ii) complement the knn-graph matrix to include the training and test images, and (iii) perform the diffusion iterations. Here we assume that the N × N graph matrix W 0 is decomposed in four blocks</p><formula xml:id="formula_5">W 0 = W LL W LB W BL W BB ∈ {0, 1} (nL+nB)×(nL+nB) (6)</formula><p>The largest matrix W BB ∈ {0, 1} nB×nB is computed offline. On-line we compute the three other matrices. We combine W BL and W BB by merging similarity search result lists, hence each row of W 0 contains exactly k non-zero values, requiring to store the distances along with W BB .</p><p>We are mostly interested in the complexity of the online phase. Therefore we exclude the descriptor extraction, which is independent of the classification complexity, and the complexity of handling the test images, which is negligible compared to the training operations. We consider the logistic regression as a baseline for the complexity comparison:</p><p>Logistic regression the SGD training entails O(I logreg × B × C × d) multiply-adds, with d denotes the descriptor dimensionality and C the number of classes. The number of iterations and batch size are I logreg and B.</p><p>Diffusion the complexity is decomposed into: computing the matrices W LL , W LB and W BL , which in-</p><formula xml:id="formula_6">volves O(d × n L × n B )</formula><p>multiply-adds using bruteforce distance computations; and performing I dif iterations of sparse-dense matrix multiplications, which incurs O(k ×N ×C ×I dif ) multiply-adds (note, sparse matrix operations are more limited by irregular memory access patterns than arithmetic operations). Therefore the diffusion complexity is linear in the number of background images n B . See the supplemental for more details.</p><p>Memory usage. One important bottleneck of the algorithm is its memory usage. The sparse matrix W 0 occupies 8N k bytes in RAM, and W almost twice this amount, because most nearest neighbors are not reciprocal; the L matrix is 4CN bytes. Fortunately, the iterations can be performed one column of L at a time, reducing this to 2 × 4N bytes for L t and L t+1 (in practice, when memory is an issue, we group columns by batches of size C ′ &lt; C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and evaluation protocol</head><p>We use Imagenet 2012 <ref type="bibr" target="#b10">[11]</ref> and follow a recent setup <ref type="bibr" target="#b15">[16]</ref> previously introduced for low-shot learning. The 1000 Imagenet classes are split randomly into two groups, each containing base and novel classes. Group 1 (193 base and 300 novel classes) is used for hyper-parameter tuning and group 2 (196+311 classes) for testing with fixed hyperparameters. We assume the full Imagenet training data is available for the base classes. For the novel classes, only n images per class are available for training. Similar to <ref type="bibr" target="#b15">[16]</ref> the subset of n images is drawn randomly and the random selection is performed 5 times with different random seeds.</p><p>As a large source of unlabelled images, we use the YFCC100M dataset <ref type="bibr" target="#b32">[33]</ref>. It consists of 99 million representative images from the Flickr photo sharing site <ref type="bibr" target="#b0">1</ref> . Note that some works have used this dataset with tags or GPS metadata as weak supervision <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning the image descriptors.</head><p>We use the 50-layer Resnet trained by Hariharan et al. <ref type="bibr" target="#b15">[16]</ref> on all base classes (group 1 + group 2), to ensure that the description calculation has never seen any image of the novel classes. We run the CNN on all images, and extract a 2048-dim vector from the 49th layer, just before the last fully connected layer. This descriptor is used directly as input for the logistic regression. For the diffusion, we PCA-reduce the feature vector to 256 dimensions and L2-normalize it, which is standard in prior works on unsupervised image matching with pre-learned image representations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Performance measure and baseline In a given group (1 or 2), we classify the Imagenet validation images from both the base and novel classes, and measure the top-5 accuracy. Therefore the class distribution is heavily unbalanced. Since the seed images are drawn randomly, we repeat the random draws 5 times with different random seeds and average the obtained top-5 accuracy (the ±xx notation gives the standard deviation).</p><p>The baseline is a logistic regression applied on the labelled points. We employ a per-class image sampling strategy to circumvent the unbalanced number of examples per class. We optimize the learning rate, batch size and L2 regularization factor of the logistic regression on the group 1 The tests are performed for n = 2 and k = 30, with 5 runs on the group 1 validation images. Variants that require a parameter (e.g., the σ of the Gaussian weighting) are indicated with a "*". In this case we report only the best result, see the supplementary material for full results. In the rest of the paper, we use the variants indicated in bold, since they are simple and do not add any parameter.</p><p>images. It is worth noticing that our baseline outperforms the reported state of the art in this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background images for diffusion</head><p>We consider the following sets of background images:</p><p>1. None: the diffusion is directly from the seed images to the test images;</p><p>2. In-domain setting: the background images are the Imagenet training image from the novel classes, but without labels. This corresponds to a use case where all images are known to belong to a set of classes, but only a subset of them have been labelled; 3. Out-of-domain setting: the n B background images are taken from YFCC100M. We denote this setting by F100k, F1M, F10M or F100M, depending on the number of images we use (e.g., we note F1M for n B = 10 6 ). This corresponds to a more challenging setting where we have no prior knowledge about the image used in the diffusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Parameters of diffusion</head><p>We compare a few settings of the diffusion algorithm as discussed in section 3.4. In all cases, we set the number of nearest neighbors to k = 30 and evaluate with n = 2. The nearest neighbors are computed with Faiss <ref type="bibr" target="#b20">[21]</ref>, using the IVFFlat index. It computes exact distances but occasionally misses a few neighbors (see the supplementary material for details).</p><p>Graph edge weighting. We experimented with different weightings for W 0 , that were proposed in the literature. We compared a constant weight, a Gaussian weighting <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b2">3]</ref>, (with σ a hyper-parameter), and a weighting based on the "meaningful neighbors" proposal <ref type="bibr" target="#b27">[28]</ref>. <ref type="table">Table 1</ref> shows that results are remarkably independent of the weighting choice, which is why we set it to 1 2 . The best normalization that can be applied to the L matrix is a simple column-wise L1 normalization. Thanks to the linear iteration formula, it can be applied at the end of the iterations. <ref type="figure" target="#fig_1">Figure 2</ref> reports experiments by varying the number of background images n B and the number k of neighbors, for n = 2. All the curves have an optimal point in terms of accuracy vs computational cost at k=30. This may be a intrinsic property of the descriptor manifold. An additional number: before starting the diffusion iterations, with k=1000 and no background images (the best setting) we obtain an accuracy of 60.5%. This is a knn-classifier and this is the fastest setting because the knn-graph does not need to be constructed nor stored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Large-scale diffusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with low-shot classifiers</head><p>We compare the performance of diffusion against the logistic baseline classifiers and a recent method of the state of the art <ref type="bibr" target="#b15">[16]</ref>, using the same features.</p><p>In-domain scenario. For low-shot learning (n ≤ 5), the in-domain diffusion outperforms the other methods by a large margin, see <ref type="table">Table 2</ref>. The combination with logistic regression is not very effective.</p><p>Hariharan logistic in-domain diffusion n et al. <ref type="bibr" target="#b15">[16]</ref>   <ref type="table">Table 2</ref>. In-domain diffusion on Imagenet: We compare against logistic regression and a recent low-shot learning technique <ref type="bibr" target="#b15">[16]</ref> on this benchmark. Results are reported with k = 30 for diffusion.</p><p>Out-of-domain diffusion. <ref type="table">Table 3</ref> shows that the performance of diffusion is competitive only when 1 or 2 images are available per class. As stated in Section 3.2, we do not include the test points in the diffusion, which is standard for a classification setting. However, if we allow this, as in a fully transductive setting, we obtain a top-5 accuracy of 69.6%±0.68 with n = 2 with diffusion over F1M, i.e., on par with diffusion over F100M.</p><p>Classifier combination. We experimented with a very simple late fusion: to combine the scores of the two classifiers, we simply take a weighted average of their predictions (log-probabilities), and cross validate the weight factor. Both in the in-domain <ref type="table">(Table 2</ref>) and out-of-domain (Table 3) cases, the results are significantly above the best of the two input classifiers. This shows that the logistic regression classifier and the diffusion classifier access different aspects of image collection. We also experimented with more complicated combination methods, like using the graph edges as a regularizer during the logistic regression, which did not improve this result.</p><p>Comparison with the state of the art. With the indomain diffusion, we notice that our method outperforms the state-of-the-art result of <ref type="bibr" target="#b15">[16]</ref> and which, itself, outperforms or is closely competitive with <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> in this setting.</p><p>In the out-of-domain setting, out results are better only for n=1. However, their method is a complementary combination of a specific loss and a learned data augmentation procedure that is specifically tailored to the experimental setup with base and novel classes. In contrast, our diffusion procedure is generic and has only two parameters (n B and k). Note that the out-of-domain setting is comparable with the standard low-shot setting, because the unlabeled images from F100M are generic, and have nothing to do with Imagenet; and because the neighbor construction and diffusion are efficient enough to be run on a single workstation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Complexity: Runtime and memory</head><p>We measured the run-times of the different steps involved in diffusion process and report them in  <ref type="table">Table 3</ref>. Out-of-domain diffusion: Comparison of classifiers for different values of n, with k = 30 for the diffusion results. The "none" column indicates that the diffusion solely relies on the labelled images. The results of the rightmost column <ref type="bibr" target="#b15">[16]</ref> are state-of-the-art on this benchmark to our knowledge, generally outperforming the results of matching networks and model regression <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>   graph construction time is linear in n B , thanks to the precomputation of the graph matrix for the background images (see Section 3.5). For comparison, training the logistic regression takes between 2m27s and 12m, depending on the cross-validated parameters.</p><p>In terms of memory usage, the biggest F100M experiments need to simultaneously keep in RAM a W matrix of 5.3 billion non-zero values (39.5 GiB), and L t and L t+1 (35.8 GiB, using slices of C ′ = 96 columns). This is the main drawback of using diffusion. However <ref type="table">Table 3</ref> shows that restricting the diffusion to 10 million images already provides most of the gain, while dividing by an order of magnitude memory and computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Analysis of the diffusion process</head><p>We discuss how fast L "fills up" (it is dense after a few iterations). We consider the rate of nodes reached by the diffusion process: we consider very large graphs, few seeds and a relatively small graph degree. <ref type="figure" target="#fig_2">Figure 3</ref> measures the sparsity of the matrix L (on one run of validation), which indicates the rate of (label, image) tuples that have not been attained by the diffusion process at each diffusion step. While the graph is not necessarily fully connected, we observe that most images can be reached by all labels in practice.</p><p>The fraction of nodes reached by all labeled points grows rapidly and converges to a value close to 1 in a few iterations when k ≥ 10. In order to relate this observation with the performance attained along iterations, it is interesting to compare what happens in this plot to the one on the right. The plot on the right shows that the iteration number at which the matrix close to 1 is similar to the iteration at which accuracy is maximal, as selected by cross-validation. The maximum occurs later if n B is larger and when k is smaller. Note also that early stopping is important. <ref type="figure" target="#fig_3">Figure 4</ref> shows paths between a seed image and test images, which gives a partial view of the diffusion. Given a class, we backtrack the path: for a given node (image) and iteration i, we look up the preceding node that contributed most to the weight in L i that node at that iteration. At iteration 0, the backtracking process always ends in a source node. Each row of the figure is one such paths. For a test image (right), we show the path for the ground-truth class and that for the found class, or a single row for both when the image is classified correctly. Note that the preceding node  can be the image itself, since the diagonal of the W matrix is not set to 0. Thanks to the size of the dataset, the paths are quite "smooth": they evolve through similar images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Qualitative results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We experimented on large-scale label propagation for low-shot learning. Unsurprisingly, we have found that performing diffusion over images from the same domain works much better than images from a different domain. We clearly observe that, as the number of images over which we diffuse grows, the accuracy steadily improve. The main performance factor is the total number of edges, which also reasonably reflects the complexity. We also report neutral results for most sophisticated variants, for instance we show that edge weights are not useful. Furthermore, labeled images should be included in the diffusion process and not just used as sources, i.e., not enforced to keep their label.</p><p>The main outcome of our study is to show that diffusion over a large image set is superior to state-of-the-art methods for low-shot learning when very few labels are available. Interestingly, late-fusion with a standard classifier's result is effective. This shows the complementary of the approaches, and suggests that it could be combined with forthcoming methods for low-short learning.</p><p>When more labels are available, simple logistic regression becomes superior to the methods we describe (and to other state of the art low-shot learning methods). However, we note that there are many circumstances where even a few labels per class are more difficult to get than building (and then keeping) a graph over unlabeled data. For example, if there are a large number of "tail" classes which we will need to classify, a few examples per class can multiply to many labels. In these cases diffusion combined with logistic regression is the best method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Classification performance with n = 2, with various settings of k and nB, ordered by total number of edges (average of 5 test runs, with cross-validated number of iterations).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Statistics over iterations, for n = 2. Top: Rate of nonzero element in the matrix L. Bottom: corresponding accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Images visited during the diffusion process from a seed (left) to the test image (right). We give ground-truth class for Imagenet images (test images marked by parentheses). The first two rows are classified correctly. The two bottom ones are failure cases. Imagenet images are not shown for copyright reasons, but the labels are shown. For YFCC100M images, we provide the Flickr id of their creators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 .</head><label>4</label><figDesc>The</figDesc><table>out-of-domain diffusion 

logistic 
diffusion+logistic 
Hariharan 
n 
none 
F1M 
F10M 
F100M 
regression 
+F10M 
+ F100M 
et al. [16] 
1 58.5±0.52 61.4±0.61 62.7±0.76 63.6±0.61 60.4±0.78 63.3±0.73 64.0±0.70 
63.6 
2 63.6±0.60 66.8±0.71 68.4±0.74 69.5±0.60 68.8±0.82 70.6±0.80 71.1±0.82 
71.5 
5 69.0±0.46 72.5±0.27 74.0±0.35 75.2±0.40 79.1±0.35 79.4±0.34 79.7±0.38 
80.0 
10 73.9±0.15 76.2±0.19 77.4±0.31 78.5±0.34 83.4±0.16 83.6±0.13 83.9±0.10 
83.3 
20 78.0±0.15 79.1±0.23 80.0±0.27 80.8±0.18 86.0±0.15 86.2±0.12 86.3±0.17 
85.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>Timings for the different steps on a 24-core 2.5GHz 
machine, for a varying number of unlabelled images from 
YFCC100M. Note, the timing of 4h08m for graph completion over 
F100M takes only 23m when executed on 8 GPUs. 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Of the 100M original files, some are videos and some are not available anymore. We replace them with uniform white images.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that our parametric experiments use the set of baseline image descriptors used in the arXiv version of [16], and the table compares all methods using those underlying features, so the results are not directly comparable with the rest of the paper.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Web-scale image clustering revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Z</forename><surname>Emiris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural codes for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Label propagation and quadratic criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semi-Supervised Learning</title>
		<editor>O. Chapelle, B. Schölkopf, and A. Zien</editor>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="195" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Out-of-sample extensions for lle, isomap, mds, eigenmaps, and spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paiement</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ouimet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning feed-forward one-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Diffusion maps, spectral clustering and reaction coordinates of dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R C</forename><surname>Boaz Nadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Lafon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">G</forename><surname>Kevrekidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<pubPlace>Arxiv</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extrapolating learned manifolds for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="381" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mode-seeking on graphs via random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient k-nearest neighbor graph construction for generic similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2011-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li-Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Diffusion processes for retrieval revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1320" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An efficient algorithm for large-scale detection of protein families</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Enright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Dongen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Ouzounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised learning in gigantic image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Matrix computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Loan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>John Hopkinks University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fanng: Fast approximate nearest neighbour graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast spectral ranking for similarity search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient diffusion on region manifolds: Recovering small objects with compact CNN representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Scalable out-of-sample extension of graph embeddings using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lyzinski</surname></persName>
		</author>
		<idno>abs/1508.04422</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<title level="m">Billion-scale similarity search with GPUs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning visual features from large weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Loh and behold: Web-scale visual search, recommendation and clustering using locally optimized hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mellina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06480</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Sinkhorn-Knopp algorithm: convergence and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="261" to="275" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Power iteration clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Metric learning for large scale image classification: Generalizing to new classes at near-zero cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: a holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Highdimensional feature matching: employing the concept of meaningful nearest neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Omercevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Drbohlav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Object mining using a matching graph on very large image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Optimization as a model for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Concerning nonnegative matrices and doubly stochastic matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sinkhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Knopp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pacific Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="343" to="348" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">YFCC100M: The new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Particular object retrieval with integral max-pooling of CNN activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sicre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to learn: Model regression networks for easy small sample learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using Gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Introduction to Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
