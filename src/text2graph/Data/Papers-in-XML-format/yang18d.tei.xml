<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mean Field Multi-Agent Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaodong</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minne</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Mean Field Multi-Agent Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Existing multi-agent reinforcement learning methods are limited typically to a small number of agents. When the agent number increases largely, the learning becomes intractable due to the curse of the dimensionality and the exponential growth of agent interactions. In this paper, we present Mean Field Reinforcement Learning where the interactions within the population of agents are approximated by those between a single agent and the average effect from the overall population or neighboring agents; the interplay between the two entities is mutually reinforced: the learning of the individual agent's optimal policy depends on the dynamics of the population, while the dynamics of the population change according to the collective patterns of the individual policies. We develop practical mean field Q-learning and mean field Actor-Critic algorithms and analyze the convergence of the solution to Nash equilibrium. Experiments on Gaussian squeeze, Ising model, and battle games justify the learning effectiveness of our mean field approaches. In addition, we report the first result to solve the Ising model via model-free reinforcement learning methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-agent reinforcement learning (MARL) is concerned with a set of autonomous agents that share a common environment <ref type="bibr" target="#b5">(Busoniu et al., 2008)</ref>. Learning in MARL is fundamentally difficult since agents not only interact with the environment but also with each other. Independent Qlearning <ref type="bibr" target="#b41">(Tan, 1993</ref>) that considers other agents as a part of the environment often fails as the multi-agent setting breaks the theoretical convergence guarantee and makes the learning unstable: changes in the policy of one agent will affect that of the others, and vice versa <ref type="bibr" target="#b31">(Matignon et al., 2012)</ref>.</p><p>Instead, accounting for the extra information from conjecturing the policies of other agents is beneficial to each single learner <ref type="bibr" target="#b10">(Foerster et al., 2017;</ref><ref type="bibr" target="#b29">Lowe et al., 2017a)</ref>. Studies show that an agent who learns the effect of joint actions has better performance than those who do not in many scenarios, including cooperative games <ref type="bibr" target="#b33">(Panait &amp; Luke, 2005)</ref>, zerosum stochastic games <ref type="bibr" target="#b26">(Littman, 1994)</ref>, and general-sum stochastic games <ref type="bibr" target="#b27">(Littman, 2001;</ref><ref type="bibr" target="#b17">Hu &amp; Wellman, 2003)</ref>.</p><p>The existing equilibrium-solving approaches, although principled, are only capable of solving a handful of agents <ref type="bibr" target="#b17">(Hu &amp; Wellman, 2003;</ref><ref type="bibr" target="#b4">Bowling &amp; Veloso, 2002)</ref>. The computational complexity of directly solving (Nash) equilibrium would prevent them from applying to the situations with a large group or even a population of agents. Yet, in practice, many cases do require strategic interactions among a large number of agents, such as the gaming bots in Massively Multiplayer Online Role-Playing Game <ref type="bibr" target="#b21">(Jeong et al., 2015)</ref>, the trading agents in stock markets <ref type="bibr" target="#b42">(Troy, 1997)</ref>, or the online advertising bidding agents .</p><p>In this paper, we tackle MARL when a large number of agents co-exist. We consider a setting where each agent is directly interacting with a finite set of other agents; through a chain of direct interactions, any pair of agents is interconnected globally <ref type="bibr" target="#b2">(Blume, 1993)</ref>. The scalability is solved by employing Mean Field Theory <ref type="bibr" target="#b39">(Stanley, 1971</ref>) -the interactions within the population of agents are approximated by that of a single agent played with the average effect from the overall (local) population. The learning is mutually reinforced between two entities rather than many entities: the learning of the individual agent's optimal policy is based on the dynamics of the agent population, meanwhile, the dynamics of the population is updated according to the individual policies. Based on such formulation, we develop practical mean field Q-learning and mean field Actor-Critic algorithms, and discuss the convergence of our solution under certain assumptions. Our experiment on a simple multi-agent resource allocation shows that our mean field MARL is capable of learning over many-agent interactions when others fail. We also demonstrate that with temporaldifference learning, mean field MARL manages to learn and solve the Ising model without even explicitly knowing the energy function. At last, in a mixed cooperative-competitive battle game, we show that the mean field MARL achieves high winning rates against other baselines previously reported for many agent systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminary</head><p>MARL intersects between reinforcement learning and game theory. The marriage of the two gives rise to the general framework of stochastic game <ref type="bibr" target="#b36">(Shapley, 1953)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Stochastic Game</head><p>An N-agent (or, N-player) stochastic game Γ is formalized by the tuple Γ S, A 1 , . . . , A N , r 1 , . . . , r N , p, γ , where Sdenotes the state space, and A j is the action space of agent j ∈ {1, . . . , N}. The reward function for agent j is defined as r j : S× A 1 × · · · × A N → R, determining the immediate reward. The transition probability p : S× A 1 × · · · × A N → Ω(S) characterizes the stochastic evolution of states in time, with Ω(S) being the collection of probability distributions over the state space S. The constant γ ∈ [0, 1) represents the reward discount factor across time. At time step t, all agents take actions simultaneously, each receives the immediate reward r j t as a consequence of taking the previous actions. The agents choose actions according to their policies, also known as strategies. For agent j, the corresponding policy is defined as</p><formula xml:id="formula_0">π j : S → Ω(A j ),</formula><p>where Ω(A j ) is the collection of probability distributions over agent j's action space</p><formula xml:id="formula_1">A j . Let π [π 1 , . . . , π N ]</formula><p>denote the joint policy of all agents; we assume, as one usually does, π to be time-independent, which is referred to be stationary. Provided an initial state s, the value function of agent j under the joint policy π is written as the expected cumulative discounted future reward:</p><formula xml:id="formula_2">v j π (s) = v j (s; π) = ∞ t=0 γ t E π, p r j t |s 0 = s, π . (1)</formula><p>The Q-function (or, the action-value function) can then be defined within the framework of N-agent game based on the Bellman equation given the value function in Eq.</p><p>(1) such that the Q-function Q j π : S× A 1 × · · · × A N → R of agent j under the joint policy π can be formulated as</p><formula xml:id="formula_3">Q j π (s, a) = r j (s, a) + γE s ∼p [v j π (s )] ,<label>(2)</label></formula><p>where s is the state at the next time step. The value function v j π can be expressed in terms of the Q-function in Eq. (2) as</p><formula xml:id="formula_4">v j π (s) = E a∼π Q j π (s, a) .<label>(3)</label></formula><p>The Q-function for N-agent game in Eq. (2) extends the formulation for single-agent game by considering the joint action taken by all agents a [a 1 , . . . , a N ], and by taking the expectation over the joint action in Eq. (3).</p><p>We formulate MARL by the stochastic game with a discretetime non-cooperative setting, i.e. no explicit coalitions are considered. The game is assumed to be incomplete but to have perfect information <ref type="bibr" target="#b26">(Littman, 1994)</ref>, i.e. each agent knows neither the game dynamics nor the reward functions of others, but it is able to observe and react to the previous actions and the resulting immediate rewards of other agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Nash Q-learning</head><p>In MARL, the objective of each agent is to learn an optimal policy to maximize its value function. Optimizing the v j π for agent j depends on the joint policy π of all agents, the concept of Nash equilibrium in stochastic games is therefore of great importance <ref type="bibr" target="#b17">(Hu &amp; Wellman, 2003)</ref>. It is represented by a particular joint policy π * [π 1 * , . . . , π N * ] such that for all s ∈ S, j ∈ {1, . . . , N} and all valid π j , it satisfies</p><formula xml:id="formula_5">v j (s; π * ) = v j (s; π j * , π −j * ) ≥ v j (s; π j , π −j * ).</formula><p>Here we adopt a compact notation for the joint policy of all agents except j as π</p><formula xml:id="formula_6">−j * [π 1 * , . . . , π j−1 * , π j+1 * , . . . , π N * ].</formula><p>In a Nash equilibrium, each agent acts with the best response π j * to others, provided that all other agents follow the policy π −j * . It has been shown that, for a N-agent stochastic game, there is at least one Nash equilibrium with stationary policies <ref type="bibr" target="#b9">(Fink et al., 1964)</ref>. Given a Nash policy π * , the Nash value</p><formula xml:id="formula_7">function v Nash (s) [v 1 π * (s), . . . , v N π * (s)]</formula><p>is calculated with all agents following π * from the initial state s onward.</p><p>Nash Q-learning <ref type="bibr" target="#b17">(Hu &amp; Wellman, 2003)</ref> defines an iterative procedure with two alternating steps for computing the Nash policy: 1) solving the Nash equilibrium of the current stage game defined by {Q t } using the Lemke-Howson algorithm <ref type="bibr" target="#b25">(Lemke &amp; Howson, 1964)</ref>, 2) improving the estimation of the Q-function with the new Nash equilibrium value. It can be proved that under certain assumptions, the Nash operator H Nash defined by the following expression</p><formula xml:id="formula_8">H Nash Q(s, a) = E s ∼p r(s, a) + γv Nash (s )<label>(4)</label></formula><p>forms a contraction mapping, where</p><formula xml:id="formula_9">Q [Q 1 , . . . , Q N ], and r(s, a) [r 1 (s, a), . . . , r N (s, a)]</formula><p>. The Q-function will eventually converge to the value received in a Nash equilibrium of the game, referred to as the Nash Q-value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Mean Field MARL</head><p>The dimension of joint action a grows proportionally w.r.t. the number of agents N. As all agents act strategically and evaluate simultaneously their value functions based on the joint actions, it becomes infeasible to learn the standard Q-function Q j (s, a). To address this issue, we factorize the Q-function using only the pairwise local interactions:</p><formula xml:id="formula_10">Q j (s, a) = 1 N j k∈N(j) Q j (s, a j , a k ) ,<label>(5)</label></formula><p>where N( j) is the index set of the neighboring agents of agent j with the size N j = |N( j)| determined by the settings of different applications. It is worth noting that the pairwise approximation of the agent and its neighbors, while significantly reducing the complexity of the interactions among agents, still preserves global interactions between any pair of agents implicitly <ref type="bibr" target="#b2">(Blume, 1993)</ref>. Similar approaches can be found in factorization machine <ref type="bibr" target="#b35">(Rendle, 2012)</ref> and learning to rank <ref type="bibr" target="#b6">(Cao et al., 2007)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Mean Field Approximation</head><p>The pairwise interaction Q j (s, a j , a k ) as in Eq. (5) can be approximated using the mean field theory <ref type="bibr" target="#b39">(Stanley, 1971</ref>  </p><formula xml:id="formula_11">a k =ā j + δa j,k , whereā j = 1 N j k a k ,<label>(6)</label></formula><formula xml:id="formula_12">Q j (s, a) = 1 N j k Q j (s, a j , a k ) = 1 N j k Q j (s, a j ,ā j ) + ∇ā j Q j (s, a j ,ā j ) · δa j, k + 1 2 δa j, k · ∇ 2 a j, k Q j (s, a j ,ã j, k ) · δa j, k = Q j (s, a j ,ā j ) + ∇ā j Q j (s, a j ,ā j ) · 1 N j k δa j, k + 1 2N j k δa j, k · ∇ 2 a j, k Q j (s, a j ,ã j, k ) · δa j, k (7) = Q j (s, a j ,ā j ) + 1 2N j k R j s, a j (a k ) ≈ Q j (s, a j ,ā j ) ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_13">R j s,a j (a k ) δa j,k · ∇ 2 a j, k Q j (s, a j ,ã j,k ) · δa j,k denotes the Taylor polynomial's remainder withã j,k =ā j + j,k δa j,k and j,k ∈ [0, 1].</formula><p>In Eq. <ref type="formula">(7)</ref>, k δa k = 0 by Eq. (6) such that the first-order term is dropped. From the perspective of agent j, the action a k in the second-order remainders R j s,a j (a k ) is chosen based on the external action distribution</p><formula xml:id="formula_14">of agent k, R j s,a j (a k )</formula><p>is thus essentially a random variable.</p><p>In fact, one can further prove that the remainder</p><formula xml:id="formula_15">R j s,a j (a k ) is bounded within a symmetric interval [−2M, 2M] under the mild condition of the Q-function Q j (s, a j , a k ) being M- smooth (e.</formula><p>g. the linear function); as a result, R j s,a j (a k ) acts as a small fluctuation near zero. To stay self-contained, the derivation of the bound is put in the Appendix B. With the assumptions of homogeneity and locality on all agents within the neighborhood, the remainders tend to cancel each other, leading to the left term of Q j (s, a j ,ā j ) in Eq. (8).</p><p>As illustrated in <ref type="figure" target="#fig_1">Fig. 1</ref>, with the mean field approximation, the pairwise interactions Q j (s, a j , a k ) between agent j and each neighboring agent k are simplified as that between j, the central agent, and the virtual mean agent, that is abstracted by the mean effect of all neighbors within j's neighborhood. The interaction is thus simplified and <ref type="figure" target="#fig_1">Figure 1</ref>: Mean field approximation. Each agent is represented as a node in the grid, which is only affected by the mean effect from its neighbors (the blue area). Manyagent interactions are effectively converted into twoagent interactions.</p><p>expressed by the mean field Q-function Q j (s, a j ,ā j ) in Eq. (8). During the learning phase, given an experience e = s, {a k }, {r j }, s , the mean field Q-function is updated in a recurrent manner as</p><formula xml:id="formula_16">Q j t +1 (s, a j ,ā j ) = (1 − α)Q j t (s, a j ,ā j ) + α[r j + γv j t (s )] ,<label>(9)</label></formula><p>where α t denotes the learning rate, andā j is the mean action of all neighbors of agent j as defined in Eq. (6). The mean field value function v j t (s ) for agent j at time t in Eq. <ref type="formula" target="#formula_16">(9)</ref> is</p><formula xml:id="formula_17">v j t (s ) = a j π j t a j |s ,ā j Eā j (a − j )∼π − j t Q j t s , a j ,ā j ,<label>(10)</label></formula><p>As shown in Eqs. <ref type="formula" target="#formula_16">(9)</ref> and <ref type="formula" target="#formula_17">(10)</ref>, with the mean field approximation, the MARL problem is converted into that of solving for the central agent j's best response π j t w.r.t. the mean actionā j of all j's neighbors, which represents the action distribution of all neighboring agents of the central agent j.</p><p>We introduce an iterative procedure in computing the best response π j t of each agent j. In the stage game {Q t }, the mean actionā j of all j's neighbors is first calculated by averaging the actions a k taken by j's N j neighbors from the policies π k t parametrized by their previous mean actionsā</p><formula xml:id="formula_18">k − a j = 1 N j k a k , a k ∼ π k t (·|s,ā k − ) ,<label>(11)</label></formula><p>With eachā j calculated as in Eq. (11), the policy π j t changes consequently due to the dependence on the currentā j . The new Boltzmann policy is then determined for each j that</p><formula xml:id="formula_19">π j t (a j |s,ā j ) = exp − βQ j t (s, a j ,ā j ) a j ∈A j exp − βQ j t (s, a j ,ā j )</formula><p>. <ref type="formula" target="#formula_3">(12)</ref> By iterating Eqs. <ref type="formula" target="#formula_18">(11)</ref> and <ref type="formula" target="#formula_3">(12)</ref>, the mean actionsā j and the corresponding policies π j t for all agents improves alternatively. In spite of lacking an intuitive impression of being stationary, in the following subsections, we will show that the mean actionā j will be equilibrated at an unique point after several iterations, and hence the policy π j t converges. To distinguish from the Nash value function v Nash (s) in Eq. (4), we denote the mean field value function in Eq. <ref type="formula" target="#formula_17">(10)</ref> as</p><formula xml:id="formula_20">v MF (s) [v 1 (s), . . . , v N (s)].</formula><p>With v MF assembled, we now define the mean field operator H MF in the form of</p><formula xml:id="formula_21">H MF Q(s, a) = E s ∼p r(s, a) + γv MF (s ) .<label>(13)</label></formula><p>In fact, we can prove that H MF forms a contraction mapping; that is, one updates Q by iteratively applying the mean field operator H MF , the mean field Q-function will eventually converge to the Nash Q-value under certain assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation</head><p>We can implement the mean field Q-function in Eq. (8) by universal function approximators such as neural networks, where the Q-function is parameterized with the weights φ. The update rule in Eq. (9) can be reformulated as weights adjustment. For off-policy learning, we exploit either standard Q-learning <ref type="bibr" target="#b44">(Watkins &amp; Dayan, 1992)</ref> for discrete action spaces or DPG <ref type="bibr" target="#b37">(Silver et al., 2014)</ref> for continuous action spaces. Here we focus on the former, which we call MF-Q.</p><p>In MF-Q, agent j is trained by minimizing the loss function</p><formula xml:id="formula_22">L(φ j ) = y j − Q φ j (s, a j ,ā j ) 2 ,</formula><p>where</p><formula xml:id="formula_23">y j = r j + γ v MF φ j − (s ) is the target mean field value calculated with the weights φ j − . Differentiating L(φ j ) gives ∇ φ j L(φ j ) = y j − Q φ j (s, a j ,ā j ) ∇ φ j Q φ j (s, a j ,ā j ) ,<label>(14)</label></formula><p>which enables the gradient-based optimizers for training.</p><p>Instead of setting up Boltzmann policy using the Q-function as in MF-Q, we can explicitly model the policy by neural networks with the weights θ, which leads to the on-policy actor-critic method <ref type="bibr" target="#b23">(Konda &amp; Tsitsiklis, 2000)</ref> that we call MF-AC. The policy network π θ j , i.e. the actor, of MF-AC is trained by the sampled policy gradient:</p><formula xml:id="formula_24">∇ θ j J(θ j ) ≈ ∇ θ j log π θ j (s)Q φ j (s, a j ,ā j ) a=π θ j (s)</formula><p>.</p><p>The critic of MF-AC follows the same setting for MF-Q with Eq. (14). During the training of MF-AC, one needs to alternatively update φ and θ until convergence. We illustrate the MF-Q iterations in <ref type="figure" target="#fig_0">Fig. 2</ref>, and present the pesudocode for both MF-Q and MF-AC in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Proof of Convergence</head><p>We now prove the convergence of</p><formula xml:id="formula_25">Q t [Q 1 t , . . . , Q N t ] to the Nash Q-value Q * = [Q 1 * , . . . , Q N * ]</formula><p>as the iterations of MF-Q is applied. The proof is presented by showing that the mean field operator H MF in Eq. (13) forms a contraction mapping with the fixed point at Q * under the main assumptions. We start from introducing the assumptions: Assumption 1. Each action-value pair is visited infinitely often, and the reward is bounded by some constant K. Assumption 2. Agent's policy is Greedy in the Limit with Infinite Exploration (GLIE). In the case with the Boltzmann policy, the policy becomes greedy w.r.t. the Q-function in the limit as the temperature decays asymptotically to zero. Assumption 3. For each stage game [Q 1 t (s), ..., Q N t (s)] at time t and in state s in training, for all t, s, j ∈ {1, . . . , N}, the Nash equilibrium π * = [π 1 * , . . . , π N * ] is recognized either as 1) the global optimum or 2) a saddle point expressed as: Note that Assumption 3 imposes a strong constraint on every single stage game encountered in training. In practice, however, we find this constraint appears not to be a necessary condition for the learning algorithm to converge. This is in line with the empirical findings in <ref type="bibr" target="#b17">Hu &amp; Wellman (2003)</ref>.</p><formula xml:id="formula_26">1. E π * [Q j t (s)] ≥ E π [Q j t (s)], ∀π ∈ Ω k A k ; 2. E π * [Q j t (s)] ≥ E π j E π − j * [Q j t (s)], ∀π j ∈ Ω A j and E π * [Q j t (s)] ≤ E π j * E π − j [Q j t (s)], ∀π −j ∈ Ω k =j A k . t t+1</formula><p>Our proof is also built upon the two lemmas as follows: Lemma 1. Under Assumption 3, the Nash operator H Nash in Eq. (4) forms a contraction mapping on the complete metric space from Q to Q with the fixed point being the Nash Q-value of the entire game, i.e. H Nash t Q * = Q * .</p><p>Proof. See Theorem 17 in <ref type="bibr" target="#b17">Hu &amp; Wellman (2003)</ref>.</p><p>Lemma 2. The random process {∆ t } defined in R as</p><formula xml:id="formula_27">∆ t+1 (x) = (1 − α t (x))∆ t (x) + α t (x)F t (x)<label>(15)</label></formula><p>converges to zero with probability 1 (w.p.1) when</p><formula xml:id="formula_28">1. 0 ≤ α t (x) ≤ 1, t α t (x) = ∞, t α 2</formula><p>t (x) &lt; ∞; 2. x ∈ X, the set of possible states, and |X| &lt; ∞;</p><formula xml:id="formula_29">3. E[F t (x)|F t ] W ≤ γ ∆ t W + c t ,</formula><p>where γ ∈ [0, 1) and c t converges to zero w.p.1;</p><formula xml:id="formula_30">4. var[F t (x)|F t ] ≤ K(1 + ∆ t 2</formula><p>W ) with constant K &gt; 0. Here F t denotes the filtration of an increasing sequence of σ-fields including the history of processes; α t , ∆ t , F t ∈ F t and · W is a weighted maximum norm <ref type="bibr" target="#b0">(Bertsekas, 2012)</ref>.</p><p>Proof. See Theorem 1 in <ref type="bibr" target="#b20">Jaakkola et al. (1994)</ref> and Corollary 5 <ref type="bibr" target="#b40">Szepesvári &amp; Littman (1999)</ref> for detailed derivation. We include it here to stay self-contained.</p><p>By subtracting Q * (s, a) on both sides of Eq. (9), we present the relation from the comparison with Eq. (15) such that</p><formula xml:id="formula_31">∆ t (x) = Q t (s, a) − Q * (s, a), F t (x) = r t + γv MF t (s t+1 ) − Q * (s t , a t ),<label>(16)</label></formula><p>where x (s t , a t ) denotes the visited state-action pair at time t. In Eq. (15), α(t) is interpreted as the learning rate with α t (s , a ) = 0 for any (s , a ) = (s t , a t ); this is because that each agent only updates the Q-function with the state s t and actions a t visited at time t. Lemma 2 suggests ∆ t (x)'s convergence to zero, which means, if it holds, the sequence of Q's will asymptotically tend to the Nash Q-value Q * .</p><p>One last piece to establish the main theorem is the below: Proposition 1. Let the metric space be R N and the metric</p><formula xml:id="formula_32">be d(a, b) = j |a j − b j |, for a = [a j ] N 1 , b = [b j ] N 1 . If the Q-function is K-Lipschitz continuous w.r.t. a j , then the op- erator B(a j ) π j (a j |s,ā j ) in Eq.</formula><p>(12) forms a contraction mapping under sufficiently low temperature β.</p><p>Proof. See details in Appendix D due to the space limit. Proof. Let F t denote the σ-field generated by all random variables in the history of the stochastic game up to time t: (s t , α t , a t , r t−1 , ..., s 1 , α 1 , a 1 , Q 0 ). Note that Q t is a random variable derived from the historical trajectory up to time t. Given the fact that all Q τ with τ &lt; t are F t -measurable, both ∆ t and F t−1 are therefore also F t -measurable, which satisfies the measurability condition of Lemma 2.</p><p>To apply Lemma 2, we need to show that the mean field operator H MF meets Lemma 2's third and fourth conditions. For Lemma 2's third condition, we begin with Eq. (16) that</p><formula xml:id="formula_33">F t (s t , a t ) = r t + γv MF t (s t+1 ) − Q * (s t , a t ) = r t + γv Nash t (s t+1 ) − Q * (s t , a t ) + γ[v MF t (s t+1 ) − v Nash t (s t+1 )] = r t + γv Nash t (s t+1 ) − Q * (s t , a t ) + C t (s t , a t ) = F Nash t (s t , a t ) + C t (s t , a t ).<label>(17)</label></formula><p>Note the fact that F Nash t in Eq. <ref type="formula" target="#formula_33">(17)</ref> is essentially the F t in Lemma 2 in proving the convergence of the Nash Q-learning algorithm. From Lemma 1, it is straightforward to show that F Nash t forms a contraction mapping with the norm · ∞ being the maximum norm on a. We thus have for all t that</p><formula xml:id="formula_34">E[F Nash t (s t , a t )|F t ] ∞ ≤ γ Q t − Q * ∞ = γ ∆ t ∞ .</formula><p>In meeting the third condition, we obtain from Eq. (17) that</p><formula xml:id="formula_35">E[Ft (st, at )|Ft ] ∞ ≤ F Nash t (st, at )|Ft ∞ + Ct (st, at )|Ft ∞ ≤ γ ∆t ∞ + Ct (st, at )|Ft ∞ .<label>(18)</label></formula><p>We are left to prove that c t = C t (s t , a t )|F t converges to zero w.p.1. With Assumption 3, for each stage game, all the globally optimal equilibrium(s) share the same Nash value, so does the saddle-point equilibrium(s). Each of the two following results is essentially associated with one of the two mutually exclusive scenarios in Assumption 3:</p><p>1. For globally optimal equilibriums, all players obtain the joint maximum values that are unique and identical for all equilibriums according to the definition; 2. Suppose that the stage game {Q t } has two saddle-point equilibriums, π and ρ. It holds for agent j that</p><formula xml:id="formula_36">E π j E π − j [Q j t (s)] ≥ E ρ j E π − j [Q j t (s)], E ρ j E ρ − j [Q j t (s)] ≤ E ρ j E π − j [Q j t (s)].</formula><p>By combing the above inequalities, we obtain</p><formula xml:id="formula_37">E π j E π − j [Q j t (s)] ≥ E ρ j E ρ − j [Q j t (s)].</formula><p>By the definition of saddle points, the above inequality still holds by reversing the order of π and ρ; hence, the equilibriums for agent i at both saddle points are the same such that</p><formula xml:id="formula_38">E π j E π − j [Q j t (s)] = E ρ j E ρ − j [Q j t (s)].</formula><p>Given Proposition 1 that the policy based on the mean field Q-function forms a contraction mapping, and that all optimal/saddle points share the same Nash value in each stage game, with the homogeneity of agents, v MF will asymptotically converges to v Nash , the third condition is thus satisfied.</p><p>For the fourth condition, we exploit the conclusion that is proved above that H MF forms a contraction mapping, i.e. H MF Q * = Q * , and it follows that</p><formula xml:id="formula_39">var[F t (s t , a t )|F t ] = E[(r t + γv MF t (s t+1 ) − Q * (s t , a t )) 2 ] = E[(r t + γv MF t (s t+1 ) − H MF (Q * )) 2 ] = var[r t + γv MF t (s t+1 )|F t ] ≤ K(1 + ∆ t 2 W ).<label>(19)</label></formula><p>In the last step of Eq. (19), we employ Assumption 1 that the reward r t is always bounded by some constant. Finally, with all conditions met, it follows Lemma 2 that ∆ t converges to zero w.p.1, i.e. Q t converges to Q * w.p.1.</p><p>Apart from being convergent to the Nash Q-value, MF-Q is also Rational <ref type="bibr" target="#b3">(Bowling &amp; Veloso, 2001;</ref>. We leave the corresponding discussion in Appendix D for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>We continue our discussion on related work from Introduction and make comparisons with existing techniques in a greater scope. Our work follows the same direction as <ref type="bibr" target="#b26">Littman (1994)</ref>; <ref type="bibr" target="#b17">Hu &amp; Wellman (2003)</ref>; <ref type="bibr" target="#b4">Bowling &amp; Veloso (2002)</ref> on adapting a Stochastic Game <ref type="bibr" target="#b42">(van der Wal et al., 1981)</ref> into the MARL formulation. Specifically, <ref type="bibr" target="#b26">Littman (1994)</ref> addressed two-player zero-sum stochastic games by introducing a "minimax" operator in Q-learning, whereas <ref type="bibr" target="#b17">Hu &amp; Wellman (2003)</ref> extended it to the general-sum case by learning a Nash equilibrium in each stage game and considering a mixed strategy. Nash-Q learning is guaranteed to converge to Nash strategies under the (strong) assumption that there exists an equilibrium for every stage game. In the situation where agents can be identified as either "friends" or "foes" <ref type="bibr" target="#b27">(Littman, 2001)</ref>, one can simply solve it by alternating between fully cooperative and zero-sum learning. Considering the convergence speed, <ref type="bibr" target="#b28">Littman &amp; Stone (2005)</ref> and de <ref type="bibr" target="#b8">Cote &amp; Littman (2008)</ref> draw on the folk theorem and acquired a polynomial-time Nash equilibrium algorithm for repeated stochastic games, while <ref type="bibr" target="#b4">Bowling &amp; Veloso (2002)</ref> tried varying the learning rate to improve the convergence.</p><p>The recent treatment of MARL was using deep neural networks as the function approximator. In addressing the nonstationary issue in MARL, various solutions have been proposed including neural-based opponent modeling <ref type="bibr" target="#b15">(He &amp; Boyd-Graber, 2016)</ref>, policy parameters sharing <ref type="bibr" target="#b13">(Gupta et al., 2017)</ref>, etc. Researchers have also adopted the paradigm of centralized training with decentralized execution for multiagent policy-gradient learning: BICNET <ref type="bibr" target="#b34">(Peng et al., 2017)</ref>, COMA <ref type="bibr" target="#b11">(Foerster et al., 2018)</ref> and MADDPG <ref type="bibr" target="#b29">(Lowe et al., 2017a)</ref>, which allows the centralized critic Q-function to be trained with the actions of other agents, while the actor needs only local observation to optimize agent's policy.</p><p>The above MARL approaches limit their studies mostly to tens of agents. As the number of agents grows larger, not only the input space of Q grows exponentially, but most critically, the accumulated noises by the exploratory actions of other agents make the Q-function learning no longer feasible. Our work addresses the issue by employing the mean field approximation <ref type="bibr" target="#b39">(Stanley, 1971</ref>) over the joint action space. The parameters of the Q-function is independent of the number of agents as it transforms multiple agents interactions into two entities interactions (single agent v.s. the distribution of the neighboring agents). This would effectively alleviate the problem of the exploratory noise <ref type="bibr" target="#b7">(Colby et al., 2015)</ref> caused by many other agents, and allow each agent to determine which actions are beneficial to itself.</p><p>Our work is also closely related to the recent development of mean field games (MFG) <ref type="bibr" target="#b24">(Lasry &amp; Lions, 2007;</ref><ref type="bibr" target="#b18">Huang et al., 2006;</ref><ref type="bibr" target="#b45">Weintraub et al., 2006)</ref>. MFG studies population behaviors resulting from the aggregations of decisions taken from individuals. Mathematically, the dynamics are governed by a set of two stochastic differential equations that model the backward dynamics of individual's value function, and the forward dynamics of the aggregate distribution of agent population. Despite that the backward equation equivalently describes what Bellman equation indicates in the MDP, the primarily goal for MFG is rather for a model-based planning and to infer the movements of the individual density through time. The mean field approximation <ref type="bibr" target="#b39">(Stanley, 1971)</ref> in also employed in physics, but our work is different in that we focus on a model-free solution of learning optimal actions when the dynamics of the system and the reward function are unknown. Very recently, <ref type="bibr" target="#b46">Yang et al. (2017)</ref> built a connection between MFG and reinforcement learning. Their focus is, however, on the inverse RL in order to learn both the reward function and the forward dynamics of the MFG from the policy data, whereas our goal is to form a computable Q-learning algorithm under the framework of temporal difference learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We analyze and evaluate our algorithms in three different scenarios, including two stage games: the Gaussian Squeeze and the Ising Model, and the mixed cooperative-competitive battle game.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Gaussian Squeeze</head><p>Environment. In the Gaussian Squeeze (GS) task <ref type="bibr" target="#b16">(HolmesParker et al., 2014)</ref>, N homogeneous agents determine their individual action a j to jointly optimize the most appropriate summation x = N j=1 a j . Each agent has 10 action choices -integers 0 to 9. The system objective is defined as G(x) = xe</p><formula xml:id="formula_40">−(x−µ) 2 σ 2</formula><p>, where µ and σ are the pre-defined mean and variance of the system. In the scenario of traffic congestion, each agent is one traffic controller trying to send a j vehicles into the main road. Controllers are expected to coordinate with each other to make the full use of the main route while avoiding congestions. The goal of each agent is to learn to allocate system resources efficiently, avoiding either over-use or under-use. The GS problem here sits ideally as an ablation study on the impact of multi-agent exploratory noises toward the learning <ref type="bibr" target="#b7">(Colby et al., 2015)</ref>.</p><p>Model Settings. We implement MF-Q and MF-AC following the framework of centralized training (shared critic) with decentralized execution (independent actor). We compare against 4 baseline models: (1) Independent Learner (IL), a traditional Q-Learning algorithm that does not consider the actions performed by other agents; (2) Frequency Maximum Q-value (FMQ) <ref type="bibr" target="#b22">(Kapetanakis &amp; Kudenko, 2002)</ref>, a modified IL which increases the Q-values of actions that frequently produced good rewards in the past; (3) Recursive Frequency Maximum Q-value (Rec-FMQ) <ref type="bibr" target="#b31">(Matignon et al., 2012)</ref>, an improved version of FMQ that recursively computes the occurrence frequency to evaluate and then choose actions; (4) Multi-agent Actor-Critic (MAAC), a variant of MADDPG architecture for the discrete action space (see Eq. (4) in <ref type="bibr" target="#b30">Lowe et al. (2017b)</ref>). All models use the multilayer perception as the function approximator. The detailed settings of the implementation are in the Appendix C.1.</p><p>Results. <ref type="figure" target="#fig_2">Figure. 3</ref> illustrates the results for the GS environment of µ = 400 and σ = 200 with three different numbers of agents (N = 100, 500, 1000) that stand for 3 levels of congestions. In the smallest GS setting of <ref type="figure" target="#fig_2">Fig. 3a</ref>, all models show excellent performance. As the agent number increases, Figs. 3b and 3c show MF-Q and MF-AC's capabilities of learning the optimal allocation effectively after a few iterations, whereas all four baselines fail to learn at all. We believe this advantage is due to the awareness of other agents' actions under the mean field framework; such mechanism keeps the interactions among agents manageable while reducing the noisy effect of the exploratory behaviors from the other agents. Between MF-Q and MF-AC, MF-Q converges faster. Both FMQ and Rec-FMQ fail to reach pleasant performance, it might be because agents are essentially unable to distinguish the rewards received for the same actions, and are thus unable to update their own Q-values w.r.t. the actual contributions. It is worth noting that MAAC is surprisingly inefficient in learning when the number of agents becomes large; it simply fails to handle the non-aggregated noises due to the agents' explorations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Model-free MARL for Ising Model</head><p>Environment. In statistical mechanics, the Ising model is a mathematical framework to describe ferromagnetism <ref type="bibr" target="#b19">(Ising, 1925)</ref>. It also has wide applications in sociophysics <ref type="bibr" target="#b12">(Galam &amp; Walliser, 2010)</ref>. With the energy function explicitly defined, mean field approximation <ref type="bibr" target="#b39">(Stanley, 1971</ref>) is a typical way to solve the Ising model for every spin j, i.e. a j = a a j P(a). See the Appendix C.2 for more details.</p><p>To fit into the MARL setting, we transform the Ising model into a stage game where the reward for each spin/agent is defined by</p><formula xml:id="formula_41">r j = h j a j + λ 2 k∈N(j) a j a k ; here N( j)</formula><p>is the set of nearest neighbors of spin j, h j ∈ R is the external field affecting the spin j, and λ ∈ R is an interaction coefficient that determines how much the spins are motivated to stay aligned. Unlike the typical setting in physics, here each spin does not know the energy function, but aims to understand the environment, and to maximize its reward by learning the optimal policy of choosing the spin state: up or down.</p><p>In addition to the reward, the order parameter (OP) <ref type="bibr" target="#b39">(Stanley, 1971</ref>) is a traditional measure of purity for the Ising model. OP is defined as ξ =</p><formula xml:id="formula_42">|N ↑ −N ↓ | N</formula><p>, where N ↑ represents the number of up spins, and N ↓ for the down spins. The closer the OP is to 1, the more orderly the system is.</p><p>Model Settings. To validate the correctness of the MF-Q learning, we implement MCMC methods <ref type="bibr" target="#b1">(Binder et al., 1993)</ref> to simulate the same Ising model and provide the ground truth for comparison. The full settings of MCMC and MF-Q for Ising model are provided in the Appendix C.2. One of the learning goals is to obtain the accurate approximation of a j . Notice that agents here do not know exactly the energy function, but rather use the temporal difference learning to approximate a j during the learning procedure. Once this is accurately approximated, the Ising model as a whole should be able to converge to the same simulation result suggested by MCMC.</p><p>Correctness of MF-Q. <ref type="figure">Figure. 4</ref> illustrates the relationship between the order parameter at equilibrium under different  system temperatures. MF-Q converges nearly to the exact same plot as MCMC, this justifies the correctness of our algorithms. Critically, MF-Q finds a similar Curie temperature (the phase change point) as MCMC that is τ = 1.2. As far as we know, this is the first work that manages to solve the Ising model via model-free reinforcement learning methods. <ref type="figure">Figure. 5</ref> illustrates the mean squared error between the learned Q-value and the reward target. MF-Q is shown in <ref type="figure">Fig. 5a</ref> to be able to learn the target well under low temperature settings. When it comes to the Curie temperature, the environment enters into the phase change when the stochasticity dominates, resulting in a lower OP and higher MSE observed in <ref type="figure">Fig. 5b</ref>. We visualize the equilibrium in <ref type="figure" target="#fig_4">Fig. 6</ref>. The equilibrium points from MF-Q in fact match MCMC's results under three types of temperatures. The spins tend to stay aligned under a low temperature (τ = 0.9). As the temperature rises (τ = 1.2), some spins become volatile and patches start to form as spontaneous magnetization. This phenomenon is mostly observed around the Curie temperature. After passing the Curie temperature, the system becomes unstable and disordered due to the large thermal fluctuations, resulting in random spinning patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Mixed Cooperative-Competitive Battle Game</head><p>Environment. The Battle game in the Open-source MAgent system <ref type="bibr" target="#b47">(Zheng et al., 2018</ref>) is a Mixed CooperativeCompetitive scenario with two armies fighting against each other in a grid world, each empowered by a different RL algorithm. In the setting of <ref type="figure" target="#fig_6">Fig. 7a</ref>, each army consists of 64 homogeneous agents. The goal of each army is to get more rewards by collaborating with teammates to destroy all the opponents. Agent can takes actions to either move to or attack nearby grids. Ideally, the agents army should learn skills such as chasing to hunt after training. We adopt the default reward setting: −0.005 for every move, 0.2 for attacking an enemy, 5 for killing an enemy, −0.1 for attacking an empty grid, and −0.1 for being attacked or killed.</p><p>Model Settings. Our MF-Q and MF-AC are compared against the baselines that are proved successful on the MAgent platform. We focus on the battles between mean field methods (MF-Q, MF-AC) and their non-mean field counterparts, independent Q-learning (IL) and advantageous actor critic (AC). We exclude MADDPG/MAAC as baselines, as the framework of centralized critic cannot deal with the varying number of agents for the battle (simply because  agents could die in the battle). Also, as we demonstrated in the previous experiment of <ref type="figure" target="#fig_2">Fig. 3</ref>, MAAC tends to scale poorly and fail when the agent number is in hundreds.</p><p>Results and Discussion. We train all four models by 2000 rounds self-plays, and then use them for comparative battles. During the training, agents can quickly pick up the skills of chasing and cooperation to kill in <ref type="figure" target="#fig_6">Fig. 7a</ref>. <ref type="figure" target="#fig_8">The Fig. 8</ref> shows the result of winning rate and the total reward over 2000 rounds cross-comparative experiments. It is evident that on all the metrics mean field methods, MF-Q largely outperforms the corresponding baselines, i.e. IL and AC respectively, which shows the effectiveness of the mean field MARL algorithms. Interestingly, IL performs far better than AC and MF-AC (2nd block from the left in <ref type="figure" target="#fig_8">Fig. 8a</ref>), although it is worse than the mean field counterpart MF-Q. This might imply the effectiveness of off-policy learning with shuffled buffer replay in many-agent RL towards a more stable learning process. Also, the Q-learning family tends to introduce a positive bias <ref type="bibr" target="#b14">(Hasselt, 2010)</ref> by using the maximum action value as an approximation for the maximum expected action value, and such overestimation can be beneficial for each single agent to find the best response to others even though the environment itself is still changing. On the other hand, On-policy methods need to comply with the GLIE assumption (Assumption 2 in Sec 3.3) so as to converge properly to the optimal value <ref type="bibr" target="#b38">(Singh et al., 2000)</ref>, which is in the end a greedy policy as off-policy methods. <ref type="figure">Figure.</ref> 7b further shows the self-play learning curves of MF-AC and MF-Q. MF-Q presents a faster convergence speed than MF-AC, which is consistent with the findings in the Gaussian Squeeze task (see <ref type="figure" target="#fig_2">Fig. 3b &amp; 3c)</ref>. Apart from 64, we further test the scenarios when the agent size is 8, 144, 256, the comparative results keep the same relativity as <ref type="figure" target="#fig_8">Fig. 8</ref>; we omit the presentations for clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we developed mean field reinforcement learning methods to model the dynamics of interactions in the multi-agent systems. MF-Q iteratively learns each agent's best response to the mean effect from its neighbors; this effectively transform the many-body problem into a two-body problem. Theoretical analysis on the convergence of the MF-Q algorithm to Nash Q-value was provided. Three types of tasks have justified the effectiveness of our approaches. In particular, we report the first result to solve the Ising model using model-free reinforcement learning methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: MF-Q iterations on a 3 × 3 stateless toy example. The goal is to coordinate the agents to an agreed direction. Each agent has two choices of actions: up ↑ or down ↓. The reward of each agent's staying in the same direction as its [0, 1, 2, 3, 4] neighbors are [−2.0, −1.0, 0.0, 1.0, 2.0], respectively. The neighbors are specified by the four directions on the grid with cyclic structure on all directions, e.g. the first row and the third row are adjacent. The reward for the highlighted agent j on the bottom left at time t + 1 is 2.0, as all neighboring agents stay down in the same time. We listed the Q-tables for agent j at three time steps whereā j is the percentage of neighboring ups. Following Eq. 9, we have Q j t+1 (↑,ā j = 0) = Q j t (↑,ā j = 0) + α[r j − Q j t (↑,ā j = 0)] = 0.82 + 0.1 × (2.0 − 0.82) = 0.93. The rightmost plot shows the convergent scenario where the Q-value of staying down is 2.0, which is the largest reward in the environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 1 .</head><label>1</label><figDesc>In a finite-state stochastic game, the Q t values computed by the update rule of MF-Q in Eq. (9) converges to the Nash Q-value Q * = [Q 1 * , . . . , Q N * ], if Assumptions 1, 2 &amp; 3, and Lemma 2's first and second conditions are met.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Learning with N agents in the GS environment with µ = 400 and σ = 200.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: The order parameter at equilibrium v.s. temperature in the Ising model with 20 × 20 grid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The spins of the Ising model at equilibrium under different temperatures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The battle game: 64 v.s. 64.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Performance comparisons in the battle game.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>). Here we consider discrete action spaces, where the action a j of agent j is a discrete categorical variable represented as the one-hot encoding with each component indicating one of the D possible actions: a</figDesc><table>j 

[a 

j 

1 , . . . , a 

j 

D ]. We calculate the 
mean actionā 
j based on the neighborhood N( j) of agent 
j, and express the one-hot action a 
k of each neighbor k in 
terms of the sum ofā 
j and a small fluctuation δa 
j,k as 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">University College London, London, United Kingdom. 2 Shanghai Jiao Tong University, Shanghai, China.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We sincerely thank Ms. Yi Qu for her generous help on the graphic design.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weighted sup-norm contractions in dynamic programming: A review and some new applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Elect. Eng. Comput. Sci., Massachusetts Inst. Technol</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Rep. LIDS-P-2884</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Monte carlo simulation in statistical physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Mallinckrodt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Physics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="156" to="157" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The statistical mechanics of strategic interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Blume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Games and economic behavior</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="387" to="424" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rational and convergent learning in stochastic games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International joint conference on artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1021" to="1026" />
			<date type="published" when="2001" />
			<publisher>Lawrence Erlbaum Associates Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiagent learning using a variable learning rate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="250" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A comprehensive survey of multiagent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Busoniu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Babuska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Schutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man, and Cybernetics, Part C</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="156" to="172" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to rank: from pairwise approach to listwise approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Counterfactual exploration for improving multiagent learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Colby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kharaghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Holmesparker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems</title>
		<meeting>the 2015 International Conference on Autonomous Agents and Multiagent Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="171" to="179" />
		</imprint>
	</monogr>
	<note>International Foundation for Autonomous Agents and Multiagent Systems</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A polynomial-time nash equilibrium algorithm for repeated stochastic games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>De Cote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<editor>McAllester, D. A. and Myllymäki, P.</editor>
		<imprint>
			<date type="published" when="2008" />
			<publisher>AUAI Press</publisher>
			<biblScope unit="page" from="0" to="9749039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Journal of science of the hiroshima university, series ai (mathematics)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Fink</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="89" to="93" />
		</imprint>
	</monogr>
	<note>Equilibrium in a stochastic n-person game</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning with opponentlearning awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Shedivat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
		<idno>abs/1709.04326</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Counterfactual multi-agent policy gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">McIlraith &amp; Weinberger</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ising model versus normal form game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Galam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Walliser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">389</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="481" to="489" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cooperative multi-agent control using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Egorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAMAS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="66" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Double q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Hasselt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2613" to="2621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Opponent modeling in deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR.org</title>
		<editor>Balcan, M. and Weinberger, K. Q.</editor>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1804" to="1813" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting structure and agent-centric rewards to promote coordination in large multiagent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Holmesparker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adaptive and Learning Agents Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nash q-learning for general-sum stochastic games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Wellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1039" to="1069" />
			<date type="published" when="2003-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large population stochastic dynamic games: closed-loop mckeanvlasov systems and the nash certainty equivalence principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Malhamé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Caines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Information &amp; Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="252" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beitrag zur theorie des ferromagnetismus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ising</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zeitschrift für Physik</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="253" to="258" />
			<date type="published" when="1925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convergence of stochastic iterative dynamic programming algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="703" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Analysis of game bot&apos;s behavioral characteristics in social interaction networks of mmorpg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="99" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reinforcement learning of coordination in cooperative multi-agent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kapetanakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kudenko</surname></persName>
		</author>
		<idno>0-262-51129-0</idno>
	</analytic>
	<monogr>
		<title level="m">NCAI</title>
		<meeting><address><addrLine>Menlo Park, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="326" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Actor-critic algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Solla, S. A., Leen, T. K., and Müller, K.</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1008" to="1014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mean field games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Lasry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-L</forename><surname>Lions</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Japanese journal of mathematics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="229" to="260" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Equilibrium points of bimatrix games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Lemke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Howson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Jr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Society for Industrial and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="413" to="423" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Markov games as a framework for multiagent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Friend-or-foe q-learning in general-sum games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="322" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A polynomial-time nash equilibrium algorithm for repeated games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="66" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-agent actor-critic for mixed cooperative-competitive environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6382" to="6393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-agent actor-critic for mixed cooperativecompetitive environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">;</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Guyon, I</title>
		<editor>R.</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6382" to="6393" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Independent reinforcement learners in cooperative markov games: a survey regarding coordination problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matignon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Fort-Piat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Knowledge Engineering Review</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<editor>McIlraith, S. A. and Weinberger, K. Q.</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cooperative multi-agent learning: The state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Panait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAMAS</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="387" to="434" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Multiagent bidirectionally-coordinated nets for learning to play starcraft combat games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno>abs/1703.10069</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Factorization machines with libfm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stochastic games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Shapley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="1953" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1095" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deterministic policy gradient algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="387" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Convergence results for single-step on-policy reinforcement-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="308" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Phase transitions and critical phenomena</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Stanley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971" />
			<pubPlace>Clarendon, Oxford, 9</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A unified analysis of value-function-based reinforcement-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2017" to="2060" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning: Independent vs. cooperative agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth international conference on machine learning</title>
		<meeting>the tenth international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="330" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stochastic Dynamic Programming: successive approximations and nearly optimal strategies for Markov decision processes and Markov games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Troy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Der Wal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Der Wal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Der Wal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-B</forename><surname>Mathématicien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Der Wal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mathematician</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New York Times</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
	<note>Envisioning stock trading where the brokers are bots. Mathematisch centrum</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Display advertising with real-time bidding (rtb) and behavioural targeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="297" to="435" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Oblivious equilibrium: A mean field approximation for large-scale dynamic games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Y</forename><surname>Weintraub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Benkard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1489" to="1496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deep mean field games for learning optimal behavior policy of large populations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<idno>abs/1711.03156</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Magent: A many-agent reinforcement learning platform for artificial collective intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<editor>McIlraith &amp; Weinberger</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
