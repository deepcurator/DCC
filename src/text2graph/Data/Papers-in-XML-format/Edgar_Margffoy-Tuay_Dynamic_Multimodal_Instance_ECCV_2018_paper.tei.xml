<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Multimodal Instance Segmentation Guided by Natural Language Queries</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Margffoy-Tuay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad de los Andes</orgName>
								<address>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Pérez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad de los Andes</orgName>
								<address>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Botero</surname></persName>
							<email>e.botero10@uniandes.edu.co</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidad de los Andes</orgName>
								<address>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
							<email>pa.arbelaez@uniandes.edu.co</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidad de los Andes</orgName>
								<address>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Multimodal Instance Segmentation Guided by Natural Language Queries</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Referring expressions</term>
					<term>instance segmentation</term>
					<term>multimodal interaction</term>
					<term>dynamic convolutional filters</term>
					<term>natural language processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. We address the problem of segmenting an object given a natural language expression that describes it. Current techniques tackle this task by either (i) directly or recursively merging linguistic and visual information in the channel dimension and then performing convolutions; or by (ii) mapping the expression to a space in which it can be thought of as a filter, whose response is directly related to the presence of the object at a given spatial coordinate in the image, so that a convolution can be applied to look for the object. We propose a novel method that integrates these two insights in order to fully exploit the recursive nature of language. Additionally, during the upsampling process, we take advantage of the intermediate information generated when downsampling the image, so that detailed segmentations can be obtained. We compare our method against the state-of-the-art approaches in four standard datasets, in which it surpasses all previous methods in six of eight of the splits for this task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Consider the task of retrieving specific object instances from an image based on natural language descriptions, as illustrated in <ref type="figure" target="#fig_5">Fig. 1</ref>. In contrast to traditional instance segmentation, in which the goal is to label all pixels belonging to instances in the image for a set of predefined semantic classes <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, segmenting instances described by a natural language expression is a task that humans are able to perform without specifically focusing on a limited set of categories: we simply associate a referring expression such as "Man on the right" with what we see, as shown in <ref type="figure" target="#fig_5">Fig. 1</ref>. To learn such an association is the main goal of this paper.</p><p>In this task, the main labels to be assigned are related to query and background. Thus, the set of possible segmentation masks has few constraints, as a mask can be anything one might observe in the image, in all the ways natural language allows an object to be referred to. An algorithm to tackle this problem must then make sense of the query and relate it to what it sees and recognizes in the image, to finally output an instance segmentation map. Therefore, attempting to naively use Convolutional Neural Networks (CNNs) for this task (a) Original image.</p><p>(b) Output based on query guy.</p><p>(c) Output based on query girl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1:</head><p>Example of segmentation based on a natural language expression. A single mask is the output, in which the only two labels are member of query and background. Here, we show the raw output of our system, which is the pixelwise probability of belonging to the referred object instance.</p><p>falls short, since such networks do not model sequential information by nature, as is required when processing natural language. Given that the cornerstone of this task is the proper combination of information retrieved from multiple, dissimilar domains, we expect traditional architectures, like CNNs and Recurrent Neural Networks (RNNs), to be useful modules, but we still need to design an overall architecture that fully exploits their complementary nature.</p><p>In this paper, we introduce a modular neural network architecture that divides the task into several sub-tasks, each handling a different type of information in a specific manner. Our approach is similar to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> in that we extract visual and natural language information in an independent manner by employing networks commonly used for these types of data, i.e., CNNs and RNNs, and then focus on processing this multi-domain information by means of another neural network, yielding an end-to-end trainable architecture. However, our method also introduces the usage of Simple Recurrent Units (SRUs) for efficient segmentation based on referring expressions, a Synthesis Module that processes the linguistic and visual information jointly, and an Upsampling Module that outputs highly detailed segmentation maps.</p><p>Our network, which we refer to as Dynamic Multimodal Network (DMN), is composed of several modules, as depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>: (i) a Visual Module (VM) that produces an adequate representation of the image, (ii) a Language Module (LM) that outputs an appropriate representation of the meaning of the query up to a given word, (iii) a Synthesis Module (SM) that merges the information provided by the VM and LM at each time step and produces a single output for the whole expression and, finally, (iv) an Upsampling Module (UM) that incrementally upsamples the output of the SM by using the feature maps pro- duced by the VM. Our approach is a fully differentiable, end-to-end trainable neural network for segmentation based on natural language queries. Our main contributions are the following:</p><p>-The use of Simple Recurrent Units (SRUs) <ref type="bibr" target="#b5">[6]</ref> as language and multi-modal processors instead of standard LSTMs <ref type="bibr" target="#b6">[7]</ref>. We empirically show that they are efficient while providing high performance for the task at hand. -A Synthesis Module that takes visual and linguistic information and merges them by generating "scores" for the referring expression in a visual space. -The Synthesis Module then takes this representation as well as additional features, and exploits the spatial and sequential nature of both types of information to produce a low resolution segmentation map. -A high resolution upsampling module that takes advantage of visual features during the upsampling procedure in order to recover fine scale details.</p><p>We validate our method by performing experiments on all standard datasets, and show that DMN outperforms all the previous methods in various splits for instance segmentation based on referring expressions, and obtains state-ofthe art results. Additionally, in order to ensure reproducibility, we provide full implementation of our method and training routines, written in PyTorch 1 <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The intersection of Computer Vision (CV) and Natural Language Understanding (NLU) is an active area of research that includes multiple tasks such as object detection based on natural language expressions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, image captioning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> and visual question answering (VQA) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. Since visual and linguistic data have properties that make them fundamentally different, i.e., the former has spatial meaning and no sequentiality while the latter does not contemplate space but has a sequential nature, optimally processing both types of information is still an open question. Hence, each work in this sub-field has proposed a particular way of addressing each task.</p><p>The task studied in this paper is closest in nature to object detection based on natural language expressions, mirroring how semantic segmentation arose from object detection <ref type="bibr" target="#b19">[20]</ref>. Indeed, in <ref type="bibr" target="#b2">[3]</ref>, object detection with NLU evolved into instance segmentation using referring expressions. We review the state-of-theart on the task of segmentation based on natural language expressions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>[5], highlighting the main contributions in the fusion of multimodal information, and then compare them against our approach.</p><p>Segmentation from Natural Language Expressions <ref type="bibr" target="#b2">[3]</ref>. This work processes visual and natural language information through separate neural networks: a CNN extracts visual features from the image while an LSTM scans the query. Strided convolutions and pooling operations in the CNN downsample the feature maps to a low resolution output while producing large receptive fields for neurons in the final layers. Additionally, to explicitly model spatial information, relative coordinates are concatenated at each spatial location in the feature map obtained by the CNN. Merging of visual and natural language information is done by concatenating the LSTM's output to the visual feature map at each spatial location. Convolution layers with ReLU <ref type="bibr" target="#b20">[21]</ref> nonlinearities are applied for final classification. The loss is defined as the average over the per-pixel weighed logistic regression loss. Training has two stages: a low resolution stage, in which the ground truth mask is downsampled to have the same dimensions as the output, and a high resolution stage that trains a deconvolution layer to upsample the low resolution output to yield the final segmentation mask <ref type="bibr" target="#b2">[3]</ref>. This seminal method does not fully exploit the sequential nature of language, as it does not make use of the learned word embeddings, it merges visual and linguistic information by concatenation, and it uses deconvolution layers for upsampling, which have been shown to introduce checkerboard artifacts in images <ref type="bibr" target="#b21">[22]</ref>.</p><p>Recurrent Multimodal Interaction <ref type="bibr" target="#b3">[4]</ref>. This paper argues that segmenting the image based only on a final, memorized representation of the sentence does not fully take advantage of the sequential nature of language. Consequently, the paper proposes to perform segmentation multiple times in the pipeline. The method produces image features at every time step by generating a representation that involves visual, spatial and linguistic features. Such multimodal representation is obtained by concatenating the hidden state of the LSTM that processed the query at every spatial location of the visual representation. The segmentation mask is obtained by applying a multimodal LSTM (mLSTM) to the joint representation and then performing regular convolutions to combine the channels that were produced by the mLSTM. The mLSTM is defined as a convolutional LSTM that shares weights both across spatial location and time step, and is implemented as a 1 × 1 convolution that merges all these types of information. Bilinear upsampling is performed to the network's output at test time to produce a mask with the same dimensions of the ground-truth mask. This method reduces strides of convolutional layers and uses atrous convolution in the final layers of the CNN to compensate for the downsampling. Such modification reduces the upsampling process to bilinear interpolation, but can decrease the CNN's representation capacity while also increasing the number of computations that must be performed by the mLSTM.</p><p>Tracking by Natural Language Specification <ref type="bibr" target="#b4">[5]</ref>. In this paper, the main task is object tracking in video sequences. A typical user interaction in tracking consists in providing the bounding box of the object of interest in the first frame. However, this type of interaction has the issue that, for the duration of the video, the appearance and location of objects may change, rendering the initial bounding box useless in some cases. The main idea is to provide an alternative to this approach, by noting that (i) the semantic meaning of the object being tracked does not vary for the duration of the video as much as the appearance, and (ii) this semantic meaning may be better defined by a linguistic expression. This approach is substantially different from <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b2">[3]</ref>: visual and linguistic information is never merged per se, but rather the linguistic information is mapped to a space in which it can be interpreted as having visual meaning. The visual input is thus processed by a modified VGG <ref type="bibr" target="#b22">[23]</ref> to yield a feature map. An LSTM scans the linguistic input, and a single layer perceptron is applied to the LSTM's last hidden state to generate a vector that can be interpreted as being a filter for a 2D convolution that is to be performed on the feature map. The dynamic convolutional visual filter, generated based on the expression, is computed to produce a strong response to the elements being referred to the expression, and a weak response to those not being referred to. This response is interpreted as a "score" for the referring expression, so that a segmentation can be produced. This method proposes a new paradigm for combining information from the visual and linguistic domains, but assumes a non linear combination of the last hidden state is sufficient for modeling a filter that responds to the query.</p><p>Our approach. The approach of <ref type="bibr" target="#b2">[3]</ref> merges multi-domain information by concatenation of linguistic information, subsequent 1 × 1 convolutions for segmentation and a deconvolution layer to perform upsampling. The method in <ref type="bibr" target="#b3">[4]</ref> follows the same logic as <ref type="bibr" target="#b2">[3]</ref> but introduces recursion into the approach, exploiting the linguistic information further; however, the upsampling module is an interpolation that produces rather coarse results, to which the authors apply a post-processing DenseCRF, making the architecture not trainable end-to-end. Finally, <ref type="bibr" target="#b4">[5]</ref> has a different approach, in which linguistic information is never merged with feature maps, but is rather transformed so that it can detect the locations in the image to which the referring expression has strong response; nonetheless, like <ref type="bibr" target="#b2">[3]</ref>, it does not fully exploit linguistic information in a sequential manner. Moreover, all these methods fail to utilize information acquired in the downsampling process in the upsampling process. Our approach takes advantage of the previous insights, and consists of a modularized network that exploits both the possibility of segmentation based on combinations of multi-domain information, and the feasibility of producing filters that respond to objects being referred to by processing the linguistic information. Following the spirit of <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>, we use skip connections between the downsampling process and the upsampling module to output finely-defined segmentations. We employ the concatenation strategy of <ref type="bibr" target="#b2">[3]</ref> but include richer visual and language features. Furthermore, we make use of dynamic filter computation, like <ref type="bibr" target="#b4">[5]</ref>, but in a sequential manner. Lastly, we introduce the use of a more efficient alternative to LSTMs in this domain, namely SRUs. We demonstrate empirically that SRUs can be used for modeling language and multimodal information for this task, and that they can be up to 3× faster than LSTMs, allowing us to train more expressive models. , the Synthesis Module (SM) processes this information and produces a single feature map for the entire referring expression. This output, along with the feature maps given by the VM, is processed by the Upsampling Module (UM), that outputs a heatmap with a single channel, to which a sigmoid activation function is applied in order to produce the final prediction. <ref type="figure" target="#fig_1">Fig. 3</ref> depicts the Visual Module. We extract deep visual features from the image using as backbone a Dual Path Network 92 (DPN92) <ref type="bibr" target="#b26">[27]</ref>, which has shown competitive performance in various tasks, and is efficient in parameter usage. The VM can be written as a function returning a tuple: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Visual Module (VM)</head><formula xml:id="formula_0">{I n } N n=1 = VM(I)<label>(1)</label></formula><p>Where I is the original image, and I n , n ∈ {1, . . . , N } are the downsampled feature maps of dimensions equal to 1 2 n of the dimensions of I. In the experiments, we use N = 5, which considers all convolutional layers in the visual encoder. Note that, since our architecture is fully convolutional, we are not restricted to a fixed image size. <ref type="figure" target="#fig_3">Fig. 4</ref> shows a diagram of the Language Module. Given an expression consisting of T words {w t } T t=1 , each word is represented by an embedding (WE), e t = W E(w t ) (EM B in <ref type="figure" target="#fig_3">Fig. 4)</ref>, and the sentence is scanned by an RNN to produce a hidden state h t for each word (HID in <ref type="figure" target="#fig_3">Fig. 4)</ref>. Instead of using LSTMs as recurrent cells, we employ SRUs <ref type="bibr" target="#b5">[6]</ref>, which allow the LM to process the natural language queries more efficiently than when using LSTMs. The SRU is defined by:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Language Module (LM)</head><formula xml:id="formula_1">x t = W x t (2) f ′ t = σ (W f x t + b f ) (3) r t = σ (W r x t + b r )<label>(4)</label></formula><formula xml:id="formula_2">c t = f ′ t ⊙ c t−1 + (1 − f ′ t ) ⊙x t (5) h t = r t ⊙ g(c t ) + (1 − r t ) ⊙ x t<label>(6)</label></formula><p>Where ⊙ is the element-wise multiplication. The function g(·) can be selected based on the task; here we choose g(·) to be the sigmoid function. For further details regarding the SRU definition and implementation, please refer to <ref type="bibr" target="#b5">[6]</ref>.</p><p>We concatenate the hidden state h t with the word embedding e t to produce the final language output: r t = [e t , h t ]. This procedure yields an enriched language representation of the concept of the sentence up to word t. Moreover, we compute a set of dynamic filters f k,t based on r t , defined by: thus, we define the LM formally as:</p><formula xml:id="formula_3">f k,t = σ (W f k r t + b f k ) , k = 1, ..., K<label>(7)</label></formula><formula xml:id="formula_4">{r t } T t=1 , {{f k,t } K k=1 } T t=1 = LM {w t } T t=1<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Synthesis Module (SM)</head><p>Fig <ref type="figure" target="#fig_4">. 5</ref> illustrates the Synthesis Module. The SM is the core of our architecture, as it is responsible for merging multimodal information. We first concatenate I N and a representation of the spatial coordinates (LOC in <ref type="figure" target="#fig_4">Fig. 5</ref>), following the implementation of <ref type="bibr" target="#b2">[3]</ref>, and convolve this result with each of the filters computed by the LM to generate a response map (RESP in <ref type="figure" target="#fig_4">Fig. 5</ref>) consisting of K channels:</p><formula xml:id="formula_5">F t = {f k,t * I N } K k=1</formula><p>. Next, we concatenate I N , LOC, and F t along the channel dimension to obtain a representation I ′ , to which r t is concatenated at each spatial location, as to have all the multimodal information in a single tensor. Finally, we apply a 1 × 1 convolutional layer that merges all the multimodal information, providing tan output corresponding to each time step t, denoted by M t . Formally, M t is defined by:</p><formula xml:id="formula_6">M t = Conv 1×1 ([I N , F t , LOC, r t ])<label>(9)</label></formula><p>Next, in the pursuit of performing a recurrent operation that takes into account the sequentiality of the set and also the information of each of the channels in M t , we propose the use of a multimodal SRU (mSRU), which we define as a 1 × 1 convolution, similar to <ref type="bibr" target="#b3">[4]</ref> but using SRUs. We apply the mSRU to the whole set {M t } T t=1 , so that all the information in each M t , including the sequentiality of the set, is used in the segmentation process. The final hidden states are gathered to produce a 3D tensor that is interpreted as a feature map. This tensor, which we denominate R N , due to its size being The Upsampling Module makes use of all the feature maps that were generated in the feature extraction process to provide more detailed segmentations.</p><formula xml:id="formula_7">R N = SM {M t } T t=1 = mSRU {M t } T t=1 ,<label>(10)</label></formula><p>where M t is reshaped appropriately to make sense of the sequential nature of the information at each time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Upsampling Module (UM)</head><p>Finally, the Upsampling Module is shown in <ref type="figure">Fig. 6</ref>. Inspired by skip connections <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>, we construct an upsampling architecture that takes into account the feature maps {I n } N n=1 at all stages in order to recover fine-scale details. At each stage we concatenate R n with I n , perform 3 × 3 convolution over this result, and then scale the size by a factor of 2 via bilinear interpolation to generate R n−1 . We apply this process log 2 (N ) times, to produce an output mask of the same size of the input R 1 . We apply 1 × 1 convolution over R 1 to generate a single channel and, finally, a sigmoid layer to obtain scores between 0 and 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We conduct experiments on the four standard datasets for this task: ReferIt, UNC, UNC+ <ref type="bibr" target="#b28">[29]</ref>, and GRef <ref type="bibr" target="#b29">[30]</ref>. UNC, UNC+ and GRef are based on MS COCO <ref type="bibr" target="#b0">[1]</ref>. The type of objects that appear in the referring expressions, length of the expressions, and relative size of the referred objects are the main differences between the datasets. The high variability of those characteristics across the datasets evidences the challenge of constructing models for this task that are capable of generalization.</p><p>ReferIt <ref type="bibr" target="#b28">[29]</ref> is a crowd-sourced database that contains images and referring expressions to objects in those images. Currently it has 130,525 expressions, referring to 96,654 distinct objects, in 19,894 photographs of natural scenes.</p><p>UNC <ref type="bibr" target="#b30">[31]</ref>, was collected interactively in the ReferIt game, with images that were selected to contain two or more objects of the same object category <ref type="bibr" target="#b30">[31]</ref>, which means that an expression making reference to a determined type of object will need to be further analysed to determine which object the query is referring to, since ambiguity arises when only guided by semantic instance class cues. It consists of 142,209 referring expressions for 50,000 objects in 19,994 images.</p><p>UNC+ <ref type="bibr" target="#b30">[31]</ref>, is similar to UNC but has an additional restriction regarding words describing location: expressions must be based only on appearance rather than location. Such restriction implies that the expression will depend on the perspective of the scene and the semantic class of the object.</p><p>GRef <ref type="bibr" target="#b29">[30]</ref>, was collected on Amazon's Mechanical Turk and contains 85,474 referring expressions for 54,822 objects in 26,711 images selected to contain between two and four objects of the same class, and thus, it presents similar challenges to those of UNC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Metrics</head><p>We use the standard metrics from the literature to allow for direct comparison with respect to the state-of-the-art. We perform experiments with the proposed method on the four standard datasets described above by training on the training set and evaluating the performance in each of the validation or test sets. We evaluate results by using two standard metrics: (i) mean Intersection over Union (mIoU), defined as the total intersection area between the output and the Ground Truth (GT) mask, divided by the total union area between the output and the GT mask, added over all the images in the evaluation set, and (ii) Precision@X, or Pr @X, (X ∈ {0.5, 0.6, 0.7, 0.8, 0.9}), defined as the percentage of images with IoU higher than X. We report mIoU in the validation and test splits of each dataset, when available, using optimal thresholds from the training or validation splits, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>All the models are defined and trained with DPN92 <ref type="bibr" target="#b26">[27]</ref> as the backbone, which outputs 2688 channels in the last layer. We use N = 5 scales in the VM. We use the following hyperparameters, which we optimized on the UNC+ val set: WE size of 1000, 2-layered SRU with hidden state size of 1000, K = 10 filters, 1000 1 × 1 convolution filters in the SM, 3-layered mSRU's hidden state size of 1000. The increased number of layers presented here in both the SRU and the mSRU, with respect to usual number of layers in LSTMs, are in response to the increased need of layers for an SRU to work as expected, according to <ref type="bibr" target="#b5">[6]</ref>. We train our method in two stages: at low resolution (i.e., without using the UM) and then finetune the UM to obtain high resolution segmentation maps.</p><p>Training is done with Adam optimizer <ref type="bibr" target="#b31">[32]</ref> with an initial learning rate of 1 × 10 −5 , a scheduler that waits 2 epochs for loss stagnation to reduce the learning rate by a factor of 10, and batch size of 1 image-query pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Control Experiments</head><p>We assess the relative importance of our modules in the final result by performing ablation experiments. The control experiments were trained until convergence on the UNC dataset. Accordingly, we compare them to a version of our full method trained for a similar amount of time. <ref type="table" target="#tab_0">Table 1</ref> presents the results.</p><p>The "Only VM" experiment in row 1 consists on training only the VM and upsampling the low resolution output with bilinear interpolation, without using the query. At test time, the VM processes the image and the resulting segmentation map is upsampled using the UM and compared to the GT mask. Results show how this method performs poorly in comparison to our full approach, which confirms our hypothesis that naively using a CNN falls short for the task addressed in this paper. However, it is interesting that performance is rather high for a method that does not use linguistic information at all. This result reveals that many of the objects annotated in this dataset are salient, and so the network is able to learn to segment salient objects without help from the query.</p><p>The experiment in row 2 consists of defining r t = h t , instead of using the concatenation of h t and e t , which affects both the LM (when computing the dynamic filters) and the SM. Results show that using the learned embeddings provides a small gain in the full method, particularly for stricter overlap thresholds.</p><p>Next, in row 3 we assess the importance of skip connections in the UM, which is a measure of the usefulness of features extracted in the downsampling process for the upsampling module. The large drop in performance with respect to the full method shows that the skip connections allow the network to exploit finer details that are otherwise lost, showing how the upsampling strategy can benefit from performing convolutions followed by bilinear interpolations instead of deconvolutions, as done in <ref type="bibr" target="#b2">[3]</ref>.</p><p>We next study the effects of removing features from M t . In rows 4 and 5 we remove the set of responses to the dynamic filters F , as well as the concatenation of r t in the SM, respectively. We observe that the dynamic filters generate useful scores for the natural language queries in the visual space, and that reusing features from the LM in the SM does not help the network significantly.</p><p>Our results show that the key components of our network have significant impact in overall performance. High performance is not achieved by either using only linguistic information (r t ) or the response to filters (F ): both must be properly combined. Additionally, the UM allows the network to properly exploit features from the downsampling stage and perform detailed segmentation.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with the State-of-the-Art</head><p>Next, we proceed to compare our full method with the state-of-the-art, for which we evaluate on all the datasets described above. <ref type="table" target="#tab_1">Table 2</ref> compares the mIoU of our method with the state-of-the-art in this task <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. The results show that our method outperforms all other methods in six out of eight splits of the datasets. By including enriched linguistic features at several stages of the process, and by combining them in different ways, our network learns appropriate associations between queries and the instances they refer to. Interestingly, the performance gain in the testB splits of UNC and UNC+ is not as large as in testA. One possible reason for the smaller performance gain across splits is their difference: visual inspection of results shows how testA splits are biased towards queries related to segmenting persons. The testB splits, however, contain more varied queries and objects, which is why the increase in mIoU is not as marked. This behavior can also be observed for the method proposed by <ref type="bibr" target="#b3">[4]</ref>, as shown in the second-to-last line of <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Efficiency Comparison: SRU vs. LSTM</head><p>In order to assess the efficiency and the performance of SRUs when compared to the more commonly used LSTMs, both as language and multi-modal processors, we conduct an experiment in which we replaced the SRUs with LSTMs in our final system, both in the LM and the SM, we trained on the UNC dataset, and we measured performance on the testA split. In terms of model complexity, when using SRUs, the LM and the SM have 9M and 10M trainable parameters, respectively. When switching to LSTMs, the number of parameters increases to 24M and 24.2M, respectively, multiplying training time by a factor of three, as shown in <ref type="figure" target="#fig_8">Fig. 7a</ref>. Regarding accuracy, <ref type="figure" target="#fig_8">Fig. 7b</ref>. shows that both systems perform similarly, with a small advantage for SRUs. Therefore, when compared to LSTMs, SRUs allow us to design architectures that are more compact, train significantly faster, and generalize better. <ref type="figure" target="#fig_9">Fig. 8</ref> shows qualitative results in which the network performed well. These examples demonstrate DMN's flexibility for segmenting based on different information about a particular class or instance: attributes, location or role. We emphasize that understanding a role is not trivial, as it is related to the object's context and appearance. Additionally, a semantic difficulty that our network seems to overcome is that the role coexists with the object's class: an instance can be "batter" and be "person". Notice in <ref type="figure" target="#fig_9">Fig. 8</ref> that thanks to the upsampling module, our network segments fine details such as legs, heads and hands. In <ref type="figure" target="#fig_9">Fig. 8a</ref> the query refers to the kid by one of his attributes: the color of his shirt; in <ref type="figure" target="#fig_9">Fig. 8b</ref> the man is defined by his location and the fact that he is alone (although that could be dropped, as there is no ambiguity); in Figs. 8c and d the reference is based on the person's role. Typical failure cases are depicted in <ref type="figure" target="#fig_10">Fig. 9</ref>. In <ref type="figure" target="#fig_10">Fig. 9a</ref> the network segments (arguably) the incorrect person, since the correct segmentation was the person at the border of the image whose face is partially shown. Several failure cases we found had exactly the same issue: ambiguity in the expression that could confuse even a human. <ref type="figure" target="#fig_10">Fig. 9b</ref> shows an example of strong failure, in which a weak segmentation is produced. The model appears to have only focused on the word "right". We attribute this failure to the network's inability to make sense of such a relatively long sentence, which, while unambiguously defining an object, is a confusing way of referring to it. <ref type="figure" target="#fig_10">Fig. 9c</ref> is an interesting example of the network's confusion. While the woman is not segmented, two subjects that share several attributes (guy, gray and shirt) are confused and are both segmented. However, the network does not manage to use the word "standing" to resolve the ambiguity. Finally, in <ref type="figure" target="#fig_10">Fig. 9d</ref> a failure is observed, where nothing related to the query is segmented. The mask that is produced only reflects a weak attempt of segmenting the red object, while ignoring the upper part of the image, in which both the hand and the remote were present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We propose Dynamic Multimodal Network, a novel approach for segmentation of instances based on natural language expressions. DMN integrates insights from previous works into a modularized network, in which each module has the responsibility of handling information from a specific domain. Our Synthesis Module combines the outputs from previous modules and handles this multimodal information to produce features that can be used by the Upsampling Module. Thanks to the incremental use of feature maps obtained in the encoding part of the network, the Upsampling Module delivers great detail in the final segmentations. Our method outperforms the state-of-the-art methods in six of the eight standard dataset splits for this task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Overview of our Dynamic Multimodal Network (DMN), involving four different modules: Visual Module (VM), Language Module (LM), Synthesis Module (SM), and Upsampling Module (UM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The Visual Module outputs feature maps at N different scales with the aim of using them in the segmentation process and in the upsampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig</head><label></label><figDesc>Fig. 2 illustrates our overall architecture. Given an input consisting of an image I, and a query composed of T words, {w t } T t=1 , the Visual Module (VM) takes I as input and produces feature maps at N different scales: {I n } N n=1 . The Language Module (LM) processes {w t } T t=1 and yields a set of features {r t } T t=1 and a set of dynamic filters {{f k,t } K k=1 } T t=1 . Given the VM's last output, I N , {r t } T t=1 , and {{f k,t } K k=1 } T t=1 , the Synthesis Module (SM) processes this information and produces a single feature map for the entire referring expression. This output, along with the feature maps given by the VM, is processed by the Upsampling Module (UM), that outputs a heatmap with a single channel, to which a sigmoid activation function is applied in order to produce the final prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: The Language Module uses an SRU, instead of the traditional LSTM, to output enriched features of the query and dynamic filters based on such features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: The Synthesis Module takes into account the response to dynamic filters, language features, spatial coordinates representation, and visual features in a recurrent manner to output a single response map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 2</head><label>1</label><figDesc>Fig. 6: The Upsampling Module makes use of all the feature maps that were generated in the feature extraction process to provide more detailed segmentations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Performance (mIoU).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: SRUs vs. LSTMs comparison on UNC testA (at low resolution).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Qualitative examples of the output of the network. From left to right in each subfigure: original image, heatmap produced by our method, and ground-truth mask. Each caption is the query that produced the output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Negative examples of the output of the network. From left to right in each Subfigure: original image, heatmap produced by our method, and ground-truth mask. Each caption is the query that produced the output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Precision@X and mIoU for ablation study in the UNC testA split</figDesc><table>Method 
Pr@0.5 Pr@0.6 Pr@0.7 Pr@0.8 Pr@0.9 mIoU 
Only VM 
15.26 
6.36 
2.96 
0.91 
0.14 
30.92 
Only ht in LM and SM 65.38 
57.99 47.07 27.38 
4.63 
54.80 
No skip connections in UM 56.58 
42.77 
26.32 
9.22 
1.07 
49.26 
No dynamic filters 
57.53 
48.70 
38.27 
20.64 
3.00 
50.34 
No concatenation of rt 64.52 
56.69 
45.16 
25.56 
4.38 
54.69 
DMN 
65.83 57.82 
46.80 
27.64 5.12 
54.83 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Comparison with the state-of-the-art in mIoU performance across the dif-</figDesc><table>ferent datasets. Blank entries where authors do not report performance 
Method Referit GRef 
UNC 
UNC+ 
test 
val 
val testA testB val testA testB 
[3] 
48.03 28.14 
-
-
-
-
-
-
[33] 
49.91 34.06 
-
-
-
-
-
-
[5] 
54.30 
-
-
-
-
-
-
-
[4] 
58.73 34.52 45.18 45.69 45.57 29.86 30.48 29.50 
DMN 52.81 36.76 49.78 54.83 45.13 38.88 44.22 32.29 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/BCV-Uniandes/query-objseg</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors gratefully thank NVIDIA for donating the GPUs used in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Multimodal Instance Segmentation</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV</title>
		<editor>Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<title level="m">Segmentation from natural language expressions. European Conference on Computer Vision (ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recurrent multimodal interaction for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1280" to="1289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tracking by natural language specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Training RNNs as fast as CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<idno>CoRR abs/1709.02755</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<title level="m">Natural language object retrieval. Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4555" to="4564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding object descriptions in robotics by open-vocabulary object retrieval and detection. The International</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="265" to="280" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<title level="m">Deep compositional captioning: Describing novel object categories without paired training data. Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic compositional networks for visual captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1141" to="1150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4565" to="4574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Review networks for caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6325" to="6334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Visual7w: Grounded question answering in images. Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4995" to="5004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<title level="m">VQA: Visual question answering. International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph-structured representations for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3233" to="3241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10</title>
		<meeting>the 27th international conference on machine learning (ICML-10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deconvolution and checkerboard artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Very deep convolutional networks for large-scale image recognition. International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention -MICCAI</title>
		<imprint>
			<biblScope unit="page" from="234" to="241" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Object instance segmentation and fine-grained localization using hypercolumns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="627" to="639" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dual path networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Densely connected convolutional networks. Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Referit game: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Utilizing large scale vision and text datasets for image segmentation from referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
