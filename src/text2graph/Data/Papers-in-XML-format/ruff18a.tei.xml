<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep One-Class Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Görnitz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoaib</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Binder</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Müller</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Kloft</surname></persName>
						</author>
						<title level="a" type="main">Deep One-Class Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Despite the great advances made by deep learning in many machine learning problems, there is a relative dearth of deep learning approaches for anomaly detection. Those approaches which do exist involve networks trained to perform a task other than anomaly detection, namely generative models or compression, which are in turn adapted for use in anomaly detection; they are not trained on an anomaly detection based objective. In this paper we introduce a new anomaly detection method-Deep Support Vector Data Description-, which is trained on an anomaly detection based objective. The adaptation to the deep regime necessitates that our neural network and training procedure satisfy certain properties, which we demonstrate theoretically. We show the effectiveness of our method on MNIST and CIFAR-10 image benchmark datasets as well as on the detection of adversarial examples of GT-SRB stop signs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Anomaly detection (AD) <ref type="bibr" target="#b7">(Chandola et al., 2009;</ref><ref type="bibr" target="#b0">Aggarwal, 2016)</ref> is the task of discerning unusual samples in data. Typically, this is treated as an unsupervised learning problem where the anomalous samples are not known a priori and it is assumed that the majority of the training dataset consists of "normal" data (here and elsewhere the term "normal" means not anomalous and is unrelated to the Gaussian distribution). The aim then is to learn a model that accurately describes "normality." Deviations from this description are then deemed to be anomalies. This is also known as oneclass classification <ref type="bibr" target="#b33">(Moya et al., 1993)</ref>. AD algorithms are often trained on data collected during the normal operating state of a machine or system for monitoring <ref type="bibr" target="#b25">(Lavin &amp; Ahmad, 2015)</ref>. Other domains include intrusion detection for cybersecurity <ref type="bibr" target="#b12">(Garcia-Teodoro et al., 2009)</ref>, fraud detection <ref type="bibr" target="#b36">(Phua et al., 2005)</ref>, and medical diagnosis <ref type="bibr" target="#b43">(Salem et al., 2013;</ref><ref type="bibr" target="#b44">Schlegl et al., 2017)</ref>. As with many fields, the data in these domains is growing rapidly in size and dimensionality and thus we require effective and efficient ways to detect anomalies in large quantities of high-dimensional data.</p><p>Classical AD methods such as the One-Class SVM (OC-SVM) <ref type="bibr" target="#b46">(Schölkopf et al., 2001)</ref> or Kernel Density Estimation (KDE) <ref type="bibr" target="#b35">(Parzen, 1962)</ref>, often fail in high-dimensional, datarich scenarios due to bad computational scalability and the curse of dimensionality. To be effective, such shallow methods typically require substantial feature engineering. In comparison, deep learning <ref type="bibr" target="#b27">(LeCun et al., 2015;</ref><ref type="bibr" target="#b45">Schmidhuber, 2015)</ref> presents a way to learn relevant features automatically, with exceptional successes over classical methods <ref type="bibr" target="#b9">(Collobert et al., 2011;</ref><ref type="bibr" target="#b19">Hinton et al., 2012)</ref>, especially in computer vision <ref type="bibr" target="#b24">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b17">He et al., 2016)</ref>. How to transfer the benefits of deep learning to AD is less clear, however, since finding the right unsupervised deep objective is hard <ref type="bibr" target="#b4">(Bengio et al., 2013)</ref>. Current approaches to deep AD have shown promising results <ref type="bibr" target="#b16">(Hawkins et al., 2002;</ref><ref type="bibr" target="#b42">Sakurada &amp; Yairi, 2014;</ref><ref type="bibr" target="#b55">Xu et al., 2015;</ref><ref type="bibr" target="#b11">Erfani et al., 2016;</ref><ref type="bibr" target="#b2">Andrews et al., 2016;</ref><ref type="bibr" target="#b8">Chen et al., 2017)</ref>, but none of these methods are trained by optimizing an AD based objective function and typically rely on reconstruction error based heuristics.</p><p>In this work we introduce a novel approach to deep AD inspired by kernel-based one-class classification and minimum volume estimation. Our method, Deep Support Vector Data Description (Deep SVDD), trains a neural network while minimizing the volume of a hypersphere that encloses the network representations of the data (see <ref type="figure" target="#fig_1">Figure 1</ref>). Minimizing the volume of the hypersphere forces the network to  . Deep SVDD learns a neural network transformation φ(· ; W) with weights W from input space X ⊆ R d to output space F ⊆ R p that attempts to map most of the data network representations into a hypersphere characterized by center c and radius R of minimum volume. Mappings of normal examples fall within, whereas mappings of anomalies fall outside the hypersphere. extract the common factors of variation since the network must closely map the data points to the center of the sphere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Before introducing Deep SVDD we briefly review kernelbased one-class classification and present existing deep approaches to AD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Kernel-based One-Class Classification</head><p>Let X ⊆ R d be the data space. Let k : X × X → [0, ∞) be a PSD kernel, F k it's associated RKHS, and φ k : X → F k its associated feature mapping. So k(x,x) = φ k (x), φ k (x) F k for all x,x ∈ X where · , · F k is the dot product in Hilbert space F k <ref type="bibr" target="#b3">(Aronszajn, 1950)</ref>. We review two kernel machine approaches to AD.</p><p>Probably the most prominent example of a kernel-based method for one-class classification is the One-Class SVM (OC-SVM) <ref type="bibr" target="#b46">(Schölkopf et al., 2001)</ref>. The objective of the OC-SVM finds a maximum margin hyperplane in feature space, w ∈ F k , that best separates the mapped data from the origin. Given a dataset D n = {x 1 , . . . , x n } with x i ∈ X , the OC-SVM solves the primal problem</p><formula xml:id="formula_0">min w,ρ,ξ 1 2 w 2 F k − ρ + 1 νn n i=1 ξ i s.t. w, φ k (x i ) F k ≥ ρ − ξ i , ξ i ≥ 0, ∀i.<label>(1)</label></formula><p>Here ρ is the distance from the origin to hyperplane w. Nonnegative slack variables ξ = (ξ 1 , . . . , ξ n ) allow the margin to be soft, but violations ξ i get penalized. w 2 F k is a regularizer on the hyperplane w where · F k is the norm induced by · , · F k . The hyperparameter ν ∈ (0, 1] controls the trade-off in the objective. Separating the data from the origin in feature space translates into finding a halfspace in which most of the data lie and points lying outside this halfspace, i.e. w, φ k (x) F k &lt; ρ, are deemed to be anomalous.</p><p>Support Vector Data Description (SVDD) <ref type="bibr" target="#b49">(Tax &amp; Duin, 2004</ref>) is a technique related to OC-SVM where a hypersphere is used to separate the data instead of a hyperplane. The objective of SVDD is to find the smallest hypersphere with center c ∈ F k and radius R &gt; 0 that encloses the majority of the data in feature space F k . The SVDD primal problem is given by min R,c,ξ</p><formula xml:id="formula_1">R 2 + 1 νn i ξ i s.t. φ k (x i ) − c 2 F k ≤ R 2 + ξ i , ξ i ≥ 0, ∀i.<label>(2)</label></formula><p>Again, slack variables ξ i ≥ 0 allow a soft boundary and hyperparameter ν ∈ (0, 1] controls the trade-off between penalties ξ i and the volume of the sphere. Points which fall outside the sphere, i.e.</p><formula xml:id="formula_2">φ k (x) − c 2 F k &gt; R 2 , are deemed anomalous.</formula><p>The OC-SVM and SVDD are closely related. Both methods can be solved by their respective duals, which are quadratic programs and can be solved via a variety of methods, e.g. sequential minimal optimization <ref type="bibr" target="#b37">(Platt, 1998)</ref>. In the case of the widely used Gaussian kernel, the two methods are equivalent and are asymptotically consistent density level set estimators <ref type="bibr" target="#b50">(Tsybakov, 1997;</ref><ref type="bibr" target="#b52">Vert &amp; Vert, 2006)</ref>. Formulating the primal problems with hyperparameter ν ∈ (0, 1] as in <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_1">(2)</ref> is a handy choice of parameterization since ν ∈ (0, 1] is (i) an upper bound on the fraction of outliers, and (ii) a lower bound on the fraction of support vectors (points that are either on or outside the boundary). This result is known as the ν-property <ref type="bibr" target="#b46">(Schölkopf et al., 2001</ref>) and allows one to incorporate a prior belief about the fraction of outliers present in the training data into the model.</p><p>Apart from the necessity to perform explicit feature engineering <ref type="bibr" target="#b34">(Pal &amp; Foody, 2010)</ref>, another drawback of the aforementioned methods is their poor computational scaling due to the construction and manipulation of the kernel matrix. Kernel-based methods scale at least quadratically in the number of samples <ref type="bibr" target="#b51">(Vempati et al., 2010)</ref> unless some sort of approximation technique is used <ref type="bibr" target="#b39">(Rahimi &amp; Recht, 2007)</ref>. Moreover, prediction with kernel methods requires storing support vectors which can require large amounts of memory. As we will see, Deep SVDD does not suffer from these limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep Approaches to Anomaly Detection</head><p>Deep learning <ref type="bibr" target="#b27">(LeCun et al., 2015;</ref><ref type="bibr" target="#b45">Schmidhuber, 2015)</ref> is a subfield of representation learning <ref type="bibr" target="#b4">(Bengio et al., 2013)</ref> that utilizes model architectures with multiple processing layers to learn data representations with multiple levels of abstraction. Multiple levels of abstraction allow for the representation of a rich space of features in a very compact and distributed form. Deep (multi-layered) neural networks are especially well-suited for learning representations of data that are hierarchical in nature, such as images or text.</p><p>We categorize approaches that try to leverage deep learning for AD into either "mixed" or "fully deep." In mixed approaches, representations are learned separately in a preceding step before these representations are then fed into classical (shallow) AD methods like the OC-SVM. Fully deep approaches, in contrast, employ the representation learning objective directly for detecting anomalies.</p><p>With Deep SVDD, we introduce a novel, fully deep approach to unsupervised AD. Deep SVDD learns to extract the common factors of variation of the data distribution by training a neural network to fit the network outputs into a hypersphere of minimum volume. In comparison, virtually all existing deep AD approaches rely on the reconstruction error -either in mixed approaches for just learning representations, or directly for both representation learning as well as detection.</p><p>Deep autoencoders <ref type="bibr" target="#b18">(Hinton &amp; Salakhutdinov, 2006</ref>) (of various types) are the predominant approach used for deep AD. Autoencoders are neural networks which attempt to learn the identity function while having an intermediate representation of reduced dimension (or some sparsity regularization) serving as a bottleneck to induce the network to extract salient features from some dataset. Typically these networks are trained to minimize reconstruction error, i.e. x −x 2 . Therefore these networks should be able to extract the common factors of variation from normal samples and reconstruct them accurately, while anomalous samples do not contain these common factors of variation and thus cannot be reconstructed accurately. This allows for the use of autoencoders in mixed approaches <ref type="bibr" target="#b55">(Xu et al., 2015;</ref><ref type="bibr" target="#b2">Andrews et al., 2016;</ref><ref type="bibr" target="#b11">Erfani et al., 2016;</ref><ref type="bibr" target="#b41">Sabokrou et al., 2016)</ref>, by plugging the learned embeddings into classical AD methods, but also in fully deep approaches, by directly employing the reconstruction error as an anomaly score <ref type="bibr" target="#b16">(Hawkins et al., 2002;</ref><ref type="bibr" target="#b42">Sakurada &amp; Yairi, 2014;</ref><ref type="bibr" target="#b1">An &amp; Cho, 2015;</ref><ref type="bibr" target="#b8">Chen et al., 2017)</ref>. Some variants of the autoencoder used for the purpose of AD include denoising autoencoders <ref type="bibr" target="#b53">(Vincent et al., 2008;</ref>, sparse autoencoders <ref type="bibr" target="#b29">(Makhzani &amp; Frey, 2013)</ref>, variational autoencoders (VAEs) <ref type="bibr" target="#b22">(Kingma &amp; Welling, 2013)</ref>, and deep convolutional autoencoders (DCAEs) <ref type="bibr" target="#b31">(Masci et al., 2011;</ref><ref type="bibr" target="#b30">Makhzani &amp; Frey, 2015)</ref>, where the last variant is predominantly used in AD applications with image or video data <ref type="bibr" target="#b47">(Seeböck et al., 2016;</ref><ref type="bibr" target="#b40">Richter &amp; Roy, 2017)</ref>.</p><p>Autoencoders have the objective of dimensionality reduction and do not target AD directly. The main difficulty of applying autoencoders for AD is given in choosing the right degree of compression, i.e. dimensionality reduction. If there was no compression, an autoencoder would just learn the identity function. In the other edge case of information reduction to a single value, the mean would be the optimal solution. That is, the "compactness" of the data representation is a model hyperparameter and choosing the right balance is hard due to the unsupervised nature and since the intrinsic dimensionality of the data is often difficult to estimate <ref type="bibr" target="#b4">(Bengio et al., 2013)</ref>. In comparison, we include the compactness of representation into our Deep SVDD objective by minimizing the volume of a data-enclosing hypersphere and thus target AD directly.</p><p>Apart from autoencoders, <ref type="bibr" target="#b44">Schlegl et al. (2017)</ref> have recently proposed a novel deep AD method based on Generative Adversarial Networks (GANs) <ref type="bibr" target="#b14">(Goodfellow et al., 2014)</ref> called AnoGAN. In this method one first trains a GAN to generate samples according to the training data. Given a test point AnoGAN tries to find the point in the generator's latent space that generates the sample closest to the test input considered. Intuitively, if the GAN has captured the distribution of the training data then normal samples, i.e. samples from the distribution, should have a good representation in the latent space and anomalous samples will not. To find the point in latent space, <ref type="bibr" target="#b44">Schlegl et al. (2017)</ref> perform gradient descent in latent space keeping the learned weights of the generator fixed. AnoGAN finally defines an anomaly score also via the reconstruction error. Similar to autoencoders, a main difficulty of this generative approach is the question of how to regularize the generator for compactness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep SVDD</head><p>In this section we introduce Deep SVDD, a method for deep one-class classification. We present the Deep SVDD objective, its optimization, and theoretical properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Deep SVDD Objective</head><p>With Deep SVDD, we build on the kernel-based SVDD and minimum volume estimation by finding a data-enclosing hypersphere of smallest size. However, with Deep SVDD we learn useful feature representations of the data together with the one-class classification objective. To do this we employ a neural network that is jointly trained to map the data into a hypersphere of minimum volume.</p><p>For some input space X ⊆ R d and output space F ⊆ R p , let φ(· ; W) : X → F be a neural network with L ∈ N hidden layers and set of weights W = {W 1 , . . . , W L } where W are the weights of layer ∈ {1, . . . , L}. That is, φ(x; W) ∈ F is the feature representation of x ∈ X given by network φ with parameters W. The aim of Deep SVDD then is to jointly learn the network parameters W together with minimizing the volume of a data-enclosing hypersphere in output space F that is characterized by radius R &gt; 0 and center c ∈ F which we assume to be given for now. Given some training data D n = {x 1 , . . . , x n } on X , we define the soft-boundary Deep SVDD objective as</p><formula xml:id="formula_3">min R,W R 2 + 1 νn n i=1 max{0, φ(x i ; W) − c 2 − R 2 } + λ 2 L =1 W 2 F .<label>(3)</label></formula><p>As in kernel SVDD, minimizing R 2 minimizes the volume of the hypersphere. The second term is a penalty term for points lying outside the sphere after being passed through the network, i.e. if its distance to the center φ(x i ; W) − c is greater than radius R. Hyperparameter ν ∈ (0, 1] controls the trade-off between the volume of the sphere and violations of the boundary, i.e. allowing some points to be mapped outside the sphere. We prove in Section 3.3 that the ν-parameter in fact allows us to control the proportion of outliers in a model similar to the ν-property of kernel methods mentioned previously. The last term is a weight decay regularizer on the network parameters W with hyperparameter λ &gt; 0, where · F denotes the Frobenius norm.</p><p>Optimizing objective (3) lets the network learn parameters W such that data points are closely mapped to the center c of the hypersphere. To achieve this the network must extract the common factors of variation of the data. As a result, normal examples of the data are closely mapped to center c, whereas anomalous examples are mapped further away from the center or outside of the hypersphere. Through this we obtain a compact description of the normal class. Minimizing the size of the sphere enforces this learning process.</p><p>For the case where we assume most of the training data D n is normal, which is often the case in one-class classification tasks, we propose an additional simplified objective. We define the One-Class Deep SVDD objective as</p><formula xml:id="formula_4">min W 1 n n i=1 φ(x i ; W) − c 2 + λ 2 L =1 W 2 F . (4)</formula><p>One-Class Deep SVDD simply employs a quadratic loss for penalizing the distance of every network representation φ(x i ; W) to c ∈ F. The second term again is a network weight decay regularizer with hyperparameter λ &gt; 0. We can think of One-Class Deep SVDD also as finding a hypersphere of minimum volume with center c. But unlike in soft-boundary Deep SVDD, where the hypersphere is contracted by penalizing the radius directly and the data representations that fall outside the sphere, One-Class Deep SVDD contracts the sphere by minimizing the mean distance of all data representations to the center. Again, to map the data (on average) as close to center c as possible, the neural network must extract the common factors of variation. Penalizing the mean distance over all data points instead of allowing some points to fall outside the hypersphere is consistent with the assumption that the majority of training data is from one class.</p><p>For a given test point x ∈ X , we can naturally define an anomaly score s for both variants of Deep SVDD by the distance of the point to the center of the hypersphere, i.e.</p><formula xml:id="formula_5">s(x) = φ(x; W * ) − c 2 ,<label>(5)</label></formula><p>where W * are the network parameters of a trained model. For soft-boundary Deep SVDD, we can adjust this score by subtracting the final radius R * of the trained model such that anomalies (points with representations outside the sphere) have positive scores, whereas inliers have negative scores. Note that the network parameters W * (and R * ) completely characterize a Deep SVDD model and no data must be stored for prediction, thus endowing Deep SVDD a very low memory complexity. This also allows fast testing by simply evaluating the network φ with learned parameters W * at some test point x ∈ X which usually is just a concatenation of simple functions.</p><p>We address Deep SVDD optimization and selection of the hypersphere center c ∈ F in the following two subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Optimization of Deep SVDD</head><p>We use stochastic gradient descent (SGD) and its variants (e.g., Adam <ref type="bibr" target="#b21">(Kingma &amp; Ba, 2014)</ref>) to optimize the parameters W of the neural network in both Deep SVDD objectives using backpropagation. Training is carried out until convergence to a local minimum. Using SGD allows Deep SVDD to scale well with large datasets as its computational complexity scales linearly in the number of training batches and each batch can be processed in parallel (e.g. by processing on multiple GPUs). SGD optimization also enables iterative or online learning.</p><p>Since the network parameters W and radius R generally live on different scales, using one common SGD learning rate may be inefficient for optimizing the soft-boundary Deep SVDD. Instead, we suggest optimizing the network parameters W and radius R alternately in an alternating minimization/block coordinate descent approach. That is, we train the network parameters W for some k ∈ N epochs while the radius R is fixed. Then, after every k-th epoch, we solve for radius R given the data representations from the network using the network parameters W of the latest update. R can be easily solved for via line search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Properties of Deep SVDD</head><p>For an improperly formulated network or hypersphere center c, the Deep SVDD can learn trivial, uninformative solutions. Here we theoretically demonstrate some network properties which will yield trivial solutions (and thus must be avoided). We then prove the ν-property for soft-boundary Deep SVDD.</p><p>In the following let J soft (W, R) and J OC (W) be the softboundary and One-Class Deep SVDD objective functions as defined in <ref type="formula" target="#formula_3">(3)</ref> and (4). First, we show that including the hypersphere center c ∈ F as a free optimization variable leads to trivial solutions for both objectives.</p><p>Proposition 1 (All-zero-weights solution). Let W 0 be the set of all-zero network weights, i.e., W = 0 for every W ∈ W 0 . For this choice of parameters, the network maps any input to the same output, i.e., φ(x; W 0 ) = φ(x; W 0 ) =: c 0 ∈ F for any x,x ∈ X . Then, if c = c 0 , the optimal solution of Deep SVDD is given by W * = W 0 and R * = 0.</p><p>Proof. For every configuration (W, R) we have that J soft (R, W) ≥ 0 and J OC (W) ≥ 0 respectively. As the output of the all-zero-weights network φ(x; W 0 ) is constant for every input x ∈ X (all parameters in each network unit are zero and thus the linear projection in each network unit maps any input to zero), and the center of the hypersphere is given by c = φ(x; W 0 ), all errors in the empirical sums of the objectives become zero. Thus, R * = 0 and W * = W 0 are optimal solutions since J soft (W * , R * ) = 0 and J OC (W * ) = 0 in this case.</p><p>Stated less formally, Proposition 1 implies that if we include the hypersphere center c as a free variable in the SGD optimization, Deep SVDD would likely converge to the trivial solution (W * , R * , c * ) = (W 0 , 0, c 0 ). We call such a solution, where the network learns weights such that the network produces a constant function mapping to the hypersphere center, "hypersphere collapse" since the hypersphere radius collapses to zero. Proposition 1 also implies that we require c = c 0 when fixing c in output space F because otherwise a hypersphere collapse would again be possible. For a convolutional neural network (CNN) with ReLU activation functions, for example, this would require c = 0. We found empirically that fixing c as the mean of the network representations that result from performing an initial forward pass on some training data sample to be a good strategy. Although we obtained similar results in our experiments for other choices of c (making sure c = c 0 ), we found that fixing c in the neighborhood of the initial network outputs made SGD convergence faster and more robust.</p><p>Next, we identify two network architecture properties, that would also enable trivial hypersphere collapse solutions. Proposition 2 (Bias terms). Let c ∈ F be any fixed hypersphere center. If there is any hidden layer in network φ(· ; W) : X → F having a bias term, there exists an optimal solution (R * , W * ) of the Deep SVDD objectives (3) and (4) with R * = 0 and φ(x; W * ) = c for every x ∈ X .</p><p>Proof. Assume layer ∈ {1, . . . , L} with weights W also has a bias term b . For any input x ∈ X , the output of layer is then given by</p><formula xml:id="formula_6">z (x) = σ (W · z −1 (x) + b ),</formula><p>where "·" denotes a linear operator (e.g., matrix multiplication or convolution), σ (·) is the activation of layer , and the output z −1 of the previous layer − 1 depends on input x by the concatenation of previous layers. Then, for W = 0, we have that z (x) = σ (b ), i.e., the output of layer is constant for every input x ∈ X . Therefore, the bias term b (and the weights of the subsequent layers) can be chosen such that φ(x; W * ) = c for every x ∈ X (assuming c is in the image of the network as a function of b and the subsequent parameters W +1 , . . . , W L ). Hence, selecting W * in this way results in an empirical term of zero and choosing R * = 0 gives the optimal solution (ignoring the weight decay regularization terms for simplicity).</p><p>Put differently, Proposition 2 implies that networks with bias terms can easily learn any constant function, which is independent of the input x ∈ X . It follows that bias terms should not be used in neural networks with Deep SVDD since the network can learn the constant function mapping directly to the hypersphere center, leading to hypersphere collapse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>Proposition 3 (Bounded activation functions). Consider a network unit having a monotonic activation function σ(·) that has an upper (or lower) bound with sup z σ(z) = 0 (or inf z σ(z) = 0). Then, for a set of unit inputs {z 1 , . . . , z n } that have at least one feature that is positive or negative for all inputs, the non-zero supremum (or infimum) can be uniformly approximated on the set of inputs.</p><p>Proof. W.l.o.g. consider the case of σ being upper bounded by B := sup z σ(z) = 0 and feature k being positive for all inputs, i.e. z (k) i &gt; 0 for every i = 1, . . . , n. Then, for every ε &gt; 0, one can always choose the weight of the k-th element w k large enough (setting all other network unit weights to zero) such that sup i |σ(w k z</p><formula xml:id="formula_7">(k) i ) − B| &lt; ε.</formula><p>Proposition 3 simply says that a network unit with bounded activation function can be saturated for all inputs having at least one feature with common sign thereby emulating a bias term in the subsequent layer, which again leads to a hypersphere collapse. Therefore, unbounded activation functions (or functions only bounded by 0) such as the ReLU should be preferred in Deep SVDD to avoid a hypersphere collapse due to "learned" bias terms.</p><p>To summarize the above analysis: the choice of hypersphere center c must be something other than the all-zero-weights solution and only neural networks without bias terms or bounded activation functions should be used in Deep SVDD to prevent a hypersphere collapse solution. Lastly, we prove that the ν-property also holds for soft-boundary Deep SVDD which allows to include a prior assumption on the number of anomalies assumed to be present in the training data.</p><p>Proposition 4 (ν-property). Hyperparameter ν ∈ (0, 1] in the soft-boundary Deep SVDD objective in (3) is an upper bound on the fraction of outliers and a lower bound on the fraction of samples being outside or on the boundary of the hypersphere.</p><formula xml:id="formula_8">Proof. Define d i = φ(x i ; W) − c 2 for i = 1, . . . , n. W.l.o.g. assume d 1 ≥ · · · ≥ d n .</formula><p>The number of outliers is then given by n out = |{i : d i &gt; R 2 }| and we can write the soft-boundary objective J soft (in radius R) as</p><formula xml:id="formula_9">J soft (R) = R 2 − n out νn R 2 = 1 − n out νn R 2 .</formula><p>That is, radius R is decreased as long as n out ≤ νn holds and decreasing R gradually increases n out . Thus, nout n ≤ ν must hold in the optimum, i.e. ν is an upper bound on the fraction of outliers, and the optimal radius R * is given for the largest n out for which this inequality still holds. Finally, we have that R * 2 = d i for i = n out + 1 since radius R is minimal in this case and points on the boundary do not increase the objective. Hence, we also have |{i :</p><formula xml:id="formula_10">d i ≥ R * 2 }| ≥ n out + 1 ≥ νn.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate Deep SVDD on the well-known MNIST (LeCun et al., 2010) and CIFAR-10 ( <ref type="bibr" target="#b23">Krizhevsky &amp; Hinton, 2009)</ref> datasets. Adversarial attacks <ref type="bibr" target="#b15">(Goodfellow et al., 2015)</ref> have seen a lot attention recently and here we examine the possibility of using anomaly detection to detect such attacks.</p><p>To do this we apply Boundary Attack <ref type="bibr" target="#b5">(Brendel et al., 2018)</ref> to the GTSRB stop signs dataset <ref type="bibr" target="#b48">(Stallkamp et al., 2011)</ref>. We compare our method against a diverse collection of stateof-the-art methods from different paradigms. We use image data since they are usually high-dimensional and moreover allow for a qualitative visual assessment of detected anomalies by human observers. Using classification datasets to create one-class classification setups allows us to evaluate the results quantitatively via AUC measure by using the ground truth labels in testing (cf. <ref type="bibr" target="#b11">Erfani et al., 2016;</ref><ref type="bibr" target="#b10">Emmott et al., 2016)</ref>. For training, of course, we do not use any labels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Competing methods</head><p>Shallow Baselines (i) Kernel OC-SVM/SVDD with Gaussian kernel. We select the inverse length scale γ from γ ∈ {2 −10 , 2 −9 , . . . , 2 −1 } via grid search using the performance on a small holdout set (10 % of randomly drawn test samples). This grants shallow SVDD a small supervised advantage. We run all experiments for ν ∈ {0.01, 0.1} and report the better result. (ii) Kernel density estimation (KDE). We select the bandwidth h of the Gaussian kernel from h ∈ {2 0.5 , 2 1 , . . . , 2 5 } via 5-fold cross-validation using the log-likelihood score. (iii) Isolation Forest (IF) <ref type="bibr" target="#b28">(Liu et al., 2008)</ref>. We set the number of trees to t = 100 and the sub-sampling size to ψ = 256, as recommended in the original work. We do not compare to lazy evaluation approaches since such methods have no training phase and do not learn a model of normality (e.g. Local Outlier Factor (LOF) <ref type="bibr" target="#b6">(Breunig et al., 2000)</ref>). For all three shallow baselines, we reduce the dimensionality of the data via PCA, where we choose the minimum number of eigenvectors such that at least 95% of the variance is retained (cf. <ref type="bibr" target="#b11">Erfani et al., 2016)</ref>.</p><p>Deep Baselines and Deep SVDD We compare Deep SVDD to the two deep approaches described Section 2.2. We choose the DCAE from the various autoencoders since our experiments are on image data. For the DCAE encoder, we employ the same network architectures as we use for Deep SVDD. The decoder is then created symmetrically, where we substitute max-pooling with upsampling. We train the DCAE using the MSE loss. For AnoGAN we fix the architecture to DCGAN <ref type="bibr" target="#b38">(Radford et al., 2015)</ref> and set the latent space dimensionality to 256, following <ref type="bibr" target="#b32">Metz et al. (2017)</ref>, and otherwise follow <ref type="bibr" target="#b44">Schlegl et al. (2017)</ref>. For Deep SVDD, we remove the bias terms in all network units to prevent a hypersphere collapse as explained in Section 3.3. In soft-boundary Deep SVDD, we solve for R via line search every k = 5 epochs. We choose ν from ν ∈ {0.01, 0.1} and again report the best results. As was described in Sec- tion 3.3, we set the hypersphere center c to the mean of the mapped data after performing an initial forward pass. For optimization, we use the Adam optimizer <ref type="bibr" target="#b21">(Kingma &amp; Ba, 2014)</ref> with parameters as recommended in the original work and apply Batch Normalization <ref type="bibr" target="#b20">(Ioffe &amp; Szegedy, 2015)</ref>. For the competing deep AD methods we initialize network weights by uniform Glorot weights <ref type="bibr" target="#b13">(Glorot &amp; Bengio, 2010)</ref>, and for Deep SVDD use the weights from the trained DCAE encoder for initialization, thus establishing a pre-training procedure. We employ a simple two-phase learning rate schedule (searching + fine-tuning) with initial learning rate η = 10 −4 , and subsequently η = 10 −5 . For DCAE we train 250 + 100 epochs, for Deep SVDD 150 + 100. Leaky ReLU activations are used with leakiness α = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">One-class classification on MNIST and CIFAR-10</head><p>Setup Both MNIST and CIFAR-10 have ten different classes from which we create ten one-class classification setups. In each setup, one of the classes is the normal class and samples from the remaining classes are used to represent anomalies. We use the original training and test splits in our experiments and only train with training set examples from the respective normal class. This gives training set sizes of n ≈ 6 000 for MNIST and n = 5 000 for CIFAR-10. Both test sets have 10 000 samples including samples from the nine anomalous classes for each setup. We pre-process all images with global contrast normalization using the L 1 -norm and finally rescale to [0, 1] via min-max-scaling.</p><p>Network architectures For both datasets, we use LeNettype CNNs, where each convolutional module consists of a convolutional layer followed by leaky ReLU activations and 2 × 2 max-pooling. On MNIST, we use a CNN with two modules, 8 × (5 × 5 × 1)-filters followed by 4 × (5 × 5 × 1)-filters, and a final dense layer of 32 units. On CIFAR-10, we use a CNN with three modules, 32 × (5 × 5 × 3)-filters, 64×(5×5×3)-filters, and 128×(5×5×3)-filters, followed by a final dense layer of 128 units. We use a batch size of 200 and set the weight decay hyperparameter to λ = 10 −6 .</p><p>Results Results are presented in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Adversarial attacks on GTSRB stop signs</head><p>Setup Detecting adversarial attacks is vital in many applications such autonomous driving. In this experiment, we test how Deep SVDD compares to its competitors on detecting adversarial examples. We consider the "stop sign" class of the German Traffic Sign Recognition Benchmark (GTSRB) dataset, for which we generate adversarial examples from randomly drawn stop sign images of the test set using Boundary Attack. We train the models again only on normal stop sign samples and in testing check if adversarial examples are correctly detected. The training set contains n = 780 stop signs. The test set is composed of 270 normal examples and 20 adversarial examples. We pre-process the data by removing the 10% border around each sign, and then resize every image to 32 × 32 pixels. After that, we again apply global contrast normalization using the L 1 -norm and rescale to the unit interval [0, 1]. Network architecture We use a CNN with LeNet architecture having three convolutional modules, 16×(5×5×3)-filters, 32 × (5 × 5 × 3)-filters, and 64 × (5 × 5 × 3)-filters, followed by a final dense layer of 32 units. We train with a smaller batch size of 64, due to the dataset size and set again hyperparamter λ = 10 −6 . Results <ref type="table" target="#tab_2">Table 2</ref> shows the results. The One-Class Deep SVDD shows again the best performance. Generally, the deep methods perform better. The DCGAN of AnoGAN did not converge due to the data set size which is too small for GANs. <ref type="figure" target="#fig_5">Figure 4</ref> shows the most anomalous samples detected by One-Class Deep SVDD which are either adversarial attacks or images in odd perspectives that are cropped incorrectly. We refer to the supplementary material for more examples of the most normal images and anomalies detected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduced the first fully deep one-class classification objective for unsupervised AD in this work. Our method, Deep SVDD, jointly trains a deep neural network while optimizing a data-enclosing hypersphere in output space. Through this Deep SVDD extracts common factors of variation from the data. We have demonstrated theoretical properties of our method such as the ν-property that allows to incorporate a prior assumption on the number of outliers being present in the data. Our experiments demonstrate quantitatively as well as qualitatively the sound performance of Deep SVDD.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1</head><label>1</label><figDesc>Figure 1. Deep SVDD learns a neural network transformation φ(· ; W) with weights W from input space X ⊆ R d to output space F ⊆ R p that attempts to map most of the data network representations into a hypersphere characterized by center c and radius R of minimum volume. Mappings of normal examples fall within, whereas mappings of anomalies fall outside the hypersphere.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Most normal (left) and most anomalous (right) in-class examples determined by One-Class Deep SVDD for selected MNIST (top) and CIFAR-10 (bottom) one-class experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Most normal (left) and most anomalous (right) in-class examples determined by KDE for CIFAR-10 one-class experiments in which KDE performs best.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Most anomalous stop signs detected by One-Class Deep SVDD. Adversarial examples are highlighted in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Average AUCs in % with StdDevs (over 10 seeds) per method and one-class experiment on MNIST and CIFAR-10.</figDesc><table>NORMAL 
OC-SVM/ 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Deep SVDD clearly outperforms both its shallow and deep competitors on MNIST. On CIFAR-10 the picture is mixed. Deep SVDD, however, shows an overall strong performance. It is interest- ing to note that shallow SVDD and KDE perform better than deep methods on three of the ten CIFAR-10 classes. Figures 2 and 3 show examples of the most normal and most anoma- lous in-class samples according to Deep SVDD and KDE respectively. We can see that normal examples of the classes on which KDE performs best seem to have strong global structures. For example, TRUCK images are mostly divided horizontally into street and sky, and DEER as well as FROG have similar colors globally. For these classes, choosing lo- cal CNN features can be questioned. These cases underline the importance of network architecture choice. Notably, the One-Class Deep SVDD performs slightly better than its soft- boundary counterpart on both datasets. This may be because the assumption of no anomalies being present in the training data is valid in our scenario. Due to SGD optimization, deep methods show higher standard deviations.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Average AUCs in % with StdDevs (over 10 seeds) per method on GTSRB stop signs with adversarial attacks.SOFT-BOUND. DEEP SVDD 77.8 ± 4.9 ONE-CLASS DEEP SVDD 80.3 ± 2.8</figDesc><table>METHOD 
AUC 

OC-SVM/SVDD 
67.5 ± 1.2 
KDE 
60.5 ± 1.7 
IF 
73.8 ± 0.9 
DCAE 
79.1 ± 3.0 
ANOGAN 
− 
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Proposition 2 also explains why autoencoders with bias terms are vulnerable to converge to a constant mapping onto the mean, which is the optimal constant solution of the mean squared error.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We provide our code at https://github.com/lukasruff/Deep-SVDD.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We kindly thank the reviewers for their constructive feedback which helped to improve this work. LR acknowledges financial support from the German Federal Ministry of Transport and Digital Infrastructure (BMVI) in the project OSIMAB (FKZ: 19F2017E). AB is grateful for support by the SUTD startup grant and the STElectronics-SUTD Cybersecurity Laboratory. MK and RV acknowledge support from the German Research Foundation (DFG) award KL 2698/2-1 and from the Federal Ministry of Science and Education (BMBF) award 031B0187B.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Outlier Analysis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Variational Autoencoder based Anomaly Detection using Reconstruction Probability. SNU Data Mining Center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T A</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Griffin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Detecting Anomalous Data Using Auto-Encoders. IJMLC</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Theory of reproducing kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aronszajn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American mathematical society</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="404" />
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Representation Learning: A Review and New Perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Identifying Density-Based Local Outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIG-MOD Record</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Anomaly Detection: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="58" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Outlier Detection with Autoencoder Ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="90" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Processing (Almost) from Scratch. JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Anomaly detection meta-analysis benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Emmott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajasegarar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karunasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leckie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="121" to="134" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Anomaly-based network intrusion detection: Techniques, systems and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garcia-Teodoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diaz-Verdejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maciá-Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vázquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Security</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="18" to="28" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Generative Adversarial Nets. In NIPS</title>
		<imprint>
			<biblScope unit="page" from="2672" to="2680" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Outlier Detection Using Replicator Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DaWaK</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2454</biblScope>
			<biblScope unit="page" from="170" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reducing the Dimensionality of Data with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for Acoustic Modeling in Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A Method for Stochastic Optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1090" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluating Real-time Anomaly Detection Algorithms -the Numenta Anomaly Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th ICMLA</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="38" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
		<respStmt>
			<orgName>AT&amp;T Labs</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Isolation Forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Autoencoders</forename><surname>K-Sparse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5663</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Winner-Take-All Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2791" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction. ICANN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unrolled Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">One-class classifier networks for target recognition applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Moya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Hostetler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings World Congress on Neural Networks</title>
		<meeting>World Congress on Neural Networks</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="797" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Feature selection for classification of hyperspectral data by SVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Foody</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2297" to="2307" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">On Estimation of a Probability Density Function and Mode. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parzen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1065" to="1076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A Comprehensive Survey of Data Mining-based Fraud Detection Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Phua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gayler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Clayton School of Information Technology, Monash University, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Sequential minimal optimization: A fast algorithm for training support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Safe Visual Navigation via Deep Learning and Novelty Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Fully Convolutional Neural Network for Fast Anomaly Detection in Crowded Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.00866</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Anomaly detection using autoencoders with nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yairi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd MLSDA Workshop</title>
		<meeting>the 2nd MLSDA Workshop</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sensor Fault and Patient Anomaly Detection and Classification in Medical Wireless Sensor Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Salem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guerassimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mehaoua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Furht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICC</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="4373" to="4378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPMI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep Learning in Neural Networks: An Overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Estimating the Support of a HighDimensional Distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1443" to="1471" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Identifying and Categorizing Anomalies in Retinal Imaging Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Klimscha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Gerendas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Donner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00686</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The German Traffic Sign Recognition Benchmark: A multiclass classification competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1453" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Support Vector Data Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M J</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On Nonparametric Estimation of Density Level Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Tsybakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="948" to="969" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Generalized RBF feature maps for Efficient Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vempati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jawahar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Consistency and Convergence Rates of One-Class SVMs and Related Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="817" to="854" />
			<date type="published" when="2006-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Extracting and Composing Robust Features with Denoising Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning Deep Representations of Appearance and Motion for Anomalous Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno>8.1-8.12</idno>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
