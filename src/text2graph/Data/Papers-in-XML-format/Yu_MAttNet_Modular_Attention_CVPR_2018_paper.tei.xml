<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MAttNet: Modular Attention Network for Referring Expression Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
							<email>licheng@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
							<email>zlin@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
							<email>xshen@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
							<email>jimyang@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
							<email>xinl@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<email>mbansal@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
							<email>tlberg@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MAttNet: Modular Attention Network for Referring Expression Comprehension</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Referring expressions are natural language utterances that indicate particular objects within a scene, e.g., "the woman in the red sweater" or "the man on the right". For robots or other intelligent agents communicating with people in the world, the ability to accurately comprehend such expressions in real-world scenarios will be a necessary component for natural interactions.</p><p>Referring expression comprehension is typically formulated as selecting the best region from a set of proposals/objects O = {o i } N i=1 in image I, given an input expression r. Most recent work on referring expressions uses CNN-LSTM based frameworks to model P (r|o) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b16">17]</ref> or uses a joint vision-language embedding . Given an expression, we attentionally parse it into three phrase embeddings, which are input to three visual modules that process the described visual region in different ways and compute individual matching scores. An overall score is then computed as a weighted combination of the module scores.</p><p>framework to model P (r, o) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. During testing, the proposal/object with highest likelihood/probability is selected as the predicted region. However, most of these work uses a simple concatenation of all features (target object feature, location feature and context feature) as input and a single LSTM to encode/decode the whole expression, ignoring the variance among different types of referring expressions. Depending on what is distinctive about a target object, different kinds of information might be mentioned in its referring expression. For example, if the target object is a red ball among 10 black balls then the referring expression may simply say "the red ball". If that same red ball is placed among 3 other red balls then location-based information may become more important, e.g., "red ball on the right". Or, if there were 100 red balls in the scene then the ball's relationship to other objects might be the most distinguishing information, e.g., "red ball next to the cat". Therefore, it is natural and intuitive to think about the comprehension model as a modular network, where different visual processing modules are triggered based on what information is present in the referring expression. Modular networks have been successfully applied to address other tasks such as (visual) question answering <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, visual reasoning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref>, relationship modeling <ref type="bibr" target="#b8">[9]</ref>, and multi-task reinforcement learning <ref type="bibr" target="#b0">[1]</ref>. To the best of our knowledge, we present the first modular network for the general referring expression comprehension task. Moreover, these previous work typically relies on an off-the-shelf language parser <ref type="bibr" target="#b22">[23]</ref> to parse the query sentence/question into different components and dynamically assembles modules into a model addressing the task. However, the external parser could raise parsing errors and propagate them into model setup, adversely effecting performance.</p><p>Therefore, in this paper we propose a modular network for referring expression comprehension -Modular Attention Network (MAttNet) -that takes a natural language expression as input and softly decomposes it into three phrase embeddings. These embeddings are used to trigger three separate visual modules (for subject, location, and relationship comprehension, each with a different attention model) to compute matching scores, which are finally combined into an overall region score based on the module weights. Our model is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. There are 3 main novelties in MAttNet.</p><p>First, MAttNet is designed for general referring expressions. It consists of 3 modules: subject, location and relationship. As in <ref type="bibr" target="#b11">[12]</ref>, a referring expression could be parsed into 7 attributes: category name, color, size, absolute location, relative location, relative object and generic attribute. MAttNet covers all of them. The subject module handles the category name, color and other attributes, the location module handles both absolute and (some) relative location, and the relationship module handles subject-object relations. Each module has a different structure and learns the parameters within its own modular space, without affecting the others.</p><p>Second, MAttNet learns to parse expressions automatically through a soft attention based mechanism, instead of relying on an external language parser <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b11">12]</ref>. We show that our learned "parser" attends to the relevant words for each module and outperforms an off-the-shelf parser by a large margin. Additionally, our model computes module weights which are adaptive to the input expression, measuring how much each module should contribute to the overall score. Expressions like "red cat" will have larger subject module weights and smaller location and relationship module weights, while expressions like "woman on left" will have larger subject and location module weights.</p><p>Third, we apply different visual attention techniques in the subject and relationship modules to allow relevant attention on the described image portions. In the subject module, soft attention attends to the parts of the object itself mentioned by an expression like "man in red shirt" or "man with yellow hat". We call this "in-box" attention. In contrast, in the relationship module, hard attention is used to attend to the relational objects mentioned by expressions like "cat on chair" or "girl holding frisbee". Here the attention focuses on "chair" and "frisbee" to pinpoint the target object "cat" and "girl". We call this "out-of-box" attention. We demonstrate both attentions play important roles in improving comprehension accuracy. During training, the only supervision is object proposal, referring expression pairs, (o i , r i ), and all of the above are automatically learned in an end-to-end unsupervised manner, including the word attention, module weights, soft spatial attention, and hard relative object attention. We demonstrate MAttNet has significantly superior comprehension performance over all state-of-art methods, achieving ∼10% improvements on bounding-box localization and almost doubling precision on pixel segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Referring Expression Comprehension: The task of referring expression comprehension is to localize a region described by a given referring expression. To address this problem, some recent work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17]</ref> uses CNN-LSTM structure to model P (r|o) and looks for the object o maximizing the probability. Other recent work uses joint embedding model <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b3">4]</ref> to compute P (o|r) directly. In a hybrid of both types of approaches, <ref type="bibr" target="#b31">[32]</ref> proposed a joint speaker-listener-reinforcer model that combined CNN-LSTM (speaker) with embedding model (listener) to achieve state-of-the-art results.</p><p>Most of the above treat comprehension as bounding box localization, but object segmentation from referring expression has also been studied in some recent work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>. These papers use FCN-style <ref type="bibr" target="#b15">[16]</ref> approaches to perform expression-driven foreground/background classification. We demonstrate that in addition to bounding box prediction, we also outperform previous segmentation results. Modular Networks: Neural module networks <ref type="bibr" target="#b2">[3]</ref> were introduced for visual question answering. These networks decompose the question into several components and dynamically assemble a network to compute an answer to the given question. Since their introduction, modular networks have been applied to several other tasks: visual reasoning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref>, question answering <ref type="bibr" target="#b1">[2]</ref>, relationship modeling <ref type="bibr" target="#b8">[9]</ref>, multitask reinforcement learning <ref type="bibr" target="#b0">[1]</ref>, etc. While the early work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b1">2]</ref> requires an external language parser to do the decomposition, recent methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7]</ref> propose to learn the decomposition end-to-end. We apply this idea to referring expression comprehension, also taking an end-to-end approach bypassing the use of an external parser. We find that our soft attention approach achieves better performance over the hard decisions predicted by a parser.</p><p>The most related work to us is <ref type="bibr" target="#b8">[9]</ref>, which decomposes the expression into (Subject, Preposition/Verb, Object) triples. However, referring expressions have much richer forms than this fixed template. For example, expressions like "left dog" and "man in red" are hard to model using <ref type="bibr" target="#b8">[9]</ref>. In this paper, we propose a generic modular network addressing all kinds of referring expressions. Our network is adaptive to the input expression by assigning both word-level attention and module-level weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head><p>MAttNet is composed of a language attention network plus visual subject, location, and relationship modules. Given a candidate object o i and referring expression r, we first use the language attention network to compute a soft parse of the referring expression into three components (one for each visual module) and map each to a phrase embedding. Second, we use the three visual modules (with unique attention mechanisms) to compute matching scores for o i to their respective embeddings. Finally, we take a weighted combination of these scores to get an overall matching score, measuring the compatibility between o i and r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Language Attention Network</head><p>Instead of using an external language parser <ref type="bibr" target="#b22">[23]</ref>[3] <ref type="bibr" target="#b1">[2]</ref> or pre-defined templates <ref type="bibr" target="#b11">[12]</ref> to parse the expression, we propose to learn to attend to the relevant words automatically for each module, similar to <ref type="bibr" target="#b8">[9]</ref>. Our language attention network is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. For a given expression r = {u t } T t=1 , we use a bi-directional LSTM to encode the context for each word. We first embed each word u t into a vector e t using an one-hot word embedding, then a bidirectional LSTM-RNN is applied to encode the whole expression. The final hidden representation for each word is the concatenation of the hidden vectors in both directions:</p><formula xml:id="formula_0">e t = embedding(u t ) h t = LSTM(e t , h t−1 ) h t = LSTM(e t , h t+1 ) h t = [ h t , h t ]. Given H = {h t } T t=1</formula><p>, we apply three trainable vectors f m where m ∈ {subj, loc, rel}, computing the attention on each word <ref type="bibr" target="#b27">[28]</ref> for each module:</p><formula xml:id="formula_1">a m,t = exp (f T m ht) T k=1 exp (f T m h k )</formula><p>. The weighted sum of word embeddings is used as the modular phrase embedding: q m = T t=1 a m,t e t . Different from relationship detection <ref type="bibr" target="#b8">[9]</ref> where phrases are always decomposed as (Subject, Preposition/Verb, Object) triplets, referring expressions have no such well-posed structure. For example, expressions like "smiling boy" only contain language relevant to the subject module, while expressions like "man on left" are relevant to the subject and location modules, and "cat on the chair" are relevant to the subject and relationship modules. To handle this variance, we compute 3 module weights for the expression, weighting how much each module contributes to the expressionobject score. We concatenate the first and last hidden vectors from H which memorizes both structure and semantics of the whole expression, then use another fully-connected (FC) layer to transform it into 3 module weights:</p><formula xml:id="formula_2">[w subj , w loc , w rel ] = softmax(W T m [h 0 , h T ] + b m )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Visual Modules</head><p>While most previous work <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> evaluates CNN features for each region proposal/candidate object, we use Faster R-CNN <ref type="bibr" target="#b19">[20]</ref> as the backbone net for a faster and more principled implementation. Additionally, we use ResNet <ref type="bibr" target="#b5">[6]</ref> as our main feature extractor, but also provide comparisons to previous methods using the same VGGNet features <ref type="bibr" target="#b21">[22]</ref> (in Sec. <ref type="bibr" target="#b3">4</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.2).</head><p>Given an image and a set of candidates o i , we run Faster R-CNN to extract their region representations. Specifically, we forward the whole image into Faster R-CNN and crop the C3 feature (last convolutional output of 3rd-stage) for each o i , following which we further compute the C4 feature (last convolutional output of 4th-stage). In Faster R-CNN, C4 typically contains higher-level visual cues for category prediction, while C3 contains relatively lower-level cues including colors and shapes for proposal judgment, making both useful for our purposes. In the end, we compute the matching score for each o i given each modular phrase embedding, i.e., S(o i |q subj ), S(o i |q loc ) and S(o i |q rel ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Subject Module</head><p>Our subject module is illustrated in <ref type="figure">Fig. 3</ref>. Given the C3 and C4 features of a candidate o i , we forward them to two tasks. The first is attribute prediction, helping produce a representation that can understand appearance characteristics of the candidate. The second is the phrase-guided at-  <ref type="figure">Figure 3</ref>: The subject module is composed of a visual subject representation and phrase-guided embedding. An attribute prediction branch is added after the ResNet-C4 stage and the 1x1 convolution output of attribute prediction and C4 is used as the subject visual representation. The subject phrase embedding attentively pools over the spatial region and feeds the pooled feature into the matching function.</p><p>tentional pooling to focus on relevant regions within object bounding boxes. Attribute Prediction: Attributes are frequently used in referring expressions to differentiate between objects of the same category, e.g. "woman in red" or "the fuzzy cat". Inspired by previous work <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24]</ref>, we add an attribute prediction branch in our subject module. While preparing the attribute labels in the training set, we first run a template parser <ref type="bibr" target="#b11">[12]</ref> to obtain color and generic attribute words, with low-frequency words removed. We combine both C3 and C4 for predicting attributes as both low and high-level visual cues are important. The concatenation of C3 and C4 is followed with a 1 × 1 convolution to produce an attribute feature blob. After average pooling, we get the attribute representation of the candidate region. A binary cross-entropy loss is used for multi-attribute classification:</p><formula xml:id="formula_3">L attr subj = λ attr i j w attr j [log(p ij )+(1−y ij )log(1−p ij )]</formula><p>where w attr j = 1/ freq attr weights the attribute labels, easing unbalanced data issues. During training, only expressions with attribute words go through this branch.</p><p>Phrase-guided Attentional Pooling: The subject description varies depending on what information is most salient about the object. Take people for example. Sometimes a person is described by their accessories, e.g., "girl in glasses"; or sometimes particular clothing items may be mentioned, e.g., "woman in white pants". Thus, we allow our subject module to localize relevant regions within a bounding box through "in-box" attention. To compute spatial attention, we first concatenate the attribute blob and C4, then use a 1×1 convolution to fuse them into a subject blob, which consists of spatial grid of features V ∈ R d×G , where G = 14 × 14. Given the subject phrase embedding q subj , we compute its attention on each grid location:</p><formula xml:id="formula_4">H a = tanh(W v V + W q q subj ) a v = softmax(w T h,a H a )</formula><p>. The weighted sum of V is the final subject visual representation for the candidate region o i :</p><formula xml:id="formula_5">v subj i = G i=1 a v i v i .</formula><p>Matching Function: We measure the similarity between the subject representation v subj i and phrase embedding q subj using a matching function, i.e, S(o i |q subj ) = F ( v subj i , q subj ). As shown in top-right of <ref type="figure">Fig. 3</ref>, it consists of two MLPs (multi-layer perceptions) and two L2 normalization layers following each input. The MLPs transform the visual and phrase representation into a common embedding space. The inner-product of two l2-normalized representations computes their similarity score. The same matching function is used to compute the location score S(o i |q loc ), and relationship score S(o i |q rel ). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Location Module</head><formula xml:id="formula_6">Matching ! " # , % " &amp; , ! ' # , % ' &amp;( , )ℎ #&amp;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4: Location Module</head><p>Our location module is shown in <ref type="figure">Fig. 4</ref>  While the subject module deals with "in-box" details about the target object, some other expressions may involve its relationship with other "out-of-box" objects, e.g., "cat on chaise lounge". The relationship module is used to address these cases. As in <ref type="figure" target="#fig_2">Fig. 5</ref>, given a candidate object o i we first look for its surrounding (up-to-five) objects o ij regardless of their categories. We use the average-pooled C4 feature as the appearance feature v ij of each supporting object. Then, we encode their offsets to the candidate object via δm ij = [ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Relationship Module</head><formula xml:id="formula_7">v rel ij = W r [v ij ; δm ij ] + b r</formula><p>We compute the matching score for each of them with q rel and pick the highest one as the relationship score, i.e., S(o i |q rel ) = max j =i F ( v rel ij , q rel ). This can be regarded as weakly-supervised Multiple Instance Learning (MIL) which is similar to <ref type="bibr" target="#b8">[9]</ref>[19].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Function</head><p>The overall weighted matching score for candidate object o i and expression r is:</p><formula xml:id="formula_8">S(o i |r) = w subj S(o i |q subj ) + w loc S(o i |q loc ) + w rel S(o i |q rel ) (1)</formula><p>During training, for each given positive pair of (o i , r i ), we randomly sample two negative pairs (o i , r j ) and (o k , r i ), where r j is the expression describing some other object and o k is some other object in the same image, to calculate a combined hinge loss,</p><formula xml:id="formula_9">L rank = i [λ1max(0, ∆ + S(oi|rj) − S(oi|ri)) +λ2max(0, ∆ + S(o k |ri) − S(oi|ri))]</formula><p>The overall loss incorporates both attributes cross-entropy loss and ranking loss: L = L attr subj + L rank .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments 4.1. Datasets</head><p>We use 3 referring expression datasets: RefCOCO, RefCOCO+ <ref type="bibr" target="#b11">[12]</ref>, and RefCOCOg <ref type="bibr" target="#b17">[18]</ref> for evaluation, all collected on MS COCO images <ref type="bibr" target="#b12">[13]</ref>, but with several differences. 1) RefCOCO and RefCOCO+ were collected in an interactive game interface, while RefCOCOg was collected in a non-interactive setting thereby producing longer expressions, 3.5 and 8.4 words on average respectively. 2) RefCOCO and RefCOCO+ contain more same-type objects, 3.9 vs 1.63 respectively. 3) RefCOCO+ forbids using absolute location words, making the data more focused on appearance differentiators.</p><p>During testing, RefCOCO and RefCOCO+ provide person vs. object splits for evaluation, where images containing multiple people are in "testA" and those containing multiple objects of other categories are in "testB". There is no overlap between training, validation and testing images. RefCOCOg has two types of data partitions. The first <ref type="bibr" target="#b17">[18]</ref> divides the dataset by randomly partitioning objects into training and validation splits. As the testing split has not been released, most recent work evaluates performance on the validation set. We denote this validation split as RefCOCOg's "val*". Note, since this data is split by objects the same image could appear in both training and validation. The second partition <ref type="bibr" target="#b18">[19]</ref> is composed by randomly partitioning images into training, validation and testing splits. We denote its validation and testing splits as RefCOCOg's "val" and "test", and run most experiments on this split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results: Referring Expression Comprehension</head><p>Given a test image, I, with a set of proposals/objects,</p><formula xml:id="formula_10">O = {o i } N i=1</formula><p>, we use Eqn. 1 to compute the matching score S(o i |r) for each proposal/object given the input expression r, and pick the one with the highest score. For evaluation, we compute the intersection-over-union (IoU) of the selected region with the ground-truth bounding box, considering IoU &gt; 0.5 a correct comprehension.</p><p>First, we compare our model with previous methods using COCO's ground-truth object bounding boxes as proposals. Results are shown in  <ref type="table">Table 2</ref>: Ablation study of MAttNet using different combination of modules. The feature used here is res101-frcn.</p><p>methods (Line 1-8) used a 16-layer VGGNet (vgg16) as the feature extractor, we run our experiments using the same feature for fair comparison. Note the flat fc7 is a single 4096-dimensional feature which prevents us from using the phrase-guided attentional pooling in <ref type="figure">Fig. 3</ref>, so we use average pooling for subject matching. Despite this, our results (Line 9) still outperform all previous state-of-the-art methods. After switching to the res101-based Faster R-CNN (res101-frcn) representation, the comprehension accuracy further improves another ∼3% (Line 10). Note our Faster R-CNN is pre-trained on COCO's training images, excluding those in RefCOCO, RefCOCO+, and RefCOCOg's validation+testing. Thus no training images are seen during our evaluation 3 . Our full model (Line 11) with phrase-guided attentional pooling achieves the highest accuracy over all others by a large margin.</p><p>Second, we study the benefits of each module of MAttNet by running ablation experiments <ref type="table">(Table.</ref> 2) with the same res101-frcn features. As a baseline, we use the concatenation of the regional visual feature and the location feature as the visual representation and the last hidden output of LSTM-encoded expression as the language representation, then feed them into the matching function to obtain the similarity score (Line 1). Compared with this, a simple two-module MAttNet using the same features (Line 2) already outperforms the baseline, showing the advantage of modular learning. Line 3 shows the benefit of encoding location (Sec. 3.2.2). After adding the relationship module, the performance further improves (Line 4). Lines 5 and Line 6 show the benefits brought by the attribute subbranch and the phrase-guided attentional pooling in our subject module. We find the attentional pooling (Line 6) greatly improves on the person category (testA of RefCOCO and RefCOCO+), demonstrating the advantage of modular attention on understanding localized details like "girl with red hat".</p><p>Third, we tried training our model using 3 hard-coded phrases from a template language parser <ref type="bibr" target="#b11">[12]</ref>, shown in Line 7 of <ref type="table">Table.</ref> 2, which is ∼5% lower than our end-toend model (Line 6). The main reason for this drop is errors made by the external parser which is not tuned for referring expressions.</p><p>Fourth, we show results using automatically detected objects from Faster R-CNN, providing an analysis of fully automatic comprehension performance. <ref type="table">Table.</ref> 3 shows the ablation study of fully-automatic MAttNet. While performance drops due to detection errors, the overall improvements brought by each module are consistent with <ref type="table">Table.</ref> 2, showing the robustness of MAttNet. Our results also outperform the state-of-the-art <ref type="bibr" target="#b31">[32]</ref> (Line 1,2) with a big margin. Besides, we show the performance when using the detector branch of Mask R-CNN <ref type="bibr" target="#b4">[5]</ref> (res101-mrcn) in Line 9.</p><p>Finally, we show some example visualizations of comprehension using our full model in <ref type="figure">Fig. 6</ref>    alizations of the attention predictions. We observe that our language model is able to attend to the right words for each module even though it is learned in a weakly-supervised manner. We also observe the expressions in RefCOCO and RefCOCO+ describe the location or details of the target object more frequently while RefCOCOg mentions the relationship between target object and its surrounding object more frequently, which accords with the dataset property. Note that for some complex expressions like "woman in plaid jacket and blue pants on skis" which contains several relationships (last row in <ref type="figure">Fig. 6</ref>), our language model is able to attend to the portion that should be used by the "in-box" subject module and the portion that should be used by the "out-of-box" relationship module. Additionally our subject module also displays reasonable spatial "in-box" attention, which qualitatively explains why attentional pooling <ref type="table">(Table.</ref> 2 Line 6) outperforms average pooling <ref type="table">(Table.</ref> 2 Line 5). For comparison, some incorrect comprehension are shown in <ref type="figure" target="#fig_5">Fig. 7</ref>. Most errors are due to sparsity in the training data, ambiguous expressions, or detection error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Segmentation from Referring Expression</head><p>Our model can also be used to address referential object segmentation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>. Instead of using Faster R-CNN as the backbone net, we now turn to res101-based Mask R-CNN <ref type="bibr" target="#b4">[5]</ref> (res101-mrcn). We apply the same procedure described in Sec. 3 on the detected objects, and use the one with highest matching score as our prediction. Then we feed the predicted bounding box to the mask branch to obtain a pixel-wise segmentation. We evaluate the full model of MAttNet and compare with the best results reported in <ref type="bibr" target="#b13">[14]</ref>. We use Precision@X Expression="a man with a silver ring is holding a phone" Expression="woman in plaid jacket and blue pants on skis" Expression="bottom left bowl" Expression="suit guy under umbrella" <ref type="figure">Figure 6</ref>: Examples of fully automatic comprehension. The blue dotted boxes show our prediction with the relative regions in yellow dotted boxes, and the green boxes are the ground-truth. The word attention is multiplied by module weight.  (X ∈ {0.5, 0.6, 0.7, 0.8, 0.9}) <ref type="bibr" target="#b3">4</ref> and overall Intersectionover-Union (IoU) as metrics. Results are shown in <ref type="table" target="#tab_7">Table. 4</ref> with our model outperforming state-of-the-art results by a large margin under all metrics <ref type="bibr" target="#b4">5</ref> . As both <ref type="bibr" target="#b13">[14]</ref> and MAttNet use res101 features, such big gains may be due to our proposed model. We believe decoupling box localization (comprehension) and segmentation brings a large gain over FCN-style <ref type="bibr" target="#b15">[16]</ref> foreground/background mask classification <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref> for this instance-level segmentation prob-4 Precision@0.5 is the percentage of expressions where the IoU of the predicted segmentation and ground-truth is at least 0.5. <ref type="bibr" target="#b4">5</ref> There is no experiments on RefCOCOg's val/test splits in <ref type="bibr" target="#b13">[14]</ref>, so we show our performance only for reference in <ref type="table" target="#tab_7">Table 4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Our modular attention network addresses variance in referring expressions by attending to both relevant words and visual regions in a modular framework, and dynamically computing an overall matching score. We demonstrate our model's effectiveness on bounding-box-level and pixellevel comprehension, significantly outperforming state-ofthe-art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1Figure 1 :</head><label>1</label><figDesc>Figure 1: Modular Attention Network (MAttNet). Given an expression, we attentionally parse it into three phrase embeddings, which are input to three visual modules that process the described visual region in different ways and compute individual matching scores. An overall score is then computed as a weighted combination of the module scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Language Attention Network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Relationship Module</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Examples of incorrect comprehensions. Red dotted boxes show our wrong prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Examples of fully-automatic MAttNet referential segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>. Location is fre- quently used in referring expressions with about 41% ex-pressions from RefCOCO and 36% expressions from Ref- COCOg containing absolute location words [12], e.g. "cat on the right" indicating the object location in the image. Following previous work [31][32], we use a 5-d vector l i to encode the top-left position, bottom-right position and relative area to the image for the candidate object, i.e., l i = [Additionally, expressions like "dog in the middle" and "second left person" imply relative positioning among objects of the same category. We encode the relative location representation of a candidate object by choos- ing up to five surrounding objects of the same cate- gory and calculating their offsets and area ratio, i.e., δl ij = [= W l [l i ; δl i ] + b l and the location module matching score be- tween o i and q</figDesc><table>x tl 

W , 

y tl 

H , 

x br 

W , 

y br 

H , 

w·h 

W ·H ]. 
[△x tl ]ij 
wi 

, 

[△y tl ]ij 
hi 

, 

[△x br ]ij 
wi 

, 

[△y br ]ij 
hi 

, 

wj hj 

wihi ]. The fi-
nal location representation for the target object is l 

loc 
i 

loc is S(o i |q 
loc ) = F ( l 

loc 

i , q 
loc ). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>Table. 1. As all of the previous</figDesc><table>RefCOCO 

RefCOCO+ 
RefCOCOg 
feature 
val 
testA 
testB 
val 
testA 
testB 
val* 
val 
test 
1 
Mao [18] 
vgg16 
-
63.15 64.21 
-
48.73 42.13 
62.14 
-
-
2 
Varun [19] 
vgg16 
76.90 75.60 78.00 
-
-
-
-
-
68.40 
3 
Luo [17] 
vgg16 
-
74.04 73.43 
-
60.26 55.03 
65.36 
-
-
4 
CMN [9] 
vgg16-frcn 
-
-
-
-
-
-
69.30 
-
-
5 
Speaker/visdif [31] 
vgg16 
76.18 74.39 77.30 
58.94 61.29 56.24 
59.40 
-
-
6 
Listener [32] 
vgg16 
77.48 76.58 78.94 
60.50 61.39 58.11 
71.12 
69.93 
69.03 
7 
Speaker+Listener+Reinforcer [32] 
vgg16 
79.56 78.95 80.22 
62.26 64.60 59.62 
72.63 
71.65 
71.92 
8 
Speaker+Listener+Reinforcer [32] 
vgg16 
78.36 77.97 79.86 
61.33 63.10 58.19 
72.02 
71.32 
71.72 
9 
MAttN:subj(+attr)+loc(+dif)+rel 
vgg16 
80.94 79.99 82.30 
63.07 65.04 61.77 
73.08 
73.04 
72.79 
10 MAttN:subj(+attr)+loc(+dif)+rel 
res101-frcn 
83.54 82.66 84.17 
68.34 69.93 65.90 
-
76.63 
75.92 
11 MAttN:subj(+attr+attn)+loc(+dif)+rel res101-frcn 
85.65 85.26 84.57 
71.01 75.13 66.17 
-
78.10 
78.12 

Table 1: Comparison with state-of-the-art approaches on ground-truth MS COCO regions. 

RefCOCO 
RefCOCO+ 
RefCOCOg 
val 
testA 
testB 
val 
testA 
testB 
val 
test 
1 Matching:subj+loc 
79.14 
79.42 
80.42 
62.17 
63.53 
59.87 
70.45 
70.92 
2 MAttN:subj+loc 
79.68 
80.20 
81.49 
62.71 
64.20 
60.65 
72.12 
72.62 
3 MAttN:subj+loc(+dif) 
82.06 
81.28 
83.20 
64.84 
65.77 
64.55 
75.33 
74.46 
4 MAttN:subj+loc(+dif)+rel 
82.54 
81.58 
83.34 
65.84 
66.59 
65.08 
75.96 
74.56 
5 MAttN:subj(+attr)+loc(+dif)+rel 
83.54 
82.66 
84.17 
68.34 
69.93 
65.90 
76.63 
75.92 
6 MAttN:subj(+attr+attn)+loc(+dif)+rel 
85.65 
85.26 
84.57 
71.01 
75.13 
66.17 
78.10 
78.12 
7 parser+MAttN:subj(+attr+attn)+loc(+dif)+rel 80.20 
79.10 
81.22 
66.08 
68.30 
62.94 
73.82 
73.72 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>as well as visu-</figDesc><table>RefCOCO 

RefCOCO+ 
RefCOCOg 
detector 
val 
testA 
testB 
val 
testA 
testB 
val 
test 
1 
Speaker+Listener+Reinforcer [32] 
res101-frcn 
69.48 73.71 
64.96 
55.71 
60.74 48.80 
60.21 59.63 
2 
Speaker+Listener+Reinforcer [32] 
res101-frcn 
68.95 73.10 
64.85 
54.89 
60.04 49.56 
59.33 59.21 
3 
Matching:subj+loc 
res101-frcn 
72.28 75.43 
67.87 
58.42 
61.46 52.73 
64.15 63.25 
4 
MAttN:subj+loc 
res101-frcn 
72.72 76.17 
68.18 
58.70 
61.65 53.41 
64.40 63.74 
5 
MAttN:subj+loc(+dif) 
res101-frcn 
72.96 76.61 
68.20 
58.91 
63.06 55.19 
64.66 63.88 
6 
MAttN:subj+loc(+dif)+rel 
res101-frcn 
73.25 76.77 
68.44 
59.45 
63.31 55.68 
64.87 64.01 
7 
MAttN:subj(+attr)+loc(+dif)+rel 
res101-frcn 
74.51 77.81 
68.39 
62.13 
66.33 55.75 
65.33 65.19 
8 
MAttN:subj(+attr+attn)+loc(+dif)+rel 
res101-frcn 
76.40 80.43 
69.28 
64.93 
70.26 56.00 
66.67 67.01 

9 
MAttN:subj(+attr+attn)+loc(+dif)+rel res101-mrcn 
76.65 81.14 
69.99 
65.33 
71.62 56.02 
66.58 67.27 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of MAttNet on fully-automatic comprehension task using different combination of modules. The features used here are res101-frcn, except the last row using res101-mrcn.</figDesc><table>RefCOCO 
Model 
Backbone Net 
Split 
Pr@0.5 
Pr@0.6 
Pr@0.7 
Pr@0.8 
Pr@0.9 
IoU 
D+RMI+DCRF [14] res101-DeepLab 
val 
42.99 
33.24 
22.75 
12.11 
2.23 
45.18 
MAttNet 
res101-mrcn 
val 
75.16 
72.55 
67.83 
54.79 
16.81 
56.51 
D+RMI+DCRF [14] res101-DeepLab testA 
42.99 
33.59 
23.69 
12.94 
2.44 
45.69 
MAttNet 
res101-mrcn 
testA 
79.55 
77.60 
72.53 
59.01 
13.79 
62.37 
D+RMI+DCRF [14] res101-DeepLab 
testB 
44.99 
32.21 
22.69 
11.84 
2.65 
45.57 
MAttNet 
res101-mrcn 
testB 
68.87 
65.06 
60.02 
48.91 
21.37 
51.70 

RefCOCO+ 
Model 
Backbone Net 
Split 
Pr@0.5 
Pr@0.6 
Pr@0.7 
Pr@0.8 
Pr@0.9 
IoU 
D+RMI+DCRF [14] res101-DeepLab 
val 
20.52 
14.02 
8.46 
3.77 
0.62 
29.86 
MAttNet 
res101-mrcn 
val 
64.11 
61.87 
58.06 
47.42 
14.16 
46.67 
D+RMI+DCRF [14] res101-DeepLab testA 
21.22 
14.43 
8.99 
3.91 
0.49 
30.48 
MAttNet 
res101-mrcn 
testA 
70.12 
68.48 
63.97 
52.13 
12.28 
52.39 
D+RMI+DCRF [14] res101-DeepLab 
testB 
20.78 
14.56 
8.80 
4.58 
0.80 
29.50 
MAttNet 
res101-mrcn 
testB 
54.82 
51.73 
47.27 
38.58 
17.00 
40.08 

RefCOCOg 
Model 
Backbone Net Split Pr@0.5 Pr@0.6 Pr@0.7 Pr@0.8 Pr@0.9 
IoU 
MAttNet 
res101-mrcn 
val 
64.48 
61.52 
56.50 
43.97 
14.67 
47.64 
MAttNet 
res101-mrcn 
test 
65.60 
62.92 
57.31 
44.44 
12.55 
48.61 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Comparison of segmentation performance on RefCOCO, RefCOCO+, and our results on RefCOCOg.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Such constraint forbids us to evaluate on RefCOCOg's val* using the res101-frcn feature in Table 1.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This research is supported by NSF Awards #1405822, 1562098, 1633295, NVidia, Google Research, Microsoft Research, and Adobe Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Modular multitask reinforcement learning with policy sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NAACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Query-guided regression network with context policy for phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kovvuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segmentation from natural language expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling relationship in referential expressions with compositional modular networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbacnh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<title level="m">Natural language object retrieval. CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent multimodal interaction for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Referring expression generation and comprehension via attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Comprehension-guided referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling context between objects for referring expression understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Grounding of textual phrases in images by reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Reasoning about fine-grained attribute phrases using reference games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning deep structurepreserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning two-branch neural networks for image-text matching tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03470</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image captioning and visual question answering based on attributes and external knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01646</idno>
		<title level="m">Boosting image captioning with attributes</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A joint speakerlistener-reinforcer model for referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
