<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stabilizing Training of Generative Adversarial Networks through Regularization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Roth</surname></persName>
							<email>kevin.roth@inf.ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
							<email>aurelien.lucchi@inf.ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
							<email>sebastian.nowozin@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
							<email>thomas.hofmann@inf.ethz.ch</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">ETH Zürich</orgName>
								<orgName type="institution" key="instit2">ETH Zürich</orgName>
								<orgName type="institution" key="instit3">Microsoft Research Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">ETH Zürich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Stabilizing Training of Generative Adversarial Networks through Regularization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Deep generative models based on Generative Adversarial Networks (GANs) have demonstrated impressive sample quality but in order to work they require a careful choice of architecture, parameter initialization, and selection of hyper-parameters. This fragility is in part due to a dimensional mismatch or non-overlapping support between the model distribution and the data distribution, causing their density ratio and the associated f -divergence to be undefined. We overcome this fundamental limitation and propose a new regularization approach with low computational cost that yields a stable GAN training procedure. We demonstrate the effectiveness of this regularizer accross several architectures trained on common benchmark image generation tasks. Our regularization turns GAN models into reliable building blocks for deep learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b0">1</ref> </div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A recent trend in the world of generative models is the use of deep neural networks as data generating mechanisms. Two notable approaches in this area are variational auto-encoders (VAEs) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28]</ref> as well as generative adversarial networks (GAN) <ref type="bibr" target="#b7">[8]</ref>. GANs are especially appealing as they move away from the common likelihood maximization viewpoint and instead use an adversarial game approach for training generative models. Let us denote by P(x) and Q ✓ (x) the data and model distribution, respectively. The basic idea behind GANs is to pair up a ✓-parametrized generator network that produces Q ✓ with a discriminator which aims to distinguish between P and Q ✓ , whereas the generator aims for making Q ✓ indistinguishable from P. Effectively the discriminator represents a class of objective functions F that measures dissimilarity of pairs of probability distributions. The final objective is then formed via a supremum over F, leading to the saddle point problem F (P, Q ✓ ) .</p><p>The standard way of representing a specific F is through a family of statistics or discriminants 2 , typically realized by a neural network <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26]</ref>. In GANs, we use these discriminators in a logistic classification loss as follows</p><formula xml:id="formula_1">F (P, Q; ) = E P [g( (x))] + E Q [g( (x))] ,<label>(2)</label></formula><p>where g(z) = ln( (z)) is the log-logistic function (for reference, ( (x)) = D(x) in <ref type="bibr" target="#b7">[8]</ref>).</p><p>As shown in <ref type="bibr" target="#b7">[8]</ref>, for the Bayes-optimal discriminator ⇤ 2 , the above generator objective reduces to the Jensen-Shannon (JS) divergence between P and Q. The work of <ref type="bibr" target="#b24">[25]</ref> later generalized this to a more general class of f -divergences, which gives more flexibility in cases where the generative model may not be expressive enough or where data may be scarce.</p><p>We consider three different challenges for learning the model distribution:</p><p>(A) empirical estimation: the model family may contain the true distribution or a good approximation thereof, but one has to identify it based on a finite training sample drawn from P. This is commonly addressed by the use of regularization techniques to avoid overfitting, e.g. in the context of estimating f -divergences with M -estimators <ref type="bibr" target="#b23">[24]</ref>. In our work, we suggest a novel (Tikhonov) regularizer, derived and motivated from a training-with-noise scenario, where P and Q are convolved with white Gaussian noise <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b2">3]</ref>, namely</p><formula xml:id="formula_2">F (P, Q; ) := F (P ⇤ ⇤, Q ⇤ ⇤; ), ⇤ = N (0, I) .<label>(3)</label></formula><p>(B) density misspecification: the model distribution and true distribution both have a density function with respect to the same base measure but there exists no parameter for which these densities are sufficiently similar. Here, the principle of parameter estimation via divergence minimization is provably sound in that it achieves a well-defined limit <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref>. It therefore provides a solid foundation for statistical inference that is robust with regard to model misspecifications.</p><p>(C) dimensional misspecification: the model distribution and the true distribution do not have a density function with respect to the same base measure or -even worse -supp(P) \ supp(Q) may be negligible. This may occur, whenever the model and/or data are confined to low-dimensional manifolds <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref>. As pointed out in <ref type="bibr" target="#b2">[3]</ref>, a geometric mismatch can be detrimental for f -GAN models as the resulting f -divergence is not finite (the sup in Eq. (1) is +1). As a remedy, it has been suggested to use an alternative family of distance functions known as integral probability metrics <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31]</ref>. These include the Wasserstein distance used in Wasserstein GANs (WGAN) <ref type="bibr" target="#b2">[3]</ref> as well as RKHS-induced maximum mean discrepancies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b5">6]</ref>, which all remain well-defined. We will provide evidence (analytically and experimentally) that the noise-induced regularization method proposed in this paper effectively makes f -GAN models robust against dimensional misspecifications. While this introduces some dependency on the (Euclidean) metric of the ambient data space, it does so on a well-controlled length scale (the amplitude of noise or strength of the regularization ) and by retaining the benefits of f -divergences. This is a rather gentle modification compared to the more radical departure taken in Wasserstein GANs, which rely solely on the ambient space metric (through the notion of optimal mass transport).</p><p>In what follows, we will take Eq. (3) as the starting point and derive an approximation via a regularizer that is simple to implement as an integral operator penalizing the squared gradient norm. As opposed to a naïve norm penalization, each f -divergence has its own characteristic weighting function over the input space, which depends on the discriminator output. We demonstrate the effectiveness of our approach on a simple Gaussian mixture as well as on several benchmark image datasets commonly used for generative models. In both cases, our proposed regularization yields stable GAN training and produces samples of higher visual quality. We also perform pairwise tests of regularized vs. unregularized GANs using a novel cross-testing protocol.</p><p>In summary, we make the following contributions:</p><p>• We systematically derive a novel, efficiently computable regularization method for f -GAN.</p><p>• We show how this addresses the dimensional misspecification challenge.</p><p>• We empirically demonstrate stable GAN training across a broad set of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>The fundamental way to learn a generative model in machine learning is to (i) define a parametric family of probability densities {Q ✓ }, ✓ 2 ⇥ ✓ R d , and (ii) find parameters ✓ ⇤ 2 ⇥ such that Q ✓ is closest (in some sense) to the true distribution P. There are various ways to measure how close model and real distribution are, or equivalently, various ways to define a distance or divergence function between P and Q. In the following we review different notions of divergences used in the literature.</p><p>f -divergence. GANs <ref type="bibr" target="#b7">[8]</ref> are known to minimize the Jensen-Shannon divergence between P and Q. This was generalized in <ref type="bibr" target="#b24">[25]</ref> to f -divergences induced by a convex functions f . An interesting property of f -divergences is that they permit a variational characterization <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27]</ref> via</p><formula xml:id="formula_3">D f (P||Q) := E Q  f dP dQ = Z X sup u ✓ u · dP dQ f c (u) ◆ dQ,<label>(4)</label></formula><p>where dP/dQ is the Radon-Nikodym derivative and f c (t) ⌘ sup u2dom f {ut f (u)} is the Fenchel dual of f . By defining an arbitrary class of statistics 3 : X ! R we arrive at the bound</p><formula xml:id="formula_4">D f (P||Q) sup Z ✓ · dP dQ f c ◆ dQ = sup {E P [ ] E Q [f c ]} .<label>(5)</label></formula><p>Eq. (5) thus gives us a variational lower bound on the f -divergence as an expectation over P and Q, which is easier to evaluate (e.g. via sampling from P and Q, respectively) than the density based formulation. We can see that by identifying = g and with the choice of f such that</p><formula xml:id="formula_5">f c = ln(1 exp), we get f c = ln(1 ( )) = g( ) thus recovering Eq. (2).</formula><p>Integral Probability Metrics (IPM). An alternative family of divergences are integral probability metrics <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31]</ref>, which find a witness function to distinguish between P and Q. This class of methods yields an objective similar to Eq. (2) that requires optimizing a distance function between two distributions over a function class F. Particular choices for F yield the kernel maximum mean discrepancy approach of <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref> or Wasserstein GANs <ref type="bibr" target="#b2">[3]</ref>. The latter distance is defined as</p><formula xml:id="formula_6">W (P, Q) = sup kf k L 1 {E P [f ] E Q [f ]},<label>(6)</label></formula><p>where the supremum is taken over functions f which have a bounded Lipschitz constant.</p><p>As shown in <ref type="bibr" target="#b2">[3]</ref>, the Wasserstein metric implies a different notion of convergence compared to the JS divergence used in the original GAN. Essentially, the Wasserstein metric is said to be weak as it requires the use of a weaker topology, thus making it easier for a sequence of distribution to converge. The use of a weaker topology is achieved by restricting the function class to the set of bounded Lipschitz functions. This yields a hard constraint on the function class that is empirically hard to satisfy. In <ref type="bibr" target="#b2">[3]</ref>, this constraint is implemented via weight clipping, which is acknowledged to be a "terrible way" to enforce the Lipschitz constraint. As will be shown later, our regularization penalty can be seen as a soft constraint on the Lipschitz constant of the function class which is easy to implement in practice. Recently, <ref type="bibr" target="#b9">[10]</ref> has also proposed a similar regularization; while their proposal was motivated for Wasserstein GANs and does not extend to f -divergences it is interesting to observe that both their and our regularization work on the gradient.</p><p>Training with Noise. As suggested in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30]</ref>, one can break the dimensional misspecification discussed in Section 1 by adding continuous noise to the inputs of the discriminator, therefore smoothing the probability distribution. However, this requires to add high-dimensional noise, which introduces significant variance in the parameter estimation process. Counteracting this requires a lot of samples and therefore ultimately leads to a costly or impractical solution. Instead we propose an approach that relies on analytic convolution of the densities P and Q with Gaussian noise. As we demonstrate below, this yields a simple weighted penalty function on the norm of the gradients. Conceptually we think of this noise not as being part of the generative process (as in <ref type="bibr" target="#b2">[3]</ref>), but rather as a way to define a smoother family of discriminants for the variational bound of f -divergences.</p><p>Regularization for Mode Dropping. Other regularization techniques address the problem of mode dropping and are complementary to our approach. This includes the work of <ref type="bibr" target="#b6">[7]</ref> which incorporates a supervised training signal as a regularizer on top of the discriminator target. To implement supervision the authors use an additional auto-encoder as well as a two-step training procedure which might be computationally expensive. A similar approach was proposed by <ref type="bibr" target="#b19">[20]</ref> that stabilizes GANs by unrolling the optimization of the discriminator. The main drawback of this approach is that the computational cost scales with the number of unrolling steps. In general, it is not clear to what extent these methods not only stabilize GAN training, but also address the conceptual challenges listed in Section 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Noise-Induced Regularization</head><p>From now onwards, we consider the general f -GAN <ref type="bibr" target="#b24">[25]</ref> objective defined as</p><formula xml:id="formula_7">F (P, Q; ) ⌘ E P [ ] E Q [f c</formula><p>].</p><p>(7)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Noise Convolution</head><p>From a practitioners point of view, training with noise can be realized by adding zero-mean random variables ⇠ to samples x ⇠ P, Q during training. Here we focus on normal white noise ⇠ ⇠ ⇤ = N (0, I) (the same analysis goes through with a Laplacian noise distribution for instance). From a theoretical perspective, adding noise is tantamount to convolving the corresponding distribution as</p><formula xml:id="formula_8">E P E ⇤ [ (x + ⇠)] = Z (x) Z p(x ⇠) (⇠)d⇠ dx = Z (x)(p ⇤ )(x)dx = E P⇤⇤ [ ].<label>(8)</label></formula><p>where p and are probability densities of P and ⇤, respectively, with regard to the Lebesgue measure. The noise distribution ⇤ as well as the resulting P⇤⇤ are guaranteed to have full support in the ambient space, i.e. (x) &gt; 0 and (p ⇤ )(x) &gt; 0 (8x). Technically, applying this to both P and Q makes the resulting generalized f -divergence well-defined, even when the generative model is dimensionally misspecified. Note that approximating E ⇤ through sampling was previously investigated in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b2">3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolved Discriminants</head><p>With symmetric noise, (⇠) = ( ⇠), we can write Eq. <ref type="formula" target="#formula_8">(8)</ref> equivalently as</p><formula xml:id="formula_9">E P⇤⇤ [ ] = E P E ⇤ [ (x + ⇠)] = Z p(x) Z (x ⇠) ( ⇠) d⇠ dx = E P [ ⇤ ].<label>(9)</label></formula><p>For the Q-expectation in Eq. <ref type="formula">(7)</ref> one gets, by the same argument,</p><formula xml:id="formula_10">E Q⇤⇤ [f c ] = E Q [(f c ) ⇤ ].</formula><p>Formally, this generalizes the variational bound for f -divergences in the following manner:</p><formula xml:id="formula_11">F (P ⇤ ⇤, Q ⇤ ⇤; ) = F (P, Q; ⇤ , (f c ) ⇤ ), F(P, Q; ⇢, ⌧ ) := E P [⇢] E Q [⌧ ]<label>(10)</label></formula><p>Assuming that F is closed under ⇤ convolutions, the regularization will result in a relative weakening of the discriminator as we take the sup over a smaller, more regular family. Clearly, the low-pass effect of ⇤-convolutions can be well understood in the Fourier domain. In this equivalent formulation, we leave P and Q unchanged, yet we change the view the discriminator can take on the ambient data space: metaphorically speaking, the generator is paired up with a short-sighted adversary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analytic Approximations</head><p>In general, it may be difficult to analytically compute ⇤ or -equivalently -</p><formula xml:id="formula_12">E ⇤ [ (x + ⇠)].</formula><p>However, for small we can use a Taylor approximation of around ⇠ = 0 (cf. <ref type="bibr" target="#b4">[5]</ref>):</p><formula xml:id="formula_13">(x + ⇠) = (x) + [r (x)] T ⇠ + 1 2 ⇠ T [r 2 (x)] ⇠ + O(⇠ 3 )<label>(11)</label></formula><p>where r 2 denotes the Hessian, whose trace Tr(r 2 ) = 4 is known as the Laplace operator. The properties of white noise result in the approximation</p><formula xml:id="formula_14">E ⇤ [ (x + ⇠)] = (x) + 2 4 (x) + O( 2 )<label>(12)</label></formula><p>and thereby lead directly to an approximation of F (see Eq. (3)) via F = F 0 plus a correction, i.e.</p><formula xml:id="formula_15">F (P, Q; ) = F(P, Q; ) + 2 {E P [4 ] E Q [4(f c )]} + O( 2 ) .<label>(13)</label></formula><p>We can interpret Eq. <ref type="formula" target="#formula_0">(13)</ref>  The Laplace operator is a sum of d terms, where d is the dimensionality of the ambient data space. As such it does not suffer from the quadratic blow-up involved in computing the Hessian. If we realize the discriminator via a deep network, however, then we need to be able to compute the Laplacian of composed functions. For concreteness, let us assume that = h G, <ref type="figure">G = (g 1 , .</ref> . . , g k ) and look at a single input x, i.e. g i : R ! R, then</p><formula xml:id="formula_16">(h G) 0 = X i g 0 i · (@ i h G), (h G) 00 = X i g 00 i · (@ i h G) + X i,j g 0 i · g 0 j · (@ i @ j h G) (14)</formula><p>So at the intermediate layer, we would need to effectively operate with a full Hessian, which is computationally demanding, as has already been observed in <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Efficient Gradient-Based Regularization</head><p>We would like to derive a (more) tractable strategy for regularizing , which (i) avoids the detrimental variance that comes from sampling ⇠, (ii) does not rely on explicitly convolving the distributions P and Q, and (iii) avoids the computation of Laplacians as in Eq. <ref type="bibr" target="#b12">(13)</ref>. Clearly, this requires to make further simplifications. We suggest to exploit properties of the maximizer ⇤ of F that can be characterized by <ref type="bibr" target="#b23">[24]</ref> (f</p><formula xml:id="formula_17">c0 ⇤ ) dQ = dP =) E P [h] = E Q [(f c0 ⇤ ) · h] (8h, integrable).<label>(15)</label></formula><p>The relevance of this becomes clear, if we apply the chain rule to 4(f</p><formula xml:id="formula_18">c ), assuming that f c is twice differentiable 4(f c ) = (f c00 ) · ||r || 2 + f c0 4 ,<label>(16)</label></formula><p>as now we get a convenient cancellation of the Laplacians at =</p><formula xml:id="formula_19">⇤ + O( ) F (P, Q; ⇤ ) = F(P, Q; ⇤ ) 2 E Q h (f c00 ⇤ ) · kr ⇤ k 2 i + O( 2 ) .<label>(17)</label></formula><p>We can (heuristically) turn this into a regularizer by taking the leading terms,</p><formula xml:id="formula_20">F (P, Q; ) ⇡ F(P, Q; ) 2 ⌦ f (Q; ), ⌦ f (Q; ) := E Q h (f c00 ) · kr k 2 i .<label>(18)</label></formula><p>Note that we do not assume that the Laplacian terms cancel far away from the optimum, i.e. we do not assume Eq. <ref type="formula" target="#formula_0">(15)</ref> to hold for far away from ⇤ . Instead, the underlying assumption we make is that optimizing the gradient-norm regularized objective F (P, Q; ) makes converge to ⇤ + O( ), for which we know that the Laplacian terms cancel <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>The convexity of f c implies that the weighting function of the squared gradient norm is non-negative, i.e. f c00 0, which in turn implies that the regularizer 2 ⌦ f (Q; ) is upper bounded (by zero). Maximization of F (P, Q; ) with respect to is therefore well-defined. Further considerations regarding the well-definedness of the regularizer can be found in sec. 7.2 in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Regularizing GANs</head><p>We have shown that training with noise is equivalent to regularizing the discriminator. Inspired by the above analysis, we propose the following class of f -GAN regularizers:</p><formula xml:id="formula_21">Regularized f -GAN F (P, Q; ) = E P [ ] E Q [f c ] 2 ⌦ f (Q; ) ⌦ f (Q; ) := E Q h (f c00 ) kr k 2 i<label>(19)</label></formula><p>The regularizer corresponding to the commonly used parametrization of the Jensen-Shannon GAN can be derived analogously as shown in the Appendix. We obtain, Regularized Jensen-Shannon GAN</p><formula xml:id="formula_22">F (P, Q; ') = E P [ln(')] + E Q [ln(1 ')] 2 ⌦ JS (P, Q; ') ⌦ JS (P, Q; ') := E P ⇥ (1 '(x)) 2 ||r (x)|| 2 ⇤ + E Q ⇥ '(x) 2 ||r (x)|| 2 ⇤<label>(20)</label></formula><p>where = 1 (') denotes the logit of the discriminator '. We prefer to compute the gradient of as it is easier to implement and more robust than computing gradients after applying the sigmoid. Require: Initial noise variance 0 , annealing decay rate ↵, number of discriminator update steps n <ref type="bibr">'</ref> per generator iteration, minibatch size m, number of training iterations T Require: Initial discriminator parameters ! 0 , initial generator parameters ✓ 0 for t = 1, ..., T do 0 · ↵ t/T # annealing for 1, ..., n ' do Sample minibatch of real data {x <ref type="bibr" target="#b0">(1)</ref> , ..., x (m) } ⇠ P. Sample minibatch of latent variables from prior {z <ref type="bibr" target="#b0">(1)</ref> , ..., z (m) } ⇠ p(z).</p><formula xml:id="formula_23">F (!, ✓) = 1 m m X i=1 h ln ⇣ ' ! (x (i) ) ⌘ + ln ⇣ 1 ' ! (G ✓ (z (i) )) ⌘i ⌦(!, ✓) = 1 m m X i=1  ⇣ 1 ' ! (x (i) ) ⌘ 2 r ! (x (i) ) 2 + ' ! G ✓ (z (i) ) 2 rx ! (x) x=G ✓ (z (i) ) 2 ! ! + r ! ⇣ F (!, ✓) 2 ⌦(!, ✓)</formula><p>⌘ # gradient ascent end for Sample minibatch of latent variables from prior {z <ref type="bibr" target="#b0">(1)</ref> , ..., z (m) } ⇠ p(z).</p><formula xml:id="formula_24">F (!, ✓) = 1 m m X i=1 ln ⇣ 1 ' ! (G ✓ (z (i) )) ⌘ or F alt (!, ✓) = 1 m m X i=1 ln ⇣ ' ! (G ✓ (z (i) )) ⌘ ✓ ✓ r ✓ F(!, ✓) # gradient descent end for</formula><p>The gradient-based updates can be performed with any gradient-based learning rule. We used Adam in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Algorithm</head><p>Regularizing the discriminator provides an efficient way to convolve the distributions and is thereby sufficient to address the dimensional misspecification challenges outlined in the introduction. This leaves open the possibility to use the regularizer also in the objective of the generator. On the one hand, optimizing the generator through the regularized objective may provide useful gradient signal and therefore accelerate training. On the other hand, it destabilizes training close to convergence (if not dealt with properly), since the generator is incentiviced to put probability mass where the discriminator has large gradients. In the case of JS-GANs, we recommend to pair up the regularized objective of the discriminator with the "alternative" or "non-saturating" objective for the generator, proposed in <ref type="bibr" target="#b7">[8]</ref>, which is known to provide strong gradients out of the box (see Algorithm 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Annealing</head><p>The regularizer variance lends itself nicely to annealing. Our experimental results indicate that a reasonable annealing scheme consists in regularizing with a large initial early in training and then (exponentially) decaying to a small non-zero value. We leave to future work the question of how to determine an optimal annealing schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">2D submanifold mixture of Gaussians in 3D space</head><p>To demonstrate the stabilizing effect of the regularizer, we train a simple GAN architecture <ref type="bibr" target="#b19">[20]</ref> on a 2D submanifold mixture of seven Gaussians arranged in a circle and embedded in 3D space (further details and an illustration of the mixture distribution are provided in the Appendix). We emphasize that this mixture is degenerate with respect to the base measure defined in ambient space as it does not have fully dimensional support, thus precisely representing one of the failure scenarios commonly <ref type="bibr">UNREG.</ref> 0.01 1.0 <ref type="figure">Figure 1</ref>: 2D submanifold mixture. The first row shows one of several unstable unregularized GANs trained to learn the dimensionally misspecified mixture distribution. The remaining rows show regularized GANs (with regularized objective for the discriminator and unregularized objective for the generator) for different levels of regularization . Even for small but non-zero noise variance, the regularized GAN can essentially be trained indefinitely without collapse. The color of the samples is proportional to the density estimated from a Gaussian KDE fit. The target distribution is shown in <ref type="figure">Fig. 5</ref>. GANs were trained with one discriminator update per generator update step (indicated).</p><p>described in the literature <ref type="bibr" target="#b2">[3]</ref>. The results are shown in <ref type="figure">Fig. 1</ref> for both standard unregularized GAN training as well as our regularized variant.</p><p>While the unregularized GAN collapses in literally every run after around 50k iterations, due to the fact that the discriminator concentrates on ever smaller differences between generated and true data (the stakes are getting higher as training progresses), the regularized variant can be trained essentially indefinitely (well beyond 200k iterations) without collapse for various degrees of noise variance, with and without annealing. The stabilizing effect of the regularizer is even more pronounced when the GANs are trained with five discriminator updates per generator update step, as shown in <ref type="figure">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Stability across various architectures</head><p>To demonstrate the stability of the regularized training procedure and to showcase the excellent quality of the samples generated from it, we trained various network architectures on the CelebA <ref type="bibr" target="#b16">[17]</ref>, CIFAR-10 <ref type="bibr" target="#b14">[15]</ref> and LSUN bedrooms <ref type="bibr" target="#b31">[32]</ref> datasets. In addition to the deep convolutional GAN (DCGAN) of <ref type="bibr" target="#b25">[26]</ref>, we trained several common architectures that are known to be hard to train <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b18">19]</ref>, therefore allowing us to establish a comparison to the concurrently proposed gradientpenalty regularizer for Wasserstein GANs <ref type="bibr" target="#b9">[10]</ref>. Among these architectures are a DCGAN without any normalization in either the discriminator or the generator, a DCGAN with tanh activations and a deep residual network (ResNet) GAN <ref type="bibr" target="#b10">[11]</ref>. We used the open-source implementation of <ref type="bibr" target="#b9">[10]</ref> for our experiments on CelebA and LSUN, with one notable exception: we use batch normalization also for the discriminator (as our regularizer does not depend on the optimal transport plan or more precisely the gradient penalty being imposed along it).</p><p>All networks were trained using the Adam optimizer <ref type="bibr" target="#b12">[13]</ref> with learning rate 2 ⇥ 10 4 and hyperparameters recommended by <ref type="bibr" target="#b25">[26]</ref>. We trained all datasets using batches of size 64, for a total of 200K generator iterations in the case of LSUN and 100k iterations on CelebA. The results of these experiments are shown in Figs. 3 &amp; 2. Further implementation details can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training time</head><p>We empirically found regularization to increase the overall training time by a marginal factor of roughly 1.4 (due to the additional backpropagation through the computational graph of the discriminator gradients). More importantly, however, (regularized) f -GANs are known to converge (or at least generate good looking samples) faster than their WGAN relatives <ref type="bibr" target="#b9">[10]</ref>. The initial level of regularization 0 is shown below each batch of images. 0 was exponentially annealed as described in Algorithm 1. The regularized GANs can be trained essentially indefinitely without collapse, the superior quality is again evident. Samples were produced after 100k generator iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Regularization vs. explicitly adding noise</head><p>We compare our regularizer against the common practitioner's approach to explicitly adding noise to images during training. In order to compare both approaches (analytic regularizer vs. explicit noise), we fix a common batch size (64 in our case) and subsequently train with different noise-to-signal ratios (NSR): we take (batch-size/NSR) samples (both from the dataset and generated ones) to each of which a number of NSR noise vectors is added and feed them to the discriminator (so that overall both models are trained on the same batch size). We experimented with NSR 1, 2, 4, 8 and show the best performing ratio (further ratios in the Appendix). Explicitly adding noise in high-dimensional ambient spaces introduces additional sampling variance which is not present in the regularized variant. The results, shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, confirm that the regularizer stabilizes across a broad range of noise levels and manages to produce images of considerably higher quality than the unregularized variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Cross-testing protocol</head><p>We propose the following pairwise cross-testing protocol to assess the relative quality of two GAN models: unregularized GAN (Model 1) vs. regularized GAN (Model 2). We first report the confusion matrix (classification of 10k samples from the test set against 10k generated samples) for each model separately. We then classify 10k samples generated by Model 1 with the discriminator of Model 2 and vice versa. For both models, we report the fraction of false positives (FP) (Type I error) and false negatives (FN) (Type II error). The discriminator with the lower FP (and/or lower FN) rate defines the better model, in the sense that it is able to more accurately classify out-of-data samples, which indicates better generalization properties. We obtained the following results on CIFAR-10: For both models, the discriminator is able to recognize his own generator's samples (low FP in the confusion matrix). The regularized GAN also manages to perfectly classify the unregularized GAN's samples as fake (cross-testing FP 0.0) whereas the unregularized GAN classifies the samples of the regularized GAN as real (cross-testing FP 1.0). In other words, the regularized model is able to fool the unregularized one, whereas the regularized variant cannot be fooled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduced a regularization scheme to train deep generative models based on generative adversarial networks (GANs). While dimensional misspecifications or non-overlapping support between the data and model distributions can cause severe failure modes for GANs, we showed that this can be addressed by adding a penalty on the weighted gradient-norm of the discriminator. Our main result is a simple yet effective modification of the standard training algorithm for GANs, turning them into reliable building blocks for deep learning that can essentially be trained indefinitely without collapse. Our experiments demonstrate that our regularizer improves stability, prevents GANs from overfitting and therefore leads to better generalization properties (cf cross-testing protocol). Further research on the optimization of GANs as well as their convergence and generalization can readily be built upon our theoretical results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>as follows: the Laplacian measures how much the scalar fields and f c differ at each point from their local average. It is thereby an infinitesimal proxy for the (exact) convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Stability accross various architectures: ResNet, DCGAN, DCGAN without normalization and DCGAN with tanh activations (details in the Appendix). All samples were generated from regularized GANs with exponentially annealed 0 = 2.0 (and alternative generator loss) as described in Algorithm 1. Samples were produced after 200k generator iterations on the LSUN dataset (see also Fig. 8 for a full-resolution image of the ResNet GAN). Samples for the unregularized architectures can be found in the Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: CIFAR-10 samples generated by (un)regularized DCGANs (with alternative generator loss), as well as by training a DCGAN with explicitly added noise (noise-to-signal ratio 4). The level of regularization or noise is shown above each batch of images. The regularizer stabilizes across a broad range of noise levels and manages to produce images of higher quality than the unregularized variants. Samples were produced after 50 training epochs.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code available at https://github.com/rothk/Stabilizing_GANs 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Devon Hjelm for pointing out that the regularizer works well with ResNets. KR is thankful to Yannic Kilcher, Lars Mescheder and the dalab team for insightful discussions. Big thanks also to Ishaan Gulrajani and Taehoon Kim for their open-source GAN implementations. This work was supported by Microsoft Research through its PhD Scholarship Programme.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Methods of information geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Shun-Ichi Amari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nagaoka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>American Mathematical Soc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The effects of adding noise during backpropagation training on a generalization performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guozhong</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="page" from="643" to="674" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research. PMLR</title>
		<meeting>Machine Learning Research. PMLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Training with noise is equivalent to tikhonov regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="108" to="116" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Disco nets: Dissimilarity coefficients networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Bouchacourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Mudigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="352" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Tong Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02136</idno>
		<title level="m">Mode regularized generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative moment matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1718" to="1727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The numerics of gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unrolled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Divergence measures and message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Minka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>Microsoft Research</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Integral probability metrics and their generating classes of functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Probability</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="429" to="443" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sample complexity of testing the manifold hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hariharan</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjoy</forename><surname>Mitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1786" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Estimating divergence functionals and the likelihood ratio by convex risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanlong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5847" to="5861" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka. F-Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Information, divergence and risk for binary experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="731" to="817" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Danilo J Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Multivariate density estimation: theory, practice, and visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Amortised map inference for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Casper Kaae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huszár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04490</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">On integral probability metrics, phi-divergences and binary classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bharath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gert Rg</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanckriet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0901.2698</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
