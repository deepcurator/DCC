<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:13+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-Time Seamless Single Shot 6D Object Pose Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>EPFL</roleName><forename type="first">Bugra</forename><surname>Tekin</surname></persName>
							<email>bugra.tekin@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudipta</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
							<email>sudipta.sinha@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>EPFL</roleName><forename type="first">Pascal</forename><surname>Fua</surname></persName>
							<email>pascal.fua@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Real-Time Seamless Single Shot 6D Object Pose Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We propose a single-shot approach for simultaneously detecting    <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>that directly predicts the 2D image locations of the projected vertices of the object's 3D bounding box. The object's 6D pose is then estimated using a PnP algorithm.</head><p>For single object and multiple object pose estimation on the LINEMOD and OCCLUSION datasets, our approach substantially outperforms other recent CNN-based approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">25]</ref> when they are all used without postprocessing. During post-processing, a pose refinement step can be used to boost the accuracy of these two methods, but at 10 fps or less, they are much slower than our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Real-time object detection and 6D pose estimation is crucial for augmented reality, virtual reality, and robotics. Currently, methods relying on depth data acquired by RGB-D cameras are quite robust <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref>. However, active depth sensors are power hungry, which makes 6D object detection methods for passive RGB images more attractive for mobile and wearable cameras. There are many fast keypoint and edge-based methods <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b36">36]</ref> that are effective for textured objects. However, they have difficulty handling weakly textured or untextured objects and processing low-resolution video streams, which are quite common when dealing with cameras on wearable devices.</p><p>Deep learning techniques have recently been used to address these limitations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">25]</ref>. BB8 <ref type="bibr" target="#b25">[25]</ref> is a 6D object detection pipeline made of one CNN to coarsely segment the object and another to predict the 2D locations of the projections of the object's 3D bounding box given the segmentation, which are then used to compute the 6D pose using a PnP algorithm <ref type="bibr" target="#b15">[16]</ref>. The method is effective but slow due to its multi-stage nature. SSD-6D <ref type="bibr" target="#b9">[10]</ref> is a different pipeline that relies on the SSD architecture <ref type="bibr" target="#b18">[19]</ref> to predict 2D bounding boxes and a very rough estimate of the object's orientation in a single step. This is followed by an approximation to predict the object's depth from the size of its 2D bounding box in the image, to lift the 2D detections to 6D. Both BB8 and SSD-6D require a further pose refinement step for improved accuracy, which increases their running times linearly with the number of objects being detected.</p><p>In this paper, we propose a single-shot deep CNN architecture that takes the image as input and directly detects the 2D projections of the 3D bounding box vertices. It is end-to-end trainable and accurate even without any a posteriori refinement. And since, we do not need this refinement step, we also do not need a precise and detailed textured 3D object model that is needed by other methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">25]</ref>. We only need the 3D bounding box of the object shape for training. This can be derived from other easier to acquire and approximate 3D shape representations.</p><p>We demonstrate state-of-the-art accuracy on the LINEMOD dataset <ref type="bibr" target="#b7">[8]</ref>, which has become a de facto standard benchmark for 6D pose estimation. However, we are much faster than the competing techniques by a factor of more than five, when dealing with a single object. Furthermore, we pay virtually no time-penalty when handling several objects and our running time remains constant whereas that of other methods grow proportional to the number of objects, which we demonstrate on the OCCLUSION dataset <ref type="bibr" target="#b0">[1]</ref>. Therefore, our contribution is an architecture that yields a fast and accurate one-shot 6D pose prediction without requiring any post-processing. It extends single shot CNN architectures for 2D detection in a seamless and natural way to the 6D detection task. Our implementation is based on YOLO <ref type="bibr" target="#b28">[28]</ref> but the approach is amenable to other singleshot detectors such as SSD <ref type="bibr" target="#b18">[19]</ref> and its variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We now review existing work on 6D pose estimation ranging from classical feature and template matching methods to newer end-to-end trainable CNN-based methods.</p><p>Classical methods. Traditional RGB object instance recognition and pose estimation works used local keypoints and feature matching. Local descriptors needed by such methods were designed for invariance to changes in scale, rotation, illumination and viewpoints <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b36">36]</ref>. Such methods are often fast and robust to occlusion and scene clutter. However, they only reliably handle textured objects in high resolution images <ref type="bibr" target="#b14">[15]</ref>. Other related methods include 3D model-based registration <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b35">35]</ref>, Hausdorff matching <ref type="bibr" target="#b8">[9]</ref>, oriented Chamfer matching for edges <ref type="bibr" target="#b17">[18]</ref> and 3D chamfer matching for aligning 3D curve-based models to images <ref type="bibr" target="#b26">[26]</ref>.</p><p>RGB-D methods. The advent of commodity depth cameras has spawned many RGB-D object pose estimation methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b39">39]</ref>. For example, Hinterstoisser proposed template matching algorithms suitable for both color and depth images <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Rios et al. <ref type="bibr" target="#b30">[30]</ref> extended their work using discriminative learning and cascaded detections for higher accuracy and efficiency respectively. RGB-D methods were used on indoor robots for 3D object recognition, pose estimation, grasping and manipulation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b40">40]</ref>. Brachmann et al. <ref type="bibr" target="#b0">[1]</ref> proposed using regression forests to predict dense object coordinates, to segment the object and recover its pose from dense correspondences. They also extended their method to handle uncertainty during inference and deal with RGB images <ref type="bibr" target="#b1">[2]</ref>. Zach et al. <ref type="bibr" target="#b38">[38]</ref> explored fast dynamic programming based algorithms for RGB-D images.</p><p>CNN-based methods. In recent years, research in most pose estimation tasks has been dominated by CNNs. Techniques such as Viewpoints and Keypoints <ref type="bibr" target="#b34">[34]</ref> and Render for CNN <ref type="bibr" target="#b33">[33]</ref> cast object categorization and 3D pose estimation into classification tasks, specifically by discretizing the pose space. In contrast, PoseNet <ref type="bibr" target="#b11">[12]</ref> proposes using a CNN to directly regress from an RGB image to a 6D pose, albeit for camera pose estimation, a slightly different task. Since PoseNet outputs a translational and a rotational component, the two associated loss terms have to be balanced carefully by tuning a hyper-parameter during training.</p><p>To avoid this problem, the newer PoseCNN architecture <ref type="bibr" target="#b37">[37]</ref> is trained to predict 6D object pose from a single RGB image in multiple stages, by decoupling the translation and rotation predictors. A geodesic loss function more suitable for optimizing over 3D rotations have been suggested in <ref type="bibr" target="#b22">[22]</ref>. Another way to address this issue has recently emerged. In <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">25]</ref>, the CNNs do not directly predict object pose. Instead, they output 2D coordinates, 2D masks, or discrete orientation predictions from which the 6D pose can be inferred. Because all the predictions are in the 2D image, the problem of weighting different loss terms goes away. Also training becomes numerically more stable, resulting in better performance on the LINEMOD dataset <ref type="bibr" target="#b7">[8]</ref>. We also adopt this philosophy in our work.</p><p>In parallel to these developments, on the 2D object detection task, there has been a progressive trend towards single shot CNN frameworks as an alternative to two-staged methods such as Faster-RCNN <ref type="bibr" target="#b29">[29]</ref> that first find a few candidate locations in the image and then classifies them as objects or background. Recently, single shot architectures such as YOLO <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28]</ref> and SSD <ref type="bibr" target="#b18">[19]</ref> have been shown to be fast and accurate. SSD has been extended to predict the object's identity, its 2D bounding box in the image and a discrete estimate of the object's orientation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">24]</ref>. In this paper, we go beyond such methods by extending a YOLO-like architecture <ref type="bibr" target="#b28">[28]</ref> to directly predict a few 2D coordinates from which the full 6D object pose can be accurately recovered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>With our goal of designing an end-to-end trainable network that predicts the 6D pose in real-time, we were inspired by the impressive performance of single shot 2D object detectors such as YOLO <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28]</ref>. This led us to design the CNN architecture <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28]</ref> shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. We designed our network to predict the 2D projections of the corners of the 3D bounding box around our objects. The main insight was that YOLO was originally designed to regress 2D bounding boxes and to predict the projections of the 3D bounding box corners in the image, a few more 2D points had to be predicted for each object instance in the image. Then given these 2D coordinates and the 3D ground control points for the bounding box corners, the 6D pose can be calculated algebraically with an efficient PnP algorithm <ref type="bibr" target="#b15">[16]</ref>. BB8 <ref type="bibr" target="#b25">[25]</ref> takes a similar approach. However, they first find a 2D segmentation mask around the object and present a cropped image to a second network that predicts the eight 2D corners in the image. We now describe our network architecture and explain various aspects of our approach in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model</head><p>We formulate the 6D pose estimation problem in terms of predicting the 2D image coordinates of virtual 3D control points associated with the 3D models of our objects of interest. Given the 2D coordinate predictions, we calculate the object's 6D pose using a PnP algorithm. We parameterize the 3D model of each object with 9 control points. For these control points, we select the 8 corners of the tight 3D bounding box fitted to the 3D model, similar to <ref type="bibr" target="#b25">[25]</ref>. In addition, we use the centroid of the object's 3D model as the 9th point. This parameterization is general and can be used for any rigid 3D object with arbitrary shape and topology. In addition, these 9 control points are guaranteed to be well spread out in the 2D image and could be semantically meaningful for many man-made objects.</p><p>Our model takes as input a single full color image, processes it with a fully-convolutional architecture shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a) and divides the image into a 2D regular grid containing S × S cells as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(c). In our model, each grid location in the 3D output tensor will be associated with a multidimensional vector, consisting of predicted 2D image locations of the 9 control points, the class probabilities of the object and an overall confidence value. At test time, predictions at cells with low confidence values, ie. where the objects of interest are not present, will be pruned.</p><p>The output target values for our network are stored in a 3D tensor of size S × S × D visualized in <ref type="figure" target="#fig_0">Fig. 1</ref>(e). The target values for an object at a specific spatial cell location i ∈ S × S is placed in the i-th cell in the 3D tensor in the form of a D dimensional vector v i . When N objects are present in different cells, we have N such vectors, v 1 , v 2 , . . . , v n in the 3D tensor. We train our network to predict these target values. The 9 control points in our case are the 3D object model's center and bounding box corners but could be defined in other ways as well. To train our network, we only need to know the 3D bounding box of the object, not a detailed mesh or an associated texture map.</p><p>As in YOLO, it is crucial that a trained network is able to predict not only the precise 2D locations but also high confidence values in regions where the object is present and low confidence where it isn't present. In case of 2D object detection, YOLO uses for its confidence values, an intersection over union (IoU) score associated with the predicted (and true 2D rectangles) in the image. In our case, the objects are in 3D and to compute an equivalent IoU score with two arbitrary cuboids, we would need to calculate a 3D convex hull corresponding to their intersections. This would be tedious and would slow down training, as also analyzed in our supplemental material. Therefore, we take a different approach. We model the predicted confidence value using a confidence function shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The confidence function, c(x), returns a confidence value for a predicted 2D point denoted by x based on its distance D T (x) from the ground truth i.e. target 2D point. Formally, we define the confidence function c(x) as follows:</p><formula xml:id="formula_0">c(x) = e α(1− D T (x) d th ) , if D T (x) &lt; d th 0 otherwise (1)</formula><p>The distance D T (x) is defined as the 2D Euclidean distance in the image space. To achieve precise localization with this function, we choose a sharp exponential function with a cut-off value d th instead of a monotonically decreasing linear function. The sharpness of the exponential function is defined by the parameter α. In practice, we apply the confidence function to all the control points and calculate the mean value and assign it as the confidence. As mentioned earlier, we also predict C conditional class probabilities at each cell. The class probability is conditioned on the cell containing an object. Overall, our output 3D tensor depicted in <ref type="figure" target="#fig_0">Figure 1</ref>(e) has dimension S × S × D, where the 2D spatial grid corresponding to the image dimensions has S × S cells and each such cell has a D dimensional vector. Here, D = 9×2+C +1, because we have 9 (x i , y i ) control points, C class probabilities and one confidence value.</p><p>Our network architecture follows the fully convolutional YOLO v2 architecture <ref type="bibr" target="#b28">[28]</ref>. Thus, our network has 23 convolutional layers and 5 max-pooling layers. Similar to YOLO v2, we choose S = 13 and have a 13 × 13 2D spatial grid on which we make our predictions. We also allow higher layers of our network to use fine-grained features by adding a passthrough layer. Specifically, we bring features from an earlier layer at resolution 26 × 26, apply batch normalization and resize the input image during training onthe-fly. As the network downsamples the image by a factor of 32, we change the input resolution to a multiple of 32 randomly chosen from the set {320, 352, . . . , 608} to be robust to objects of different size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training Procedure</head><p>Our final layer outputs class probabilities, (x, y) coordinate locations for the control points, and the overall confidence score. During training, this confidence value is computed on the fly using the function defined in Eq. 1 to measure the distance between the current coordinate predictions and the ground-truth, D T (x). We predict offsets for the 2D coordinates with respect to (c x , c y ), the top-left corner of the associated grid cell. For the centroid, we constrain this offset to lie between 0 and 1. However, for the corner points, we do not constrain the network's output as those points should be allowed to fall outside the cell. The predicted control point (g x , g y ) is defined as</p><formula xml:id="formula_1">g x = f (x) + c x (2) g y = f (y) + c y<label>(3)</label></formula><p>where f (·) is chosen to be a 1D sigmoid function in case of the centroid and the identity function in case of the eight corner points. This has the effect of forcing the network to first find the approximate cell location for the object and later refine its eight corner locations. We minimize the following loss function to train our complete network.</p><formula xml:id="formula_2">L = λ pt L pt + λ conf L conf + λ id L id (4)</formula><p>Here, the terms L pt , L conf and L id denote the coordinate, confidence and the classification loss, respectively. We use mean-squared error for the coordinate and confidence losses, and cross entropy for the classification loss. As suggested in <ref type="bibr" target="#b27">[27]</ref>, we downweight the confidence loss for cells that don't contain objects by setting λ conf to 0.1. This improves model stability. For cells that contain objects, we set λ conf to 5.0. We set λ pt and λ id simply to 1. When multiple objects are located close to each other in the 3D scene, they are more likely to appear close together in the images or be occluded by each other. In these cases, certain cells might contain multiple objects. To be able to predict the pose of such multiple objects that lie in the same cell, we allow up to 5 candidates per cell and therefore predict five sets of control points per cell. This essentially means that we assumed that at most 5 objects could occlude each other in a single grid cell. This is a reasonable assumption to make in practical pose estimation scenarios. As in <ref type="bibr" target="#b28">[28]</ref>, we precompute with k-means, five anchor boxes that define the size, ie. the width and height of a 2D rectangle tightly fitted to a masked region around the object in the image. During training, we assign whichever anchor box has the most similar size to the current object as the responsible one to predict the 2D coordinates for that object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pose Prediction</head><p>We detect and estimate the pose of objects in 6D by invoking our network only once. At test time, we estimate the class-specific confidence scores for each object by multiplying the class probabilities and the score returned by the confidence function. Each grid cell produces predictions in one network evaluation and cells with predictions with low confidence are pruned using a confidence threshold. For large objects and objects whose projections lie at the intersection of two cells, multiple cells are likely to predict highly confident detections. To obtain a more robust and well localized pose estimate, we inspect the cells in the 3×3 neighborhood of the cell which has the maximum confidence score. We combine the individual corner predictions of these adjacent cells by computing a weighted average of the individual detections, where the weights used are the confidence scores of the associated cells.</p><p>At run-time, the network gives the 2D projections of the object's centroid and corners of its 3D bounding box along with the object identity. We estimate the 6D pose from the correspondences between the 2D and 3D points using a Perspective-n-Point (PnP) pose estimation method <ref type="bibr" target="#b15">[16]</ref>. In our case, PnP uses only 9 such control point correspondences and provides an estimate of the 3D rotation R and 3D translation t of the object in camera coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation Details</head><p>We initialize the parameters of our network by training the original network on the ImageNet classification task. As the pose estimates in the early stages of training are inaccurate, the confidence values computed using Eq. 1 are initially unreliable. To remedy this, we pretrain our network parameters by setting the regularization parameter for confidence to 0. Subsequently, we train our network by setting λ conf to 5 for the cells that contain an object, and to 0.1 otherwise, to have more reliable confidence estimates in the early stages of the network. In practice, we set the sharpness of the confidence function α to 2 and the distance threshold to 30 pixels. We use stochastic gradient descent for optimization. We start with a learning rate of 0.001 and divide the learning rate by 10 at every 100 epochs. To avoid overfitting, we use extensive data augmentation by randomly changing the hue, saturation and exposure of the image by up to a factor of 1.5. We also randomly scale and translate the image by up to a factor of 20% of the image size. Our implementation is based on PyTorch. We will make our code publicly available for the sake of reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We first evaluate our method for estimating the 6D pose of single objects and then we evaluate it in the case where multiple objects are present in the image. We use the same datasets and evaluation protocols as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">25]</ref>, which we review below. We then present and compare our results to the state of the art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>We test our approach on two datasets that were designed explicitly to benchmark 6D object pose estimation algorithms. We describe them briefly below.</p><p>LineMod <ref type="bibr" target="#b7">[8]</ref> has become a de facto standard benchmark for 6D object pose estimation of textureless objects in cluttered scenes. The central object in each RGB image is assigned a ground-truth rotation, translation, and ID. A full 3D mesh representing the object is also provided. There are 15783 images in LINEMOD for 13 objects. Each object features in about 1200 instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OCCLUSION [1]</head><p>is a multi-object detection and pose estimation dataset that contains additional annotations for all objects in a subset of the LINEMOD images. As its name suggests, several objects in the images are severely occluded due to scene clutter, which makes pose estimation  <ref type="table">Table 1</ref>: Comparison of our approach with state-of-the-art algorithms on LINEMOD in terms of 2D reprojection error. We report percentages of correctly estimated poses. In <ref type="table" target="#tab_2">Tables 1, 2</ref> and 4 bold face numbers denote the best overall methods, bold italic numbers denote the best methods among those that do not use refinement as opposed to the ones that use, if different. Note that even though we do not rely on the knowledge of a detailed 3D object model our method consistently outperforms the baselines.</p><p>extremely challenging. With the exception of <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">25]</ref>, it has primarily been used to test algorithms that require depth images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation Metrics</head><p>We use three standard metrics to evaluate 6D pose accuracy, namely -2D reprojection error, IoU score and average 3D distance of model vertices (referred to as ADD metric) as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">25]</ref>. In all cases, we calculate the accuracy as the percentage of correct pose estimates for certain error thresholds.</p><p>When using the reprojection error, we consider a pose estimate to be correct when the mean distance between the 2D projections of the object's 3D mesh vertices using the estimate and the ground truth pose is less than 5 pixels <ref type="bibr" target="#b1">[2]</ref>. This measures the closeness of the true image projection of the object to that obtained by using the estimated pose. This metric is suitable for augmented reality applications.</p><p>To compute the IoU score, we measure the overlap between the projections of the 3D model given the groundtruth and predicted pose and accept a pose as correct if the overlap is larger than 0.5, as in <ref type="bibr" target="#b9">[10]</ref>.</p><p>When comparing 6D poses using the ADD metric, we take a pose estimate to be correct if the mean distance between the true coordinates of 3D mesh vertices and those estimated given the pose is less than 10% of the object's diameter <ref type="bibr" target="#b7">[8]</ref>. For most objects, this is approximately a 2cm threshold but for smaller objects, such as ape, the threshold drops to about 1cm. For rotationally symmetric objects whose pose can only be computed up to one degree of rotational freedom, we modify slightly the metric as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref> and compute <ref type="formula">(5)</ref> where (R, t) are the ground-truth rotation and translation, (R,t) the predicted ones, and M the vertex set of the 3D model. We use this metric when evaluating the pose accuracy for the rotationally invariant objects, eggbox and glue as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref>.</p><formula xml:id="formula_3">s = 1 |M| x1∈M min M (Rx + t) − (Rx +t) ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Single Object Pose Estimation</head><p>We first estimate the 6D pose of the central object in the RGB only LINEMOD images, without reference to the depth ones. We compare our approach to those of <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">25]</ref>, which operate under similar conditions.</p><p>In this dataset, the training images are selected such that the relative orientation between corresponding pose annotations are larger than a threshold. As in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">25]</ref>, to avoid being influenced by the scene context and overfitting to the background, we segment the training images using the segmentation masks provided with the dataset and replace the background by a random image from the PASCAL VOC dataset <ref type="bibr" target="#b5">[6]</ref>.</p><p>We use exactly the same training/test splits as in <ref type="bibr" target="#b25">[25]</ref>. We report our results in terms of 2D reprojection error in <ref type="table">Table 1</ref>, 6D pose error in <ref type="table" target="#tab_2">Table 2</ref> and IoU metric in <ref type="table">Table 4</ref>. We provide example pose predictions of our approach in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Comparative Accuracy</head><p>6D Accuracy in terms of projection error. In <ref type="table">Table 1</ref>, we compare our results to those of Brachmann et al. <ref type="bibr" target="#b1">[2]</ref> and to BB8 <ref type="bibr" target="#b25">[25]</ref>. Both of these competing methods involve a multi-stage pipeline that comprises a 2D detection step followed by pose prediction and refinement. Since we do not have a refinement stage, we show in the table their results without and with it. In both cases, we achieve better 6D pose estimation accuracies.</p><p>In <ref type="table">Table 4</ref>, we perform a similar comparison with SSD-6D <ref type="bibr" target="#b9">[10]</ref>, whose authors report their projection accuracy in terms of the IoU metric. That method also requires a posteriori refinement and our results are again better in both cases, even though SSD-6D relies on a large training set of rendered images that are sampled over a wide range of viewpoints and locations.</p><p>6D Accuracy in terms of the ADD metric. In <ref type="table" target="#tab_2">Tables 2  and 3</ref>, we compare our methods against the other in terms of the average of the 3D distances, as described in Section 5.2. In <ref type="table" target="#tab_2">Table 2</ref>, we give numbers before and after refinement for   <ref type="table">Table 3</ref>: Comparison of our approach with SSD-6D <ref type="bibr" target="#b9">[10]</ref> without refinement using different thresholds for the 6D pose metric.</p><p>the competing methods. Before refinement, we outperform all the methods by a significant margin of at least 12%. After refinement, our pose estimates are still better than Brachmann et al. <ref type="bibr" target="#b1">[2]</ref>. By assuming the additional knowledge of a full 3D CAD model and using it to further refine the pose, BB8 1 and SSD-6D 2 boost their pose estimation accuracy.</p><p>Without any bells and whistles, our approach achieves state-of-the-art pose estimation accuracy in all the metrics without refinement. When compared against methods that rely on the additional knowledge of full 3D CAD models and pose refinement, it still achieves state-of-the-art performance in 2D projection error and IoU metrics and yields comparable accuracy in the ADD metric. Our approach could be used in conjunction with such refinement strategies to further increase the accuracy however this comes at a heavy computational cost as we describe below.  <ref type="table">Table 4</ref>: Comparison of our approach against <ref type="bibr" target="#b9">[10]</ref> on LINEMOD using IoU metric. The authors of <ref type="bibr" target="#b9">[10]</ref> were able to provide us the results of our approach w/o the refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Accuracy / Speed Trade-off</head><p>In <ref type="table">Table 5</ref>, we report the computational efficiency of our approach for single object pose estimation in comparison to the state-of-the-art approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">25]</ref>. Our approach runs at real-time performance in contrast to the existing approaches which fall short of it. In particular, our algorithm runs at least 5 times faster than the state-of-the-art techniques for single object pose estimation.</p><p>As can be seen in <ref type="table" target="#tab_2">Table 2</ref>, pose refinement in Brachmann et al. increase the accuracy significantly by 17.9% at an additional run-time of 100 miliseconds per object. BB8 also gets a substantial improvement of 19.1% in accuracy at an additional run-time of 21 miliseconds per object. Even without correcting for the pose error, our approach outperforms Brachmann et al. and yields close accuracy to BB8 while being 16 times faster for single object pose estimation. As discussed also in <ref type="bibr" target="#b9">[10]</ref>, the unrefined poses computed from the bounding boxes of the SSD 2D object detector, are rather approximate. We confirmed this by running their publicly available code with the provided pretrained models. We report the accuracy numbers without the refinement using the ADD metric in <ref type="table">Table 3</ref> for different thresholds. While providing a good initialization for the subsequent pose processing, the pose estimates of SSD-6D without refinement are much less accurate than our approach. The further refinement increases the pose estimation accuracy significantly, however at the cost of a computational time of 24 miliseconds per object. Moreover, in contrast to our approach, the refinement requires the knowledge of the full 3D object CAD model.</p><p>In <ref type="figure">Figure 3</ref>, we show example results of our method on the LINEMOD. We include more visual results of our method in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Multiple Object Pose Estimation</head><p>We use the OCCLUSION dataset to compare our approach to Brachmann et al. <ref type="bibr" target="#b1">[2]</ref> for multi-object detection and report pose estimation accuracy as in <ref type="bibr" target="#b25">[25]</ref>. The identity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Overall Speed Refinement runtime Brachmann et al. <ref type="bibr" target="#b1">[2]</ref> 2 fps 100 ms/object Rad &amp; Lepetit <ref type="bibr" target="#b25">[25]</ref> 3 fps 21 ms/object Kehl et al. <ref type="bibr" target="#b9">[10]</ref> 10 fps 24 ms/object OURS 50 fps - <ref type="table">Table 5</ref>: Comparison of the overall computational runtime of our approach in comparison to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">25]</ref>. We further provide the computational runtime induced by the pose refinement stage of <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">25]</ref> of the objects cannot be assumed to be known a priori and has to be guessed. To this end, the method of <ref type="bibr" target="#b25">[25]</ref> assumes that it has access to image crops based on the ground-truth 2D bounding boxes <ref type="bibr" target="#b2">3</ref> . We make no such assumptions. Instead, we jointly detect the object in 2D, estimate its identity and predict its 6D pose. We generate our training images with the approach explained in Section 5.2. We further augment the LINEMOD training data by adding into the images objects extracted from other training sequences. We report our pose estimation accuracy in <ref type="figure">Figure 4</ref> and demonstrate that even without assuming ground-truth information as in the case of <ref type="bibr" target="#b25">[25]</ref>, our method yields satisfactory pose accuracy in the case of severe occlusions. For object detection purposes, we consider an estimate to be correct if its detection IoU is larger than 0.5. Note that here the detection IoU corresponds to the overlap of the 2D bounding boxes of the object, rather than the overlap of the projected masks as is the case for the IoU metric defined in Sec 5.2. In <ref type="table">Table 6</ref>, we report a mean average precision (MAP) of 0.48 which is similar to the accuracy reported by <ref type="bibr" target="#b1">[2]</ref> and outperforms the ones reported by <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method MAP</head><p>Hinterstoisser et al. <ref type="bibr" target="#b6">[7]</ref> 0.21 Brachmann et al. <ref type="bibr" target="#b1">[2]</ref> 0.51 Kehl et al. <ref type="bibr" target="#b9">[10]</ref> 0.38 OURS 0.48 <ref type="table">Table 6</ref>: The detection experiment on the Occlusion dataset <ref type="bibr" target="#b1">[2]</ref>. (Left) Precision-recall plot. (Right) Our approach provides accurate 6D poses with real-time performance. Upon one network invocation, our only computational overhead is an efficient PnP algorithm which operates on just 9 points per object. Furthermore we do not require full 3D colored object models to further refine our initial pose estimates. Our approach is therefore scalable to handle multiple objects as shown in <ref type="figure">Figure 5</ref> and has only a negligible computational overhead of PnP (0.2 miliseconds/object) while the competing approaches <ref type="bibr" target="#b9">[10]</ref> have a linear runtime growth.</p><p>We also evaluated the accuracy and speed of our ap- <ref type="figure">Figure 3</ref>: Pose estimation results of our approach. Note that our method can recover the 6D pose in these challenging scenarios, which involve significant amounts of clutter, occlusion and orientation ambiguity. In the last column, we show failure cases due to motion blur, severe occlusion and specularity (this figure is best viewed on a computer screen). The runtime of our approach with increasing number of objects as compared to that of <ref type="bibr" target="#b9">[10]</ref>.</p><p>proach for different input resolutions. As explained in Section 3.1, we adopt a multi-scale training procedure and change the input resolution during training randomly as in <ref type="bibr" target="#b28">[28]</ref>. This allows us to be able to change the input resolution at test-time and predict from images with higher resolution. This is especially useful for predicting the pose of small objects more robustly. As we do not have an initial step for 2D object detection and produce image crops which are then resized to higher resolutions for pose prediction as in <ref type="bibr" target="#b25">[25]</ref>, our approach requires better handling of the small objects. In <ref type="table">Table 7</ref>, we compare the accuracy and computational efficiency of our approach for different input resolutions. With only 1% decrease in accuracy the average runtime per image is 94 ms and the runtime virtually remains the same for estimating the pose of multiple objects.  <ref type="table">Table 7</ref>: Accuracy/speed trade-off of our method on the LINEMOD dataset. Accuracy reported is the percentage of correctly estimated poses w.r.t the 2D projection error. The same network model is used for all four input resolutions. Timings are on a Titan X (Pascal) GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have proposed a new CNN architecture for fast and accurate single-shot 6D pose prediction that naturally extends the single shot 2D object detection paradigm to 6D object detection. Our network predicts 2D locations of the projections of the objects 3D bounding box corners which involves predicting just a few more 2D points than for 2D bounding box regression. Given the predicted 2D corner projections, the 6D pose is computed via an efficient PnP method. For high accuracy, existing CNN-based 6D object detectors all refine their pose estimates during postprocessing, a step that requires an accurate 3D object model and also incurs a runtime overhead per detected object. In contrast, our single shot predictions are very accurate which alleviates the need for refinement. Due to this, our method is not dependent on access to 3D object models and there is virtually no overhead when estimating the pose of multiple objects. Our method is real-time; it runs at 50 -94 fps depending on the image resolution. This makes it substantially faster than existing methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview: (a) The proposed CNN architecture. (b) An example input image with four objects. (c) The S × S grid showing cells responsible for detecting the four objects. (d) Each cell predicts 2D locations of the corners of the projected 3D bounding boxes in the image. (e) The 3D output tensor from our network, which represents for each cell a vector consisting of the 2D corner locations, the class probabilities and a confidence value associated with the prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Confidence c(x) as a function of the distance D T (x) between a predicted point and the true point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Percentage of correctly estimated poses as a function of the projection error for different objects of the Occlusion dataset [2].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison of our approach with state-of-the-art 
algorithms on LINEMOD in terms of ADD metric. We re-
port percentages of correctly estimated poses. 

Threshold 
10% 
30% 
50% 
Object 
[10] OURS 
[10] OURS 
[10] OURS 
Ape 
0 
21.62 
5.62 
70.67 19.95 88.10 
Benchvise 
0.18 81.80 
2.07 
91.07 10.62 98.85 
Cam 
0.41 36.57 34.52 81.57 63.54 94.80 
Can 
1.35 68.80 61.43 99.02 85.49 99.90 
Cat 
0.51 41.82 36.87 90.62 64.04 98.80 
Driller 
2.58 63.51 56.01 99.01 84.86 99.80 
Duck 
0 
27.23 
5.56 
70.70 32.65 89.39 
Eggbox 
8.9 
69.58 24.61 81.31 48.41 98.31 
Glue 
0 
80.02 14.18 89.00 26.94 97.20 
Holepuncher 0.30 42.63 18.23 85.54 38.75 96.29 
Iron 
8.86 74.97 59.26 98.88 88.31 99.39 
Lamp 
8.20 71.11 57.64 98.85 81.03 99.62 
Phone 
0.18 47.74 35.55 91.07 61.22 98.85 
Average 
2.42 55.95 31.65 88.25 54.29 96.78 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The authors do not report results without refinement, however they provided us with the accuracy numbers reported in Table 2. 2 The authors were not able to provide their accuracy numbers without refinement for this metric, but made their code publicly available. We ran their code with provided pretrained models to obtain the 6D pose errors.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This it is not explicitly stated in [25], but the authors confirmed this to us in private email communication.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported in part by the Swiss National Science Foundation. We would like to thank Mahdi Rad, Vincent Lepetit, Wadim Kehl, Fabian Manhardt and Slobodan Ilic for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning 6D Object Pose Estimation Using 3D Object Coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Uncertainty-Driven 6D Pose Estimation of Objects and Scenes from a Single RGB Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3D Textureless Object Detection and Tracking: An Edge-Based Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">I</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">RGB-D Object Pose Estimation in Unstructured Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">I</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="595" to="613" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The MOPED Framework: Object Recognition and Pose Estimation for Manipulation. The International Journal of Robotics Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1284" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The PASCAL Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimodal Templates for RealTime Detection of Texture-less Objects in Heavily Cluttered Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model Based Training, Detection and Pose Estimation of Texture-less 3D Objects in Heavily Cluttered Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Comparing images using the Hausdorff distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Klanderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Rucklidge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="850" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SSD-6D: Making RGB-Based 3D Detection and 6D Pose Estimation Great Again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Learning of Local RGB-D Patches for 3D Object Detection and 6D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PoseNet: A Convolutional Network for Real-Time 6-DOF camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Large-Scale Hierarchical Multi-View RGB-D Object Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Scalable Tree-Based Approach for Joint Object and Pose Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Monocular Model-Based 3D Tracking of Rigid Objects: A Survey. Foundations and Trends in Computer Graphics and Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Epnp: An Accurate O(n) Solution to the PnP Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="166" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Robustly Aligning a Shape Model and Its Application to Car Alignment of Unknown Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1860" to="1876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast Directional Chamfer Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SSD: Single Shot Multibox Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fitting parameterized three-dimensional models to images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="441" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">3D Pose Regression using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Global Hypothesis Generation for 6D Object Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fast Single Shot Detection and Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ammirato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Car make and model recognition using 3d curve alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hsiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">YOLO9000: Better, Faster, Stronger. CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discriminatively trained templates for 3d object detection: A real time scalable approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rios-Cabrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3d Object Modeling and Recognition Using Local AffineInvariant Image Descriptors and Multi-View Spatial Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rothganger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="259" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multiview 6D Object Pose Estimation and Camera Motion Planning using RGBD Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Kasaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Render for CNN: Viewpoint Estimation in Images Using CNNs Trained with Rendered 3D Model Views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Viewpoints and Keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stable Real-Time 3D Tracking Using Online and Offline Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pose Tracking from Natural Features on Mobile Phones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Reitmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mulloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schmalstieg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMAR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00199</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A dynamic programming approach for fast and robust object pose recognition from range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Penate-Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Combined Holistic and Local Patches for Recovering 6D Object Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Single Image 3D Object Detection and Pose Estimation for Grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brahmbhatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lecce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
