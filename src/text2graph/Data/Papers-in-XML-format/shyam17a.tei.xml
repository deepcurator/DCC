<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attentive Recurrent Comparators</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambedkar</forename><surname>Dukkipati</surname></persName>
						</author>
						<title level="a" type="main">Attentive Recurrent Comparators</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Rapid learning requires flexible representations to quickly adopt to new evidence. We develop a novel class of models called Attentive Recurrent Comparators (ARCs) that form representations of objects by cycling through them and making observations. Using the representations extracted by ARCs, we develop a way of approximating a dynamic representation space and use it for oneshot learning. In the task of one-shot classification on the Omniglot dataset, we achieve the state of the art performance with an error rate of 1.5%. This represents the first super-human result achieved for this task with a generic model that uses only pixel information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Utilizing the success and the potential of Deep Neural Networks to solve hard Artificial Intelligence tasks requires neural models that are capable of performing rapid learning <ref type="bibr" target="#b13">(Lake et al., 2016)</ref>. For models to embody such rich learning capabilities, we believe that a crucial characteristic will be the employment of dynamic representations -representations that are formed by observing a growing and continually evolving set of features. We call the space that is formed by such evolving representations the dynamic representation space.</p><p>In this paper, we present a novel model for one-shot learning that utilizes a crude approximation of such a dynamic representation space. This is done by constructing the representation space lazily and relative to a particular (test) sample every time. For the purpose of producing such relative representations, we develop a novel class of models called Attentive Recurrent Comparators (ARCs).</p><p>We first test ARCs across many tasks that require assessment of visual similarity. We find that ARCs that do not use any convolutions show comparable performance to Deep Convolutional Neural Networks on challenging datasets like CASIA WebFace and Omniglot. Though dense ARCs are as capable as ConvNets, a combination of both ARCs and convolutions (ConvARCs) produces much more superior models. In the task of estimating the similarity of two characters from the Omniglot dataset, ARCs and Deep ConvNets both achieve about 93.4% accuracy, whereas ConvARCs achieve 96.10% accuracy. In the task of face verification on the CASIA Webface dataset, ConvARCs achieved 81.73% accuracy surpassing the 79.48% accuracy achieved by a CNN baseline considered.</p><p>We then use ARCs as a means for developing a lazy, relative representation space and use it for one-shot learning. On the challenging Omniglot one-shot classification task, our model achieved an accuracy of 98.5%, significantly surpassing the current state-of-the-art set by all other methods. This is also the first super-human result achieved for this task with a generic model that uses only pixel information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Comparing Objects</head><p>ARCs are inspired by our interpretation of how humans generally compare a set of objects. When a person is asked to compare two objects and estimate their similarity, the person does so by repeatedly looking back and forth between the two objects. With each glimpse of the object, a specific observation is made. These observations which are made in both objects are then cumulatively used to come to a conclusion about their similarity. A crucial characteristic of this process is that new observations are made conditioned on the previous context that has been investigated so far by the observer. The observation and it's contextual location are all based on intermediate deductions -deductions that are themselves based on the observations made so far in the two objects. A series of such guided observations and their entailing inferences are accumulated to form a final the judgement on their similarity. We will refer to how humans compare objects as the human way.</p><p>In stark contrast to this, current similarity estimating systems in Deep Learning are analogues of the Siamese sim-ilarity learning system <ref type="bibr" target="#b2">(Bromley et al., 1993)</ref>. In this system, a fixed set of features is detected in both the objects. The two objects are compared based on mutual agreement of the detected features. More concretely, comparison between two objects in this system consists of measuring the distance between their vector embeddings or representations. A neural network that is specifically trained to detect the most salient features in an object for a task defines the object to embedding mapping. Detection of features in one object is independent of the features present in the other object.</p><p>There is a major underlying difference between the human approach discussed above and the siamese approach to the problem. In the human way, the information from the two objects is fused from the very beginning and this combined information primes the subsequent steps in comparison. There are multiple lookups on each of the objects and each of these lookups are conditioned on the observations of both the objects so far. In the siamese way, when the embeddings are compared the information fuses mostly at an abstract level and only in the last stage.</p><p>Inspired by the human way, we develop an end-to-end differentiable model that can learn to compare objects called Attentive Recurrent Comparators (ARCs).</p><p>Fundamentally, the excellent performance of ARCs shows the value of "early fusion" of information across the context and the value of dynamic representations. Further, it also gives merit to the view that attention and recurrence together can be as good as convolutions in a few special cases.</p><p>Finally, the superior similarity learning capability of ARCs can be of independent interest as an alternative to siamese neural networks for tasks such as face recognition and voice verification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Attentive Recurrent Comparators</head><p>Our ARC model is essentially an algorithmic imitation of the human way discussed in Section 1.1 and built with Deep Neural Networks. Using attention and recurrence, an ARC makes an observation in one object conditioned on the observations made so far in both objects. The exposition of an ARC model that can compare two images and judge their similarity is given below. But it can be trivially generalised to more images or other modalities.</p><p>The model consists of a recurrent neural network controller and an attention mechanism that takes in a specially constructed presentation sequence as the input. Given two images {x a , x b }, we alternate between the two images for a finite number of presentations of each image to form the presentation sequence x a , x b , x a , x b , ..., x a , x b . The model <ref type="figure">Figure 1</ref>. The abstract graph of an ARC comparing two images. The controller which is an RNN primes the whole process. The two images are alternatively and repeatedly attended to. At each time-step the glimpse taken from the image is based on the attention parameters Ωt which is calculated using the previous state of RNN ht−1 by projecting it with Wg. The glimpse obtained Gt and the previous state ht−1 together used to update the state of controller to ht.</p><p>repeatedly cycles through both the images, attending to one image at one time-step.</p><p>For time-step t the image presented is given by:</p><formula xml:id="formula_0">I t ←− x a if t % 2 is 0 else x b</formula><p>The attention mechanism focuses on a specific region of the image current image I t to get the glimpse G t .</p><formula xml:id="formula_1">G t ←− attend(I t , Ω t )</formula><p>where</p><formula xml:id="formula_2">Ω t = W g h t−1</formula><p>attend(.) is the attention mechanism that acts on image I t (described in the Section 2.1). Ω t are the attention glimpse parameters which specify the location and size of the attention window. At each step, we use the previous hidden state of the RNN controller h t−1 to compute Ω t . W g is the projection matrix that maps the hidden state to the required number of attention parameters.</p><p>Next, both the glimpse and previous hidden state are utilized to form the next hidden state. If we make g glimpses (or observations) of each image, the hidden state of the RNN controller at the final time-step h T = h 2g can then be used as the relative representation of x a with respect to x b or vice versa. Note that I t for some t alternates between x a and x b , while the rest of the equations are exactly the same for all time steps.</p><formula xml:id="formula_3">h t ←− RN N (G t , h t−1 ) RN N (.)</formula><p>We arrived at the iterative attention paradigm after trying out many approaches to attend to multiple images at once on a few toy datasets. Other approaches for early fusion like attending to both images in the same time-step or having 2 controllers with shared weights failed or had poor empirical performance. Iteratively attending to the inputs is more computationally efficient, scalable and more consistent statistically than the other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Attention Mechanism</head><p>The attention mechanism we used is incrementally derived from zoom-able and differentiable image observation mechanism of <ref type="bibr">DRAW Gregor et al. (2015)</ref>. The attention window is defined by an N ×N 2D grid of Cauchy kernels. We found that the heavy tail of the Cauchy curve alleviates some of the vanishing gradient issues and it also increases the speed of training.</p><p>The grid's location and size is defined based on the glimpse parameters. The N × N grid of kernels is placed at (x, y) on the S × S image, with the central Cauchy kernel being located at (x, y). The elemental square in the grid has a side of length δ. The glimpse parameter set Ω t is unpacked to get Ω t → ( x, y, δ). x, y and δ are computed from x, y and, δ using the following transforms:</p><formula xml:id="formula_4">x = (S − 1) ( x+1) 2 y = (S − 1) ( y+1) 2 δ = S N (1 − | δ|) γ = e 1−2| δ|</formula><p>The location of a i th row, j th column's Cauchy kernel in terms of the pixel coordinates of the image is given by:</p><formula xml:id="formula_5">µ i X = x + (i − (N + 1)/2) δ µ j Y = y + (j − (N + 1)/2) δ</formula><p>The horizontal and vertical filterbank matrices are then calculated as:</p><formula xml:id="formula_6">F X [i, a] = 1 Z X πγ 1 + a−µ i X γ 2 −1 F Y [j, b] = 1 Z Y πγ 1 + b−µ j Y γ 2 −1</formula><p>Z X and Z Y are normalization constants such that they make</p><formula xml:id="formula_7">Σ a F X [i, a] = 1 and Σ b F X [j, b] = 1</formula><p>Final result of attention on an image is given by:</p><formula xml:id="formula_8">attend(I t , Ω t ) = F Y I t F T X</formula><p>attend thus gets an N × N patch of the image, which is flattened and used in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Use of Convolutions</head><p>As seen in the experimental sections that follow, use of convolutional feature extractors gave a significant boost in performance. Given an image, the application of several layers of convolution produces a 3D solid of activations (or a stack of 2D feature maps). Attention over this corresponds to applying the same 2D attention (described in Section 2.1 above) over the entire depth of the 3D feature map. The attended sub-solid is then flattened and used as the glimpse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dynamic Representations and One-shot Classification</head><p>One-shot learning requires learning models to be at the apotheosis of data efficiency. In the case of one-shot classification, only a single example of each individual class is given and the model is expected to generalise to new samples of the same class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dynamic Representations</head><p>Deep Neural Networks learn useful representations of objects from data. Representation of a sample is computed by identifying a fixed set of features in it, and these features are learnt from scratch and are purely based on data provided during training. In the end, a trained neural network can be interpreted as being composed of two componentsa function that maps the input sample to a point in representation space and a classifier that learns a decision boundary in this representation space.</p><p>Rapid learning expects that this representation space to be dynamic -representations should change with newly found data. All features that form a good representation aren't known during initial learning and entirely new concepts with never-before-seen features should be expected. Ideally, the entire representation space should change when the new concept is introduced. This would enable the assimilation of new concepts in conjunction with the old con-cepts. One way of training such systems is to have a metalearning system where the model is trained to represent entities in space (rather than being trained to represent an entity) <ref type="bibr" target="#b18">(Schaul &amp; Schmidhuber, 2010)</ref>. Deep Learning research in this direction recently <ref type="bibr" target="#b17">(Santoro et al., 2016)</ref> has explored developing complex models that are trained in an end-to-end manner. But empirically, we found that such top-down hierarchical models are difficult to train, suffer from reduced supervision and require specially constructed datasets.</p><p>However, there is another alternative strategy that could be employed as crude approximation of this ideal scenario. This involves lazily developing a representation space that is conditioned on the test sample only at inference time. Until then, all samples presented to the model are just stored as-is in a repository. When the test sample is given, we compare this sample with every other sample in our repository using ARCs to form a relative representation of each sample (the representation being the final hidden state of the recurrent controller). In this relative representation space, which is relative to a test sample, we use a trained classifier that can identify the most similar sample pair, given the entire context of relative representation space. This relative representation space is dynamic as it changes relative to the test sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">One-shot Learning Models</head><p>The standard one-shot classification setup consists of a support set and a test sample. In an one-shot learning episode, the support set containing a single example of each class is first provided to the model. Next, a test sample is given and the model is expected to make its classification prediction. Finally, the classification accuracy is calculated based on all the predictions. We developed the following two models with ARCs for this task:</p><formula xml:id="formula_9">3.2.1. NAIVE ARC MODEL</formula><p>This is a trivial extension of ARCs for used for the verification task. A test sample is compared against all the images in the support set. It is matched to the sample with maximum similarity and the corresponding class is the prediction of the model. Here, we are reducing the relative representations to similarity scores directly. The entire context of the relative representation space is not incorporated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">FULL CONTEXT ARC</head><p>This model incorporates the full knowledge of the relative representation space generated before making a prediction. While Naive ARC model is simple and efficient, it does not incorporate the whole context in which our model is expected to make the decision of similarity. When the test sample is being compared with all support samples, the comparisons are all independently done.</p><p>It is highly desirable to have a 20-way ARC, where each observation is conditioned on the all images in the background set. Unfortunately, such a model is not practical. This would require maintaining a lot of context in the controller state. But, scaling up the controller memory incurs a huge (quadratic) parameter burden. So instead, we use a hierarchical setup, which decomposes the comparisons to be at two levels -first local pairwise comparison and a second global comparison. We found that this model reduces the information that has to be crammed in the controller state, while still providing sufficient context.</p><p>As with the Naive method, we compare test sample from evaluation set with each image from support set in pairs.</p><p>But instead of emitting a similarity score immediately, we process the relative representations of each comparison. Relative representations are the final hidden state of the controller when the test image T is being compared to image S j from the support set: e j = h L T,Sj . These embeddings are further processed by a Bi-Directional LSTM layer. This merges the information from all comparisons, thus providing the necessary context before prediction. The approach taken here is very similar to Matching Networks <ref type="bibr" target="#b21">(Vinyals et al., 2016)</ref>, but it is slightly more intuitive and provides superior results.</p><formula xml:id="formula_10">c j = [ −−−−→ LST M (e j ); ←−−−− LST M (e j ) ] ∀j ∈ [1, 20]</formula><p>Each embedding is mapped to a single score s j = f (c j ), where f (.) is an affine transform followed by a nonlinearity. The final output is the normalized similarity with respect to all similarity scores.</p><formula xml:id="formula_11">p j = sof tmax(s j ) ∀j ∈ [1, 20]</formula><p>This whole process is to make sure that we adhere to the fundamental principle of Deep Learning, which is to optimise objectives that directly reflect the task. The softmax normalisation allows for the expression of relative similarity rather than absolute similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first detail the analysis done to better understand the empirical functioning of ARCs, both qualitatively and quantitatively. We then benchmark ARCs on standard similarity learning tasks on two datasets and present the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model Analysis</head><p>For the analysis presented below, we use the simple ARC model described in Section 2 trained for the verification (or similarity learning) task on the Omniglot dataset. The ver-(a) It can be seen that the two characters look very similar in their stroke pattern and differ only in their looping structure. ARC has learnt to focus on these crucial aspects.</p><p>(b) ARC parses over the characters in a left to right, top to bottom fashion. Finally, it ends up focussing in the region where the first character has a prolonged downward stroke, whereas the second one does not. ification task is a binary classification problem wherein the model is trained to predict whether the 2 drawings provided are of the same character or not.</p><p>The final hidden state of the RNN controller h T is used to output at a single logistic neuron that estimates the probabilty of similarity. The particular model under consideration has an LSTM controller <ref type="bibr" target="#b10">(Hochreiter &amp; Schmidhuber, 1997)</ref> with forget gates <ref type="bibr" target="#b7">(Gers et al., 2000)</ref>. The number of glimpses per image was fixed to 8, thus making the total number of recurrent steps 16. 32 × 32 greyscale images of characters were used and the attention glimpse resolution of 4 × 4 was used. Similar/dissimilar pairs of character drawings were randomly chosen from within the same language to make the task more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">OMNIGLOT DATASET</head><p>Omniglot is a dataset by <ref type="bibr" target="#b12">(Lake et al., 2015)</ref> that is specially designed to compare and contrast the learning abilities of humans and machines. The dataset contains handwritten characters of 50 languages (alphabets) with 1623 total characters. The dataset is divided into a background set and an evaluation set. Background set contains 30 alphabets (964 characters) and only this set should be used to perform all learning (e.g. hyper-parameter inference or feature learning). The remaining 20 alphabets are for pure evaluation purposes only. Each character is a 105 × 105 greyscale image. There are only 20 samples for each character, each drawn by a distinct individual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">QUALITATIVE ANALYSIS</head><p>ARCs tend to adopt a left to right parsing strategy for most pairs, during which the attention window gradually reduces in size. As seen in Figures 2(a) and 2(b), the observations made by ARC in one image are definitely being conditioned on the observations in the other image. We also frequently encountered cases wherein the attention window, would end up focusing on a blank region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">QUANTITATIVE ANALYSIS</head><p>We performed simple yet insightful ablation studies to understand ARC's dynamics. The ARC accumulates information about both the input images by a series of attentive observations. To see how the information content varied with observations, we trained 8 separate binary classifiers to classify images as being similar or not based on hidden states of the LSTM controller at every even time-step. The performance of these classifiers is summarized in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Since the ARC has an attention window of only 4×4 pixels, it can barely see anything in the first time step, where its attention is spread throughout the whole image. With more glimpses, finer observations bring in more precise information and the recurrent transitions make use of this knowledge, leading to higher accuracies. We also used the 8 binary classifiers to study how models confidence grows with more glimpses and such examples are provided in <ref type="figure">Figure 3</ref>.  <ref type="bibr" target="#b22">(Zagoruyko &amp; Komodakis, 2016)</ref> which are the current state of the art models in image classification. Wide ResNets used contain 4 blocks of convolutional feature extractors. ConvARC models also used Wide Resnets for feature extraction but with one less block of convolutions. We used moderate data augmentation consisting of translation, flipping, rotation and shearing, which (a) ARC is very unsure of similarity at the beginning. But at 5th glimpse (4th column), the attention goes over the region where there are strokes in the first image and no strokes in the second one resulting in dropping of the score.</p><p>(b) Initially ARC is unsure or thinks that the characters are similar. But towards the end, at 6th glimpse (5th column), the model focusses on the region where the connecting strokes are different. The similarity score drops and with more "ponder", it falls down significantly. <ref type="figure">Figure 3</ref>. Attention windows over time and instantaneous predictions from independent binary classifiers. The first glimpse is omitted as it covers the whole image. In the graph: x-axis: glimpse number, y-axis: similarity score. The red line is the decision threshold, above which the images are considered to be similar. Both of the cases above are examples of a dissimilar pair.</p><p>we found to be critical for training ARC models (WRNs also were trained with the same augmentation). Hyper parameters were set for reasonable values for all our ARC models and no hyper-parameter tuning of any kind was employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">OMNIGLOT</head><p>The same exact model used in the previous section was used for this comparison as well. The data split up of the Omniglot dataset used for this comparison is different from the above: 30 alphabets were used for training, 10 for validation and 10 for testing (this was in order to be comparable to the ConvNets in <ref type="bibr">(Koch et al.)</ref>).The results are aggregated in <ref type="table" target="#tab_2">Table 2</ref>. Our simple ARC model without using any convolutional layers obtains a performance that matches a AlexNet style 6 layer Deep Convnet. Using convolutional feature extractors, ARCs outperform the Wide ResNet based Siamese ConvNet baselines, even the ones containing an order of magnitude more parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">CASIA WEBFACE</head><p>CASIA Webface is the largest public repository of faces consisting of 494,414 distinct images of over 10 thousand people. We split the data as follows: Training set: 70% (7402 people), validation set: 15% (1586 people) and Test set: 15% (1587 people). The images were downscaled to 32x32 pixels and an attention window of 4x4 pixels was used. The rest of the architecture is same as the Omniglot  <ref type="bibr">ET AL.)</ref> 93.42% SIAMESE RESNET (D=24, <ref type="bibr">W=1)</ref> 93.47% SIAMESE RESNET (D=30, <ref type="bibr">W=2)</ref> 94.61% SIAMESE RESNET (D=60, <ref type="bibr">W=4)</ref> 93.57% ARC 93.31% CONVARC 96.10% model. Results are tabluated in <ref type="table" target="#tab_3">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">One Shot Classification</head><p>One-shot classification on the Omniglot dataset is a very challenging task as most Deep Learning systems do not work well on this dataset. <ref type="bibr" target="#b12">(Lake et al., 2015)</ref> developed a dedicated system for such rapid knowledge acquisition called Bayesian Programming Learning, which surpasses human performance and is the current state of the art method.</p><p>The details of the Omniglot dataset are given in Section 4.1.1 . One-shot classification task on this dataset is setup as follows: from a randomly chosen alphabet, 20 characters are chosen which becomes the support set classes. One character among these 20 becomes the test character. 2 drawers are chosen, one each for the support set drawings and the test character drawing. The task is to match the test drawing to the correct character's class in the support set.</p><p>Assigning an image to one of the 20 characters results in a 20-way, 1-shot classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Baselines and Other Methods</head><p>We compare the two models discussed in Section 3.2 with other methods in literature: starting from the simplest baseline of k-Nearest Neighbours to the latest meta-learning methods. The training and evaluation practices are nonconsistent and the two main threads of variation are detailed below.</p><p>Across Alphabets: Many papers recently, like Matching Networks <ref type="bibr" target="#b21">(Vinyals et al., 2016)</ref> and MANNs <ref type="bibr" target="#b17">(Santoro et al., 2016)</ref> have used 1200 chars for the background set (instead of 964 specified by <ref type="bibr" target="#b12">(Lake et al., 2015)</ref>). The remaining 423 characters are used for testing. Most importantly, the characters sampled for both training and evaluation are across all the alphabets in the training set.</p><p>Within Alphabets: This corresponds to standard Omniglot setting where characters are sampled within an alphabet and only the 30 background characters are used for training and validation.</p><p>The across alphabet task is much simpler as it is easy to distinguish characters belonging to different languages, compared to distinguishing characters belonging to the same language.</p><p>There are large variations in the resolution of the images used as well. The Deep Siamese Network of Koch et al. uses 105x105 images and thus not directly comparable to out model, but we include it as it is the current best result using deep neural nets. The performance of MANNs in this standard setup is interpreted from the graph in the paper as the authors did not report it. It should also be noted that Bayesian Program Learning (BPL) <ref type="bibr" target="#b12">(Lake et al., 2015)</ref> incorporates human stroke data into the model. Lake et al estimate the human performance to be at 95.5%.</p><p>Results are presented in <ref type="table" target="#tab_4">Table 4</ref> and 5. Our ARC models outperform all previous methods according to both of the testing protocols and establish the corresponding state of the art results.  It uses a testing protocol that is very similar to Omniglot. The dataset consists of 60,000 colour images of size 84×84 with 100 classes of 600 examples each. As with the original paper, we used 80 classes for training and tested on the remaining 20 classes. We report results on 5-way one-shot task in <ref type="table" target="#tab_6">Table 6</ref>, which is a one-shot learning with 5 classes at a time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>Deep Neural Networks <ref type="bibr" target="#b20">(Schmidhuber, 2015</ref><ref type="bibr" target="#b15">) (LeCun et al., 2015</ref> are very complex parametrised functions which can be adapted to have the required behaviour by specifying a suitable objective function. Our overall model is a simple combination of the attention mechanism and recurrent neural networks (RNNs).</p><p>It is known that attention brings in selectivity in processing information while reducing the processing load <ref type="bibr" target="#b5">(Desimone &amp; Duncan, 1995)</ref>. Attention and (Recurrent) Neural Networks were combined in <ref type="bibr" target="#b19">Schmidhuber &amp; Huber (1991)</ref> to learn fovea trajectories. Later attention was used in conjunction with RBMs to learn what and where to attend in <ref type="bibr" target="#b14">Larochelle &amp; Hinton (2010)</ref> and in <ref type="bibr" target="#b4">Denil et al. (2012)</ref>.</p><p>Hard Attention mechanism based on Reinforcement Learning was used in <ref type="bibr" target="#b16">Mnih et al. (2014)</ref> and further extended to multiple objects in <ref type="bibr" target="#b0">Ba et al. (2014)</ref>; both of these models showed that the computation required at inference is significantly less compared to highly parallel Convolutional Networks, while still achieving good performance. A soft or differentiable attention mechanisms have been used in <ref type="bibr" target="#b8">Graves (2013)</ref>. A specialised form of location based soft attention mechanism, well suited for 2D images was developed for the DRAW architecture <ref type="bibr" target="#b9">(Gregor et al., 2015)</ref>, and this forms the basis of our attention mechanism in ARC.</p><p>A survey of the methods and importance of measuring similarity of samples in Machine Learning is presented in <ref type="bibr" target="#b1">Bellet et al. (2013)</ref>. With respect to Deep Learning methods, the most popular architecture family is that of Siamese Networks <ref type="bibr" target="#b2">(Bromley et al., 1993)</ref>. The energy based derivation of the same is presented in <ref type="bibr" target="#b3">Chopra et al. (2005)</ref>.</p><p>A bayesian framework for one-shot visual recognition was presented in <ref type="bibr" target="#b6">Fe-Fei et al. (2003)</ref>. <ref type="bibr" target="#b12">Lake et al. (2015)</ref> extensively study one-  <ref type="bibr" target="#b21">(Vinyals et al., 2016)</ref> and Memory Augmented Neural Networks <ref type="bibr" target="#b17">(Santoro et al., 2016)</ref> are other approaches to perform continual or meta learning in the low data regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Future Work</head><p>We presented a model that uses attention and recurrence to cycle through a set of images repeatedly and estimate their similarity. We showed that this model is not only viable but is also much better than the popular siamese neural networks in wide use today in terms of performance and generalization. We showed the value of having dynamic representations and presented a novel way of approximating it. Our main result is in the task of one-shot classification on the Omniglot dataset, where we achieved state of the art performance surpassing human performance using only raw pixel data.</p><p>Though presented in the context of images, ARCs can be used for any modality. There are innumerable ways to extend ARCs. Better attention mechanisms, higher resolution images, careful hyper-parameter tuning, more complicated controllers etc ., can be employed to achieve better performance. However, one potential downside of this model is that due to sequential execution of the recurrent core and by the very design of the model, it might be more computationally expensive than distance metric methods.</p><p>More interesting directions would involve developing more complex architectures using this bottom-up, lazy approach to solve even more challenging AI tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Attention windows over time when comparing the two Omniglot characters. The top row has the first image and the bottom row has the second. Each column represents a glimpse step. (a) Comparing two dissimilar characters and (b) Comparing two similar characters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Glimpses</figDesc><table>per image versus classification accuracy of 
ARC. Out of the 50 alphabets provided in the Omniglot dataset, 
30 were used for training and validation and the last 20 for testing 

GLIMPSES ACCURACY (TEST SET) 

1 
58.2% 
2 
65.0% 
4 
80.8% 
6 
89.25% 
8 
92.08% 

4.2. Similarity Learning 

In this section we compare ARCs with other Deep Learning 
methods in the task of similarity learning on two datasets: 
Omniglot and CASIA WebFace Dataset. We consider 
strong convolutional baselines, which have been shown 
time and again to excel at such visual tasks. Particularly, 
we use Wide Resnets (WRNs) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Performance</figDesc><table>of ARC vs conventional methods on the 
verification task on Omniglot dataset. Wide ResNets suffixes 
specify the depth and width. Example, (d=60, w=4) means that it 
is a ResNet that 60 is layers deep with each residual block having 
a width multiplier of 4. Out of the 50 alphabets provided in the 
Omniglot dataset, 30 were used for training, 10 for validation and 
the last 10 for testing 

MODEL 
ACCURACY (TEST SET) 

SIAMESE NETWORK 
60.52% 
DEEP SIAMESE NET (KOCH </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Performance of ARC vs conventional methods on the verification task on CASIA Webface dataset. Wide ResNets suf- fixes notation is same as used in Table 2.</figDesc><table>MODEL 
ACCURACY (TEST SET) 

SIAMESE RESNET (D=36, W=4) 
79.48% 
ARC 
72% 
CONVARC 
81.73% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 .</head><label>4</label><figDesc>One-shot classification accuracies of various methods and our ARC models on Omniglot dataset -Across Alphabets</figDesc><table>MODEL 
ACCURACY 

KNN 

26.7% 
CONV SIAMESE NETWORK 
88.1% 
MANN 
≈ 60% 
MATCHING NETWORKS 
93.8% 
NAIVE ARC 
90.30% 
NAIVE CONVARC 
96.21% 
FULL CONTEXT CONVARC 
97.5% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 .</head><label>5</label><figDesc>One-shot classification accuracies of various methods and our ARC models on Omniglot dataset -Within AlphabetsRecently, Vinyals et al. (2016) introduced a one-shot learn- ing benchmark based off of the popular ImageNet dataset.</figDesc><table>MODEL 
ACCURACY 

KNN 

21.7% 
SIAMESE NETWORK 
58.3% 
DEEP SIAMESE NETWORK (KOCH ET AL.) 
92.0% 
HUMANS 
95.5% 
BPL 
96.7% 
NAIVE ARC 
91.75% 
NAIVE CONVARC 
97.75% 
FULL CONTEXT CONVARC 
98.5% 

5.2. miniImageNet 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 6 .</head><label>6</label><figDesc>5 way one-shot Classification accuracies of various meth- ods and our ARC models -miniImageNet</figDesc><table>MODEL 
ACCURACY 

RAW PIXELS W/ COSINE SIMILARITY 
23.0% 
BASELINE CLASSIFIER 
38.4% 
MATCHING NETWORKS 
46.6% 
NAIVE CONVARC 
49.14% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head></head><label></label><figDesc>shot Learning and present a novel probabilistic framework called Bayesian Program Learn- ing (BPL) for rapid learning. They have also released the Omniglot dataset, which has become a testing ground for one-shot learning techniques. Recently, many Deep Learning methods have been developed to do one-shot learning: Koch et al. use Deep Convolutional Siamese Networks for performing one-shot classification. Match- ing Networks</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Akshay Mehrotra, Gaurav Pandey and Siddharth Agrawal for their extensive support and feedback while developing the ideas in this work. We would like to thank Soumith Chintala for his feedback on this manuscript and the idea and Ankur Handa for helping us with the CASIA dataset. We would also like to thank Sanyam Agarwal, Wolfgang Richter, Shreyas Karanth and Aadesh Bagmar for their valuable feedback on the manuscript. Finally, we would like to thank the anonymous reviewers for helping us significantly improve the quality of this paper.</p><p>Authors acknowledge financial support for the "CyberGut" expedition project by the Robert Bosch Centre for Cyber Physical Systems at the Indian Institute of Science, Bengaluru.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7755</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A survey on metric learning for feature vectors and structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaury</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Sebban</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.6709</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Signature verification using a siamese time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">W</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Léon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Isabelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="669" to="688" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sumit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning where to attend with deep architectures for image tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2151" to="2184" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Neural mechanisms of selective visual attention. Annual review of neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Desimone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duncan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="193" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A bayesian approach to unsupervised one-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fe-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1134" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ivo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04623</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Building machines that learn and think like people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tomer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Samuel</surname></persName>
		</author>
		<idno>abs/1604.00289</idno>
		<ptr target="http://arxiv.org/abs/1604.00289" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to combine foveal glimpses with a third-order boltzmann machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1243" to="1251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volodymyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">One-shot learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06065</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schmidhuber</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jürgen. Metalearning. Scholarpedia</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">4650</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to generate artificial fovea trajectories for target detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Neural Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">01n02</biblScope>
			<biblScope unit="page" from="125" to="134" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04080</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Wide residual networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>abs/1605.07146</idno>
		<ptr target="http://arxiv.org/abs/1605.07146" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
