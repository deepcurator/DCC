<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Transfer: A Hierarchical Deep Convolutional Neural Network for Fast Artistic Style Transfer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Adobe Research</orgName>
								<address>
									<settlement>San Francisco</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Oxholm</surname></persName>
							<email>oxholm@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Adobe Research</orgName>
								<address>
									<settlement>San Francisco</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
							<email>yfwang@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Transfer: A Hierarchical Deep Convolutional Neural Network for Fast Artistic Style Transfer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Style transfer, or to repaint an existing photograph with the style of another, is considered a challenging but interesting problem in arts. Recently, this task has become an active topic both in academia and industry due to the influential work by Gatys et al. <ref type="bibr" target="#b7">[8]</ref>, where a pre-trained deep learning network for visual recognition is used to capture both style and content representations, and achieves visually stunning results. Unfortunately, the transfer run time is prohibitively long because of the online iterative optimization procedure. To resolve this issue, a feed-forward network can be trained offline with the same loss criterion to generate stylized results that are visually close (but still somewhat inferior). In this way, only one single inference pass of the feed-forward network is needed at the application time. This results in a computational algorithm that is hundreds of times faster <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Though past work creates visually pleasing results for many different types of artworks, two important drawbacks stand out: (1) The current feed-forward networks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> are trained on a specific resolution of the style image, so deviating from that resolution (bigger or smaller) results in a scale mismatch. For example, applying a model trained with a style guide of size 256 on higher-resolution images would generate results whose texture scale is smaller than that of the artistic style, and (2) Current networks often fail to capture small, intricate textures, like brushwork, of many kinds of artworks on high-resolution images. While it has been shown that these feed-forward networks function quite well on artworks with abstract, large-scale textures and easily discernible strokes, e.g., The Starry Night by Vincent van Gogh, artistic styles are much more encompassing than what has been demonstrated. That is, different artistic styles may be characterized by exquisite, subtle brushes and strokes, and hence, our observations are that results of these style-transfer networks are often not satisfactory for a large variety of artistic styles.</p><p>In this paper, we propose a novel hierarchical deep convolutional neural network architecture for fast style transfer. Our contribution is fourfold: (1) We introduce a hierarchical network and design an associated training scheme that is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) Our hierarchical training scheme and end-to-end CNN network architecture allow us to combine multiple models into one network to handle increasingly larger image sizes; (3) Instead of taking only RGB color channels into consideration, our network utilizes representations of both color and luminance channels for style transfer; and (4) Through experimentation, we show that our hierarchical style transfer network can better capture both coarse and intricate texture patterns. Our hierarchical style transfer network is trained with multiple stylization losses at different scales using a mixture of modalities, so we distinguish it as multimodal transfer from the feed-forward style transfer networks with only one stylization loss <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>, which we call singular transfer. In <ref type="figure" target="#fig_0">Fig. 1</ref> we give an example that compares results from our multimodal transfer network with those from the current state-of-the-art singular transfer networks. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the advantages of multimodal transfer on learning different levels of textures, including style, color, large texture distortion and fine brushwork. Note specifically that our method can simulate more closely the brushwork of the artwork. In Sec. 4 we will show that multimodal transfer can also be used to train a combination model to stylize a single image with multiple, distinct artistic styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Understanding representations of deep neural networks. Recently, seminal work was done on understanding deep neural networks. The DeconvNet method of Zeiler and Fergus <ref type="bibr" target="#b29">[30]</ref> learns how certain network outputs are obtained by identifying which image patches are responsible for certain neural activation. Yosinski et al. <ref type="bibr" target="#b28">[29]</ref> aims to understand what computation is performed by deep networks through visualizing the internal neurons. Mahendran and Vedaldi <ref type="bibr" target="#b18">[19]</ref> inverts the image representations of certain layers to learn what information is preserved by the networks. The latter two approaches generate visualization images with an optimization procedure whose objective is for perceptual understanding of network functions. Similar optimization procedure is also adopted in other cases <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Based on the better understanding of the powerful representations of deep convoluntional networks <ref type="bibr" target="#b14">[15]</ref>, many traditional vision tasks have been addressed with much more improved outcomes. Optimization-based style transfer is one such example. Different from previous texture synthesis algorithms that are usually non-parametric methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>, Gatys et al. first proposed an optimization method of synthesizing texture images where the objective loss is computed based on the representations of a pre-trained convolutional neural network <ref type="bibr" target="#b5">[6]</ref>. This texture loss is then combined with content loss derived from Mahendran and Vedaldi <ref type="bibr" target="#b18">[19]</ref> to perform the style transfer task <ref type="bibr" target="#b7">[8]</ref>.</p><p>Feed-forward networks for image generation. The optimization-based methods for image generation are computationally expensive due to the iterative optimization procedure. On the contrary, many deep learning methods use the perceptual objective computed from a neural network as the loss function to construct feed-forward neural networks to synthesize images <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Fast style transfer has achieved great results and is receiving a lot of attention. Johnson et al. <ref type="bibr" target="#b12">[13]</ref> proposed a feed-forward network for both fast style transfer and super- resolution using the perceptual losses defined in Gatys et al. <ref type="bibr" target="#b7">[8]</ref>. A similar architecture texture net is introduced to synthesize textured and stylized images <ref type="bibr" target="#b24">[25]</ref>. More recently, Ulyanov et al. <ref type="bibr" target="#b25">[26]</ref> shows that replacing spatial batch normalization <ref type="bibr" target="#b11">[12]</ref> in the feed-forward network with instance normalization can significantly improve the quality of generated images for fast style transfer. Here we present further improvement of such style transfer algorithms to handle progressively larger images using hierarchical networks with mixed modalities. Furthermore, it allows the use of multiple, distinct styles for repainting a single input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multimodal Transfer Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Architecture and Learning Schemes</head><p>Our proposed network, which is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, is comprised of two main components: a feed-forward multimodal network and a loss network. The feed-forward multimodal network (MT Network) is a hierarchical deep residual convolutonal neural network. It consists of three subnetworks: style subnet, enhance subnet and refine subnet. These subnets are parameterized by Θ 1 , Θ 2 , and Θ 3 respectively (these parameters will be made explicit later). At a high level, the MT Network takes an image x as the input and is trained to generate multiple output imagesŷ k of increasing sizes,ŷ</p><formula xml:id="formula_0">k = f (∪ k i=1 Θ i , x) .<label>(1)</label></formula><p>These output images are then taken separately as inputs to the loss network to calculate a stylization loss for each. The total loss is a weighted combination of all stylization losses. We will show later in Sec. 3.2 the loss network and the definition of the total loss.</p><p>At test time, in order to produce the same stylization effect and correct texture scale of the artworks when applied to larger images, the MT network stylizes the image hierarchically: The input image is first resized into 256 with a bilinear downsampling layer and stylized by the style subnet, capturing the large color and texture traits of the artwork. Next the stylized result, which is the first outputŷ 1 , is upsampled into 512 and transferred to the outputŷ 2 by the enhance subnet, which enhances the stylization strength. Then it is resized back to 1024. Finally, the refine subnet removes the local pixelization artifacts and further refines the result. The high-resolution and most visually appealing resultŷ 3 is obtained after these three-stage processing. Note that while we illustrate the process using a two-level hierarchy, the same concept can be extended recursively to enable stylization of progressively larger images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Loss Functions</head><p>In this section, we first introduce the single stylization loss funtion and then present a hierarchical stylization loss function that is adopted to train our multimodal transfer network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Single Stylization Loss Function</head><p>Similar to the loss definition in previous work for fast style transfer <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>, the stylization loss is also derived from Gatys et al. <ref type="bibr" target="#b7">[8]</ref>, where a loss network (a pre-trained VGG-19 network optimized for object recognition <ref type="bibr" target="#b23">[24]</ref>) is used to extract the image representations.</p><p>Two perceptual losses are defined to measure to what extent the generated imageŷ k combines the content of the content target y c with the texture and style cues of the style target y s (see <ref type="figure" target="#fig_2">Fig. 3</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Content Loss</head><p>The content loss function is used to measure the dissimilarity betweenŷ k and y c . Let F l i (x) denote the i-th feature map in the l-th layer of the loss network applied to image x. The content loss is the squared-error loss between the two feature representations at layer l</p><formula xml:id="formula_1">L content (ŷ k , y c , l) = N l i=1 F l i (ŷ k ) − F l i (y c ) 2 2</formula><p>. <ref type="formula">(2)</ref> That is, the content loss directly compares the feature maps computed from the corresponding layers and thus is suitable for characterizing spatial content similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Texture or Style Loss Gatys et al.</head><p>propose that the correlations between feature maps in each layer of the loss network can be seen as texture representations of an image <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>. Those correlations are given by the Gram matrix, whose elements are pairwise scalar products between those feature maps:</p><formula xml:id="formula_2">G l ij (x) = F l i (x), F l j (x) .<label>(3)</label></formula><p>A set of Gram matrices G l , l ∈ L is used as the texture representations, which discard the spatial information but retain the statistic profiles of color and intensity distribution of an input image. So the texture loss function is defined as</p><formula xml:id="formula_3">L text (ŷ k , y s ) = l∈L G l (ŷ k ) − G l (y s ) 2 2 .<label>(4)</label></formula><p>Finally, the stylization loss for each outputŷ k from the MT network is defined as a weighted sum of the content loss and the texture loss</p><formula xml:id="formula_4">L S (ŷ k , y c , y s ) = αL content (ŷ k , y c ) + βL text (ŷ k , y s ),<label>(5)</label></formula><p>where α and β are the weights of the content loss and texture loss, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Hierarchical Stylization Loss Function</head><p>The multimodal transfer network can generate K output results of K increasing sizes (K = 3 in the network shown in <ref type="figure" target="#fig_1">Fig. 2)</ref>. Then a stylization loss is computed for each output resultŷ k</p><formula xml:id="formula_5">L k S (ŷ k , y k c , y k s ) = αL content (ŷ k , y k c ) + βL text (ŷ k ,</formula><note type="other">y k s ) (6) where y k c and y k s are the corresponding content target and style target, which are the input to the subnet that outputŝ y k , and are the scaled versions of the artwork y s . By training the subnets with different style scales, we control the types of artistic features that are learned for different subnets. Again, we want to emphasize that the concept can be easily extended for more layers.</note><p>Since such stylization losses are computed based on the outputs of different layers of the whole network, a total loss (e.g., a weighted combination of all stylization losses) cannot be used here to directly propagate and update the weights backward. Thus, a parallel criterion is adopted so that different stylization losses are used to back-propagate the weights for different ranges of layers. We define the hierarchical stylization loss function L H , which is a weighted sum of such stylization losses, as</p><formula xml:id="formula_6">L H = K k=1 λ k L k S (ŷ k , y k c , y k s ) ,<label>(7)</label></formula><p>where λ k is the weight of stylization loss L k S . Therefore, during the end-to-end learning on natural images x ∼ X , each subnet denoted by Θ k is trained to minimize the parallel weighted stylization losses that are computed from the latter outputsŷ i (i ≥ k) (latter means it comes later in the feed-forward direction) as in</p><formula xml:id="formula_7">Θ k = arg min Θ k E x∼X K i≥k λ i L k S (f (∪ k j=1 Θ j , x), y i c , y i s )<label>(8)</label></formula><p>In practice, suppose the general back-propagation function is denoted by f −1 , then for every iteration, the weight updates (gradients) of the subnet Θ k can be written as</p><formula xml:id="formula_8">∆Θ k = f −1 (λ k L k S ) k = K f −1 (λ k L k S , ∆Θ k+1 ) 1 ≤ k &lt; K ,<label>(9)</label></formula><p>so the weights of the current subnet Θ k are influenced by both the stylization loss at the current level L k S and the gradients of the latter subnets.</p><p>From Eq. (8), we can see that even though all those subnets are designed for different purposes, they are not totally independent. Former subnets also contribute to minimize losses of the latter. Thus, shallower CNN structure can be used for latter subnets, which saves both computing memory and running time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Architecture</head><p>One of the key drawbacks of singular transfer networks (e.g. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>) is that the scale at which the singular transfer network is trained limits the range of style details that are captured. Since it is trained with one particular scale of the style image, during training we need to choose if it learns the coarse texture or the fine brushwork. That is, it learns one at the expense of the other.</p><p>To remedy this problem, we design the hierarchical architecture where different subnets are trained with different scales of the style image to learn different levels of artistic texture cues. This design enables a test image to be transferred using different levels of the style in increasing resolutions. Furthermore, because all these subnets are combined into one network and trained hierarchically, the latter subnets are also able to enhance and refine the results from previous ones, making ours a collaborative scheme for improved efficiency and robustness.</p><p>We have experimented with several architectures that have varying levels of hierarchy and different internal structures. Here we introduce the general architecture of the network shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, which has the best stylization quality from our experience.</p><p>As stated before, the multimodal transfer network consists of three learnable subnetworks, style subnet, enhance subnet and refine subnet, each following a fixed bilinear upsampling/downsampling layer. Note that the upsampling layer between enhance subnet and refine subnet is only inserted at test time, so during training the input to refine subnet is still of size 512, which hugely reduces the required memory and speeds up the training process. The salient features of these networks are explained below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Style Subnet</head><p>Luminance-Color Joint Learning To better address the issue of preserving small intricate textures, our network utilizes representations of both color and luminance channels, because visual perception is far more sensitive to changes in luminance than in color <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b6">7]</ref>. We separate the luminance channel from the RGB color image and use two independent branches (RGB-Block and L-Block) to learn their representations distinctively. The feature maps calculated from both branches are then joined together along the depth dimension and further processed by the ensuing Conv-Block.</p><p>RGB-Block comprises three strided convolutional layers (9 × 9, 3 × 3, 3 × 3 respectively, the latter two are used for downsampling) and three residual blocks <ref type="bibr" target="#b9">[10]</ref>, while LBlock has a similar structure except that the depth of convolution is different. Conv-Block is composed of three residual blocks, two resize-convolution layers for upsampling and the last 3 × 3 convolutional layer to obtain the output RGB imageŷ. All non-residual convolutional layers are followed by instance normalization <ref type="bibr" target="#b25">[26]</ref> and ReLU nonlinearity. Part of our style subnet is designed based on the work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref>. A nearest neighbor interpolation upsampling layer and a convolutional layer called resize-convolution layer is used here instead of deconvolutions to avoid the checkerboard artifacts of generated images <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Enhance Subnet and Refine Subnet</head><p>Although the style subnet is intended to stylize the input image with large texture distortion to match that of the style guide, we have found that it is difficult to optimally adjust texture and content weights to achieve style transfer while preserving the content for a large variety of styles. Thus, we allow the style subnet to perform texture mapping with an eye toward preserving the content, and train a separate enhance subnet with a large texture weight to further enhance the stylization. <ref type="figure" target="#fig_3">Fig. 4</ref> illustrates the specific role of each subnet. Evidently, the style subnet changes both color and texture heavily, but the enhance subnet also contributes greatly to the texture mapping while adding more detail. The refine net further refines and adds more detail into the final result. Accordingly, for the sake of enhancing the stylization, we adopt a similar structure as the style subnet for the enhance subnet. The only difference is that the enhance subnet has one more convolutional layer for downsampling and one more resize-convolution layer for upsampling, which enlarges the receptive field sizes. This is needed because the input to the enhance subnet is twice larger than that to the style subnet.</p><p>Finally, the refine net consists of three convolutional layers, three residual blocks, two resize-convolution layers and one last convolutional layer to obtain the final output, which is much shallower than the style and enhance subnet. This is because from Eq. <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_7">(8)</ref> we know former subnets can also contribute to the learning tasks of the latter. Shortening the refine subnet is advantageous. It significantly reduces memory and computational complexity, which is critical for images of size 1024. Furthermore, we add an identity connection from its beginning to the end, forcing it to learn just the difference between its input and output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Training Details The MT Network was trained on a subset of the Microsoft COCO dataset <ref type="bibr" target="#b17">[18]</ref>, which contained 32,059 images (whose width and height were ≥ 480). We cropped those images and resized them to 512 × 512. Adam optimization <ref type="bibr" target="#b13">[14]</ref> was used to train models for 10,000 iterations with batch size 1. The learning rate was initially set as 1 × 10 −3 and then reduced by a factor 0.8 every 2,000 iterations. Content losses were computed at layer relu4 2 of VGG-19 and texture losses at layers relu1 1, relu2 1, relu3 1 and relu4 1 for all subnets. The content weights were all set to 1, while the texture weights depended on different styles, because a universal ratio of texture to content did not fit all artistic styles. As for the weights of stylization losses, we set λ 1 : λ 2 : λ 3 = 1 : 0.5 : 0.25. The rationale was that, during training, the parameters of former subnets were updated to incorporate the current and latter stylization losses. The latter losses should have smaller weights in order not to totally dominate the optimization process of the former subnets. Experiments revealed, however, that results were fairly robust to changes in λ k . It took around one hour to fully train a hierarchical model on an NVIDIA GTX 1080. The model size on disk was about 35MB.</p><p>Comparison among Different Singular Transfer Networks As mentioned before, singular transfer was a feedforward style-transfer network with a single stylization loss and multimodal transfer was the hierarchical network with multiple stylization losses and a mixture of modallities (color and luminance). Here we separated the style subnet from our MT network as a singular transfer network and compared it with the other state-of-the-art networks by Johnson et al. <ref type="bibr" target="#b12">[13]</ref> and Ulyanov et al. <ref type="bibr" target="#b25">[26]</ref>. All three networks were trained on images of size 256 with the same : We compared our multimodal transfer with two singular transfer networks trained with different scales of the style image, 256 and 1024 (the singular transfer networks had the same architecture and had the same number of parameters as our multimodal transfer network). All generated results were 1024 × 1024 pixels. As can be seen, there was a big texture scale mismatch issue in the results of singular transfer 256 (column (b)) that the texture scale was apparently smaller than that of the original artworks. While singular transfer 1024 (column (c)) failed to learn the texture distortion and brushwork although being rendering with the correct color. The results of multimodal transfer (column (d)) resolved those issues and managed to learn both coarse texture and fine, intricate brushwork. content to texture weights and used instance normalization. Generally speaking, our style subnet generated qualitatively comparable results. Particularly, it performed better than others on capturing texture details in some cases. In <ref type="figure" target="#fig_4">Fig. 5</ref> we gave two comparison examples. In the first example, the result of Ulyanov et al. was visibly darker than the style image in color, and the texture scale in Johnson et al's result did not match that of the style image very well. The result of our style subnet seemed better in both aspects. In the second example, comparing with the other two networks, our style subnet performed better on simulating small, detailed texture. Therefore, we chose the style subnet as a representative of singular transfer to be compared with multimodal transfer next.</p><p>Singular Transfer VS Multimodal Transfer on Highresolution Images We tested our method on numerous artistic styles. In <ref type="figure" target="#fig_5">Fig. 6</ref>, we compared our multimodal transfer network on high-resolution images (1024 × 1024) with a singular transfer network with the same number of learning weights. (More exactly, we duplicated our style subnet to a deeper network that had the same number of parameters to provide a fair comparison). Examining the results shown in <ref type="figure" target="#fig_5">Fig. 6</ref>, comparing with singular transfer, multimodal transfer results were visually more similar to the original artistic styles both in coarse texture structure and fine brushwork, while singular transfer with the style size 256 caused a texture scale mismatch that the texture scale was much smaller than that of the original artwork. Furthermore, singular transfer with style size 1024 failed to learn the distortion and fine brushwork.</p><p>Multimodal Transfer with Multiple Styles Our multimodal transfer allowed an interesting application that was not possible before: It could be trained with multiple styles such that the final stylized result fused the content of one test image, the coarse texture distortion of one style image, and the fine brushwork of another style image. Here we gave an example in <ref type="figure" target="#fig_6">Fig. 7</ref> where the model was trained with two different styles.</p><p>Processing Speed and Memory Use We compared quantitatively the speed and memory usage of our multimodal transfer network (MT Net) with other singular transfer networks (Here we used Johnson Net, the network by <ref type="bibr">Johnson et al.)</ref>. We also constructed a deep singular transfer network for comparison (called DS Net), which had the same structure as the MT Net. We took the average of the test time for 1,000 generations (excluding model loading time  As shown in <ref type="table" target="#tab_1">Table 1</ref>, although MT Net was more than twice deeper than Johnson Net, its speed and memory usage were close to those of Johnson Net (0.54s vs 0.42s, 3100 MB vs 2400 MB) when generating high-resolution images, which benefited from the hierarchical transfer procedure where most computation was done on low resolutions. The singular transfer network DS Net performed the worst even with same number of parameters. Therefore, multimodal transfer is suitable for real-world applications that usually required high image resolution, because it was able to generate results more similar to the desired artistic styles with a small cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Here we present a hierarchical training scheme (multimodal transfer) for fast style transfer to learn artistic style cues at multiple scales, including color, coarse texture structure and fine, exquisite brushwork. The scheme solves the texture scale mismatch issue and generates much more visually appealing stylized results on high-resolution images.</p><p>In the future, we plan to investigate other losses that can better capture the artistic style at different scales. We also want to explore alternate loss networks that costs less memory to extend our scheme onto much larger images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Top row: (a) The style guide is At the Close of Day by Tomas King, and (f) is the content image. (b) is the result of Gatys et al's optimization-based method. (Result size is 512 due to memory limitation of the method) (c), (d) and (e) are results generated by different feed-forward networks (all are of size 1024). Bottom row: the zoom-in display of the regions enclosed in the red boxes from the top row. As can be seen, all results are repainted with the color of the style image. However, a closer examination shows that the brush strokes are not captured well in (c) and (d). The zoom-in region in (b) is a little blurry. Comparing with the others, our multimodal transfer (e) is capable of simulating more closely the brushwork of the original artwork on high-resolution images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overall Architecture. Please see Sec. 3.1 for explanation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Loss network. Please see Sec. 3.2 for explanation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) is the content image and (b) is the style image. (c)(d)(e) show the outputs of the three subnets,ŷ1,ŷ2 andŷ3, whose sizes are 256, 512 and 1024 respectively. The third row depicts the absolute difference between: (f) the content image and the output imageŷ1, (g) the output imagesŷ1 andŷ2, and (h) the output imagesŷ2 andŷ3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison between our style subnet (b) and other singular transfer networks (c) and (d). They were tested on two styles here, Mountain No. 2 by Jay DeFeo (top) and Venus by Manierre Dawson (bottom). All results were of size 512. Notice that for the second style, we also zoomed in on the region in the red box to better compare the fine texture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6</head><label>6</label><figDesc>Figure 6: We compared our multimodal transfer with two singular transfer networks trained with different scales of the style image, 256 and 1024 (the singular transfer networks had the same architecture and had the same number of parameters as our multimodal transfer network). All generated results were 1024 × 1024 pixels. As can be seen, there was a big texture scale mismatch issue in the results of singular transfer 256 (column (b)) that the texture scale was apparently smaller than that of the original artworks. While singular transfer 1024 (column (c)) failed to learn the texture distortion and brushwork although being rendering with the correct color. The results of multimodal transfer (column (d)) resolved those issues and managed to learn both coarse texture and fine, intricate brushwork.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Multimodal transfer with two styles. The model was trained with style (a) Still Life with Skull, Leeks and Pitcher by Pablo Picasso, and (b) At the Close of Day by Tomas King. (c) was the test image, and (f) was the final stylized result that had large texture distortion from (a) and small, detailed brushwork from (b). For comparison, we gave the results transferred by the models trained on a single style in (d) and (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>).</figDesc><table>Network 
Test Time Memory Usage 

MT Net 
0.54s 
3100 MB 
Johnson Net 
0.42s 
2400 MB 
DS Net 
0.63s 
6700 MB 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Comparison results of speed and memory use when ap- plied on 1024 × 1024 images.</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast texture transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ashikhmin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="38" to="43" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to generate chairs with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1538" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image quilting for texture synthesis and transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="341" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Texture synthesis by nonparametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 1999. The Proceedings of the Seventh IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Texture synthesis using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05897</idno>
		<title level="m">Preserving color in neural artistic style transfer</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="327" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graphcut textures: image and video synthesis using graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schödl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="277" to="286" />
			<date type="published" when="2003" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Directional texture transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Symposium on Non-Photorealistic Animation and Rendering</title>
		<meeting>the 8th International Symposium on Non-Photorealistic Animation and Rendering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5188" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="http://distill.pub/2016/deconv-checkerboard/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Texture networks: Feed-forward synthesis of textures and stylized images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Foundations of vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Wandell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Sinauer Associates</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast texture synthesis using treestructured vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 27th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06579</idno>
		<title level="m">Understanding neural networks through deep visualization</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
