<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Triangle Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
							<email>zhe.gan@duke.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqun</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Triangle Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>A Triangle Generative Adversarial Network (∆-GAN) is developed for semisupervised cross-domain joint distribution matching, where the training data consists of samples from each domain, and supervision of domain correspondence is provided by only a few paired samples. ∆-GAN consists of four neural networks, two generators and two discriminators. The generators are designed to learn the two-way conditional distributions between the two domains, while the discriminators implicitly define a ternary discriminative function, which is trained to distinguish real data pairs and two kinds of fake data pairs. The generators and discriminators are trained together using adversarial learning. Under mild assumptions, in theory the joint distributions characterized by the two generators concentrate to the data distribution. In experiments, three different kinds of domain pairs are considered, image-label, image-image and image-attribute pairs. Experiments on semi-supervised image classification, image-to-image translation and attribute-based image generation demonstrate the superiority of the proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative adversarial networks (GANs) <ref type="bibr" target="#b0">[1]</ref> have emerged as a powerful framework for learning generative models of arbitrarily complex data distributions. When trained on datasets of natural images, significant progress has been made on generating realistic and sharp-looking images <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. The original GAN formulation was designed to learn the data distribution in one domain. In practice, one may also be interested in matching two joint distributions. This is an important task, since mapping data samples from one domain to another has a wide range of applications. For instance, matching the joint distribution of image-text pairs allows simultaneous image captioning and textconditional image generation <ref type="bibr" target="#b3">[4]</ref>, while image-to-image translation <ref type="bibr" target="#b4">[5]</ref> is another challenging problem that requires matching the joint distribution of image-image pairs.</p><p>In this work, we are interested in designing a GAN framework to match joint distributions. If paired data are available, a simple approach to achieve this is to train a conditional GAN model <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>, from which a joint distribution is readily manifested and can be matched to the empirical joint distribution provided by the paired data. However, fully supervised data are often difficult to acquire. Several methods have been proposed to achieve unsupervised joint distribution matching without any paired data, including DiscoGAN <ref type="bibr" target="#b6">[7]</ref>, CycleGAN <ref type="bibr" target="#b7">[8]</ref> and DualGAN <ref type="bibr" target="#b8">[9]</ref>. Adversarially Learned Inference (ALI) <ref type="bibr" target="#b9">[10]</ref> and Bidirectional GAN (BiGAN) <ref type="bibr" target="#b10">[11]</ref> can be readily adapted to this case as well. Though empirically achieving great success, in principle, there exist infinitely many possible mapping functions that satisfy the requirement to map a sample from one domain to another. In order to alleviate this nonidentifiability issue, paired data are needed to provide proper supervision to inform the model the kind of joint distributions that are desired.</p><p>This motivates the proposed Triangle Generative Adversarial Network (∆-GAN), a GAN framework that allows semi-supervised joint distribution matching, where the supervision of domain correspondence is provided by a few paired samples. ∆-GAN consists of two generators and two discriminators. The generators are designed to learn the bidirectional mappings between domains, while the discriminators are trained to distinguish real data pairs and two kinds of fake data pairs. Both the generators and discriminators are trained together via adversarial learning.</p><p>∆-GAN bears close resemblance to Triple GAN <ref type="bibr" target="#b11">[12]</ref>, a recently proposed method that can also be utilized for semi-supervised joint distribution mapping. However, there exist several key differences that make our work unique. First, ∆-GAN uses two discriminators in total, which implicitly defines a ternary discriminative function, instead of a binary discriminator as used in Triple GAN. Second, ∆-GAN can be considered as a combination of conditional GAN and ALI, while Triple GAN consists of two conditional GANs. Third, the distributions characterized by the two generators in both ∆-GAN and Triple GAN concentrate to the data distribution in theory. However, when the discriminator is optimal, the objective of ∆-GAN becomes the Jensen-Shannon divergence (JSD) among three distributions, which is symmetric; the objective of Triple GAN consists of a JSD term plus a Kullback-Leibler (KL) divergence term. The asymmetry of the KL term makes Triple GAN more prone to generating fake-looking samples <ref type="bibr" target="#b12">[13]</ref>. Lastly, the calculation of the additional KL term in Triple GAN is equivalent to calculating a supervised loss, which requires the explicit density form of the conditional distributions, which may not be desirable. On the other hand, ∆-GAN is a fully adversarial approach that does not require that the conditional densities can be computed; ∆-GAN only require that the conditional densities can be sampled from in a way that allows gradient backpropagation.</p><p>∆-GAN is a general framework, and can be used to match any joint distributions. In experiments, in order to demonstrate the versatility of the proposed model, we consider three domain pairs: image-label, image-image and image-attribute pairs, and use them for semi-supervised classification, image-to-image translation and attribute-based image editing, respectively. In order to demonstrate the scalability of the model to large and complex datasets, we also present attribute-conditional image generation on the COCO dataset <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generative Adversarial Networks (GANs)</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b0">[1]</ref> consist of a generator G and a discriminator D that compete in a two-player minimax game, where the generator is learned to map samples from an arbitray latent distribution to data, while the discriminator tries to distinguish between real and generated samples. The goal of the generator is to "fool" the discriminator by producing samples that are as close to real data as possible. Specifically, D and G are learned as</p><formula xml:id="formula_0">min G max D V (D, G) = E x∼p(x) [log D(x)] + E z∼pz(z) [log(1 − D(G(z)))] ,<label>(1)</label></formula><p>where p(x) is the true data distribution, and p z (z) is usually defined to be a simple distribution, such as the standard normal distribution. The generator G implicitly defines a probability distribution p g (x) as the distribution of the samples G(z) obtained when z ∼ p z (z). For any fixed generator</p><formula xml:id="formula_1">G, the optimal discriminator is D(x) = p(x)</formula><p>pg(x)+p(x) . When the discriminator is optimal, solving this adversarial game is equivalent to minimizing the Jenson-Shannon Divergence (JSD) between p(x) and p g (x) <ref type="bibr" target="#b0">[1]</ref>. The global equilibrium is achieved if and only if p(x) = p g (x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Triangle Generative Adversarial Networks (∆-GANs)</head><p>We now extend GAN to ∆-GAN for joint distribution matching. We first consider ∆-GAN in the supervised setting, and then discuss semi-supervised learning in Section 2.4. Consider two related domains, with x and y being the data samples for each domain. We have fully-paired data samples that are characterized by the joint distribution p(x, y), which also implies that samples from both the marginal p(x) and p(y) can be easily obtained.</p><p>∆-GAN consists of two generators: (i) a generator G x (y) that defines the conditional distribution p x (x|y), and (ii) a generator G y (x) that characterizes the conditional distribution in the other direction p y (y|x). G x (y) and G y (x) may also implicitly contain a random latent variable z as input, i.e., G x (y, z) and G y (x, z). In the ∆-GAN game, after a sample x is drawn from p(x), the generator G y produces a pseudo sampleỹ following the conditional distribution p y (y|x). Hence, the fake data pair (x,ỹ) is a sample from the joint distribution p y (x, y) = p y (y|x)p(x). Similarly, a fake data pair (x, y) can be sampled from the generator G x by first drawing y from p(y) and then drawing x from p x (x|y); hence (x, y) is sampled from the joint distribution p x (x, y) = p x (x|y)p(y). As such, the generative process between p x (x, y) and p y (x, y) is reversed.</p><p>The objective of ∆-GAN is to match the three joint distributions: p(x, y), p x (x, y) and p y (x, y). If this is achieved, we are ensured that we have learned a bidirectional mapping p x (x|y) and p y (y|x) that guarantees the generated fake data pairs (x, y) and (x,ỹ) are indistinguishable from the true data pairs (x, y). In order to match the joint distributions, an adversarial game is played. Joint pairs are drawn from three distributions: p(x, y), p x (x, y) or p y (x, y), and two discriminator networks are learned to discriminate among the three, while the two conditional generator networks are trained to fool the discriminators.</p><p>The value function describing the game is given by</p><formula xml:id="formula_2">min Gx,Gy max D1,D2 V (G x , G y , D 1 , D 2 ) = E (x,y)∼p(x,y) [log D 1 (x, y)] (2) + E y∼p(y),x∼px(x|y) log (1 − D 1 (x, y)) · D 2 (x, y) + E x∼p(x),ỹ∼py(y|x) log (1 − D 1 (x,ỹ)) · (1 − D 2 (x,ỹ)) .</formula><p>The discriminator D 1 is used to distinguish whether a sample pair is from p(x, y) or not, if this sample pair is not from p(x, y), another discriminator D 2 is used to distinguish whether this sample pair is from p x (x, y) or p y (x, y). D 1 and D 2 work cooperatively, and the use of both implicitly defines a ternary discriminative function D that distinguish sample pairs in three ways. See <ref type="figure" target="#fig_0">Figure 1</ref> for an illustration of the adversarial game and Appendix B for an algorithmic description of the training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Theoretical analysis</head><p>∆-GAN shares many of the theoretical properties of GANs <ref type="bibr" target="#b0">[1]</ref>. We first consider the optimal discriminators D 1 and D 2 for any given generator G x and G y . These optimal discriminators then allow reformulation of objective <ref type="formula">(2)</ref>, which reduces to the Jensen-Shannon divergence among the joint distribution p(x, y), p x (x, y) and p y (x, y). Proposition 1. For any fixed generator G x and G y , the optimal discriminator D 1 and D 2 of the game defined by</p><formula xml:id="formula_3">V (G x , G y , D 1 , D 2 ) is D * 1 (x, y) = p(x, y) p(x, y) + p x (x, y) + p y (x, y) , D * 2 (x, y) = p x (x, y) p x (x, y) + p y (x, y)</formula><p>.</p><p>Proof. The proof is a straightforward extension of the proof in <ref type="bibr" target="#b0">[1]</ref>. See Appendix A for details.</p><formula xml:id="formula_4">Proposition 2. The equilibrium of V (G x , G y , D 1 , D 2 ) is achieved if and only if p(x, y) = p x (x, y) = p y (x, y) with D * 1 (x, y) = 1 3 and D * 2 (x, y) = 1 2</formula><p>, and the optimum value is −3 log 3.</p><p>Proof. Given the optimal D * 1 (x, y) and D * 2 (x, y), the minimax game can be reformulated as:</p><formula xml:id="formula_5">C(G x , G y ) = max D1,D2 V (G x , G y , D 1 , D 2 ) (3) = −3 log 3 + 3 · JSD p(x, y), p x (x, y), p y (x, y) ≥ −3 log 3 ,<label>(4)</label></formula><p>where JSD denotes the Jensen-Shannon divergence (JSD) among three distributions. See Appendix A for details.</p><p>Since p(x, y) = p x (x, y) = p y (x, y) can be achieved in theory, it can be readily seen that the learned conditional generators can reveal the true conditional distributions underlying the data, i.e., p x (x|y) = p(x|y) and p y (y|x) = p(y|x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Semi-supervised learning</head><p>In order to further understand ∆-GAN, we write (2) as</p><formula xml:id="formula_6">V = E p(x,y) [log D 1 (x, y)] + E px(x,y) [log(1 − D 1 (x, y))] + E py(x,ỹ) [log(1 − D 1 (x,ỹ))] conditional GAN (5) + E px(x,y) [log D 2 (x, y)] + E py(x,ỹ) [log(1 − D 2 (x,ỹ))] BiGAN/ALI .<label>(6)</label></formula><p>The objective of ∆-GAN is a combination of the objectives of conditional GAN and BiGAN. The BiGAN part matches two joint distributions: p x (x, y) and p y (x, y), while the conditional GAN part provides the supervision signal to notify the BiGAN part what joint distribution to match. Therefore, ∆-GAN provides a natural way to perform semi-supervised learning, since the conditional GAN part and the BiGAN part can be used to account for paired and unpaired data, respectively.</p><p>However, when doing semi-supervised learning, there is also one potential problem that we need to be cautious about. The theoretical analysis in Section 2.3 is based on the assumption that the dataset is fully supervised, i.e., we have the ground-truth joint distribution p(x, y) and marginal distributions p(x) and p(y). In the semi-supervised setting, p(x) and p(y) are still available but p(x, y) is not. We can only obtain the joint distribution p l (x, y) characterized by the few paired data samples. Hence, in the semi-supervised setting, p x (x, y) and p y (x, y) will try to concentrate to the empirical distribution p l (x, y). We make the assumption that p l (x, y) ≈ p(x, y), i.e., the paired data can roughly characterize the whole dataset. For example, in the semi-supervised classification problem, one usually strives to make sure that labels are equally distributed among the labeled dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Relation to Triple GAN</head><p>∆-GAN is closely related to Triple GAN <ref type="bibr" target="#b11">[12]</ref>. Below we review Triple GAN and then discuss the main differences. The value function of Triple GAN is defined as follows:</p><formula xml:id="formula_7">V =E p(x,y) [log D(x, y)] + (1 − α)E px(x,y) [log(1 − D(x, y))] + αE py(x,ỹ) [log(1 − D(x,ỹ))] +E p(x,y) [− log p y (y|x)] ,<label>(7)</label></formula><p>where α ∈ (0, 1) is a contant that controls the relative importance of the two generators. Let Triple GAN-s denote a simplified Triple GAN model with only the first three terms. As can be seen, Triple GAN-s can be considered as a combination of two conditional GANs, with the importance of each condtional GAN weighted by α. It can be proven that Triple GAN-s achieves equilibrium if and only if p(x, y) = (1 − α)p x (x, y) + αp y (x, y), which is not desirable. To address this problem, in Triple GAN a standard supervised loss R L = E p(x,y) [− log p y (y|x)] is added. As a result, when the discriminator is optimal, the cost function in Triple GAN becomes:</p><formula xml:id="formula_8">2JSD p(x, y)||((1 − α)p x (x, y) + αp y (x, y)) + KL(p(x, y)||p y (x, y)) + const.<label>(8)</label></formula><p>This cost function has the good property that it has a unique minimum at p(x, y) = p x (x, y) = p y (x, y). However, the objective becomes asymmetrical. The second KL term pays low cost for generating fake-looking samples <ref type="bibr" target="#b12">[13]</ref>. By contrast ∆-GAN directly optimizes the symmetric Jensen-Shannon divergence among three distributions. More importantly, the calculation of</p><formula xml:id="formula_9">E p(x,y) [− log p y (y|x)]</formula><p>in Triple GAN also implies that the explicit density form of p y (y|x) should be provided, which may not be desirable. On the other hand, ∆-GAN only requires that p y (y|x) can be sampled from. For example, if we assume p y (y|x) = δ(y − G y (x, z))p(z)dz, and δ(·) is the Dirac delta function, we can sample y through sampling z, however, the density function of p y (y|x) is not explicitly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Applications</head><p>∆-GAN is a general framework that can be used for any joint distribution matching. Besides the semi-supervised image classification task considered in <ref type="bibr" target="#b11">[12]</ref>, we also conduct experiments on image-to-image translation and attribute-conditional image generation. When modeling image pairs, both p x (x|y) and p y (y|x) are implemented without introducing additional latent variables, i.e.,</p><formula xml:id="formula_10">p x (x|y) = δ(x − G x (y)), p y (y|x) = δ(y − G y (x)).</formula><p>A different strategy is adopted when modeling the image-label/attribute pairs. Specifically, let x denote samples in the image domain, y denote samples in the label/attribute domain. y is a one-hot vector or a binary vector when representing labels and attributes, respectively. When modeling p x (x|y), we assume that x is transformed by the latent style variables z given the label or attribute vector y, i.e., p x (x|y) = δ(x − G x (y, z))p(z)dz, where p(z) is chosen to be a simple distribution (e.g., uniform or standard normal). When learning p y (y|x), p y (y|x) is assumed to be a standard multi-class or multi-label classfier without latent variables z. In order to allow the training signal backpropagated from D 1 and D 2 to G y , we adopt the REINFORCE algorithm as in <ref type="bibr" target="#b11">[12]</ref>, and use the label with the maximum probability to approximate the expectation over y, or use the output of the sigmoid function as the predicted attribute vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related work</head><p>The proposed framework focuses on designing GAN for joint-distribution matching. Conditional GAN can be used for this task if supervised data is available. Various conditional GANs have been proposed to condition the image generation on class labels <ref type="bibr" target="#b5">[6]</ref>, attributes <ref type="bibr" target="#b14">[15]</ref>, texts <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref> and images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17]</ref>. Unsupervised learning methods have also been developed for this task. BiGAN <ref type="bibr" target="#b10">[11]</ref> and ALI <ref type="bibr" target="#b9">[10]</ref> proposed a method to jointly learn a generation network and an inference network via adversarial learning. Though originally designed for learning the two-way transition between the stochastic latent variables and real data samples, BiGAN and ALI can be directly adapted to learn the joint distribution of two real domains. Another method is called DiscoGAN <ref type="bibr" target="#b6">[7]</ref>, in which two generators are used to model the bidirectional mapping between domains, and another two discriminators are used to decide whether a generated sample is fake or not in each individual domain. Further, additional reconstructon losses are introduced to make the two generators strongly coupled and also alleviate the problem of mode collapsing. Similiar work includes CycleGAN <ref type="bibr" target="#b7">[8]</ref>, DualGAN <ref type="bibr" target="#b8">[9]</ref> and DTN <ref type="bibr" target="#b17">[18]</ref>. Additional weight-sharing constraints are introduced in CoGAN <ref type="bibr" target="#b18">[19]</ref> and UNIT <ref type="bibr" target="#b19">[20]</ref>.</p><p>Our work differs from the above work in that we aim at semi-supervised joint distribution matching. The only work that we are aware of that also achieves this goal is Triple GAN. However, our model is distinct from Triple GAN in important ways (see Section 2.5). Further, Triple GAN only focuses on image classification, while ∆-GAN has been shown to be applicable to a wide range of applications.</p><p>Various methods and model architectures have been proposed to improve and stabilize the training of GAN, such as feature matching <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, Wasserstein GAN <ref type="bibr" target="#b23">[24]</ref>, energy-based GAN <ref type="bibr" target="#b24">[25]</ref>, and unrolled GAN <ref type="bibr" target="#b25">[26]</ref> among many other related works. Our work is orthogonal to these methods, which could also be used to improve the training of ∆-GAN. Instead of using adversarial loss, there also exists work that uses supervised learning <ref type="bibr" target="#b26">[27]</ref> for joint-distribution matching, and variational autoencoders for semi-supervised learning <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. Lastly, our work is also closely related to the recent work of <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, which treats one of the domains as latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We present results on three tasks: (i) semi-supervised classification on CIFAR10 <ref type="bibr" target="#b32">[33]</ref>; (ii) imageto-image translation on MNIST <ref type="bibr" target="#b33">[34]</ref> and the edges2shoes dataset <ref type="bibr" target="#b4">[5]</ref>; and (iii) attribute-to-image generation on CelebA <ref type="bibr" target="#b34">[35]</ref> and COCO <ref type="bibr" target="#b13">[14]</ref>. We also conduct a toy data experiment to further demonstrate the differences between ∆-GAN and Triple GAN. We implement ∆-GAN without introducing additional regularization unless explicitly stated. All the network architectures are provided in the Appendix.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Toy data experiment</head><p>We first compare our method with Triple GAN on a toy dataset. We synthesize data by drawing</p><formula xml:id="formula_11">(x, y) ∼ 1 4 N (µ 1 , Σ 1 ) + 1 4 N (µ 2 , Σ 2 ) + 1 4 N (µ 3 , Σ 3 ) + 1 4 N (µ 4 , Σ 4 ), where µ 1 = [0, 1.5] , µ 2 = [−1.5, 0] , µ 3 = [1.5, 0] , µ 4 = [0, −1.5] , Σ 1 = Σ 4 = ( 3 0</formula><p>0 0.025 ) and Σ 2 = Σ 3 = ( 0.025 0 0 3 ). We generate 5000 (x, y) pairs for each mixture component. In order to implement ∆-GAN and Triple GAN-s, we model p x (x|y) and p y (y|x) as p x (x|y) = δ(x − G x (y, z))p(z)dz, p y (y|x) = δ(y − G y (x, z))p(z)dz where both G x and G y are modeled as a 4-hidden-layer multilayer perceptron (MLP) with 500 hidden units in each layer. p(z) is a bivariate standard Gaussian distribution. Triple GAN can be implemented by specifying both p x (x|y) and p y (y|x) to be distributions with explicit density form, e.g., Gaussian distributions. However, the performance can be bad since it fails to capture the multi-modality of p x (x|y) and p y (y|x). Hence, only Triple GAN-s is implemented.</p><p>Results are shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The joint distributions p x (x, y) and p y (x, y) learned by ∆-GAN successfully match the true joint distribution p(x, y). Triple GAN-s cannot achieve this, and can only guarantee 1 2 (p x (x, y) + p y (x, y)) matches p(x, y). Although this experiment is limited due to its simplicity, the results clearly support the advantage of our proposed model over Triple GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semi-supervised classification</head><p>We evaluate semi-supervised classification on the CIFAR10 dataset with 4000 labels. The labeled data is distributed equally across classes and the results are averaged over 10 runs with different random splits of the training data. For fair comparison, we follow the publically available code of Triple GAN and use the same regularization terms and hyperparameter settings as theirs. Results are summarized in <ref type="table" target="#tab_0">Table 1</ref>. Our ∆-GAN achieves the best performance among all the competing methods. We also show the ability of ∆-GAN to disentangle classes and styles in <ref type="figure">Figure 3</ref>. ∆-GAN can generate realistic data in a specific class and the injected noise vector encodes meaningful style patterns like background and color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Image-to-image translation</head><p>We first evaluate image-to-image translation on the edges2shoes dataset. Results are shown in <ref type="figure">Figure 4</ref>(bottom). Though DiscoGAN is an unsupervised learning method, it achieves impressive results. However, with supervision provided by 10% paired data, ∆-GAN generally generates more accurate edge details of the shoes. In order to provide quantitative evaluation of translating shoes to edges, we use mean squared error (MSE) as our metric. The MSE of using DiscoGAN is 140.1; with 10%, 20%, 100% paired data, the MSE of using ∆-GAN is 125.3, 113.0 and 66.4, respectively.</p><p>To further demonstrate the importance of providing supervision of domain correspondence, we created a new dataset based on MNIST <ref type="bibr" target="#b33">[34]</ref>, where the two image domains are the MNIST images and their corresponding tranposed ones. As can be seen in <ref type="figure">Figure 4</ref>(top), ∆-GAN matches images   <ref type="figure">Figure 5</ref>: Results on the face-to-attribute-to-face experiment. The 1st row is the input images; the 2nd row is the predicted attributes given the input images; the 3rd row is the generated images given the predicted attributes. betwen domains well, while DiscoGAN fails in this task. For supporting quantitative evaluation, we have trained a classifier on the MNIST dataset, and the classification accuracy of this classifier on the test set approaches 99.4%, and is, therefore, trustworthy as an evaluation metric. Given an input MNIST image x, we first generate a transposed image y using the learned generator, and then manually transpose it back to normal digits y T , and finally send this new image y T to the classifier. Results are summarized in <ref type="table" target="#tab_1">Table 2</ref>, which are averages over 5 runs with different random splits of the training data. ∆-GAN achieves significantly better performance than Triple GAN and DiscoGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Attribute-conditional image generation</head><p>We apply our method to face images from the CelebA dataset. This dataset consists of 202,599 images annotated with 40 binary attributes. We scale and crop the images to 64 × 64 pixels. In order to qualitatively evaluate the learned attribute-conditional image generator and the multi-label classifier, given an input face image, we first use the classifier to predict attributes, and then use the image generator to produce images based on the predicted attributes. <ref type="figure">Figure 5</ref> shows example results. Both the learned attribute predictor and the image generator provides good results. We further show another set of image editing experiment in <ref type="figure" target="#fig_3">Figure 6</ref>. For each subfigure, we use a same set of attributes with different noise vectors to generate images. For example, for the top-right subfigure, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Predicted attributes Generated images Input Predicted attributes Generated images ! <ref type="figure">Figure 7</ref>: Results on the image-to-attribute-to-image experiment.</p><p>all the images in the 1st row were generated based on the following attributes: black hair, female, attractive, and we then added the attribute of "sunglasses" when generating the images in the 2nd row. It is interesting to see that ∆-GAN has great flexibility to adjust the generated images by changing certain input attribtutes. For instance, by switching on the wearing hat attribute, one can edit the face image to have a hat on the head.</p><p>In order to demonstrate the scalablility of our model to large and complex datasets, we also present results on the COCO dataset. Following <ref type="bibr" target="#b36">[37]</ref>, we first select a set of 1000 attributes from the caption text in the training set, which includes the most frequent nouns, verbs, or adjectives. The images in COCO are scaled and cropped to have 64 × 64 pixels. Unlike the case of CelebA face images, the networks need to learn how to handle multiple objects and diverse backgrounds. Results are provided in <ref type="figure">Figure 7</ref>. We can generate reasonably good images based on the predicted attributes. The input and generated images also clearly share a same set of attributes. We also observe diversity in the samples by simply drawing multple noise vectors and using the same predicted attributes.</p><p>Precision (P) and normalized Discounted Cumulative Gain (nDCG) are two popular evaluation metrics for multi-label classification problems. <ref type="table" target="#tab_3">Table 3</ref> provides the quantatitive results of P@10 and nDCG@10 on CelebA and COCO, where @k means at rank k (see the Appendix for definitions). For fair comparison, we use the same network architecures for both Triple GAN and ∆-GAN. ∆-GAN consistently provides better results than Triple GAN. On the COCO dataset, our semi-supervised learning approach with 50% labeled data achieves better performance than the results of Triple GAN using the full dataset, demonstrating the effectiveness of our approach for semi-supervised joint distribution matching. More results for the above experiments are provided in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented the Triangle Generative Adversarial Network (∆-GAN), a new GAN framework that can be used for semi-supervised joint distribution matching. Our approach learns the bidirectional mappings between two domains with a few paired samples. We have demonstrated that ∆-GAN may be employed for a wide range of applications. One possible future direction is to combine ∆-GAN with sequence GAN <ref type="bibr" target="#b37">[38]</ref> or textGAN <ref type="bibr" target="#b22">[23]</ref> to model the joint distribution of image-caption pairs. Acknowledgements This research was supported in part by ARO, DARPA, DOE, NGA and ONR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the Triangle Generative Adversarial Network (∆-GAN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Toy data experiment on ∆-GAN and Triple GAN. (a) the joint distribution p(x, y) of real data. For (b) and (c), the left and right figure is the learned joint distribution px(x, y) and py(x, y), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Generated CIFAR10 samples, where each row shares the same label and each column uses the same noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Results on the image editing experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Error rates (%) on the par-

tially labeled CIFAR10 dataset. 

Algorithm 
n = 4000 

CatGAN [36] 
19.58 ± 0.58 
Improved GAN [21] 18.63 ± 2.32 
ALI [10] 
17.99 ± 1.62 
Triple GAN [12] 
16.99 ± 0.36 

∆-GAN (ours) 
16.80 ± 0.42 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Classification accuracy (%) on the MNIST-to-

MNIST-transpose dataset. 

Algorithm 
n = 100 
n = 1000 
All 

DiscoGAN 
− 
− 
15.00± 0.20 
Triple GAN 63.79 ± 0.85 84.93 ± 1.63 86.70 ± 1.52 

∆-GAN 
83.20± 1.88 
88.98± 1.50 
93.34± 1.46 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Results of P@10 and nDCG@10 for attribute predicting on CelebA and COCO.Triple GAN 40.97/50.74 62.13/73.56 70.12/79.37 32.64/35.91 34.00/37.76 35.35/39.60 ∆-GAN 53.21/58.39 63.68/75.22 70.37/81.47 34.38/37.91 36.72/40.39 39.05/42.86</figDesc><table>Dataset 
CelebA 
COCO 

Method 
1% 
10% 
100% 
10% 
50% 
100% 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning to discover crossdomain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeksoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moonsu</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungkwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Dualgan: Unsupervised dual learning for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zili</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minglun</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Triple generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guim</forename><surname>Perarnau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Joost Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Raducanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Álvarez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06355</idno>
		<title level="m">Invertible conditional gans for image editing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Photo-realistic single image superresolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generating text via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on Adversarial Training</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adversarial feature matching for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<title level="m">Soumith Chintala, and Léon Bottou. Wasserstein gan</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unrolled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Dual supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Variational autoencoder for deep learning of images, labels and captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vae learning via stein variational gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaobo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Alice: Towards understanding adversarial learning for joint distribution matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adversarial symmetric variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Symmetric variational autoencoder and connections to adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unsupervised and semi-supervised learning with categorical generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06390</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semantic compositional networks for visual captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Seqgan: sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
