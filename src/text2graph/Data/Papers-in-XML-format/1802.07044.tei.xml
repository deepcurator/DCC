<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Description Length of Deep Learning Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-11-01">1 Nov 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léonard</forename><surname>Blier</surname></persName>
							<email>leonard.blier@normalesup.org</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook Artificial Intelligence Research</orgName>
								<orgName type="institution">École Normale Supérieure Paris</orgName>
								<address>
									<settlement>Paris</settlement>
									<country>France, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Ollivier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook Artificial Intelligence Research</orgName>
								<orgName type="institution">École Normale Supérieure Paris</orgName>
								<address>
									<settlement>Paris</settlement>
									<country>France, France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Description Length of Deep Learning Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-11-01">1 Nov 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Solomonoff's general theory of inference (Solomonoff, 1964)  and the Minimum Description Length principle <ref type="bibr" target="#b12">(Grünwald, 2007;</ref><ref type="bibr" target="#b29">Rissanen, 2007)</ref> formalize Occam's razor, and hold that a good model of data is a model that is good at losslessly compressing the data, including the cost of describing the model itself. Deep neural networks might seem to go against this principle given the large number of parameters to be encoded. We demonstrate experimentally the ability of deep neural networks to compress the training data even when accounting for parameter encoding. The compression viewpoint originally motivated the use of variational methods in neural networks <ref type="bibr" target="#b15">(Hinton and Van Camp, 1993;</ref><ref type="bibr" target="#b32">Schmidhuber, 1997)</ref>. Unexpectedly, we found that these variational methods provide surprisingly poor compression bounds, despite being explicitly built to minimize such bounds. This might explain the relatively poor practical performance of variational methods in deep learning. On the other hand, simple incremental encoding methods yield excellent compression values on deep networks, vindicating Solomonoff's approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning has achieved remarkable results in many different areas <ref type="bibr" target="#b23">(LeCun et al., 2015)</ref>. Still, the ability of deep models not to overfit despite their large number of parameters is not well understood. To quantify the complexity of these models in light of their generalization ability, several metrics beyond parameter-counting have been measured, such as the number of degrees of freedom of models <ref type="bibr" target="#b10">(Gao and Jojic, 2016)</ref>, or their intrinsic dimension <ref type="bibr" target="#b24">(Li et al., 2018)</ref>. These works concluded that deep learning models are significantly simpler than their numbers of parameters might suggest.</p><p>In information theory and Minimum Description Length (MDL), learning a good model of the data is recast as using the model to losslessly transmit the data in as few bits as possible. More complex models will compress the data more, but the model must be transmitted as well. The overall codelength can be understood as a combination of quality-of-fit of the model (compressed data length), together with the cost of encoding (transmitting) the model itself. For neural networks, the MDL viewpoint goes back as far as <ref type="bibr" target="#b15">(Hinton and Van Camp, 1993)</ref>, which used a variational technique to estimate the joint compressed length of data and parameters in a neural network model.</p><p>Compression is strongly related to generalization and practical performance. Standard sample complexity bounds (VC-dimension, PAC-Bayes...) are related to the compressed length of the data in a model, and any compression scheme leads to generalization bounds <ref type="bibr" target="#b4">(Blum and Langford, 2003)</ref>. Specifically for deep learning, <ref type="bibr" target="#b1">(Arora et al., 2018)</ref> showed that compression leads to generalization bounds (see also <ref type="bibr" target="#b8">(Dziugaite and Roy, 2017)</ref>). Several other deep learning methods have been inspired by information theory and the compression viewpoint. In unsupervised learning, autoencoders and especially variational autoencoders <ref type="bibr" target="#b19">(Kingma and Welling, 2013)</ref> are compression methods of the data <ref type="bibr" target="#b28">(Ollivier, 2014)</ref>. In supervised learning, the information bottleneck method studies 32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada. Figure 1: Fake labels cannot be compressed Measuring codelength while training a deep model on MNIST with true and fake labels. The model is an MLP with 3 hidden layers of size 200, with RELU units. With ordinary SGD training, the model is able to overfit random labels. The plot shows the effect of using variational learning instead, and reports the variational objective (encoding cost of the training data, see Section 3.3), on true and fake labels. We also isolated the contribution from parameter encoding in the total loss (KL term in (3.2)). With true labels, the encoding cost is below the uniform encoding, and half of the description length is information contained in the weights. With fake labels, on the contrary, the encoding cost converges to a uniform random model, with no information contained in the weights: there is no mutual information between inputs and outputs.</p><p>how the hidden representations in a neural network compress the inputs while preserving the mutual information between inputs and outputs <ref type="bibr" target="#b38">(Tishby and Zaslavsky, 2015;</ref><ref type="bibr" target="#b35">Shwartz-Ziv and Tishby, 2017;</ref><ref type="bibr" target="#b0">Achille and Soatto, 2017)</ref>.</p><p>MDL is based on Occam's razor, and on Chaitin's hypothesis that "comprehension is compression" <ref type="bibr" target="#b6">(Chaitin, 2007)</ref>: any regularity in the data can be exploited both to compress it and to make predictions. This is ultimately rooted in Solomonoff's general theory of inference <ref type="bibr">(Solomonoff, 1964)</ref> (see also, e.g., <ref type="bibr" target="#b17">(Hutter, 2007;</ref><ref type="bibr" target="#b32">Schmidhuber, 1997)</ref>), whose principle is to favor models that correspond to the "shortest program" to produce the training data, based on its Kolmogorov complexity <ref type="bibr" target="#b25">(Li and Vitányi, 2008)</ref>. If no structure is present in the data, no compression to a shorter program is possible.</p><p>The problem of overfitting fake labels is a nice illustration: convolutional neural networks commonly used for image classification are able to reach 100% accuracy on random labels on the train set . However, measuring the associated compression bound <ref type="figure">(Fig. 1)</ref> immediately reveals that these models do not compress fake labels (and indeed, theoretically, they cannot, see Appendix A), that no information is present in the model parameters, and that no learning has occurred.</p><p>In this work we explicitly measure how much current deep models actually compress data. (We introduce no new architectures or learning procedures.) As seen above, this may clarify several issues around generalization and measures of model complexity. Our contributions are:</p><p>• We show that the traditional method to estimate MDL codelengths in deep learning, variational inference <ref type="bibr" target="#b15">(Hinton and Van Camp, 1993)</ref>, yields surprisingly inefficient codelengths for deep models, despite explicitly minimizing this criterion. This might explain why variational inference as a regularization method often does not reach optimal test performance.</p><p>• We introduce new practical ways to compute tight compression bounds in deep learning models, based on the MDL toolbox <ref type="bibr" target="#b12">(Grünwald, 2007;</ref><ref type="bibr" target="#b29">Rissanen, 2007)</ref>. We show that prequential coding on top of standard learning, yields much better codelengths than variational inference, correlating better with test set performance. Thus, despite their many parameters, deep learning models do compress the data well, even when accounting for the cost of describing the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Probabilistic Models, Compression, and Information Theory</head><p>Imagine that Alice wants to efficiently transmit some information to Bob. Alice has a dataset D = {(x 1 , y 1 ), ..., (x n , y n )} where x 1 , ..., x n are some inputs and y 1 , ..., y n some labels. We do not assume that these data come from a "true" probability distribution. Bob also has the data x 1 , ..., x n , but he does not have the labels. This describes a supervised learning situation in which the inputs x may be publicly available, and a prediction of the labels y is needed. How can deep learning models help with data encoding? One key problem is that Bob does not necessarily know the precise, trained model that Alice is using. So some explicit or implicit transmission of the model itself is required.</p><p>We study, in turn, various methods to encode the labels y, with or without a deep learning model. Encoding the labels knowing the inputs is equivalent to estimating their mutual information (Section 2.4); this is distinct from the problem of practical network compression (Section 3.2) or from using neural networks for lossy data compression. Our running example will be image classification on the MNIST <ref type="bibr" target="#b22">(LeCun et al., 1998)</ref> and CIFAR10 <ref type="bibr" target="#b20">(Krizhevsky, 2009</ref>) datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Definitions and notation</head><p>Let X be the input space and Y the output (label) space. In this work, we only consider classification tasks, so</p><formula xml:id="formula_0">Y = {1, ..., K}. The dataset is D := {(x 1 , y 1 ), ..., (y n , x n )}. Denote x k:l := (x k , x k+1 , ..., x l−1 , x l )</formula><p>. We define a model for the supervised learning problem as a conditional probability distribution p(y|x), namely, a function such that for each x ∈ X , y∈Y p(y|x) = 1. A model class, or architecture, is a set of models depending on some parameter θ:</p><formula xml:id="formula_1">M = {p θ , θ ∈ Θ}. The Kullback-Leibler divergence between two distributions is KL(µ ν) = E X∼µ [log 2 µ(x) ν(x) ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Models and codelengths</head><p>We recall a basic result of compression theory <ref type="bibr" target="#b34">(Shannon, 1948</ref> </p><formula xml:id="formula_2">L p (y 1:n |x 1:n ) = − n i=1 log 2 p(y i |x i ) (2.1)</formula><p>This bound is known to be optimal if the data are independent and coming from the model p <ref type="bibr" target="#b27">(Mackay, 2003)</ref>. The one additional bit in the Shannon-Huffman code is incurred only once for the whole dataset <ref type="bibr" target="#b27">(Mackay, 2003)</ref>. With large datasets this is negligible. Thus, from now on we will systematically omit the +1 as well as admit non-integer codelengths <ref type="bibr" target="#b12">(Grünwald, 2007)</ref>. We will use the terms codelength or compression bound interchangeably.</p><p>This bound is exactly the categorical cross-entropy loss evaluated on the model p. Hence, trying to minimize the description length of the outputs over the parameters of a model class is equivalent to minimizing the usual classification loss.</p><p>Here we do not consider the practical implementation of compression algorithms: we only care about the theoretical bit length of their associated encodings. We are interested in measuring the amount of information contained in the data, the mutual information between input and output, and how it is captured by the model. Thus, we will directly work with codelength functions.</p><p>An obvious limitation of the bound (2.1) is that Alice and Bob both have to know the model p in advance. This is problematic if the model must be learned from the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Uniform encoding</head><p>The uniform distribution p unif (y|x) = 1 K over the K classes does not require any learning from the data, thus no additional information has to be transmitted. Using p unif (y|x) (2.1) yields a codelength  <ref type="bibr" target="#b13">(Han et al., 2015a)</ref> and <ref type="bibr" target="#b41">(Xu et al., 2017)</ref>, and for the intrinsic dimension code, results from <ref type="bibr" target="#b24">(Li et al., 2018)</ref>. The values in the table for these codelengths and compression ratio are lower bounds, only taking into account the codelength of the weights, and not the codelength of the data encoded with the model (the final loss is not always available in these publications). For variational and prequential codes, we selected the model and hyperparameters providing the best compression bound. This uniform encoding will be a sanity check against which to compare the other encodings in this text. For MNIST, the uniform encoding cost is 60000 × log 2 10 = 199 kbits. For CIFAR, the uniform encoding cost is 50000 × log 2 10 = 166 kbits.</p><formula xml:id="formula_3">L unif (y 1:n |x 1:n ) = n log 2 K (2.2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CODE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Mutual information between inputs and outputs</head><p>Intuitively, the only way to beat a trivial encoding of the outputs is to use the mutual information (in a loose sense) between the inputs and outputs.</p><p>This can be formalized as follows. Assume that the inputs and outputs follow a "true" joint distribution q(x, y). Then any transmission method with codelength L satisfies <ref type="bibr" target="#b27">(Mackay, 2003</ref>)</p><formula xml:id="formula_4">E q [L(y|x)] ≥ H(y|x) (2.3)</formula><p>Therefore, the gain (per data point) between the codelength L and the trivial codelength H(y) is</p><formula xml:id="formula_5">H(y) − E q [L(y|x)] ≤ H(y) − H(y|x) = I(y; x) (2.4)</formula><p>the mutual information between inputs and outputs <ref type="bibr" target="#b27">(Mackay, 2003)</ref>.</p><p>Thus, the gain of any codelength compared to the uniform code is limited by the amount of mutual information between input and output. (This bound is reached with the true model q(y|x).) Any successful compression of the labels is, at the same time, a direct estimation of the mutual information between input and output. The latter is the central quantity in the Information Bottleneck approach to deep learning models <ref type="bibr" target="#b35">(Shwartz-Ziv and Tishby, 2017)</ref>.</p><p>Note that this still makes sense without assuming a true underlying probabilistic model, by replacing the mutual information H(y) − H(y|x) with the "absolute" mutual information K(y) − K(y|x) based on Kolmogorov complexity K <ref type="bibr" target="#b25">(Li and Vitányi, 2008)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Compression Bounds via Deep Learning</head><p>Various compression methods from the MDL toolbox can be used on deep learning models. (Note that a given model can be stored or encoded in several ways, some of which may have large codelengths. A good model in the MDL sense is one that admits at least one good encoding.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Two-Part Encodings</head><p>Alice and Bob can first agree on a model class (such as "neural networks with two layers and 1,000 neurons per layer"). However, Bob does not have access to the labels, so Bob cannot train the parameters of the model. Therefore, if Alice wants to use such a parametric model, the parameters themselves have to be transmitted. Such codings in which Alice first transmits the parameters of a model, then encodes the data using this parameter, have been called two-part codes <ref type="bibr" target="#b12">(Grünwald, 2007)</ref>.</p><p>Definition 1 (Two-part codes). Assume that Alice and Bob have first agreed on a model class (p θ ) θ∈Θ . Let L param (θ) be any encoding scheme for parameters θ ∈ Θ. Let θ * be any parameter. The corresponding two-part codelength is</p><formula xml:id="formula_6">L 2-part θ * (y 1:n |x 1:n ) := L param (θ * ) + L p θ * (y 1:n |x 1:n ) = L param (θ * ) − n i=1 log 2 p θ * (y i |x i ) (3.1)</formula><p>An obvious possible code L param for θ is the standard float32 binary encoding for θ, for which L param (θ) = 32 dim(θ). In deep learning, two-part codes are widely inefficient and much worse than the uniform encoding <ref type="bibr" target="#b11">(Graves, 2011)</ref>. For a model with 1 million parameters, the two-part code with float32 binary encoding will amount to 32 Mbits, or 200 times the uniform encoding on CIFAR10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Compression</head><p>The practical encoding of trained models is a well-developed research topic, e.g., for use on small devices such as cell phones. Such encodings can be seen as two-part codes using a clever code for θ instead of encoding every parameter on 32 bits. Possible strategies include training a student layer to approximate a well-trained network <ref type="bibr" target="#b2">(Ba and Caruana, 2014;</ref><ref type="bibr" target="#b31">Romero et al., 2015)</ref>, or pipelines involving retraining, pruning, and quantization of the model weights <ref type="bibr">(Han et al., 2015a,b;</ref><ref type="bibr" target="#b36">Simonyan and Zisserman, 2014;</ref><ref type="bibr" target="#b26">Louizos et al., 2017;</ref><ref type="bibr" target="#b33">See et al., 2016;</ref>.</p><p>Still, the resulting codelengths (for compressing the labels given the data) are way above the uniform compression bound for image classification <ref type="table" target="#tab_1">(Table 1)</ref>.</p><p>Another scheme for network compression, less used in practice but very informative, is to sample a random low-dimensional affine subspace in parameter space and to optimize in this subspace <ref type="bibr" target="#b24">(Li et al., 2018)</ref>. The number of parameters is thus reduced to the dimension of the subspace and we can use the associated two-part encoding. (The random subspace can be transmitted via a pseudorandom seed.) Our methodology to derive compression bounds from <ref type="bibr" target="#b24">(Li et al., 2018</ref>) is detailed in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Variational and Bayesian Codes</head><p>Another strategy for encoding weights with a limited precision is to represent these weights by random variables: the uncertainty on θ represents the precision with which θ is transmitted. The variational code turns this into an explicit encoding scheme, thanks to the bits-back argument <ref type="bibr" target="#b16">(Honkela and Valpola, 2004)</ref>. Initially a way to compute codelength bounds with neural networks <ref type="bibr" target="#b15">(Hinton and Van Camp, 1993)</ref>, this is now often seen as a regularization technique <ref type="bibr" target="#b5">(Blundell et al., 2015)</ref>. This method yields the following codelength.</p><p>Definition 2 (Variational code). Assume that Alice and Bob have agreed on a model class (p θ ) θ∈Θ and a prior α over Θ. Then for any distribution β over Θ, there exists an encoding with codelength</p><formula xml:id="formula_7">L var β (y 1:n |x 1:n ) = KL (β α) + E θ∼β L p θ (y 1:n |x 1:n ) = KL (β α) − E θ∼β n i=1 log 2 p θ (y i |x i ) (3.2)</formula><p>This can be minimized over β, by choosing a parametric model class (β φ ) φ∈Φ , and minimizing (3.2) over φ. A common model class for β is the set of multivariate Gaussian distributions {N (µ, Σ), µ ∈ R d , Σ diagonal}, and µ and Σ can be optimized with a stochastic gradient descent algorithm <ref type="bibr" target="#b11">(Graves, 2011;</ref><ref type="bibr" target="#b21">Kucukelbir et al., 2017)</ref>. Σ can be interpreted as the precision with which the parameters are encoded.</p><p>The variational bound L var β is an upper bound for the Bayesian description length bound of the Bayesian model p θ with parameter θ and prior α. Considering the Bayesian distribution of y,</p><formula xml:id="formula_8">p Bayes (y 1:n |x 1:n ) = θ∈Θ p θ (y 1:n |x 1:n )α(θ)dθ, (3.3)</formula><p>then Proposition 1 provides an associated code via (2.1) with model p Bayes : L Bayes (y 1:n |x 1:n ) = − log 2 p Bayes (y 1:n |x 1:n ) Then, for any β we have <ref type="bibr" target="#b11">(Graves, 2011</ref>)</p><formula xml:id="formula_9">L var β (y 1:n |x 1:n ) ≥ L Bayes (y 1:n |x 1:n ) (3.4)</formula><p>with equality if and only if β is equal to the Bayesian posterior p Bayes (θ|x 1:n , y 1:n ). Variational methods can be used as approximate Bayesian inference for intractable Bayesian posteriors.</p><p>We computed practical compression bounds with variational methods on MNIST and CIFAR10. Neural networks that give the best variational compression bounds appear to be smaller than networks trained the usual way. We tested various fully connected networks and convolutional networks (Appendix C): the models that gave the best variational compression bounds were small LeNet-like networks. To test the link between compression and test accuracy, in <ref type="table" target="#tab_1">Table 1</ref> we report the best model based on compression, not test accuracy. This results in a drop of test accuracy with respect to other settings.</p><p>On MNIST, this provides a codelength of the labels (knowing the inputs) of 24.1 kbits, i.e., a compression ratio of 0.12. The corresponding model achieved 95.5% accuracy on the test set.</p><p>On CIFAR, we obtained a codelength of 89.0 kbits, i.e., a compression ratio of 0.54. The corresponding model achieved 61.6% classification accuracy on the test set.</p><p>We can make two observations. First, choosing the model class which minimizes variational codelength selects smaller deep learning models than would cross-validation. Second, the model with best variational codelength has low classification accuracy on the test set on MNIST and CIFAR, compared to models trained in a non-variational way. This aligns with a common criticism of Bayesian methods as too conservative for model selection compared with cross-validation <ref type="bibr" target="#b30">(Rissanen et al., 1992;</ref><ref type="bibr" target="#b9">Foster and George, 1994;</ref><ref type="bibr" target="#b3">Barron and Yang, 1999;</ref><ref type="bibr" target="#b12">Grünwald, 2007)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Prequential or Online Code</head><p>The next coding procedure shows that deep neural models which generalize well also compress well.</p><p>The prequential (or online) code is a way to encode both the model and the labels without directly encoding the weights, based on the prequential approach to statistics <ref type="bibr" target="#b7">(Dawid, 1984)</ref>, by using prediction strategies. Intuitively, a model with default values is used to encode the first few data; then the model is trained on these few encoded data; this partially trained model is used to encode the next data; then the model is retrained on all data encoded so far; and so on.</p><p>Precisely, we call p a prediction strategy for predicting the labels in Y knowing the inputs in X if for all k, p(y k+1 |x 1:k+1 , y 1:k ) is a conditional model; namely, any strategy for predicting the k + 1-label after already having seen k input-output pairs. In particular, such a model may learn from the first k data samples. Any prediction strategy p defines a model on the whole dataset:</p><formula xml:id="formula_10">p preq (y 1:n |x 1:n ) = p(y 1 |x 1 ) · p(y 2 |x 1:2 , y 1 ) · . . . · p(y n |x 1:n , y 1:n−1 ) (3.5)</formula><p>Let (p θ ) θ∈Θ be a deep learning model. We assume that we have a learning algorithm which computes, from any number of data samples (x 1:k , y 1:k ), a trained parameter vectorθ(x 1:k , y 1:k ). Then the data is encoded in an incremental way: at each step k,θ(x 1:k , y 1:k ) is used to predict y k+1 .</p><p>In practice, the learning procedureθ may only reset and retrain the network at certain timesteps. We choose timesteps 1 = t 0 &lt; t 1 &lt; ... &lt; t S = n, and we encode the data by blocks, always using the model learned from the already transmitted data (Algorithm 2 in Appendix D). A uniform encoding is used for the first few points. (Even though the encoding procedure is called "online", it does not mean that only the most recent sample is used to update the parameterθ: the optimization procedurê θ can be any predefined technique using all the previous samples (x 1:k , y 1:k ), only requiring that the algorithm has an explicit stopping criterion.) This yields the following description length:</p><p>Definition 3 (Prequential code). Given a model p θ , a learning algorithmθ(x 1:k , y 1:k ), and retraining timesteps 1 = t 0 &lt; t 1 &lt; ... &lt; t S = n, the prequential codelength is</p><formula xml:id="formula_11">L preq (y 1:n |x 1:n ) = t 1 log 2 K + S−1 s=0</formula><p>− log 2 pθ ts (y ts+1:ts+1 |x ts+1:ts+1 ) (3.6) where for each s,θ ts =θ(x 1:ts , y 1:ts ) is the parameter learned on data samples 1 to t s .</p><p>The model parameters are never encoded explicitly in this method. The difference between the prequential codelength L preq (y 1:n |x 1:n ) and the log-loss n t=1 − log 2 pθ t K (y t |x t ) of the final trained model, can be interpreted as the amount of information that the trained parameters contain about the data contained: the former is the data codelength if Bob does not know the parameters, while the latter is the codelength of the same data knowing the parameters.</p><p>Prequential codes depend on the performance of the underlying training algorithm, and take advantage of the model's generalization ability from the previous data to the next. In particular, the model training should yield good generalization performance from data [1; t s ] to data [t s + 1; t s+1 ].</p><p>In practice, optimization procedures for neural networks may be stochastic (initial values, dropout, data augmentation...), and Alice and Bob need to make all the same random actions in order to get the same final model. A possibility is to agree on a random seed ω (or pseudorandom numbers) beforehand, so that the random optimization procedureθ(x 1:ts , y 1:ts ) is deterministic given ω, Hyperparameters may also be transmitted first (the cost of sending a few numbers is small).</p><p>Prequential coding with deep models provides excellent compression bounds. On MNIST, we computed the description length of the labels with different networks (Appendix D). The best compression bound was given by a convolutional network of depth 8. It achieved a description length of 4.10 kbits, i.e., a compression ratio of 0.021, with 99.5% test set accuracy <ref type="table" target="#tab_1">(Table 1)</ref>. This codelength is 6 times smaller than the variational codelength.</p><p>On CIFAR, we tested a simple multilayer perceptron, a shallow network, a small convolutional network, and a VGG convolutional network <ref type="bibr" target="#b36">(Simonyan and Zisserman, 2014</ref>) first without data augmentation or batch normalization (VGGa) <ref type="bibr" target="#b18">(Ioffe and Szegedy, 2015)</ref>, then with both of them (VGGb) (Appendix D). The results are in <ref type="figure" target="#fig_1">Figure 2</ref>. The best compression bound was obtained with VGGb, achieving a codelength of 45.3 kbits, i.e., a compression ratio of 0.27, and 93% test set accuracy <ref type="table" target="#tab_1">(Table 1</ref>). This codelength is twice smaller than the variational codelength. The difference between VGGa and VGGb also shows the impact of the training procedure on codelengths for a given architecture.</p><p>Model Switching. A weakness of prequential codes is the catch-up phenomenon <ref type="bibr" target="#b40">(Van Erven et al., 2012)</ref>. Large architectures might overfit during the first steps of the prequential encoding, when the model is trained with few data samples. Thus the encoding cost of the first packs of data might be worse than with the uniform code. Even after the encoding cost on current labels becomes lower, the cumulated codelength may need a lot of time to "catch up" on its initial lag. This can be observed in practice with neural networks: in <ref type="figure" target="#fig_1">Fig. 2</ref>, the VGGb model needs 5,000 samples on CIFAR to reach a cumulative compression ratio &lt; 1, even though the encoding cost per label becomes drops below uniform after just 1,000 samples. This is efficiently solved by switching <ref type="bibr" target="#b40">(Van Erven et al., 2012)</ref> between models (see Appendix E). Switching further improves the practical compression bounds, even when just switching between copies of the same model with different SGD stopping times <ref type="figure" target="#fig_1">(Fig. 3, Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Too Many Parameters in Deep Learning Models? &gt;From an information theory perspective, the goal of a model is to extract as much mutual information between the labels and inputs as possible-equivalently (Section 2.4), to compress the labels. This cannot be achieved with 2-part codes or practical network compression. With the variational code, the models do compress the data, but with a worse prediction performance: one could conclude that deep learning models that achieve the best prediction performance cannot compress the data. Thanks to the prequential code, we have seen that deep learning models, even with a large number of parameters, compress the data well: from an information theory point of view, the number of parameters is not an obstacle to compression. This is consistent with Chaitin's hypothesis that "comprehension is compression", contrary to previous observations with the variational code.</p><p>Prequential Code and Generalization. The prequential encoding shows that a model that generalizes well for every dataset size, will compress well. The efficiency of the prequential code is directly due to the generalization ability of the model at each time.</p><p>Theoretically, three of the codes (two-parts, Bayesian, and prequential based on a maximum likelihood or MAP estimator) are known to be asymptotically equivalent under strong assumptions (d-dimensional identifiable model, data coming from the model, suitable Bayesian prior, and technical assumptions ensuring the effective dimension of the trained model is not lower than d): in that case, these three methods yield a codelength L(y 1:n |x 1:n ) = nH(Y |X) + d 2 log 2 n + O(1) <ref type="bibr" target="#b12">(Grünwald, 2007)</ref>. This corresponds to the BIC criterion for model selection. Hence there was no obvious reason for the prequential code to be an order of magnitude better than the others.</p><p>However, deep learning models do not usually satisfy any of these hypotheses. Moreover, our prequential codes are not based on the maximum likelihood estimator at each step, but on standard deep learning methods (so training is regularized at least by dropout and early stopping).</p><p>Inefficiency of Variational Models for Deep Networks. The objective of variational methods is equivalent to minimizing a description length. Thus, on our image classification tasks, variational methods do not have good results even for their own objective, compared to prequential codes. This makes their relatively poor results at test time less surprising.</p><p>Understanding this observed inefficiency of variational methods is an open problem. As stated in (3.4), the variational codelength is an upper bound for the Bayesian codelength. More precisely, L var β (y 1:n |x 1:n ) = L Bayes (y 1:n |x 1:n ) + KL (p Bayes (θ|x 1:n , y 1:n ) β) (4.1) with notation as above, and with p Bayes (θ|x 1:n , y 1:n ) the Bayesian posterior on θ given the data. Empirically, on MNIST and CIFAR, we observe that L preq (y 1:n |x 1:n ) ≪ L var β (y 1:n |x 1:n ). Several phenomena could contribute to this gap. First, the optimization of the parameters φ of the approximate Bayesian posterior might be imperfect. Second, even the optimal distribution β * in the variational class might not approximate the posterior p Bayes (θ|x 1:n , y 1:n ) well, leading to a large KL term in (4.1); this would be a problem with the choice of variational posterior class β. On the other hand we do not expect the choice of Bayesian prior to be a key factor: we tested Gaussian priors with various variances as well as a conjugate Gaussian prior, with similar results. Moreover, Gaussian initializations and L2 weight decay (acting like a Gaussian prior) are common in deep learning. Finally, the (untractable) Bayesian codelength based on the exact posterior might itself be larger than the prequential codelength. This would be a problem of underfitting with parametric Bayesian inference, perhaps related to the catch-up phenomenon or to the known conservatism of Bayesian model selection (end of Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Deep learning models can represent the data together with the model in fewer bits than a naive encoding, despite their many parameters. However, we were surprised to observe that variational inference, though explicitly designed to minimize such codelengths, provides very poor such values compared to a simple incremental coding scheme. Understanding this limitation of variational inference is a topic for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Fake labels are not compressible</head><p>In the introduction, we stated that fake labels could not be compressed. This means that the optimal codelength for this labels is almost the uniform one. This can be formalized as follows. We define a code for y 1:n as any program (in a reference Turing machine) that outputs y 1:n , and denote L(y 1:n ) the length of this program, or L(y 1:n |x 1:n ) for programs that may use x 1:n as their input.</p><p>Proposition 2. Assume that x 1 , ..., x n are inputs, and that Y 1 , ..., Y n are iid random labels uniformly sampled in {1, ..., K}. Then for any δ ∈ N * , with probability 1 − 2 −δ the values Y 1 , . . . , Y n satisfy that for any possible coding procedure L (even depending on the values of x 1:n ), the codelength of</p><formula xml:id="formula_12">Y 1:n is at least L(Y 1:n |x 1:n ) ≥ nH(Y ) − δ − 1 (A.1) = n log 2 K − δ − 1. (A.2)</formula><p>We insist that this does not require any assumptions on the coding procedure used, so this result holds for all possible models. Moreover, this is really a property of the sampled values Y 1 , . . . Y n : most values of Y 1:n can just not be compressed by any algorithm.</p><p>Proof. This proposition is a standard counting argument, or an immediate consequence of Theorem 2.2.1 in <ref type="bibr" target="#b25">(Li and Vitányi, 2008)</ref>. Let A = {1, ..., K} n be the set of all possible outcomes for the sequence of random labels. We have |A| = K n . Let δ be an integer, δ ∈ N * , we want to know how many elements in A can be encoded in less than log 2 |A| − δ bits. We consider, on a given Turing machine, the number of programs of length less than ⌊log 2 |A| − δ⌋. This number is less than :</p><formula xml:id="formula_13">⌊log 2 |A|⌋−δ−1 i=0 2 i = 2 ⌊log 2 |A|⌋−δ − 1 (A.3) ≤ 2 −δ |A| − 1 (A.4)</formula><p>Therefore, the number of elements in A which can be described in less than log 2 |A| − δ bits is less than 2 −δ |A| − 1. We can deduce from this that the number of elements in A which cannot be described by any program in less than 2 −δ |A|−1 bits is at least |A|(1−2 −δ ). Equivalently, there are at least |A|(1 − 2 −δ ) elements (y 1 , ..., y n ) in |A| such that for any coding scheme, L(y 1:n |x 1:n ) ≥ n log 2 K −δ−1. Since the random labels Y 1 , ..., Y n are uniformly distributed, the result follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Technical details on compression bounds with random affine subspaces</head><p>We describe in Algorithm 1 the detailed procedure which allows to compute compression bounds with the random affine subspace method <ref type="bibr" target="#b24">(Li et al., 2018)</ref>. To compute the numerical results in <ref type="table" target="#tab_1">Table 1</ref>, we took the intrinsic dimension computed in the original paper, and considered that the precision of the parameter was 32 bits, following the authors' suggestion. Then, the description length of the model itself is 32× the intrinsic dimension. This does not take into account the description length of the labels given the model, which is non-negligible (to take this quantity into account, we would need to know the loss on the training set of the model, which was not specified in the original paper). Thus we only get a lower bound.</p><p>For MNIST, the model with the smaller intrinsic dimension is the LeNet, which has an intrinsic dimension of 290 for an accuracy of 90% (the threshold at which <ref type="bibr" target="#b24">(Li et al., 2018)</ref> stop by definition, hence the performance in <ref type="table" target="#tab_1">Table 1</ref>). This leads to a description length for the model of 9280 bits, which corresponds to a 0.05 compression ratio, without taking into account the description length of the labels given the model.</p><p>For CIFAR, again with the LeNet architecture, the intrinsic dimension is 2,900. This leads to a description length for the model of 92800 bits, which corresponds to a 0.05 compression ratio, without taking into account the description length of the labels given the model. These bounds could be improved by optimizing the precision ε. Indeed, reducing the precision makes the model less accurate and increases the encoding cost of the labels with the model, but it decreases the encoding cost of the parameters. Therefore, we could find an optimal precision ε * to improve the compression bound. This would be a topic for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Technical Details on Variational Learning for Section 3.3</head><p>Variational learning was performed using the library Pyvarinf (Tallec and Blier, 2018).</p><p>We used a prior α = N (0, σ We optimize the bound (3.2) as a function of (µ, ρ) with a gradient descent method, and estimate its values and gradient with a Monte-Carlo method. Since the prior and posteriors are gaussian, we have an explicit formula for the first part of the variational loss KL(β µ,ρ α) <ref type="bibr" target="#b15">(Hinton and Van Camp, 1993)</ref>. Therefore, we can easily compute its values and gradients. For the second part</p><formula xml:id="formula_14">(µ, ρ) → E θ∼βµ,ρ n i=1 − log 2 p θ (y i |x i ) , (C.1)</formula><p>we can use the following proposition <ref type="bibr" target="#b11">(Graves, 2011)</ref>. For any function f : Θ → R, we have</p><formula xml:id="formula_15">∂ ∂µ i E θ∼βµ,ρ [f (θ)] = E θ∼βµ,ρ ∂f ∂θ i (θ) (C.2) ∂ ∂ρ i E θ∼βµ,ρ [f (θ)] = ∂σ i ∂ρ i · E θ∼βµ,ρ ∂f ∂θ i · θ i − µ i σ i (C.3)</formula><p>Therefore, we can estimate the values and gradients of (3.2) with a Monte-Carlo algorithm:</p><formula xml:id="formula_16">∂ ∂µ i E θ∼βµ,ρ [f (θ)] ≈ S s=1 ∂f ∂θ i (θ s ) (C.4) ∂ ∂ρ i E θ∼βµ,ρ [f (θ)] ≈ ∂σ i ∂ρ i · S s=1 ∂f ∂θ i (θ s ) · θ s i − µ i σ i (C.5)</formula><p>where θ 1 , ..., θ S are sampled from β µ,ρ . In practice, we used S = 1 both for the computations of the variational loss and its gradients.</p><p>We used both convolutional and fully connected architectures, but in our experiments fully connected models were better for compression. For CIFAR and MNIST, we used fully connected networks with two hidden layers of width 256, trained with SGD, with a 0.005 learning rate and mini-batchs of size 128.</p><p>For CIFAR and MNIST, we used a LeNet-like network with 2 convolutional layers with 6 and 16 filters, both with kernels of size 5 and 3 fully connected layers. Each convolutional is followed by a ReLU activation and a max-pooling layer. The code will be publicly available. The first and the second fully connected layers are of dimension 120 and 84 and are followed by ReLU activations. The last one is followed by a softmax activation layer. The code for all models will be publicly available.</p><p>During the test phase, we sampled parametersθ from the learned distribution β, and used the model pθ for prediction. This explains why our test accuracy on MNIST is lower than other numerical results <ref type="bibr" target="#b5">(Blundell et al., 2015)</ref>, since they use for prediction the averaged model with parameterŝ θ = E θ∼βm,r [θ] = µ. But our goal was not to get the best prediction score, but to evaluate the model which was used for compression on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Technical details on prequential learning</head><p>Prequential Learning on MNIST. On MNIST, we used three different models:</p><p>1. The uniform probability over the labels.</p><p>2. A fully connected network or Multilayer Perceptron (MLP) with two hidden layers of dimension 256.</p><p>3. A VGG-like convolutional network with 8 convolutional layers with <ref type="bibr">32,</ref><ref type="bibr">32,</ref><ref type="bibr">64,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">128,</ref><ref type="bibr">256</ref> and 256 filters respectively and max pooling operators every two convolutional layers, followed by two fully connected layers of size 256.</p><p>For the two neural networks we used Dropout with probability 0.5 between the fully connected layers, and optimized the network with the Adam algorithm with learning rate 0.001.</p><p>The successive timestep for the prequential learning t 1 , t 2 , ..., t s are 8, <ref type="bibr">16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384 and 32768.</ref> For the prequential code results in <ref type="table" target="#tab_1">Table 1</ref>, we selected the best model, which was the VGG-like network.</p><p>Prequential Learning on CIFAR. On CIFAR, we used five different models:</p><p>1. The uniform probability over the labels.</p><p>2. A fully connected network or Multilayer Perceptron (MLP) with two hidden layers of dimension 512.</p><p>3. A shallow network, with one hidden layer and width 5000.</p><p>4. A convolutional network (tinyCNN) with four convolutional layers with 32 filters, and a maxpooling operator after every two convolutional layers. Then, two fully connected layers of dimension 256. We used Dropout with probability 0.5 between the fully connected layers.</p><p>5. A VGG-like network with 13 convolutional layers from <ref type="bibr" target="#b42">(Zagoruyko, 2015)</ref>. We trained this architecture with two learning procedures. The first one (VGGa) without batchnormalization and data augmentation, and the second one (VGGb) with both of them, as introduced in <ref type="bibr" target="#b42">(Zagoruyko, 2015)</ref>. In both of them, we used dropout regularization with parameter 0.5.</p><p>We optimized the network with the Adam algorithm with learning rate 0.001.</p><p>For prequential learning, the timesteps t 1 , t 2 , ..., t s were: <ref type="bibr">10,</ref><ref type="bibr">20,</ref><ref type="bibr">40,</ref><ref type="bibr">80,</ref><ref type="bibr">160,</ref><ref type="bibr">320,</ref><ref type="bibr">640,</ref><ref type="bibr">1280,</ref><ref type="bibr">2560,</ref><ref type="bibr">5120,</ref><ref type="bibr">10240,</ref><ref type="bibr">20480,</ref><ref type="bibr">40960</ref>. The training results can be seen in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>For the prequential code, all the results are in <ref type="figure" target="#fig_1">Figure 2</ref>. For the results in <ref type="table" target="#tab_1">Table 1</ref>, we selected the best model for the prequential code, which was VGGb.</p><p>E Switching between models against the catch-up phenomenon</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Switching between model classes</head><p>The solution introduced by <ref type="bibr" target="#b40">(Van Erven et al., 2012)</ref> against the catch-up phenomenon described in Section 3.4, is to switch between models, to always encode a data block with the best model at that point. That way, the encoding adapts itself to the number of data samples seen. The switching pattern itself has to be encoded. Results of the self-switch code on CIFAR with 2 different models: the shallow network, and the VGG-like network trained with data augmentation and batch normalization (VGGb). Performance is reported during online training, as a function of the number of samples seen so far. Top: test accuracy on a pack of data [t k ; t k+1 ) given data [1; t k ), as a function of t k . Second: codelength per sample (log loss) on a pack of data [t k ; t k+1 ) given data [1; t k ). Third: difference between the prequential cumulated codelength on data [1; t k ], and the uniform encoding. Bottom: compression ratio of the prequential code on data [1; t k ]. The catch-up phenomenon is clearly visible for both models: even if models with and without the selfswitch have similar performances after a training on the entire dataset, the standard model has lower performances than the uniform model (for the 1280 first labels for the VGGb network, and for the 10,000 first labels for the shallow network), and the code length for these first labels is large. The self-switch method solves this problem.</p><p>Let (p θ ) θ∈Θ be a model class. Letθ j (x 1:k , y 1:k ) be the parameter obtained by some optimization procedure after j epochs of training on data [1; k]. For instance, j = 0 would correspond to using an untrained model (usually close to the uniform model).</p><p>We call self-switch code the switch code obtained by switching among the family of models with different gradient descent stopping times j (based on the same parametric family (p θ ) θ∈Θ ). In practice, this means that at each step of the prequential encoding, after having seen data [1; t k ), we train the model on those data and record, at each epoch j, the loss obtained on data [t k ; t k+1 ). We then switch optimally between those. We incur the small additional cost of encoding the best number of epochs to be used (which was limited to 10) at each step.</p><p>The catch-up phenomenon and the beneficial effect of the self-switch code can be seen in <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>The self-switch code achieves similar compression bounds to the switch code, while storing only one network. On MNIST, there is no observable difference. On CIFAR, self-switch is only 300 bits (0.006 bit/label) worse than full 4-architecture switch.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Prequential code results on CIFAR. Results of prequential encoding on CIFAR with 5 different models: a small Multilayer Perceptron (MLP), a shallow network, a small convolutional layer (tinyCNN), a VGG-like network without data augmentation and batch normalization (VGGa) and the same VGG-like architecture with data augmentation and batch normalization (VGGb) (see Appendix D). Performance is reported during online training, as a function of the number of samples seen so far. Top left: codelength per sample (log loss) on a pack of data [t k ; t k+1 ) given data [1; t k ). Bottom left: test accuracy on a pack of data [t k ; t k+1 ) given data [1; t k ), as a function of t k . Top right: difference between the prequential cumulated codelength on data [1; t k ], and the uniform encoding. Bottom right: compression ratio of the prequential code on data [1; t k ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 0</head><label>2</label><figDesc>I d ) with σ 0 = 0.05, chosen to optimize the compression bounds. The chosen class of posterior was the class of multivariate gaussian distributions with diagonal covariance matrix {N (µ, Σ) , µ ∈ R d Σ diagonal}. It was parametrized by (β µ,ρ ) (µ,ρ)∈R d ×R d , with σ ∈ R d defined as σ i = log(1 + exp(ρ i )), and the covariance matrix Σ as the diagonal matrix with diagonal values σ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Compression with the self-switch method: Results of the self-switch code on CIFAR with 2 different models: the shallow network, and the VGG-like network trained with data augmentation and batch normalization (VGGb). Performance is reported during online training, as a function of the number of samples seen so far. Top: test accuracy on a pack of data [t k ; t k+1 ) given data [1; t k ), as a function of t k . Second: codelength per sample (log loss) on a pack of data [t k ; t k+1 ) given data [1; t k ). Third: difference between the prequential cumulated codelength on data [1; t k ], and the uniform encoding. Bottom: compression ratio of the prequential code on data [1; t k ]. The catch-up phenomenon is clearly visible for both models: even if models with and without the selfswitch have similar performances after a training on the entire dataset, the standard model has lower performances than the uniform model (for the 1280 first labels for the VGGb network, and for the 10,000 first labels for the shallow network), and the code length for these first labels is large. The self-switch method solves this problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>).</figDesc><table>Proposition 1 (Shannon-Huffman code). Suppose that Alice and Bob have agreed in advance on a 
model p, and both know the inputs x 1:n . Then there exists a code to transmit the labels y 1:n losslessly 
with codelength (up to at most one bit on the whole sequence) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Compression bounds via Deep Learning. Compression bounds given by different codes on two datasets, MNIST and CIFAR10. The Codelength is the number of bits necessary to send the labels to someone who already has the inputs. This codelength includes the description length of the model. The compression ratio for a given code is the ratio between its codelength and the codelength of the uniform code. The test accuracy of a model is the accuracy of its predictions on the test set. For 2-part and network compression codes, we report results from</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>First, we would like to thank the reviewers for their careful reading and their questions and comments. We would also like to thank Corentin Tallec for his technical help, and David Lopez-Paz, Moustapha Cissé, Gaétan Marceau Caron and Jonathan Laurent for their helpful comments and advice.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>s</head><p>. This costs − log 2 pθ s (y ts+1:ts+1 |x ts+1:ts+1 ) bits Bob decodes y ts+1:ts+1 end for <ref type="table">Table 2</ref>: Compression bounds by switching between models. Compression bounds given by different codes on two datasets, MNIST and CIFAR10. The Codelength is the number of bits necessary to send the labels to someone who already has the inputs. This codelength includes the description length of the model. The compression ratio for a given code is the ratio between its codelength and the codelength of the uniform code. The test accuracy of a model is the accuracy of its predictions on the test set. For variational and prequential codes, we selected the model and hyperparameters providing the best compression bound. Assume that Alice and Bob have agreed on a set of prediction strategies M = {p k , k ∈ I}. We define the set of switch sequences, S = { <ref type="figure">((t 1 , k 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CODE</head><p>) be a switch sequence. The associated prediction strategy p s (y 1:n |x 1:n ) uses model p ki on the time interval [t i ; t i+1 ), namely</p><p>where K i is such that K i = k l for t l ≤ i &lt; t l+1 . Fix a prior distribution π over switching sequences (see <ref type="bibr" target="#b40">(Van Erven et al., 2012)</ref> for typical examples).</p><p>Definition 4 (Switch code). Assume that Alice and Bob have agreed on a set of prediction strategies M and a prior π over S. The switch code first encodes a switch sequence s strategy, then uses the prequential code with this strategy:</p><p>where K i is the model used by switch sequence s at time i.</p><p>We then choose the switching strategy s * wich minimizes L sw s (y 1:n , x 1:n ). We tested switching between the uniform model, a small convolutional network (tinyCNN), and a VGG-like network with two training methods (VGGa, VGGb) (Appendix D). On MNIST, switching between models does not make much difference. On CIFAR10, switching by taking the best model on each interval [t k ; t k+1 ) saves more than 11 kbits, reaching a codelength of 34.6 kbits, and a compression ratio of 0.21. The cost L π (s) of encoding the switch s is negligible (see <ref type="table">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Self-Switch: Switching between variants of a model or hyperparameters</head><p>With standard switch, it may be cumbersome to work with different models in parallel. Instead, for models learned by gradient descent, we may use the same architecture but with different parameter values corresponding obtained at different gradient descent stopping times. This is a form of regularization via early stopping.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01350</idno>
		<ptr target="http://arxiv.org/abs/1706.01350" />
		<title level="m">On the Emergence of Invariance and Disentangling in Deep Representations</title>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Stronger generalization bounds for deep nets via a compression approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05296</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Do Deep Nets Really Need to be Deep?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2654" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Information-theoretic determination of minimax rates of convergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1564" to="1599" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PAC-MDL Bounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<idno>978-3-540-45167-9</idno>
	</analytic>
	<monogr>
		<title level="m">Learning Theory and Kernel Machines</title>
		<editor>B. Schölkopf and M. K. Warmuth</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="344" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weight Uncertainty in Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1613" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the intelligibility of the universe and the notions of simplicity, complexity and irreducibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Chaitin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thinking about Godel and Turing: Essays on Complexity</title>
		<imprint>
			<publisher>World scientific</publisher>
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Present Position and Potential Developments: Some Personal Views: Statistical Theory: The Prequential Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series A (General)</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">278</biblScope>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Thirty-Third Conference on Uncertainty in Artificial Intelligence<address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Risk Inflation Criterion for Multiple Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">I</forename><surname>George</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1947" to="1975" />
			<date type="published" when="1994-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jojic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09260</idno>
		<title level="m">Degrees of Freedom in Deep Neural Networks</title>
		<imprint>
			<date type="published" when="2016-03" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Practical Variational Inference for Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The Minimum Description Length principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Grünwald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<title level="m">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning both Weights and Connections for Efficient Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Keeping Neural Networks Simple by Minimizing the Description Length of the Weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Van Camp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth annual conference on Computational learning theory</title>
		<meeting>the sixth annual conference on Computational learning theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Variational Learning and Bits-Back Coding: An Information-Theoretic View to Bayesian Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Honkela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On Universal Prediction and Bayesian Confirmation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">384</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2007-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">Auto-Encoding Variational Bayes. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic Differentiation Variational Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="45" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient-Based Learning Applied to Document Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Measuring the Intrinsic Dimension of Objective Landscapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Farkhoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08838</idno>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">An introduction to Kolmogorov complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vitányi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bayesian compression for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ullrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3290" to="3300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
		<title level="m">Information Theory, Inference, and Learning Algorithms</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>cambridge edition</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Auto-encoders: reconstruction versus compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ollivier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.7752</idno>
		<ptr target="http://arxiv.org/abs/1403.7752" />
		<imprint>
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Information and complexity in statistical modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Density estimation by stochastic complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Speed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="315" to="323" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Discovering Neural Nets with Low Kolmogorov Complexity and High Generalization Capability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="857" to="873" />
			<date type="published" when="1997-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09274</idno>
		<title level="m">Compression of Neural Machine Translation Models via Pruning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shwartz-Ziv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00810</idno>
		<title level="m">Opening the Black Box of Deep Neural Networks via Information</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A formal theory of inductive inference. Information and control, 1964. C. Tallec and L. Blier. Pyvarinf : Variational Inference for PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Solomonoff</surname></persName>
		</author>
		<ptr target="https://github.com/ctallec/pyvarinf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep Learning and the Information Bottleneck Principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Theory Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Soft Weight-Sharing for Neural Network Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ullrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Meeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04008</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Catching Up Faster by Switching Sooner: A predictive approach to adaptive estimation with an application to the AIC-BIC Dilemma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Erven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grünwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De Rooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="361" to="417" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Margin-Aware Binarized Weight Networks for Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image and Graphics</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="590" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<ptr target="http://torch.ch/blog/2015" />
		<title level="m">92.45% on CIFAR-10 in Torch</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">This costs t 1 log 2 K bits. Bob decodes y 1:t1 . for s = 1 to S − 1 do Alice and Bob both computeθ s =θ</title>
		<imprint/>
	</monogr>
	<note>Alice encodes y 1:t1 with the uniform code. x 1:ts , y 1:ts , ω)</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Alice encodes y ts+1:ts+1 with model pθ</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
