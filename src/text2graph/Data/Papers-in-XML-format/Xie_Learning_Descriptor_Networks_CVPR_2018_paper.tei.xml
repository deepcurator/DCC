<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Descriptor Networks for 3D Shape Synthesis and Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hikvision Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Descriptor Networks for 3D Shape Synthesis and Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1. Introduction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Statistical models of 3D shapes</head><p>Recently, with the introduction of large 3D CAD datasets, e.g., ShapeNet <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b3">4]</ref>, some interesting attempts <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b16">17]</ref> have been made on object recognition and synthesis based on voxelized 3D shape data. From the perspective of statistical modeling, the existing 3D models can be grouped into two main categories: (1) 3D discriminators, such as Voxnet <ref type="bibr" target="#b15">[16]</ref>, which aim to learn a mapping from 3D voxel input to semantic labels for the purpose of 3D object classification and recognition, and (2) 3D generators, such as 3D-GAN <ref type="bibr" target="#b27">[28]</ref>, which are in the form of latent variable models that assume that the 3D voxel signals are generated by some latent variables. The training of discriminators usually relies on big data with annotations and is accomplished by a direct minimization of the prediction errors, while the training of the generators learns a mapping from the latent space to 3D voxel data space.</p><p>The generator model, while useful for synthesizing 3D shape patterns, involves a challenging inference step (i.e., sampling from the posterior distribution) in maximum likelihood learning, therefore variational inference <ref type="bibr" target="#b11">[12]</ref> and ad- * Equal contributions. versarial learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28]</ref> methods are commonly used, where an extra network is incorporated into the learning algorithm to get around the difficulty of the posterior inference.</p><p>The past few years have witnessed impressive progress on developing discriminator models and generator models for 3D shape data, however, there has not been much work in the literature on modeling 3D shape data based on energy-based models. We call this type of models the descriptive models or descriptor networks following <ref type="bibr" target="#b33">[34]</ref>, because the models describe the data based on bottom-up descriptive features learned from the data. The focus of the present paper is to develop a volumetric 3D descriptor network for voxelized shape data. It can be considered an alternative to 3D-GAN <ref type="bibr" target="#b27">[28]</ref> for 3D shape generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">3D shape descriptor network</head><p>Specifically, we present a novel framework for probabilistic modeling of volumetric shape patterns by combining the merits of energy-based model <ref type="bibr" target="#b13">[14]</ref> and volumetric convolutional neural network <ref type="bibr" target="#b15">[16]</ref>. The model is a probability density function directly defined on voxelized shape signal, and the model is in the form of a deep convolutional energy-based model, where the feature statistics or the energy function is defined by a bottom-up volumetric ConvNet that maps the 3D shape signal to the features. We call the proposed model the 3D DescriptorNet, because it uses a volumetric ConvNet to extract 3D shape features from the voxelized data.</p><p>The training of the proposed model follows an "analysis by synthesis" scheme <ref type="bibr" target="#b6">[7]</ref>. Different from the variational inference or adversarial learning, the proposed model does not need to incorporate an extra inference network or an adversarial discriminator in the learning process. The learning and sampling process is guided by the same set of parameters of a single model, which makes it a particularly natural and statistically rigorous framework for probabilistic 3D shape modeling.</p><p>Modeling 3D shape data by a probability density function provides distinctive advantages: First, it is able to synthesize realistic 3D shape patterns by sampling examples from the distribution via MCMC, such as Langevin dynamics. Second, the model can be modified into a conditional version, which is useful for 3D object recovery and 3D object superresolution. Specifically, a conditional probability density function that maps the corrupted (or low resolution) 3D object to the recovered (or high resolution) 3D object is trained, and then the 3D recovery (or 3D super-resolution) can be achieved by sampling from the learned conditional distribution given the corrupted or low resolution 3D object as the conditional input. Third, the model can be used in a cooperative training scheme <ref type="bibr" target="#b30">[31]</ref>, as opposed to adversarial training, to train a 3D generator model via MCMC teaching. The training of 3D generator in such a scheme is stable and does not encounter mode collapsing issue. Fourth, the model is useful for semi-supervised learning. After learning the model from unlabeled data, the learned features can be used to train a classifier on the labeled data.</p><p>We show that the proposed 3D DescriptorNet can be used to synthesize realistic 3D shape patterns, and its conditional version is useful for 3D object recovery and 3D object superresolution. The 3D generator trained by 3D DescriptorNet in a cooperative scheme carries semantic information about 3D objects. The feature maps trained by 3D DescriptorNet in an unsupervised manner are useful for 3D object classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Related work</head><p>3D object synthesis. Researchers in the fields of graphics and vision have studied the 3D object synthesis problems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref>. However, most of these object synthesis methods are nonparametric and they generate new patterns by retrieving and merging parts from an existing database. Our model is a parametric probabilistic model that requires learning from the observed data. 3D object synthesis can be achieved by running MCMC such as Langevin dynamics to draw samples from the learned distribution.</p><p>3D deep learning. Recently, the vision community has witnessed the success of deep learning, and researchers have used the models in the field of deep learning, such as convolutional deep belief network <ref type="bibr" target="#b28">[29]</ref>, deep convolutional neural network <ref type="bibr" target="#b15">[16]</ref>, and deep convolutional generative adversarial nets (GAN) <ref type="bibr" target="#b27">[28]</ref>, to model 3D objects for the sake of synthesis and analysis. Our proposed 3D model is also powered by the ConvNets. It incorporates a bottom-up 3D ConvNet structure for defining the probability density, and learns the parameters of the ConvNet by an "analysis by synthesis" scheme.</p><p>Descriptive models for synthesis. Our model is related to the following descriptive models. The FRAME (Filters, Random field, And Maximum Entropy) <ref type="bibr" target="#b34">[35]</ref> model, which was developed for modeling stochastic textures. The sparse FRAME model <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref>, which was used for modeling object patterns. Inspired by the successes of deep convolutional neural networks (CNNs or ConvNets), <ref type="bibr" target="#b14">[15]</ref> proposes a deep FRAME model, where the linear filters used in the original FRAME model are replaced by the non-linear filters at a certain convolutional layer of a pre-trained deep ConvNet. Instead of using filters from a pre-trained ConvNet, <ref type="bibr" target="#b32">[33]</ref> learns the ConvNet filters from the observed data by maximum likelihood estimation. The resulting model is called generative ConvNet, which can be considered a recursive multi-layer generalization of the original FRAME model.</p><p>Building on the early work of <ref type="bibr" target="#b24">[25]</ref>, recently <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13]</ref> have developed an introspective learning method to learn the energy-based model, where the energy function is discriminatively learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.">Contributions</head><p>(1) We propose a 3D deep convolutional energy-based model that we call 3D DescriptorNet for modeling 3D object patterns by combining the volumetric ConvNets <ref type="bibr" target="#b15">[16]</ref> and the generative ConvNets <ref type="bibr" target="#b32">[33]</ref>. <ref type="formula" target="#formula_3">(2)</ref> We present a mode seeking and mode shifting interpretation of the learning process of the model. <ref type="formula" target="#formula_5">(3)</ref> We present an adversarial interpretation of the zero temperature limit of the learning process. <ref type="formula" target="#formula_7">(4)</ref> We propose a conditional learning method for recovery tasks. <ref type="formula" target="#formula_8">(5)</ref> we propose metrics that can be useful for evaluating 3D generative models. (6) A 3D cooperative training scheme is provided as an alternative to the adversarial learning method to train 3D generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">3D DescriptorNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Probability density</head><p>The 3D DescriptorNet is a 3D deep convolutional energybased model defined on the volumetric data Y , which is in the form of exponential tilting of a reference distribution <ref type="bibr" target="#b32">[33]</ref>:</p><formula xml:id="formula_0">p(Y ; θ ) = 1 Z(θ ) exp [ f (Y ; θ )] p 0 (Y ),<label>(1)</label></formula><p>where p 0 (Y ) is the reference distribution such as Gaussian white noise model, i.e.,</p><formula xml:id="formula_1">p 0 (Y ) ∝ exp − Y 2 /2s 2 , f (Y ; θ )</formula><p>is defined by a bottom-up 3D volumetric ConvNet whose parameters are denoted by θ .</p><formula xml:id="formula_2">Z(θ ) = exp [ f (Y ; θ )] p 0 (Y )dY</formula><p>is the normalizing constant or partition function that is analytically intractable. The energy function is</p><formula xml:id="formula_3">E (Y ; θ ) = Y 2 2s 2 − f (Y ; θ ).<label>(2)</label></formula><p>We may also take p 0 (Y ) as uniform distribution within a bounded range. Then E (Y ; θ ) = − f (Y ; θ ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Analysis by synthesis</head><p>The maximum likelihood estimation (MLE) of the 3D DescriptorNet follows an "analysis by synthesis" scheme. Suppose we observe 3D training examples</p><formula xml:id="formula_4">{Y i , i = 1, ..., n} from an unknown data distribution P data (Y ). The MLE seeks to maximize the log-likelihood function L(θ ) = 1 n ∑ n i=1 log p(Y i ; θ )</formula><p>. If the sample size n is large, the maximum likelihood estimator minimizes KL(P data p θ ), the Kullback-Leibler divergence from the data distribution P data to the model distribution</p><formula xml:id="formula_5">p θ . The gradient of the L(θ ) is L ′ (θ ) = 1 n n ∑ i=1 ∂ ∂ θ f (Y i ; θ ) − E θ ∂ ∂ θ f (Y ; θ ) ,<label>(3)</label></formula><p>where E θ denotes the expectation with respect to p(Y ; θ ). The expectation term in equation <ref type="formula" target="#formula_5">(3)</ref> is due to</p><formula xml:id="formula_6">∂ ∂ θ log Z(θ ) = E θ [ ∂ ∂ θ f (Y ; θ )]</formula><p>, which is analytically intractable and has to be approximated by MCMC, such as Langevin dynamics, which iterates the following step:</p><formula xml:id="formula_7">Y τ+∆τ = Y τ − ∆τ 2 ∂ ∂Y E (Y τ ; θ ) + √ ∆τε τ = Y τ − ∆τ 2 Y τ s 2 − ∂ ∂Y f (Y τ ; θ ) + √ ∆τε τ ,<label>(4)</label></formula><p>where τ indexes the time steps of the Langevin dynamics, ∆τ is the discretized step size, and ε τ ∼ N(0, I) is the Gaussian white noise term. The Langevin dynamics consists of a deterministic part, which is a gradient descent on a landscape defined by E (Y ; θ ), and a stochastic part, which is a Brownian motion that helps the chain to escape spurious local minima of the energy E (Y ; θ ). Suppose we drawñ samples {Ỹ i , i = 1, ...,ñ} from the distribution p(Y ; θ ) by runningñ parallel chains of Langevin dynamics according to <ref type="bibr" target="#b3">(4)</ref>. The gradient of the log-likelihood L(θ ) can be approximated by</p><formula xml:id="formula_8">L ′ (θ ) ≈ 1 n n ∑ i=1 ∂ ∂ θ f (Y i ; θ ) − 1 nñ ∑ i=1 ∂ ∂ θ f (Ỹ i ; θ ).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Mode seeking and mode shifting</head><p>The above "analysis by synthesis" learning scheme can be interpreted as a mode seeking and mode shifting process. We can rewrite equation <ref type="formula" target="#formula_8">(5)</ref> in the form of</p><formula xml:id="formula_9">L ′ (θ ) ≈ ∂ ∂ θ 1 nñ ∑ i=1 E (Ỹ i ; θ ) − 1 n n ∑ i=1 E (Y i ; θ ) .<label>(6)</label></formula><p>We define a value function</p><formula xml:id="formula_10">V ({Ỹ i }; θ ) = 1 nñ ∑ i=1 E (Ỹ i ; θ ) − 1 n n ∑ i=1 E (Y i ; θ ).<label>(7)</label></formula><p>The equation <ref type="bibr" target="#b5">(6)</ref> reveals that the gradient of the loglikelihood L(θ ) coincides with the gradient of V . The sampling step in (4) can be interpreted as mode seeking, by finding low energy modes or high probability modes in the landscape defined by E (Y ; θ ) via stochastic gradient descent (Langevin dynamics) and placing the synthesized examples around the modes. It seeks to decrease V . The learning step can be interpreted as mode shifting (as well as mode creating and mode sharpening) by shifting the low energy modes from the synthesized examples {Ỹ i } toward the observed examples {Y i }. It seeks to increase V .</p><p>The training algorithm of the 3D DescriptorNet is presented in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 3D DescriptorNet</head><p>Input:</p><p>(1) training data {Y i , i = 1, ..., n}; (2) number of Langevin steps l; (3) number of learning iterations T . Output:</p><p>(1) estimated parameters θ ; (2) synthesized examples</p><formula xml:id="formula_11">{Ỹ i , i = 1, ...,ñ} 1: Let t ← 0, initialize θ (0) , initializeỸ i , for i = 1, ...,ñ. 2: repeat 3:</formula><p>Mode seeking: For each i, run l steps of Langevin dynamics to reviseỸ i , i.e., starting from the currentỸ i , each step follows equation (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Mode shifting:</p><formula xml:id="formula_12">Update θ (t+1) = θ (t) + γ t L ′ (θ (t) ),</formula><p>with learning rate γ t , where L ′ (θ (t) ) is computed according to (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Let t ← t + 1 6: until t = T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Alternating back-propagation</head><p>Both mode seeking (sampling) and mode shifting (learning) steps involve the derivatives of f (Y ; θ ) with respect to Y and θ respectively. Both derivatives can be computed efficiently by back-propagation. The algorithm is thus in the form of alternating back-propagation that iterates the following two steps: (1) Sampling back-propagation: Revise the synthesized examples by Langevin dynamics or gradient descent. (2) Learning back-propagation: Update the model parameters given the synthesized and the observed examples by gradient ascent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Zero temperature limit</head><p>We can add a temperature term to the model p <ref type="bibr">T</ref> </p><formula xml:id="formula_13">(Y ; θ ) = exp(−E (Y ; θ )/T )/Z T (θ )</formula><p>, where the original model corresponds to T = 1. At zero temperature limit as T → 0, the Langevin sampling will become gradient descent where the noise term diminishes in comparison to the gradient descent term. The resulting algorithm approximately solves the minimax problem below</p><formula xml:id="formula_14">max θ min {Ỹ i } V ({Ỹ i }; θ )<label>(8)</label></formula><p>withỸ i initialized from an initial distribution and approaching local modes of V . We can regularize either the diversity of {Ỹ i } or the smoothness of E (Y ; θ ). This is an adversarial interpretation of the learning algorithm. It is also a generalized version of herding <ref type="bibr" target="#b26">[27]</ref> and is related to <ref type="bibr" target="#b0">[1]</ref>. In our experiments, we find that disabling the noise term of the Langevin dynamics in the later stage of the learning process often leads to better synthesis results. Ideally the learning algorithm should create a large number of local modes with similar low energies to capture the diverse observed examples as well as unseen examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Conditional learning for recovery</head><p>The conditional distribution p(Y |C(Y ) = c; θ ) can be derived from p(Y ; θ ). This conditional form of the 3D DescriptorNet can be used for recovery tasks such as inpainting and super-resolution. In inpinating, C(Y ) consists of the visible part of Y . In super-resolution, C(Y ) is the low resolution version of Y . For such tasks, we can learn the model from the fully observed training data {Y i , i = 1, ..., n} by maximizing the conditional log-likelihood</p><formula xml:id="formula_15">L(θ ) = 1 n n ∑ i=1 log p(Y i | C(Y i ) = c i ; θ ),<label>(9)</label></formula><p>where c i is the observed value of C(Y i ). The learning and sampling algorithm is essentially the same as maximizing the original log-likelihood, except that in the Langevin sampling step, we need to sample from the conditional distribution, which amounts to fixing C(Y τ ) in the sampling process. The zero temperature limit (with the noise term in the Langevin dynamics disabled) approximately solves the following minimax problem</p><formula xml:id="formula_16">max θ min {Ỹ i :C(Ỹ i )=c i } V ({Ỹ i }; θ ).<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Teaching 3D generator net</head><p>We can let a 3D generator network learn from the MCMC sampling of the 3D DescriptorNet, so that the 3D generator network can be used as an approximate direct sampler of the 3D DescriptorNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D generator model</head><p>The 3D generator model <ref type="bibr" target="#b5">[6]</ref> is a 3D non-linear multi-layer generalization of the traditional factor analysis model. The generator model has the following form</p><formula xml:id="formula_17">Z ∼ N(0, I d ); Y = g(Z; α) + ε; ε ∼ N(0, σ 2 I D ).<label>(11)</label></formula><p>where Z is a d-dimensional vector of latent factors that follow N(0, 1) independently, and the 3D object Y is generated by first sampling Z from its known prior distribution N(0, I d ) and then transforming Z to the D-dimensional Y by a topdown deconvolutional network g(Z; α) plus the white noise ε. α denotes the parameters of the generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">MCMC teaching of 3D generator net</head><p>The 3D generator model can be trained simultaneously with the 3D DescriptorNet in a cooperative training scheme <ref type="bibr" target="#b30">[31]</ref>. The basic idea is to use the 3D generator to generate examples to initialize a finite step Langevin dynamics for training the 3D DescriptorNet. In return, the 3D generator learns from how the Langevin dynamics changes the initial examples it generates.</p><p>Specifically, in each iteration, (1) We generate Z i from its known prior distribution, and then generate the initial synthesized examples byŶ i = g(Z i <ref type="bibr" target="#b4">(5)</ref>, and update the parameters α of the 3D generator by gradient descent</p><note type="other">; α) + ε i for i = 1, ...,ñ. (2) Starting from the initial examples {Ŷ i }, we sample from the 3D DescriptorNet by running a finite number of steps of MCMC such as Langevin dynamics to obtain the revised synthesized examples {Ỹ i }. (3) We then update the parameters θ of the 3D DescriptorNet based on {Ỹ i } according to</note><formula xml:id="formula_18">∆α ∝ − ∂ ∂ α 1 nñ ∑ i=1 Ỹ i − g(Z i ; α) 2 .<label>(12)</label></formula><p>We call it MCMC teaching because the revised examples {Ỹ i } generated by the finite step MCMC are used to teach g(Z; α). For eachỸ i , the latent factors Z i are known to the 3D generator, so that there is no need to infer Z i , and the learning becomes a much simpler supervised learning problem. Algorithm 2 presents a full description of the learning of a 3D DescriptorNet with a 3D generator as a sampler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 MCMC teaching of 3D generator net</head><p>Input:</p><p>(1) training examples {Y i , i = 1, ..., n}, (2) numbers of Langevin steps l, (3) number of learning iterations T . Output:</p><p>(1) estimated parameters θ and α, (2) synthetic examples {Ŷ i ,Ỹ i , i = 1, ...,ñ} 1: Let t ← 0, initialize θ and α. Initializing mode seeking:</p><formula xml:id="formula_19">For i = 1, ...,ñ, generate Z i ∼ N(0, I d )</formula><p>, and generateŶ i = g(Z i ; α (t) ) + ε i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Mode seeking: For i = 1, ...,ñ, starting fromŶ i , run l steps of Langevin dynamics to obtainỸ i , each step following equation (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Mode shifting:</p><formula xml:id="formula_20">Update θ (t+1) = θ (t) + γ t L ′ (θ (t) ),</formula><p>where L ′ (θ (t) ) is computed according to (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Learning from mode seeking: Update α (t+1) according to (12). Let t ← t + 1 8: until t = T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Project page: The code and more results and details can be found at http://www.stat.ucla.edu/˜jxie/ 3DDescriptorNet/3DDescriptorNet.html</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">3D object synthesis</head><p>We conduct experiments on synthesizing 3D objects of categories from ModelNet dataset <ref type="bibr" target="#b28">[29]</ref>. Specifically, we use ModelNet10, a 10-category subset of ModelNet which is commonly used as benchmark for 3D object analysis. The categories are chair, sofa, bathtub, toilet, bed, desk, table, nightstand, dresser, and monitor. The size of the training set for each category ranges from 100 to 700.</p><p>For qualitative experiment, we learn one 3-layer 3D DescriptorNet for each object category in ModelNet10. The first layer has 200 16×16×16 filters with sub-sampling of 3, the second layer has 100 6 × 6 × 6 filters with sub-sampling of 2, and the final layer is a fully connected layer with a single filter that covers the whole voxel grid. We add ReLU layers between convolutional layers. We fix the standard deviation of the reference distribution of the model to be s = 0.5. The number of Langevin dynamics steps in each learning iteration is l=20 and the step size ∆τ = 0.1. We use Adam <ref type="bibr" target="#b10">[11]</ref> for optimization with β 1 = 0.5 and β 2 = 0.999. The learning rate is 0.001. The number of learning iterations is 3, 000. We disable the noise term in the Langevin step after 100 iterations. The training data are of size 32 × 32 × 32 voxels, whose values are 0 or 1. We prepare the training data by subtracting the mean value from the data. Each voxel value of the synthesized data is discretized into 0 or 1 by comparing with a threshold 0.5. The mini-batch size is 20. The number of parallel sampling chains for each batch is 25. <ref type="figure">Figure 1</ref> displays the observed 3D objects randomly sampled from the training set, and the synthesized 3D objects generated by our models for categories chair, bed, sofa, table, dresser, and toilet. We visualize volumetric data via isosurfaces in our paper. To show that our model can synthesize new 3D objects beyond the training set, we compare the synthesized patterns with their nearest neighbors in the training set. The retrieved nearest neighbors are based on ℓ 2 distance in the voxel space. As shown in <ref type="figure">Figure 1</ref>, our model can synthesize realistic 3D shapes, and the generated 3D objects are similar, but not identical, to the training set.</p><p>To quantitatively evaluate our model, we adopt the Inception score proposed by <ref type="bibr" target="#b25">[26]</ref>, which uses a reference convolutional neural network to compute</p><formula xml:id="formula_21">I({Ỹ i , i = 1, ...,ñ}) = exp EỸ KL(p(c|Ỹ ) p(c)) ,</formula><p>where c denotes category, {Ỹ i , i = 1, ...,ñ} are synthesized examples sampled from the model, p(c|Ỹ ) is obtained from the output of the reference network, and p(c) ≈</p><formula xml:id="formula_22">1 n ∑ñ i=1 p(c|Ỹ i ).</formula><p>Both a low entropy conditional category distribution p(c|Ỹ ) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Inception score 3D ShapeNets <ref type="bibr" target="#b28">[29]</ref> 4.126±0.193 3D-GAN <ref type="bibr" target="#b27">[28]</ref> 8.658±0.450 3D VAE <ref type="bibr" target="#b11">[12]</ref> 11.015±0.420 3D DescriptorNet (ours) 11.772±0.418</p><p>(i.e., the network classifies a given sample with high certainty) and a high entropy category distribution p(c) (i.e., the network identifies a wide variety of categories among the generated samples) can lead to a high inception score. In our experiment, we use a state-of-the-art 3D multi-view convolutional neural network <ref type="bibr" target="#b16">[17]</ref> trained on ModelNet dataset for 3D object classification as the reference network.</p><p>We learn a single model from mixed 3D objects from the training sets of 10 3D object categories of ModelNet10 dataset. <ref type="table" target="#tab_0">Table 1</ref> reports the Inception scores of our model as well as a comparison with some baseline models including 3D-GAN <ref type="bibr" target="#b27">[28]</ref>, 3D ShapeNets <ref type="bibr" target="#b28">[29]</ref>, and 3D-VAE <ref type="bibr" target="#b11">[12]</ref>.</p><p>We also evaluate the quality of the synthesized 3D shapes by the model learned from single category by using average softmax class probability that reference network assigns to the synthesized examples for the underlying category. <ref type="table" target="#tab_1">Table  2</ref> displays the results for all 10 categories. It can be seen that our model generates 3D shapes with higher softmax class probabilities than other baseline models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">3D object recovery</head><p>We then test the conditional 3D DescriptorNet on the 3D object recovery task. On each testing 3D object, we randomly corrupt some voxels of the 3D object. We then seek to recover the corrupted voxels by sampling from the con-  <ref type="table" target="#tab_0">obs1   obs2  obs3  syn1  syn2  syn3  syn4  syn5  syn6  nn1  nn2  nn3  nn4   chair   bed   sofa   table   dresser</ref> toilet <ref type="figure">Figure 1</ref>: Generating 3D objects. Each row displays one experiment, where the first three 3D objects are some observed examples, columns 4, 5, 6, 7, 8, and 9 are 6 of the synthesized 3D objects sampled from the learned model by Langevin dynamics. For the last four synthesized objects (shown in columns 6, 7, 8, and 9), their nearest neighbors retrieved from the training set are shown in columns 10, 11, 12, and 13.</p><p>Langevin dynamics, which is the same as the Langevin dynamics that samples from the full distribution p(Y ; θ ), except that we fix the uncorrupted part YM and only update the corrupted part Y M throughout the Langevin dynamics.</p><p>In the learning stage, we learn the model from the fully observed training 3D objects. To specialize the learned model to this recovery task, we learn the conditional distribution p(Y M |YM; θ ) directly. That is, in the learning stage, we also randomly corrupt each fully observed training 3D object Y , and run Langevin dynamics by fixing YM to obtain the synthesized 3D object. The parameters θ are then updated by gradient ascent according to <ref type="bibr" target="#b4">(5)</ref>. The network architecture for recovery is the same as the one used in Section 4.1 for synthesis. The number of Langevin dynamics steps for recovery in each iteration is set to be l = 90 and the step size is ∆τ = 0.07. The number of learning iterations is 1, 000. The size of the mini-batch is 50. The 3D training data are of size 32 × 32 × 32 voxels.</p><p>After learning the model, we recover the corrupted voxels in each testing data Y by sampling from p(Y M |YM, θ ) by running 90 Langevin dynamics steps. In the training stage, we randomly corrupt 70% of each training 3D shape. In the testing stage, we experiment with the same percentage of corruption. We compare our method with 3D-GAN and 3D ShapeNets. We measure the recovery error by the average of per-voxel differences between the original testing data and the corresponding recovered data on the corrupted voxels.  <ref type="figure" target="#fig_3">Figure 2</ref> displays some examples of 3D object recovery. For each experiment, the first row displays the original 3D objects, the second row displays the corrupted 3D objects, and the third row displays the recovered 3D objects that are sampled from the learned conditional distributions given the corrupted 3D objects as inputs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">3D object super-resolution</head><p>We test the conditional 3D DescriptorNet on the 3D object super-resolution task. Similar to Experiment 4.2, we can perform super-resolution on a low resolution 3D objects by sampling from a conditional 3D DescriptorNet p(Y high |Y low , θ ),  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analyzing the learned 3D generator</head><p>We evaluate a 3D generator trained by a 3D DescriptorNet via MCMC teaching. The generator network g(Z; α) has 4 layers of volumetric deconvolution with 4 × 4 × 4 kernels, with up-sampling factors {1, 2, 2, 2} at different layers respectively. The numbers of channels at different layers are 256, 128, 64, and 1. There is a fully connected layer under the 100 dimensional latent factors Z. The output size is 32 × 32 × 32. Batch normalization and ReLU layers are used between deconvolution layers and tanh non-linearity is added at the bottom-layer. We train a 3D DescriptorNet with the above 3D generator as a sampler in a cooperative training scheme presented in Algorithm 2 for the categories of toilet, sofa, and nightstand in ModelNet10 dataset independently. The 3D DescriptorNet has a 4-layer network, where the first layer has 64 9 × 9 × 9 filters, the second layer has 128 7 × 7 × 7 filters, the third layer has 256 4 × 4 × 4 filters, and the fourth layer is a fully connected layer with a single filter. The sub-sampling factors are {2, 2, 2, 1}. ReLU layers are used between convolutional layers.</p><p>We use Adam for optimization of 3D DescriptorNet with β 1 = 0.4 and β 2 = 0.999, and for optimization of 3D gener-  <ref type="figure">Figure 4</ref> shows some examples of 3D objects generated by the 3D generators trained by the 3D DescriptorNet via MCMC teaching.</p><p>We show results of interpolating between two latent vectors of Z in <ref type="figure">Figure 5</ref>. For each row, the 3D objects at the two ends are generated from Z vectors that are randomly sampled from N(0, I d ). Each object in the middle is obtained by first interpolating the Z vectors of the two end objects, and then generating the objects using the 3D generator. We observe smooth transitions in 3D shape structure and that most intermediate objects are also physically plausible. This experiment demonstrates that the learned 3D generator embeds the 3D object distribution into a smooth low dimensional manifold. Another way to investigate the learned 3D generator is to show shape arithmetic in the latent space. As shown in <ref type="figure">Figure 6</ref>, the 3D generator is able to encode semantic knowledge of 3D shapes in its latent space such that arithmetic can be performed on Z vectors for visual concept manipulation of 3D shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">3D object classification</head><p>We evaluate the feature maps learned by our 3D DescriptorNet. We perform a classification experiment on ModelNet10 dataset. We first train a single model on all categories of the training set in an unsupervised manner. The network architecture and learning configuration are the same as the one used for synthesis in Section 4.1. Then we use the model as a feature extractor. Specifically, for each input 3D object, we use the model to extract its first and second layers of feature maps, apply max pooling of kernel sizes 4 × 4 × 4 and 2 × 2 × 2 respectively, and concatenate the outputs as a feature vector of length 8,100. We train a multinomial logistic regression classifier from labeled data based on the extracted feature vectors for classification. We evaluate the classification accuracy of the classifier on the testing data using the one-versus-all rule. For comparison, <ref type="table" target="#tab_4">Table 4</ref> lists 8 published results on this dataset obtained by other baseline methods. Our method outperforms the other methods in terms of classification accuracy on this dataset.  <ref type="bibr" target="#b22">[23]</ref> 88.4% PANORAMA-NN <ref type="bibr" target="#b18">[19]</ref> 91.1% ECC <ref type="bibr" target="#b21">[22]</ref> 90.0% 3D ShapeNets <ref type="bibr" target="#b28">[29]</ref> 83.5% DeepPano <ref type="bibr" target="#b20">[21]</ref> 85.5% SPH <ref type="bibr" target="#b9">[10]</ref> 79.8% VConv-DAE <ref type="bibr" target="#b19">[20]</ref> 80.5% 3D-GAN <ref type="bibr" target="#b27">[28]</ref> 91.0% 3D DescriptorNet (ours) 92.4%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose the 3D DescriptorNet for volumetric object synthesis, and the conditional 3D DescriptorNet for 3D object recovery and 3D object super resolution. The proposed model is a deep convolutional energy-based model, which can be trained by an "analysis by synthesis" scheme. The training of the model can be interpreted as a mode seeking and mode shifting process, and the zero temperature limit has an adversarial interpretation. A 3D generator can be taught by the 3D DescriptorNet via MCMC teaching. Experiments demonstrate that our models are able to generate realistic 3D shape patterns and are useful for 3D shape analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>ditional distribution p(Y M |YM; θ ) according to the learned model p(Y ; θ ), where M andM denote the corrupted and uncorrupted voxels, and Y M and YM are the corrupted part and the uncorrupted part of the 3D object Y respectively. The sampling of p(Y M |YM; θ ) is again accomplished by the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: 3D object recovery by sampling from the conditional 3D DescriptorNet models. In each category, the first row displays the original 3D objects, the second row shows the corrupted 3D objects, and the third row displays the recovered 3D objects by running Langevin dynamics starting from the corrupted objects. (a) chair, (b) night stand.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: 3D object super-resolution by conditional 3D DescriptorNet. The first row displays some original 3D objects (64×64×64 voxels). The second row shows the corresponding low resolution 3D objects (16 × 16 × 16 voxels). The last row displays the corresponding super-resolution results which are obtained by sampling from the conditional 3D DescriptorNet by running 10 steps of Langevin dynamics initialized with the objects shown in the second row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 4: Synthesis by 3D generators toilet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Inception scores of different methods of learning from 10 3D object categories.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Softmax class probability</figDesc><table>category 
ours 
[28] 
[12] 
[29] 
bathtub 
0.8348 0.7017 0.7190 0.1644 
bed 
0.9202 0.7775 0.3963 0.3239 
chair 
0.9920 0.9700 0.9892 0.8482 
desk 
0.8203 0.7936 0.8145 0.1068 
dresser 
0.7678 0.6314 0.7010 0.2166 
monitor 
0.9473 0.2493 0.8559 0.2767 
night stand 0.7195 0.6853 0.6592 0.4969 
sofa 
0.9480 0.9276 0.3017 0.4888 
table 
0.8910 0.8377 0.8751 0.7902 
toilet 
0.9701 0.8569 0.6943 0.8832 

Avg. 
0.8811 0.7431 0.7006 0.4596 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3</head><label>3</label><figDesc>displays the numerical comparison results for the 10 categories.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Recovery errors in occlusion experiments</figDesc><table>category 
ours 
[28] 
[29] 
bathtub 
0.0152 0.0266 0.0621 
bed 
0.0068 0.0240 0.0617 
chair 
0.0118 0.0238 0.0444 
desk 
0.0122 0.0298 0.0731 
dresser 
0.0038 0.0384 0.1558 
monitor 
0.0103 0.0220 0.0783 
night stand 0.0080 0.0248 0.2925 
sofa 
0.0068 0.0186 0.0563 
table 
0.0051 0.0326 0.0340 
toilet 
0.0119 0.0180 0.0977 

Avg. 
0.0092 0.0259 0.0956 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc>3D object classification on ModelNet10 dataset</figDesc><table>Method 
Accuracy 
Geometry Image </table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The work is supported by Hikvision gift fund, DARPA SIMPLEX N66001-15-C-4035, ONR MURI N00014-16-1-2007, DARPA ARO W911NF-16-1-0579, and DARPA N66001-17-2-4029. We thank Erik Nijkamp for his help on coding. We thank Siyuan Huang for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3D faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 26th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An algorithm and data structure for 3D object synthesis using surface patch intersections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="255" to="263" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3D model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a predictable and generative vector representation for objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="484" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wardefarley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pattern theory: from representation to inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Grenander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Introspective classification with convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lazarow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="823" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A probabilistic model for component-based shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">55</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rotation invariant spherical harmonic representation of 3D shape descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on geometry processing</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="156" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Introspective neural networks for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lazarow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2774" to="2783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Predicting Structured Data</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning FRAME models using CNN filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.08379</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Voxnet: A 3D convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view CNNs for object classification on 3D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploiting the panorama representation for convolutional neural network classification and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sfikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Theoharis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pratikakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Workshop on 3D Object Retrieval</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">VConv-DAE: Deep volumetric shape learning without object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="236" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DeepPano: Deep panoramic representation for 3-D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2339" to="2343" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning 3D shape surfaces using geometry images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="223" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multiview convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning generative models via discriminative approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving generative adversarial networks with denoising feature matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Herding dynamical weights to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1121" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3D shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning sparse FRAME models for natural image patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="91" to="112" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09408</idno>
		<title level="m">Cooperative training of descriptor and generator networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Inducing wavelets into random fields via generative boosting. Applied and Computational Harmonic Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="4" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A theory of generative convnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Statistical modeling and conceptualization of visual patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="691" to="712" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Filters, random fields and maximum entropy (FRAME): Towards a unified theory for texture modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="126" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
