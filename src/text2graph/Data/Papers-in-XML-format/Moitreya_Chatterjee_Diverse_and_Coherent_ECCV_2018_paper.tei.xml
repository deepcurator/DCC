<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diverse and Coherent Paragraph Generation from Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moitreya</forename><surname>Chatterjee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
							<email>aschwing@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Diverse and Coherent Paragraph Generation from Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Captioning</term>
					<term>Review Generation</term>
					<term>Variational Autoencoders</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Paragraph generation from images is an important task for video summarization, editing, and support of the disabled, which has gained popularity recently. Traditional image captioning methods fall short on this front, since they aren't designed to generate long informative descriptions. However, the naive approach of simply concatenating multiple short sentences, possibly synthesized from traditional image captioning systems, doesn't embrace the intricacies of paragraphs: coherent sentences, globally consistent structure, and diversity. To address those challenges, we propose to augment paragraph generation techniques with "coherence vectors," "global topic vectors," and modeling of the inherent ambiguity of associating paragraphs with images via a variational auto-encoder formulation. We demonstrate the effectiveness of the developed approach on two datasets, outperforming existing state-of-the-art techniques on both.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Daily, we effortlessly describe fun events to friends and family, showing them pictures to underline the main plot. The narrative ensures that our audience can follow along step by step and picture the missing pieces in their mind with ease. Key to filling in the missing pieces is a consistency in our narrative which generally follows the arrow of time.</p><p>While computer vision, natural language processing and artificial intelligence techniques, more generally, have made great progress in describing visual content via image or video captioning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36]</ref>, the obtained result is generally a single sentence of around 20 words, describing the main observation. Even if brevity caters to today's short attention span, 20 words are hardly enough to describe subtle interactions, let alone detailed plots of our experience. Those are much more meaningfully depicted in a paragraph of reasonable length.</p><p>To this end, visual paragraph generation methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b24">25]</ref>, which have been proposed very recently, provide a longer narrative when describing a given image or video. However, as argued initially, coherence between successive sentences of the narrative is a key necessity to effectively convey the plot of our experience. Importantly, models for many of the aforementioned methods provide</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regions -Hierarchical Our Approach</head><p>A man in red shirt is walking on a street. Another man is standing next to him. A building is in the background. A trash can is next to the men. There are many cars next to the building. Many green trees are behind the man.</p><p>Two men are walking outside on a city street next to a building. Several green trees are behind the two men. A trash can is next to the two men. The trash can is green in color. The background has a building. The background has many cars.</p><p>A man in a black shirt is playing a piano. A woman is standing behind the man. Behind the man there is a white wall with a window. The piano is black.</p><p>There is a tree next to the man. It has green leaves.</p><p>A man in black shirt is playing a piano inside a room.</p><p>The piano is black in color.</p><p>A woman in a white dress is standing behind the man with her right arm extended up.</p><p>Behind the woman is a tree. The room has white walls.</p><p>In the background there is a tree with green leaves and a window next to it.</p><p>There is a bus driving on the road. It is painted yellow and red.</p><p>There is a large white building.</p><p>The building has plenty of windows. A man is sitting next to the bus.</p><p>There is a tall tree with green leaves behind the bus.</p><p>A yellow bus with orange stripes is on the city street.</p><p>It is stopped at a bus stop.</p><p>A man is sitting next to the bus in the bus stop.</p><p>In the background is a large white building. The building has many glass windows. A tall tree with green leaves is in the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1.</head><p>Paragraphs generated with a prior state-of-the-art technique <ref type="bibr" target="#b20">[21]</ref> and with our developed approach. Due to the introduced 'Coherence Vectors' we observe the generated paragraphs to be much more consistent than prior work <ref type="bibr" target="#b20">[21]</ref> .</p><p>no explicit mechanisms to ensure cross-sentence topic consistency, although a notable exception is the work of Liang et al . <ref type="bibr" target="#b24">[25]</ref>. In particular, Liang et al . <ref type="bibr" target="#b24">[25]</ref> propose to ensure consistency across sentence themes by training a standard paragraph generation module <ref type="bibr" target="#b20">[21]</ref>, coupled with an attention mechanism, under a Generative Adversarial Network (GAN) <ref type="bibr" target="#b12">[13]</ref> setting which has an additional loss-term to enforce this consistency. However, difficulties associated with training GANs <ref type="bibr" target="#b2">[3]</ref>, leaves their method vulnerable to generating incoherent paragraphs.</p><p>Different from prior work, we explicitly focus on modeling the diverse yet coherent possibilities of successive sentences when generating a paragraph, while ensuring the 'big picture' underlying the image does not get lost in the details. To this end we develop a model that propagates, what we call "Coherence Vectors," which ensure cross-sentence topic smoothness, and a "Global Topic Vector," which captures the summarizing information about the image. Additionally, we observe improvements in the quality of the generated paragraphs, when our model is trained to incorporate diversity. Intuitively, the coherence vector embeds the theme of the most recently generated sentence. The topic vector of the next sentence is combined with the coherence vector from the most recently generated one and the global topic vector to generate a new topic vector, with the intention to ensure a smooth flow of the theme across sentences. <ref type="figure">Figure 1</ref> illustrates a sampling of a synthesized paragraph, given an input image, using our method vis-à-vis prior work <ref type="bibr" target="#b20">[21]</ref>. Notably, using our model we observe a smooth transition between sentence themes, while capturing summarizing information about the image. For instance, generated paragraphs corresponding to the images in the first and the third rows in <ref type="figure">Figure 1</ref> indicate that the images have been captured in a 'city' setting.</p><p>Following prior work we quantitatively evaluate our approach on the standard Stanford Image-Paragraph dataset <ref type="bibr" target="#b20">[21]</ref>, demonstrating state-of-the-art performance. Furthermore, different from all existing methods, we showcase the generalizability of our model, evaluating the proposed approach by generating reviews from the "Office-Product" category of the Amazon product review dataset <ref type="bibr" target="#b28">[29]</ref> and by showing significant gains over all baselines.</p><p>In the next section, we discuss prior relevant work before discussing the details of our proposed approach in Section 3. Section 4 discusses the results of empirical evaluation. We finally conclude in Section 5, laying out avenues for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>For a long time, associating language with visual content has been in the focus of research <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b3">4]</ref>. Early techniques in this area associated linguistic 'tag-words' with visual data. Gradually, we focused on generating entire sentences and paragraphs for visual data by bringing together techniques from both natural language processing and computer vision with the aim of building holistic AI systems that integrate naturally into common surroundings. Two tasks that spurred the growth of recent work in the language-vision area are Image Captioning <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b41">42]</ref>, and Visual Question Answering <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref>. More recently, image captioning approaches were extended to generate natural language descriptions at the level of paragraphs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref>. In the following, we review related work from the area of image captioning and visual paragraph generation, in greater detail, and point out the distinction with our work.</p><p>Image Captioning: Image Captioning is the task of generating textual descriptions, given an input image. Classical methods for image captioning, are usually non-parametric. These methods build a pool of candidate captions from the training set of image-caption pairs, and at test time, a fitness function is used to retrieve the most compelling caption for a given input image <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b3">4]</ref>. However the computationally demanding nature of the matching process imposes a bottleneck when considering a set of descriptions of a reasonable size.</p><p>To address this problem, Recurrent Neural Network (RNN)-based approaches have come into vogue <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b9">10]</ref> lately. These approaches, typically, first use a Convolutional Neural Network (CNN) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b22">23]</ref> to obtain an encoding of the given input image. This encoding is then fed into an RNN which samples a set of words (from a dictionary of words) that agree the most with the image encoding. However, the captions generated through such techniques are short, spanning typically a single sentence of at most 20 words. Our approach differs from the aforementioned image captioning techniques, in that we generate a paragraph of multiple sentences rather than a short caption. Importantly, captioning techniques generally don't have to consider coherence across sentences, which is not true for paragraph generation approaches which we review next.</p><p>Visual Paragraph Generation: From a distance, the task of Visual Paragraph Generation resembles image captioning: given an image, generate a textual description of its content <ref type="bibr" target="#b20">[21]</ref>. However, of importance for visual paragraph generation is the attention to detail in the textual description. In particular, the system is expected to generate a paragraph of sentences (typically 5 or 6 sentences per paragraph) describing the image in great detail. Moreover, in order for the paragraph to resemble natural language, there has to be a smooth transition across the themes of the sentences of the paragraph.</p><p>Early work in generating detailed captions, include an approach by Johnson et al . <ref type="bibr" target="#b15">[16]</ref>. While generating compelling sentences individually, a focus on a theme of the story underlying a given image was missing. This problem was addressed by Krause et al . <ref type="bibr" target="#b20">[21]</ref>. Their language model consists of a two-stage hierarchy of RNNs. The first RNN level generates sentence topics, given the visual representation of semantically salient regions in the image. The second RNN level translates this topic vector into a sentence. This model was further extended by Liang et al . <ref type="bibr" target="#b24">[25]</ref> to encourage coherence amongst successive sentences. To this end, the language generation mechanism of Krause et al . <ref type="bibr" target="#b20">[21]</ref>, coupled with an attention mechanism, was trained in a Generative Adversarial Network (GAN) setting, where the discriminator is intended to encourage this coherence at training time. Dai et al . <ref type="bibr" target="#b7">[8]</ref> also train a GAN for generating paragraphs. However, known difficulties of training GANs <ref type="bibr" target="#b2">[3]</ref> pose challenges towards effectively implementing such systems. Xie et al . introduce regularization terms for ensuring diversity <ref type="bibr" target="#b38">[39]</ref> but then this results in a constrained optimization problem, which does not admit a closed form solution and is thus hard to implement. Different from these approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b38">39]</ref>, we demonstrate that a change of the generation mechanism is better suited to obtain coherent sentence structure. To this end we introduce Coherence Vectors which ensure a gradual transition of themes between sentences.</p><p>Additionally, different from prior work, we also incorporate a summary of the topic vectors to sensitize the model to the 'main plot' underlying the image. Furthermore, to capture the inherent ambiguity of paragraph generation from images, i.e., multiple paragraphs can successfully describe an image, we cast our paragraph-generation model as a Variational Autoencoder (VAE) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14]</ref>, enabling our model to generate a set of diverse paragraphs, given an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Proposed Method for Paragraph Generation</head><p>As mentioned before, coherence of sampled sentences is important for automatic generation of human-like paragraphs from visual data, while not losing sight of the underlying 'big picture' story illustrated in the image. Further, another valuable element for an automated paragraph generation system is the diversity of the generated text. In the following we develop a framework which takes into account these properties. We first provide an overview of the approach in Section 3.1, before discussing our approach to generate coherent paragraphs in Section 3.2 and finally our technique to obtain diverse paragraphs in Section 3.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>To generate a paragraph y = (y 1 , . . . , y S ) consisting of S sentences y i , i ∈ {1, . . . , S}, each with N i words y i,j , j ∈ {1, . . . , N i }, for an image x, we make use of a deep net composed out of two modules which are coupled hierarchically: the Topic Generation Net and the Sentence Generation Net.</p><p>The Topic Generation Net illustrated in <ref type="figure">Figure 2</ref> seeks to extract a set of S topic vectors, T i ∈ R H ∀i ∈ {1, . . . , S}, given an appropriate visual representation of the input image x. The topic generation net is a parametric function which, recursively at every timestep, produces a topic vector T i and a probability measure u i indicating if more topics are to be generated. We implement this function using a recurrent net, subsequently also referred to as the SentenceRNN. We then leverage the topic vectors T i to construct a Global Topic Vector G ∈ R H , which captures the underlying image summary. This global topic vector is constructed via a weighted combination of the aforementioned topic vectors T i . <ref type="figure">Figure 2</ref> illustrates a detailed schematic of the topic generation net. Formally </p><formula xml:id="formula_0">we use (G, {(T i , u i )} S i=1 ) = Γ w T (x) to</formula><formula xml:id="formula_1">T i , u i )} S i=1</formula><p>are the output which also constitute the input to the second module.</p><p>The second module of the developed approach, called the Sentence Generation Net, is illustrated in <ref type="figure">Figure 3</ref>. Based on the output of the topic generation net, it is responsible for producing a paragraph y, one sentence y i at a time.</p><p>Formally, the sentence generation module is also modeled as a parametric function which synthesizes a sentence y i , one word y i,j at a time. More specifically, a recurrent net Γ ws (·, ·) is used to obtain the predicted word probabilities {p i,j } Ni j=1 = Γ ws (T i , G), where w s subsumes all the parameters of the net, and</p><formula xml:id="formula_2">p i,j ∈ [0, 1]</formula><p>V ∀j ∈ {1, . . . , N i } is a probability distribution over the set of V words in our vocabulary. We realize the function, Γ ws (·, ·) using a recurrent net, subsequently referred to as the WordRNN.</p><p>In order to incorporate cross-sentence coherence, rather than directly using the topic vector T i in the WordRNN, we first construct a modified topic vector T ′ i , which better captures the theme of the i th sentence. For every sentence i, we compute T</p><formula xml:id="formula_3">′ i ∈ R</formula><p>H via a Coupling Unit, by combining the topic vector T i , the global vector G and a previous sentence representation C i−1 , called a Coherence Vector, which captures properties of the sentence generated at step i−1. Note that the synthesis of the first sentence begins by constructing T ′ 1 , which is obtained by coupling T 1 with the global topic vector G, and an all zero vector.</p><p>Visual Representation: To obtain an effective encoding of the input image, x, we follow Johnson et al . <ref type="bibr" target="#b15">[16]</ref>. More specifically, a Convolutional Neural Network (CNN) (VGG-16 <ref type="bibr" target="#b33">[34]</ref>) coupled with a Region Proposal Network (RPN) gives fixed-length feature vectors for every detection of a semantically salient region in the image. The obtained set of vectors {v 1 , . . . , v M } with v i ∈ R D each correspond to a region in the image. We subsequently pool these vectors into a single vector, v ∈ R I -following the approach of Krause et al . <ref type="bibr" target="#b20">[21]</ref>. This pooled representation contains relevant information from the different semantically salient regions in the image, which is supplied as input to our topic generation net. Subsequently, we use v and x interchangeably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Coherent Paragraph Generation</head><p>The construction of coherent paragraphs adopts a two-step approach. In the first step, we derive a set of individual and a global topic-vector starting with the pooled representation of the image. This is followed by paragraph synthesis.</p><formula xml:id="formula_4">Topic Generation: The Topic Generation Net (G, {(T i , u i )} S i=1 ) = Γ w T (x)</formula><p>constructs a set of relevant topics T i for subsequent paragraph generation given an image x. <ref type="figure">Figure 2</ref> provides a schematic of the proposed topic generation module. At first, the pooled visual representation of the image, v, is used as input for the SentenceRNN. The SentenceRNN is a single layer Gated Recurrent Unit (GRU) <ref type="bibr" target="#b5">[6]</ref>, parameterized by w T . It takes an image representation v as input and produces a probability distribution u i , over the labels 'CONTINUE' or 'STOP,' while its hidden state is used to produce the topic vector T i ∈ R H via a 2-layer densely connected deep neural network. A 'CONTINUE' label (u i &gt; 0. <ref type="bibr" target="#b4">5)</ref>, indicates that the recurrence should proceed for another time step, while a 'STOP' symbol terminates the recurrence. However, automatic description of an image via paragraphs necessitates tying all the sentences of the paragraph to a 'big picture' underlying the scene. For example, in the first image in <ref type="figure">Figure 1</ref>, the generated paragraph should ideally reflect that it is an image captured in a 'city' setting. To encourage this ability we construct a Global Topic Vector G ∈ R H for a given input image (see <ref type="figure">Figure 2)</ref>. Intuitively, we want this global topic vector to encode a holistic understanding of the image, by combining the aforementioned individual topic vectors as follows:</p><formula xml:id="formula_5">G = n i=1 α i T i where α i = ||T i || 2 i ||T i || 2 .<label>(1)</label></formula><p>Our intention is to facilitate representation of 'meta-concepts' (like 'city') as a weighted combination of its potential constituents (like 'car,' 'street,' 'men,' etc.).</p><p>The synthesized global vector and the topic vectors are then propagated to the sentence generation net which predicts the words of the paragraph.</p><p>Sentence Generation: Given the individual topic vectors T i and the global topic vector G, the Sentence Generation Net synthesizes sentences of the paragraph by computing word probabilities {p i,j } Ni j=1 = Γ ws (T i , G), conditioned on the previous set of synthesized words (see <ref type="figure">Figure 3)</ref>. One sentence is generated for each of the S individual topic vectors T 1 , . . . , T S . Synthesis of the i th sentence commences by combining via the Coupling Unit the topic vector T i , the global topic vector G, and the consistency ensuring Coherence Vector C i−1 ∈ R H . The Coupling Unit produces a modified topic vector (T ′ i ∈ R H ), which is propagated to the WordRNN to synthesize the sentence. The WordRNN is a 2-layer GRU, which generates a sentence, y i , one word at a time, conditioned on the previously synthesized words. The j th word of the i th sentence is obtained by selecting the word with the highest posterior probability, p i,j , over the entries of the vocabulary V . A sentence is terminated when either the maximum word limit per sentence is reached or an 'EOS' token is predicted. In the following, we describe the mechanism for constructing the coherence vectors, and the coupling technique referenced above.</p><p>Coherence Vectors: An important element of human-like paragraphs is coherence between the themes of successive sentences, which ensures a smooth flow of the line of thought in a paragraph.</p><p>As shown in <ref type="figure">Figure 3</ref>, we encourage topic coherence across sentences by constructing Coherence Vectors. In the following we describe the process of building these vectors. In order to compute the coherence vector for the (i − 1) th sentence, we extract the hidden layer representation (∈ R H ) from the WordRNN, after having synthesized the last word of the (i − 1) th sentence. This encoding carries information about the (i − 1) th sentence, and if favorably coupled with the topic vector T i of the i th sentence, encourages the theme of the i th sentence to be coherent with the previous one. However, for the aforementioned coupling to be successful, the hidden layer representation of the (i − 1) th sentence still needs to be transformed to a representation that lies in the same space as the set of topic vectors. This transformation is achieved by propagating the final representation of the (i − 1) th sentence through a 2-layer deep net of fully connected units, with the intermediate layer having H activations. We used Scaled Exponential Linear Unit (SeLU) activations <ref type="bibr" target="#b19">[20]</ref> for all neurons of this deep net. The output of this network is what we refer to as 'Coherence Vector,' C (i−1) .</p><p>Coupling Unit: Having obtained the coherence vector C i−1 from the (i − 1) th sentence, a Coupling Unit combines it with the topic vector of the next sentence, T i , and the global topic representation G. This process is illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>More specifically, we first combine C i−1 and T i into a vector T C i ∈ R H which is given by the solution to the following optimization problem:</p><formula xml:id="formula_6">T C i = arg min T C i α||T i −T C i || 2 2 + β||C i−1 −T C i || 2 2</formula><p>with α, β ≥ 0.</p><p>The solution, when α, β both are not equal to 0, is given by:</p><formula xml:id="formula_7">T C i = αT i + βC i−1 α + β .</formula><p>We refer the interested reader to the supplementary for this derivation. Intuitively, this formulation encourages T C i to be 'similar' to both the coherence vector, C i−1 and the current topic vector, T i -thereby aiding cross-sentence topic coherence. Moreover, the closed form solution of this formulation permits an efficient implementation as well.</p><p>This obtained vector, T C i is then coupled with the global topic vector G, via a gating function. We implement this gating function using a single GRU layer with vector T C i as input and global topic vector G as its hidden state vector. The output of this GRU cell, T ′ i , is the final topic vector which is used to produce the i th sentence via the WordRNN.</p><p>Loss Function and Training: Both Topic Generation Net and Sentence Generation Net are trained jointly end-to-end using labeled training data, which consists of pairs (x, y) of an image x and a corresponding paragraph y. If one image is associated with multiple paragraphs, we create a separate pair for each. Our training loss function ℓ train (x, y) couples two cross-entropy losses, a binary cross-entropy sentence-level loss on the distribution u i for the i th sentence (ℓ s (u i , ✶ i≤S )), and a word-level loss, on the distribution p i,j for the j th word of the i th sentence (ℓ w (p i,j , y i,j )). Assuming S sentences in the ground-truth paragraph, with the i th sentence having N i words, our loss function is given by:</p><formula xml:id="formula_8">ℓ train (x, y) = λ s S i=1 ℓ s (u i , ✶ i=S ) + λ w S i=1 Ni j=1 ℓ w (p i,j , y i,j ),<label>(2)</label></formula><p>where ✶ {·} is the indicator function. Armed with this loss function our method is trained via the Adam optimizer <ref type="bibr" target="#b18">[19]</ref> to update the parameters w T and w s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Diverse Coherent Paragraph Generation</head><p>The aforementioned scheme for generating paragraphs lacks in one key aspect: it doesn't model the ambiguity inherent to a diverse set of paragraphs that fit a given image. In order to incorporate this element of diversity into our model, we cast the designed paragraph generation mechanism into a Variational Autoencoder (VAE) <ref type="bibr" target="#b17">[18]</ref> formulation, a generic architecture of which is shown in <ref type="figure" target="#fig_3">Figure 5</ref>. Note that we prefer a VAE formulation over other popular tools for modeling diversity, such as GANs, because of the following reasons: (1) GANs are known to suffer from training difficulties unlike VAEs <ref type="bibr" target="#b2">[3]</ref>; <ref type="formula" target="#formula_8">(2)</ref> The intermediate sampling step in the generator of a GAN (for generating text) is not differentiable and thus one has to resort to Policy Gradient-based algorithms or Gumbel softmax, which makes the training procedure non-trivial. The details of our formulation follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VAE Formulation:</head><p>The goal of our VAE formulation is to model the loglikelihood of paragraphs y conditioned on images x, i.e., ln p(y|x). To this end, a VAE assumes that the data, i.e., in our case paragraphs, arise from a lowdimensional manifold space represented by samples z. Given a sample z, we reconstruct, i.e., decode, a paragraph y by modeling p θ (y|z, x) via a deep net. The ability to randomly sample from this latent space provides diversity. In the context of our task the decoder is the paragraph generation module described in Section 3.2, augmented by taking samples from the latent space as input. We subsequently denote the parameters of the paragraph generation module by θ = [w T , w s ]. To learn a meaningful manifold space we require the decoder's posterior p θ (z|y, x). However computing the decoder's posterior p θ (z|y, x) is known to be challenging <ref type="bibr" target="#b17">[18]</ref>. Hence, we commonly approximate this distribution using a probability q φ (z|y, x), which constitutes the encoder section of the model, parameterized by φ. Further, let p(z) denote the prior distribution of samples in the latent space. Using the aforementioned distributions, the VAE formulation can be obtained from the following identity:</p><formula xml:id="formula_9">ln p(y|x)−KL(q φ (z|y, x), p θ (z|y, x)) = E q φ (z|y,x) [ln p θ (y|z, x)]−KL(q φ (z|y, x), p(z)),</formula><p>where KL(·, ·) denotes the KL divergence between two distributions. Due to the non-negativity of the KL-divergence we immediately observe the right hand side to be a lower bound on the log-likelihood ln p(y|x) which can be maximized w.r.t. its parameters φ and θ. The first term on the right hand side optimizes the reconstruction loss, i.e., the conditional likelihood of the decoded paragraph (which is equivalent to optimizing the loss in Equation 2), while the second term acts like a distributional regularizer (ensuring smoothness). Training this system end-to-end via backpropagation is hard because of the intermediate, nondifferentiable, step of sampling z. This bottleneck is mitigated by introducing the Re-parameterization Trick <ref type="bibr" target="#b17">[18]</ref>. Details of the encoder and decoder follow.</p><p>Encoder: The encoder architecture is shown in <ref type="figure" target="#fig_4">Figure 6</ref>. Given the image x and a ground-truth paragraph y we encode the sample (x, y) by passing it through the topic and sentence generation nets. We then extract the hidden state vector (E ∈ R H ) from the final WordRNN of the Sentence Generation net. This vector is passed through a 1-layer densely connected net, the output layer of which has 2H neurons. We assume the conditional distribution underlying the encoder, q φ (z|y, x) to be a Gaussian, whose mean µ is the output of the first H neurons, while the remaining H neurons give a measure of the log-variance, i.e., ln σ 2 .</p><p>Decoder: The decoding architecture is also shown in <ref type="figure" target="#fig_4">Figure 6</ref>. While decoding, we draw a sample z ∼ N (0, I) (z ∈ R H , for training, we additionally shift and scale it by: z = µ + σǫ, where ǫ ∼ N (0, I)) and pass it to the SentenceRNN, via a single-layer neural net with I output neurons. The hidden state of this RNN is then forward propagated to the SentenceRNN unit, which also receives the pooled visual vector v. Afterwards, the decoding proceeds as discussed before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluations</head><p>Datasets: We first conduct experiments on the Stanford image-paragraph dataset <ref type="bibr" target="#b20">[21]</ref>, a standard in the area of visual paragraph generation. The dataset consists of 19,551 images from the Visual Genome <ref type="bibr" target="#b21">[22]</ref> and MS COCO dataset <ref type="bibr" target="#b25">[26]</ref>. These images are annotated with human-labeled paragraphs, 67.50 words long, with each sentence having 11.91 words, on average. The experimental protocol divides this dataset into 14,575 training, 2,487 validation, and 2,489 testing examples <ref type="bibr" target="#b20">[21]</ref>. Further, in order to exhibit generalizability of our approach, different from prior work, we also undertake experiments on the much larger, Amazon Product-Review dataset ('Office-Products' category) <ref type="bibr" target="#b28">[29]</ref> for the task of generating reviews. This is a dataset of images of common categories of office-products, such as printer, pens, etc. (see <ref type="figure" target="#fig_5">Figure 7)</ref>, crawled from amazon.com. There are 129,970 objects in total, each of which belongs to a category of office products. For every object, there is an associated image, captured in an uncluttered setting with sufficient illumination. Accompanying the image, are multiple reviews by users of the product. Further, each review is supplemented by a star rating, an integer between 1 (poor) and 5 (good). On an average there are 6.4 reviews per star rating per object. A review is 71.66 words long, with 13.52 words per sentence, on average. We randomly divide the dataset into 5,000 test, and 5,000 validation examples, while the remaining examples are used for training.</p><p>Baselines: We compare our approach to several recently introduced and our own custom designed baselines. Given an image, 'Image-Flat' directly synthesizes a paragraph, token-by-token, via a single RNN <ref type="bibr" target="#b16">[17]</ref>. 'Regions-Hierarchical' on the other hand, generates a paragraph, sentence by sentence <ref type="bibr" target="#b20">[21]</ref>. Liang et al . <ref type="bibr" target="#b24">[25]</ref> essentially train the approach of Krause et al . <ref type="bibr" target="#b20">[21]</ref> in a GAN setting ('RTT-GAN'), coupled with an attention mechanism. However, Liang et al . also report results on the Stanford image-paragraph dataset by using additional training data from the MS COCO dataset, which we refer to as 'RTT-GAN (Plus). <ref type="bibr">'</ref> We also train our model in a GAN setting and indicate this baseline as 'Ours (GAN).' Additionally, we create baselines for our model without coherence vectors, essentially replacing them with a zero vector for every time-step. We refer to this baseline as 'Ours (NC).' In another setting, we only set the global topic vector to zero for every time-step. We refer to this baseline as 'Ours (NG).' Evaluation Metrics: We report the performance of all models on 6 widely used language generation metrics: BLEU-{1, 2, 3, 4} <ref type="bibr" target="#b30">[31]</ref>, METEOR <ref type="bibr" target="#b8">[9]</ref>, and CIDEr <ref type="bibr" target="#b34">[35]</ref>. While the BLEU scores largely measure just the n-gram precision, METEOR, and CIDEr are known to provide a more robust evaluation of language generation algorithms <ref type="bibr" target="#b34">[35]</ref>.</p><p>Implementation Details: For the Stanford dataset, we set the dimension of the pooled visual feature vector, v, to be 1024. For the Amazon dataset, however, we use a visual representation obtained from VGG-16 <ref type="bibr" target="#b33">[34]</ref>. Since, these images are generally taken with just the principal object in view (see <ref type="figure" target="#fig_5">Figure 7)</ref>, a standard  There is a brown truck in a car park.</p><p>A building is in the background.</p><p>Next to the truck is a white car. Many cars are parked in the background.</p><p>There is a green sign in the background. The background has many trees.</p><p>A brown truck and a white car are in a car park.</p><p>The white caris parked next to the brown truck. The background has a building. There are many green trees next to the building. The background has many cars that are parked.</p><p>There is a green sign behind the car.</p><p>A brown truck is parked outside in a parking lot. A white car is parked next to a brown truck.</p><p>There are many trees in the background.</p><p>In the background there is a building. There are multiple cars and a green sign in the background.</p><p>It is a sunny day.</p><p>A brown truck is parked in a parking lot on a sunny day. The truck is parked outside.</p><p>Next to the truck is a white car. There are trees with green leaves in the background.</p><p>There is a building next to the trees. The background gas a green sign.</p><p>This black printer is of a good quality. This is a black printer with buttons on it. The printer has color printing. The ink cartridge lasts long. The printer has multiple paper trays. The printer has yellow buttons.</p><p>It has a display screen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RegionsHierarchical</head><p>A man in white shirt is walking on a city street. There is another man next to him. A bike is driving beside the building. Vehicles are parked in a parking lot in the background. A car is parked on the city street. There is a sign next to the car.</p><p>Two men are walking on a city street. The men are beside a building. They are walking by a car. A bike is next to the building. The background has many vehicles. The background has a tall tree with long branches.</p><p>Two men are walking outside on a city street. It is a sunny day. The men are walking next to a car. A bike is riding next to a building. A tall tree with leaves is in the background. There is a sign in front of the tree.</p><p>A bike with people is riding on the city street on a sunny day. Behind them is a tree with long branches. A car is parked next to the tree. Men are walking past the car. In the background there is a large building with windows. In the background there is a parking lot.</p><p>A bike with people is riding on the street. There is a wall beside the tree. In the background several bikes are parked. Two men are walking past the bike. The background has a large building. There is a car parked in the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of stars: 1</head><p>This is a black safe which is large. The safe is heavy. It is inconvenient to carry it around. It is very expensive as well. The safe has a handle grip to the side. It is inconvenient to replace it. This is a safe which has a steel case. It opens with a key. On the side of the safe there is a handle grip. This is one of the worst ever. It comes with a high price. It is very heavy.  an object, creating an inherent ambiguity. Noticeably, our model is worse off in terms of performance, when trained under the GAN setting. This observation is along the lines of prior work <ref type="bibr" target="#b7">[8]</ref>. We surmise that this results from the inherent difficulty of training GANs properly <ref type="bibr" target="#b2">[3]</ref> in conjunction with the fact that the GAN-based setup isn't trained directly with maximum-likelihood.</p><p>Qualitative results: <ref type="figure" target="#fig_5">Figure 7</ref> presents a sampling of our generated paragraphs. The first example in the figure (the first row) shows that our model can generate coherent paragraphs, while capturing meta-concepts like 'car-park' or 'parking lot,' from images with complex scenes. Regions-Hierarchical <ref type="bibr" target="#b20">[21]</ref> faces challenges to incorporate these 'meta-concepts' into the generated paragraphs. For several of the instances in the Amazon dataset (such as the images in the third and fourth rows), both our method and Regions-Hierarchical <ref type="bibr" target="#b20">[21]</ref> successfully detect the principal object in the image. We speculate that this is due to easy object recognition for images of the Amazon dataset, and to a lesser extent due to an improved paragraph generation algorithm. Additionally in the VAE setting, we are able to generate two distinctly different paragraphs with the same set of inputs, just by sampling a different z each time (the two rightmost columns in <ref type="figure" target="#fig_5">Figure 7</ref>), permitting our results to be diverse. Moreover, for the Amazon dataset (third and fourth rows in <ref type="figure" target="#fig_5">Figure 7)</ref> we see that our model learns to synthesize 'sentiment' words depending on the number of input stars. We present additional visualizations in the supplementary.</p><p>Ablation study: In one setting, we judge the importance of coherence vectors, by just using the global vector and setting the coherence vectors to 0, in the sentence generation net. The results for this setting ('Ours (NC)') are shown in <ref type="table" target="#tab_0">Tables 1,2</ref>, while qualitative results are shown in <ref type="figure" target="#fig_5">Figure 7</ref>. These numbers reveal that just by incorporating the global topic vector it is feasible to generate reasonably good paragraphs. However, incorporating coherence vectors makes the synthesized paragraphs more human-like. A look at the second column of <ref type="figure" target="#fig_5">Figure 7</ref> shows that even without coherence vectors we are able to detect global topics like 'car-park' but the sentences seem to exhibit sharp topic transition, quite like Regions-Hierarchical approach. We rectify this by introducing coherence vectors.</p><p>In another setting, we set the global topic vector to 0, at every time-step, while retaining the coherence vectors. The performance in this setting is indicated by 'Ours (NG)' in <ref type="table" target="#tab_0">Tables 1,2</ref>. The results suggest that incorporating the coherence vectors is much more critical for improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this work, we developed 'coherence vectors' which explicitly ensure consistency of themes between generated sentences during paragraph generation. Additionally, the 'global topic vector' was designed to capture the underlying main plot of an image. We demonstrated the efficacy of the proposed technique on two datasets, showing that our model when trained with effective autoencoding techniques can achieve state-of-the-art performance for both caption and review generation tasks. In the future we plan to extend our technique for the task of generation of even longer narratives, such as stories.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Overview of the Topic Generation Net of our proposed approach illustrating the construction of the individual and 'Global Topic Vector'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>denote the input and output of the net Γ w T (·), where the vector w T subsumes the parameters of the function. The global topic vector G, and the individual topic vectors and probabilities {(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The internal architecture of the 'Coupling Unit'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. General Framework of our VAE Formulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Architecture of the Encoder and Decoder of our VAE formulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Paragraphs generated under different settings with our developed approach, vis-à-vis Regions-Hierarchical [21]. The first, and second images are from the Stanford dataset, while the third and fourth images are from the Amazon dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Comparison of captioning performance on the Stanford Dataset</figDesc><table>Method 
BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR CIDEr 
Image-Flat [17] 
34.04 
19.95 
12.2 
7.71 
12.82 
11.06 
Regions-Hierarchical [21] 
41.9 
24.11 
14.23 
8.69 
15.95 
13.52 
RTT-GAN [25] 
41.99 
24.86 
14.89 
9.03 
17.12 
16.87 
RTT-GAN (Plus) [25] 
42.06 
25.35 
14.92 
9.21 
18.39 
20.36 
Ours (NC) 
42.03 
24.84 
14.47 
8.82 
16.89 
16.42 
Ours (NG) 
42.05 
25.05 
14.59 
8.96 
17.26 
18.23 
Ours 
42.12 
25.18 
14.74 
9.05 
17.81 
19.95 
Ours (with GAN) 
42.04 
24.96 
14.53 
8.95 
17.21 
18.05 
Ours (with VAE) 
42.38 
25.52 
15.15 
9.43 
18.62 
20.93 
Human (as in [21]) 
42.88 
25.68 
15.55 
9.66 
19.22 
28.55 

CNN suffices. We extract representations from the penultimate fully connected 
layer of the CNN, giving us a vector of 4,096 dimensions. Hence, we use a single-
layer neural network to map this vector to the input vector of 1,024 dimensions.</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Comparison of captioning performance on the Amazon Dataset</figDesc><table>Method 
BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR CIDEr 
Image-Flat [17] 
40.31 
30.63 
25.32 
15.64 
10.97 
9.63 
Regions-Hierarchical [21] 45.74 
34.8 
27.54 
16.67 
14.23 
12.02 
RTT-GAN [25] 
45.93 
36.42 
28.28 
17.26 
16.29 
15.67 
Ours (NC) 
45.85 
35.97 
27.96 
16.98 
15.86 
15.39 
Ours (NG) 
45.88 
36.33 
28.15 
17.17 
16.04 
15.54 
Ours 
46.01 
36.86 
28.73 
17.45 
16.58 
16.05 
Ours (with GAN) 
45.86 
36.25 
28.07 
17.06 
15.98 
15.43 
Ours (with VAE) 
46.32 
37.45 
29.42 
18.01 
17.64 
17.17 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>This black printer is lightweight. The printer has many buttons and has a scanner. The ink cartridge lasted for several years. The print is impressive. The packaging is good. It comes for a cheap price. The black printer comes with a scanner. It has paper cases. It comes with buttons. It comes with a warranty. It is available for good price. The printer use is convenient. A black printer with paper feed. The printer has a scanner and buttons for use. It is light weight. It prints good color. It has good availability. It has a good price.A brown truck is parked on the street. There is a white truck beside it. The background has a green sign. In the background there is a building. Behind the truck there are trees with green leaves. Next to the trees there are many cars.</figDesc><table>The printer is tall. 
It has trays and buttons 
for use. 
The scanner use is convenient. 
The ink cartridge is available. 
The printer has yellow buttons on it. 

Number of stars: 5 

Ours (NC) 
Ours 
Ours (with VAE) -I 
Ours (with VAE) -II 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>This is a black safe with a black steel case. The case is very large. It is very pricy. It is very heavy. The safe comes with two handle grips. It has a dark border. This is a black safe with a steel case. The safe is large. It has a black colored handle grip. The safe opens with a key. It is very pricy. It is not very portable. This black safe has a steel case. It comes with a handle grip on two sides. The space inside the safe is little. The safe is heavy and inconvenient. Other models are far better. It is very expensive.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">For both SentenceRNN and WordRNN, the GRUs have hidden layers (H) of 512 dimension. For the Amazon dataset, we condition the first SentenceRNN, with an H-dimensional embedding of the number of stars. We set λ s , λ w to be 5.0, and 1.0 respectively, the maximum number of sentences per paragraph, S max , to be 6, while the maximum number of words per sentence is set to be 30, for both datasets. In the coupling unit, α is set to 1.0, and β is set to 1.5 for the Stanford dataset, while for the Amazon dataset the corresponding values are 1.0 and 3.0. The learning rate of the model is to 0.01 for the first 5 epochs and is halved every 5 epochs after that, for both datasets. These hyper-parameters are chosen by optimizing the performance, based on the average of METEOR and CIDEr scores, on the validation set for both datasets. We use the same vocabulary as Krauseet al . [21], for the Stanford dataset, while a vocabulary size of the 11, 000 most frequent words is used for the Amazon dataset. Additional implementational details can be found on the project website 1 . For purposes of comparison, for the Amazon dataset, we run our implementation of all baselines, with their hyper-parameters picked based on a similar protocol, while for the Stanford dataset we report performance for prior approaches directly from [25]. Results: Tables 1 and 2 show the performance of our algorithm vis-à-vis other comparable baselines. Our model, especially when trained in the VAE setting, outperforms all other baselines (on all 6 metrics). Even the models trained under the regular (non-VAE) setup outperform most of the baselines and are comparable to the approach of Liang et al . [25], an existing state-of-the-art for this task. Our performance on the rigorous METEOR and CIDEr metrics, on both datasets, attest to our improved paragraph generation capability. The capacity to generate diverse paragraphs, using our VAE setup, pays off especially well on the Amazon dataset, since multiple reviews with the same star rating are associated with 1 https://sites.google.com/site/metrosmiles/research/research-projects/capg_revg</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This material is based upon work supported in part by the National Science Foundation under Grant No. 1718221, Samsung, and 3M. We thank NVIDIA for providing the GPUs used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Convolutional Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Wasserstein gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A novel statistical approach for image and video retrieval and its adaption for active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leuski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mind&apos;s eye: A recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<title level="m">Towards diverse and natural image descriptions via a conditional gan</title>
		<imprint>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ninth workshop on statistical machine translation</title>
		<meeting>ninth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Diverse and Controllable Image Captioning with Part-of-Speech Guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1805.12589" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Are you talking to a machine? dataset and methods for multilingual image question</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Creativity: Generating diverse questions using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A hierarchical approach for generating descriptive image paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<title level="m">Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A model for learning the semantics of pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent Topic-Transition GAN for Visual Paragraph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<title level="m">Deep captioning with multimodal recurrent neural networks (m-rnn)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGIR</title>
		<meeting>ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICME</title>
		<meeting>ICME</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Where to look: Focus regions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fusion of region and image-based techniques for automatic image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Multimedia Modeling</title>
		<meeting>International Conference on Multimedia Modeling</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<ptr target="http://www.cs.cmu.edu/~pengtaox/thesis_proposal_pengtaoxie.pdf" />
		<title level="m">Diversity-Promoting and Large-Scale Machine Learning for Healthcare</title>
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
	<note>Online; accessed 25</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Video paragraph captioning using hierarchical recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
