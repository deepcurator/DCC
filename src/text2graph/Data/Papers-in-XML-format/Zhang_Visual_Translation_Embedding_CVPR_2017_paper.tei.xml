<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Translation Embedding Network for Visual Relation Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
							<email>hanwangzhang@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zawlin</forename><surname>Kyaw</surname></persName>
							<email>kzl.zawlin@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
							<email>sfchang@ee.columbia.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Translation Embedding Network for Visual Relation Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Visual relations, such as "person ride bike" and "bike next to car", offer a comprehensive scene understanding of an image, and have already shown their great utility in connecting computer vision and natural language. However, due to the challenging combinatorial complexity of modeling subject-predicate-object relation triplets, very little work has been done to localize and predict visual relations. Inspired by the recent advances in relational representation learning of knowledge bases and convolutional object detection networks, we propose a Visual Translation Embedding network (VTransE) for visual relation detection. VTransE places objects in a low-dimensional relation space where a relation can be modeled as a simple vector translation, i.e., subject + predicate ≈ object. We propose a novel feature extraction layer that enables object-relation knowledge transfer in a fully-convolutional fashion that supports training and inference in a single forward/backward pass. To the best of our knowledge, VTransE is the first end-toend relation detection network. We demonstrate the effectiveness of VTransE over other state-of-the-art methods on two large-scale datasets: Visual Relationship and Visual Genome. Note that even though VTransE is a purely visual model, it is still competitive to the Lu's multi-modal model with language priors <ref type="bibr" target="#b26">[27]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We are witnessing the impressive development in connecting computer vision and natural language, from the arguably mature visual detection <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35]</ref> to the burgeoning visual captioning and question answering <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>. However, most existing efforts to the latter vision-language tasks attempt to directly bridge the visual model (e.g., CNN) and the language model (e.g., RNN), but fall short in modeling and understanding the relationships between objects. As a result, poor generalization ability was observed as those models are often optimized on specialized datasets for specific tasks such as image captioning or image QA. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>As illustrated in <ref type="figure">Figure 1</ref>, we take a step forward from The man in red is talking with the man in gray on a street.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q: What color is the jacket of the man under the clock? A:Gray</head><p>Image Captioning Visual QA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Relation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High-level Language</head><p>Low-level Vision <ref type="figure">Figure 1</ref>. We focus on detecting visual relations (dashed boxes in the middle layer) in this paper. Different from the direct connection between low-level vision and high-level language, visual relations offer the direct understanding of object interactions, which provide further semantic information for applications such as image captioning and QA.</p><p>the lower-level object detection and a step backward from the higher-level language modeling, focusing on the visual relations between objects in an image. We refer to a visual relation as a subject-predicate-object triplet <ref type="bibr" target="#b0">1</ref> , where the predicate can be verb (person1-talkperson2), spatial (clock-above-person2), preposition (car-with-wheel), and comparative (person1-taller-person2) <ref type="bibr" target="#b22">[ 23,</ref><ref type="bibr" target="#b26">27]</ref>. Visual relations naturally bridge the vision and language by placing objects in a semantic context of what, where, and how objects are connected with each other. For example, if we can detect clock-above-person2 and person2-wear-jacket successfully, the reasoning behind the answer "gray" to the question asked in <ref type="figure">Figure 1</ref> will be explicitly interpretable using dataset-independent inference, e.g., QA over knowl- . An illustration of translation embedding for learning predicate ride. Instead of modeling from a variety of ride images, VTransE learns consistent translation vector in the relation space regardless of the diverse appearances of subjects (e.g., person) and objects (e.g., horse, bike, etc.) involved in the predicate relation (e.g., ride).</p><p>edge bases <ref type="bibr" target="#b7">[8]</ref>, and thus permits better generalization or even zero-shot learning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>In this paper, we present a convolutional localization network for visual relation detection dubbed Visual Translation Embedding network (VTransE). It detects objects and predicts their relations simultaneously from an image in an end-to-end fashion. We highlight two key novelties that make VTransE effective and distinguishable from other visual relation models <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>:</p><p>Translation Embedding. Since relations are compositions of objects and predicates, their distribution is much more long-tailed than objects. For N objects and R predicates, one has to address the fundamental challenge of learning O(N 2 R) relations with few examples <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37]</ref>. A common solution is to learn separate models for objects and predicates, reducing the complexity to O(N + R). However, the drastic appearance change of predicates makes the learning even more challenging. For example, ride appearance largely varies from person-ridebike to person-ride-elephant. To this end, inspired by Translation Embedding (TransE) in representing largescale knowledge bases <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25]</ref>, we propose to model visual relations by mapping the features of objects and predicates in a low-dimensional space, where the relation triplet can be interpreted as a vector translation, e.g., person+ride ≈ bike. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, by avoiding learning the diverse appearances of subject-ride-object with large variance, we only need to learn the ride translation vector in the relation space, even though the subjects and/or objects can be quite diverse.</p><p>Knowledge Transfer in Relation. Cognitive evidences show that the recognition of objects and their interactions is reciprocal <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref>. For example, person and bike detections serve as the context for ride prediction, which in turn constrains the articulation of the two objects, and thus benefiting object detection. Inspired by this, we explicitly incorporate knowledge transfer between objects and predicates in VTransE. Specifically, we propose a novel feature extraction layer that extracts three types of object features used in translation embedding: classeme (i.e., class probabilities), locations (i.e., bounding boxes coordinates and scales), and RoI visual features. In particular, we use the bilinear feature interpolation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref> instead of RoI pooling <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">35]</ref> for differentiable coordinates. Thus, the knowledge between object and relation-confidence, location, and scale-can be transfered by a single forward/backward pass in an end-toend fashion.</p><p>We evaluate the proposed VTransE on two recently released relation datasets: Visual Relationship <ref type="bibr" target="#b26">[27]</ref> with 5,000 images and 6,672 unique relations, and Visual Genome <ref type="bibr" target="#b22">[23]</ref> with 99,658 images and 19,237 unique relations. We show significant performance improvement over several state-of-the-art visual relation models. In particular, our purely visual VTransE can even outperform the multimodal method with vision and language priors <ref type="bibr" target="#b26">[27]</ref> in detection and retrieval, and a bit shy of it in zero-shot learning.</p><p>In summary, our contributions are as follows: 1) We propose a visual relation detection model dubbed Visual Translation Embedding network (VTransE), which is a convolutional network that detects objects and relations simultaneously. To the best of our knowledge, this is the first end-toend relation detection network; 2) We propose a novel visual relation learning model for VTransE that incorporates translation embedding and knowledge transfer; 3) VTransE outperforms several strong baselines on visual relation detection by a large performance gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work falls in the recent progress on grounding compositional semantics in an image <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32]</ref>. It has been shown that high-quality groundings provide more comprehensive scene understanding, which underpins many visionlanguage tasks such as VQA <ref type="bibr" target="#b0">[1]</ref>, captioning <ref type="bibr" target="#b20">[21]</ref> and complex query retrieval <ref type="bibr" target="#b19">[20]</ref>. Visual relation detection not only ground regions with objects, but also describes their interactions. In particular, our VTransE network draws on recent works in relation learning and object detection.</p><p>Visual Relation Detection. Different from considering relations as hidden variables <ref type="bibr" target="#b41">[42]</ref>, we relate to explicit relation models which can be divided into two categories: joint model and separate model. For joint models, a relation triplet is considered as a unique class <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37]</ref>. However, the long-tailed distribution is an inherent defect for scalability. Therefore, we follow the separate model that learns subject, object, and predicate individually <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b26">27]</ref>. But, modeling the large visual variance of predicates is challenging. Inspired by TransE that has been successfully used in relation learning in large-scale knowledge base <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25]</ref>, our VTransE extends TransE for modeling visual relations by mapping subjects and objects into a lowdimensional relation space with less variance, and modeling the predicate as a translation vector between the subject and  <ref type="figure">Figure 3</ref>. The VTransE network overview. An input image is first through the Object Detection Module, which is a convolutional localization network that outputs a set of detected objects. Then, every pair of objects are fed into the Relation Prediction Module for feature extraction and visual translation embedding. In particular, the visual feature of an object is smoothly extracted from the last convolutional feature map using Bilinear Interpolation.</p><p>• denotes vector concatenation and ⊖ denotes element-wise subtraction.</p><p>object. Note that there are works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b23">24]</ref> that exploit language priors to boost relation detection, but we are only interested in visual models. Object Detection. VTransE is based on an object detection module composed of a region proposal network (RPN) and a classification layer. In particular, we use Faster-RCNN <ref type="bibr" target="#b34">[35]</ref>, which is evolved from its predecessors <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> that requires additional input of region proposals. Note that VTransE cannot be simply considered as appending a relation prediction layer to Faster-RCNN. In fact, we propose a novel feature extraction layer that allows knowledge transfer between objects and relations. The layer exploits the bilinear interpolation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> instead of the non-smooth RoI pooling in Faster-RCNN and thus the reciprocal learning of objects and predicates can be achieved in a single forward/backward pass. Note that VTransE can be married to any object detection network such as the very recent SSD <ref type="bibr" target="#b25">[26]</ref> and YOLO <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach: VTransE Network</head><p>VTransE is an end-to-end architecture that completes object detection and relation prediction simultaneously. As illustrated in <ref type="figure">Figure 3</ref>, it builds upon an object detection module (e.g., Faster-RCNN), and then incorporates the proposed feature extraction layer and the translation embedding for relation prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Visual Translation Embedding</head><p>Given any valid relation, Translation Embedding (TransE) <ref type="bibr" target="#b4">[5]</ref> represents subject-predicate-object in low-dimensional vectors s, p, and o, respectively, and the relation is represented as a translation in the embedding space: s + p ≈ o when the relation holds, and s + p ≈ o otherwise. TransE offers a simple yet effective linear model for representing the long-tail relations in large knowledge databases <ref type="bibr" target="#b30">[31]</ref>. Suppose x s , x o ∈ R M are the M -dimensional features of subject and object, respectively. Besides learning a relation translation vector</p><formula xml:id="formula_0">t p ∈ R r (r ≪ M ) as in TransE 2 , VTransE learns two pro- jection matrices W s , W o ∈ R</formula><p>r×M from the feature space to the relation space, i.e., s = W s x s and o = W o x o :</p><formula xml:id="formula_1">Wsxs + tp ≈ Woxo.<label>(1)</label></formula><p>Unlike the relations in a knowledge base that are generally facts, e.g., AlanTuring-bornIn-London, visual relations are volatile to specific visual examples, e.g., the validity of car-taller-person depends on the heights of the specific car and person in an image, resulting in problematic sampling negative triplets if the relation annotation is incomplete. Instead, we propose to use a simple yet efficient softmax for prediction loss that only rewards the deterministically accurate predicates <ref type="bibr" target="#b2">3</ref> , but not the agnostic object compositions of specific examples:</p><formula xml:id="formula_2">L rel = (s,p,o)∈R − log softmax t T p (Woxo − Wsxs) , (2)</formula><p>where the softmax is computed over p. Although Eq. (2) learns a rotational approximation for the translation model in Eq. <ref type="formula" target="#formula_1">(1)</ref>, we can retain the translational property by proper regularizations such as weight decay <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>The final score for relation detection is the sum of object detection score and predicate prediction score in Eq. <ref type="formula">(2)</ref>: S s,p,o = S s +S p +S o , where S s or S o is the object detection score and S p is the relation predicate prediction score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Extraction</head><p>We propose a Feature Extraction Layer in VTransE to extract x s and x o . There are three types of features that characterize the multiple facets of objects in relations: Classeme.Itisan(N +1)-d vector of object classification probabilities (i.e., N classes and 1 background) from the object detection network. Classeme is widely used as semantic attributes in various vision tasks <ref type="bibr" target="#b38">[39]</ref>. For example, in relation detection, classeme is a useful prior for rejecting unlikely relations such as cat-ride-person. Location.I t i s a 4-d vector (t x ,t y ,t w ,t h ), which is the bounding box parameterization in <ref type="bibr" target="#b11">[12]</ref>, where (t x ,t y ) specifies a scale-invariant translation and (t w ,t h ) specifies the log-space height/width shift relative to its counterpart object or subject. Take subject as an example:</p><formula xml:id="formula_3">tx = x − x ′ w ′ ,ty = y − y ′ h ′ ,tw =log w w ′ ,t h =log h h ′<label>(3)</label></formula><p>where (x, y, w, h) and (x ′ ,y ′ ,w ′ ,h ′ ) are the box coordinates of subject and object, respectively. Location feature is not only useful for detecting spatial or preposition relation, but also useful for verbs, e.g., subject is usually above object when the predicate is ride. Visual Feature.ItisaD-d vector transformed from a convolutional feature of the shape X × Y × C. Although it is as the same size as the RoI pooling features used in Faster-RCNN, our features are bilinearly interpolated from the last conv-feature map, so as to achieve end-to-end training that allows knowledge transfer (cf. Section 3.3).</p><p>The overall feature x s or x o is a weighted concatenation of the above three features (M = N + D +5), where the weights are learnable scaling layers since the feature contribution dynamically varies from relation to relation. As shown in <ref type="figure">Figure 3</ref>, the proposed feature extraction layer couples the Object Detection Module and the Relation Prediction Module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Architecture Details</head><p>A training image for VTransE is labeled with a list of subject-predicate-object triplets, where every unique subject or object is annotated with a bounding box. At testing time, VTransE inputs an image and outputs a set of detected objects and the relation prediction scores for every pair of objects.</p><p>Object Detection Network. VTransE network starts from the Faster-RCNN <ref type="bibr" target="#b34">[35]</ref> object detection network with the VGG-16 architecture <ref type="bibr" target="#b37">[38]</ref>. At training time, we sample a mini-batch cotaining 256 region proposal boxes generated by the RPN of Faster-RCNN, each of which is positive if it has an intersection over union (IoU) of at least 0.7 with some ground truth regions and it is negative if the IoU &lt; 0.3. The positive proposals are fed into the classification layer, where each proposal outputs an (N +1)class probabilities and N bounding box estimations. Then, we perform non-maximum suppression (NMS) for every class with the IoU &gt; 0.4, resulting in 15.6 detected objects on average, each of which has only one bounding box. The reasons of performing NMS for object detection are two folds: 1) we need a specific object class for each region to match with the relation ground truth, and 2) we need to down-sample the objects for a reasonable number of candidate relations. At test time, we sample 300 proposal regions generated by RPN with IoU &gt; 0.7. After the classification layer, we perform NMS with IoU &gt; 0.6 on the 300 proposals, resulting in 15-20 detections per image on average.</p><p>Bilinear Interpolation. By removing the final pooling layer of VGG-16, we use the last convolutional feature map F of the shape W ′ × H ′ × C (the pink cube in <ref type="figure">Figure 3</ref>), where C = 512 is the number of channels, W ′ = ⌊</p><note type="other">W 16 ⌋, and H ′ = ⌊ H 16 ⌋, where W and H are the width and height of the input image. F encodes the visual appearance of the whole image and is used for extracting visual features for the object detection and relation prediction.</note><p>In order to achieve object-relation knowledge transfer, the relation error should be back-propagated to the object detection network and thus refines the objects. However, the widely-used RoI pooling visual feature in Fast/Faster R-CNN is not a smooth function of coordinates since it requires discrete grid split for the proposal region, resulting in zero coordinate gradients back-propagated from the feature extraction layer.</p><p>To this end, we replace the RoI pooling layer with bilinear interpolation <ref type="bibr" target="#b17">[18]</ref>. It is a smooth function of two inputs: the feature map F and an object bounding box projected onto F, and the output is a feature V of the size X × Y × C (the orange cube in <ref type="figure">Figure 3</ref>). Each entry value in V can be efficiently interpolated from F in a convolutional way:</p><formula xml:id="formula_4">V i,j,c = W ′ i ′ =1 H ′ j ′ =1 F i ′ ,j ′ ,c k(i ′ − G i,j,1 )k(j ′ − G i,j,2 ),<label>(4)</label></formula><p>where G ∈ R X×Y ×2 records the positions of the X × Y grid split in the input bounding box and k(x)=max(0, 1 − |x|) is the bilinear interpolation kernel. Note that the grid position G matrix is a linear function of the input box. Therefore, the gradients from V can be back-propagated to the bounding box coordinates.</p><p>Optimization. We train the VTransE network end-toend by stochastic gradient descent with momentum <ref type="bibr" target="#b21">[22]</ref>. We follow the "image-centric" training strategy <ref type="bibr" target="#b34">[35]</ref>, i.e., the mini-batch arises from a single image that contains many object regions and relations. The loss function is a multi-task loss combining the object detection loss L obj and the relation detection loss L rel in Eq. (2), allowing reciprocal learning for objects and relations. In particular, we find that a reasonable loss trade-off is L obj +0.4L rel . Since object detection and relation prediction have different sample sizes, we normalize L obj and L rel by the mini-batch size.</p><p>For model initializations, we pre-train Faster-RCNN on the objects in the relation datasets to initialize the object detection network and randomly initialize the VTransE component with Gaussian weights. For end-to-end training, we also replace the RoI pooling layer in the object detection network with bilinear interpolation. For efficiency, we do not fine-tune the VGG-16 CNN. Generally, we need 2 -3 epochs for the model to converge. For a single image that has been resized to the longer side of 720 pixels, the train-ride park on <ref type="figure">Figure 4</ref>. Top 5 confident regions of subject and object retrieved by ride and park on models of JointBox (1st row) and VTransE (2nd row with ground-truth bounding boxes) from VRD.</p><p>ing runs in 2.0 fps and the testing runs in 6.7 fps on a Titan X GPU using Caffe and Python. Note that we can always plug-in faster object detection networks such as SSD <ref type="bibr" target="#b25">[26]</ref> and YOLO <ref type="bibr" target="#b33">[34]</ref> for more efficient training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We are going to validate the effectiveness of the proposed VTransE network by answering the following questions. Q1: Is the idea of embedding relations effective in the visual domain? Q2: What are the effects of the features in relation detection and knowledge transfer? Q3: How does the overall VTransE network perform compared to the other state-of-the-art visual relation models?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Metrics</head><p>To the best of our knowledge, there are only two datasets for visual relation detection at a large scale. We used both: VRD. It is the Visual Relationships dataset <ref type="bibr" target="#b26">[27]</ref>. It contains 5,000 images with 100 object categories and 70 predicates. In total, VRD contains 37,993 relation annotations with 6,672 unique relations and 24.25 predicates per object category. We followed the same train/test split as in <ref type="bibr" target="#b26">[27]</ref>, i.e., 4,000 training images and 1,000 test images, where 1,877 relationships are only in the test set for zero-shot evaluations. VG. It is the latest Visual Genome Version 1.2 relation dataset <ref type="bibr" target="#b22">[23]</ref>. Unlike VRD that is constructed by computer vision experts, VG is annotated by crowd workers and thus the objects and relations are noisy. Therefore, we contact the authors for an official pruning of them. For example, "young woman" and "lady" are merged to the WordNet hypernym "woman". We filtered out relations with less than 5 samples. In summary, VG contains 99,658 images with 200 object categories and 100 predicates, resulting in 1,174,692 relation annotations with 19,237 unique relations and 57 predicates per object category. We split the data into 73,801 for training and 25,857 for testing.</p><p>Following <ref type="bibr" target="#b26">[27]</ref>, we used Recall@50 (R@50) and Re- call@100 (R@100) as evaluation metrics for detection. R@K computes the fraction of times a true relation is predicted in the top K confident relation predictions in an image. Note that precision and average precision (AP) are not proper metrics as visual relations are labeled incompletely and they will penalize the detection if we do not have that particular ground truth. For the relation retrieval task (cf. Section 4.4), we adopted the Recall rate@5 (Rr@5), which computes the fraction of times the correct result was found among the top 5, and Median rank (Med r), which is the median rank of the first correctly retrieved image <ref type="bibr" target="#b19">[20]</ref>. In fact, for datasets with more complete annotations (e.g., VG), even if the recall is low, the actual precision could be high since the number of ground truth in an image is usually larger than 50/100. Therefore, the retrieval task measured by Rr@5 and Med r provides a complementary evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluations of Translation Embedding (Q1)</head><p>Setup. Visual relation detection requires both object detection and predicate prediction. To investigate whether VTransE is a good model for relations, we need to isolate it from object detection and perform the task of Predicate Prediction: predicting predicates given the ground-truth objects with bounding boxes.</p><p>Comparing Methods. We compared 1) JointBox, a softmax classifier that classifies the images of the subject and object joint bounding boxes into predicates, and 2) VTransE that classifies the predicate of a pair of subject and object boxes. For fair comparison, we only use the RoI pooling visual features of boxes for the two methods. Note that JointBox represents many visual relation models in predicate prediction <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37]</ref> Results. From <ref type="table" target="#tab_0">Table 1</ref>, we can see that VTransE formulated in Eq. (2) outperforms conventional visual models like JointBox. This is because the predicate model parameters of VTransE-the translation vectors-are able to capture the essential meanings of relations between two objects mapped into a low-dimensional relation space. <ref type="figure">Figure 4</ref> illustrates that VTransE can predict correct predicates with diversity while JointBox is more likely to bias on certain visual patterns. For example, JointBox limits park on in cars, but VTransE can generalize to other subjects like plane and bus. Moreover, by inspecting the semantic affinities between the predicate parameter vectors in <ref type="figure">Figure 5</ref>, we can speculate that JointBox does not actually model relations but the joint object co-occurrence. For example, in JointBox, the reason why beneath is close to drive on and park on is largely due to the co-occurrence of road-beneath-car and car-drive on-road;h o wever, VTransE is more likely to understand the meaning of beneath as its neighbors are below and under, and it is far from on and above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluations of Features (Q2)</head><p>Setup. We evaluated how the features proposed in Section 3.1 affect visual relation detection. We performed Relation Detection <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b36">37]</ref>: the input is an image and the output is a set of relation triplets and localizations of both subject and object in the image having at least 0.5 overlap with their ground-truth boxes simultaneously.</p><p>Comparing Methods. We ablated VTransE into four methods in terms of using different features: 1) Classeme, 2) Location,3 )Visual, and 4) All that uses classeme, locations, visual features, and the fusion of the above with a scaling layer (cf. <ref type="figure">Figure 3)</ref>, respectively. Note that all the above models are trained end-to-end including the object detection module. To further investigate the feature influence on relations, we categorized the predicates into four categories: verb, spatial, preposition and comparative (cf. <ref type="figure">Supplementary Material for the detailed category list)</ref>.</p><p>Results. From <ref type="figure">Figure 6</ref>, we can see the details of what features are good at detecting what relations: 1) fusing all the features with a learned scaling layer can achieve the best performance on all types of relations; 2) classeme can generally outperform visual features in various kinds of relations as it characterizes both the high-level visual appearances (e.g., what an object looks like) and composition priors (e.g., person is more likely to ride-bike than cat); 3) for spatial relations, location features are better; however, for preposition relations, all features perform relatively poor. This is because the spatial and visual cues of prepositions are volatile such as person-with-watch and car-with-wheel.  <ref type="figure">Figure 6</ref>. Performances (R@100%) of relation detection of the four relation types using the four ablated VTransE methods from VRD (left) and VG (right).  <ref type="table" target="#tab_1">Table 2</ref> shows that the end-to-end training of VTransE can improve the object detection. This is mainly due to that the proposed feature extraction layer allows knowledge transfer so that the errors made by relation prediction can be back-propagated to the front object detection module. In fact, the improvement can be expected since we incorporate additional relation labels besides object labels. As shown in <ref type="figure" target="#fig_3">Figure 7</ref>, compared to the pre-trained Faster-RCNN module, the object detection module trained by VTransE can generally improve bounding boxes, such as minor refinement or even recovery from drastic dislocation and corrections for wrong detections. This demonstrates that relations place objects in a contextual scene. For example, relation can recover shorts from the wrong detection bag,e v e n though the correct detection should be pants, which is semantically similar to shorts. This correction is likely inferred by the relation person-wear-shorts/pants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-The-Arts (Q3)</head><p>Setup. As we will introduce later, some joint relation models can only detect a joint bounding box for an entire relation; thus, besides relation detection, we performed Phrase Detection <ref type="bibr" target="#b26">[27]</ref>: the input is an image and the output is a set of relation triplets and localizations of the entire bounding box for each relation that having at least 0.5 overlap with the ground-truth joint subject and object box.</p><p>For more extensive evaluations, we also performed two additional tasks. 1) Relation Retrieval: image search with the query of a relation triplet. We first detect the relation <ref type="table">Table 3</ref>. Performances of phrase detection, relation detection, relation retrieval using various methods on both datasets. "-" denotes that the result is not applicable. (cf. Supplementary Material for the incomplete annotation in VRD that causes low retrieval performances.)  query in gallery (i.e., test) images and then score them according to the average detection scores of the query relation. An image with at least one successful query relation detection is considered as a hit. This task is a representation of the compositional semantic retrieval <ref type="bibr" target="#b19">[20]</ref>; We selected the top 1,000 frequent relations as queries. 2) Zeroshot Learning <ref type="bibr" target="#b26">[27]</ref>: individual subject, object, and predicate are seen in both training and test, but some specific triplet compositions are only in the test set. Due to the long-tailed relation distribution, it is a practical setting since it is impossible to collect data for all triplets.</p><p>Comparing Methods. We compared the VTransE network to four state-of-the-art visual relation detection models. 1) VisualPhrase <ref type="bibr" target="#b36">[37]</ref>: a joint relation model that considers every unique relation triplet as an relation class. For fair comparison, we replace the original DPM object detection model <ref type="bibr" target="#b9">[10]</ref> with Faster-RCNN <ref type="bibr" target="#b34">[35]</ref>; 2) DenseCap <ref type="bibr" target="#b18">[19]</ref>: it detects sub-image regions and generate their descriptions simultaneously. It is an end-to-end model using bilinear interpolated visual features for region localizations. We replace its LSTM classification layer with softmax for relation prediction. Thus, it can be considered as an joint relation model; 3) Lu's-V (V-only in <ref type="bibr" target="#b26">[27]</ref>): it is a two-stage separate model that first uses R-CNN <ref type="bibr" target="#b11">[12]</ref> for object detection and then adopts a large-margin JointBox model for predicate classification; 4) Lu's-VLK (V+L+K in <ref type="bibr" target="#b26">[27]</ref>): a twostage separate model that combines Lu's-V and word2vec language priors <ref type="bibr" target="#b29">[30]</ref>. In addition, we compared VTransE to its two-stage training model VTransE-2stage that apply Faster-RCNN for object detection and then perform predicate predication using translation embedding as in Q1.</p><p>As we have no training source codes of Lu's methods, we cannot apply them in VG and we quoted the results of VRD reported in their paper <ref type="bibr" target="#b26">[27]</ref>. Moreover, as the joint relation models such as VisualPhrase and DenseCap can only detect relation triplet as a whole, they are not applicable in zeroshot learning. Therefore, we only report zero-shot results (detection and retrieval) on VRD for the official 1,877 zeroshot relations <ref type="bibr" target="#b26">[27]</ref>.</p><p>Results. From the quantitative results in <ref type="table">Table 3</ref> and the qualitative results in <ref type="figure" target="#fig_4">Figure 8</ref> perform joint models like VisualPhrase and DenseCap significantly, especially on VRD. This is because the classification space of joint models for all possible relationships is large (e.g., 6,672 and 19,237 training relations in VRD and VG, respectively), leading to insufficient samples for training infrequent relations. 2) For separate models, better object detection networks, such as Faster-RCNN v.s. R-CNN used in VTrasnE and Lu's, are beneficial for relation detections. As shown in <ref type="figure" target="#fig_4">Figure 8</ref>, on VRD dataset, Lu's-VLK mistakes soundbox as person and plate as bowl. We believe that this is a significant reason why their visual model Lu's-V is considerably worse than VTransE. 3) Even though VTransE is a purely visual model, we can still outperform Lu's-VLK which incorporates language priors, e.g., on VRD measured by R@50 and Med r, we are 20%, 2%, and 230% relatively better in phrase detection, relation detection, and relation retrieval, respectively. First, the classeme feature can serve as a similar role as language priors. Second, location feature is indispensable to relations. Take the person-wear-tie relation query as an example in <ref type="figure" target="#fig_4">Figure 8</ref>, when there are multiple person detections in an image, Lu's-VLK usually relates tie to the wrong person, regardless the fact that the spatial distance is far. Similar examples can be also found in the false detection shirt-on-cup of Lu's-VLK. 4) The end-to-end VTransE is better than VTransE-2stage across all the tasks on both datasets. Together with the results in Q2, they demonstrate the effectiveness of reciprocal learning between objects and relations.</p><p>From the zero-shot quantitative results in <ref type="table" target="#tab_4">Table 4</ref> and the qualitative results in <ref type="figure" target="#fig_5">Figure 9</ref>,wehave: 1) The performances of ours and the compared methods degrade drastically, e.g., for relation detection, VTransE and Lu's-VLK suffer 88% and 79% performance (R@100) drop, respectively. This is the key limitation of VTransE. Perhaps this is because our transformation from feature space to relation space in Eq. (1) is too generic, especially for verbs, and thus fails to capture the relation-specific visual deformations. For example, VTransE cannot discriminate between person-lying on-  predicate and object models <ref type="bibr" target="#b28">[29]</ref>, although it will increase the model complexity from O(N +R) to O(NR), where N is the number of objects and R is the number of predicates.</p><p>2) Both as visual models, our VTransE is significantly better than Lu's-V in zero-shot relation predictions; nevertheless, as a multi-modal model, Lu's-VLK surpasses VTransE by exploiting language priors. But, since visual relations are volatile to specific examples, language priors are not always correct-Lu's-VLK can be misled by frequent language collocations which are invalid in visual examples, e.g., the mismatch of subject and object in sofabeneath-person and person-play with-laptop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We focused on the visual relation detection task that is believed to offer a comprehensive scene understanding for connecting computer vision and natural language. Towards this task we introduced the VTransE network for simultaneous object detection and relation prediction. VTransE is an end-to-end and fully-convolutional architecture that consists of an object detection module, a novel differentiable feature extraction layer, and a novel visual translation embedding layer for predicate classification. Moving forward, we are going to 1) model higher-order relations such as person-throw-ball-to-dog, 2) tackle the challenge of zero-shot relation learning, and 3) apply VTransE in a VQA system based on relation reasoning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2. An illustration of translation embedding for learning predicate ride. Instead of modeling from a variety of ride images, VTransE learns consistent translation vector in the relation space regardless of the diverse appearances of subjects (e.g., person) and objects (e.g., horse, bike, etc.) involved in the predicate relation (e.g., ride).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Qualitative object detection examples before (red box and font) and after (green box and font) training VTransE from VRD (top row) and VG (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Qualitative examples of relation detection (4 top-1 detections from 4 predicate types) and retrieval (top-5 images). We compare our VTransE with its best competitors: Lu's-VLK on VRD and DenseCap on VG. Green and red borders denote correct and incorrect results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Qualitative examples of zero-shot relation detection (top-4) and retrieval (top-5) using VTransE and Lu's-VLK on VRD. Green and red borders denote correct and incorrect results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Predicate prediction performances of the two methods.JointBox and VTransE from VRD. Please zoom in.</figDesc><table>Method 
JointBox 
VTransE 

Dataset VRD 
VG 
VRD 
VG 

R@50 
25.78 46.59 44.76 62.63 

R@100 25.78 46.77 44.76 62.87 

JointBox 

VTransE 

Figure 5. t-SNE visualizations [28] of the 70 predicate model pa-
rameters of </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Object detection mAP% before (Faster-RCNN) and after training VTransE from VRD (100 objects) and VG (200 objects). Low mAP is mainly due to the incomplete object annotation.</figDesc><table>VRD 
VG 

Before After Before After 

13.32 13.98 
6.21 
6.58 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>table and person- sit next to-table. One remedy is to incorporate</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Performances of zero-shot phrase detection, relation de- tection, relation retrieval using various methods on VRD. Note that joint models like VisualPhrase and DenseCap do not apply in zero- shot setting.</figDesc><table>Task 
Phrase Det. 
Relation Det. 
Retrieval 

Metric 
R@50 R@100 
R@50 
R@100 
Rr@5 Med r 

Lu's-V [27] 
0.95 
1.12 
0.67 
0.78 
0.54 
454 

Lu's-VLK [27] 
3.36 
3.75 
3.13 
3.52 
1.24 
434 

VTransE 
2.65 
3.51 
1.71 
2.14 
1.42 
422 

Random 
0.02 
0.04 
7.14×10 −3 1.43×10 −2 
0.45 
499 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">When the context is clear, we always refer to object in normal font as a general object and object in teletype to the tail object in a relation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In experiments, we tested r ∈{ 100, 200, ..., 1000} and found that r = 500 is a good default. 3 In fact, predicate is multi-labeled, e.g., both person-on-bike and person-ride-bike are correct. However, most relations are singlelabeled in the datasets, e.g., 58% in VRD [27] and 67% in VG [23].</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>NExT research is supported by the National Research Foundation, Prime Minister's Office, Singapore under its IRC@SG Funding Initiative.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep compositional question answering with neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to generalize to new compositions in image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kezami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic description generation from images: A survey of models, datasets, and evaluation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cakici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ikizlercinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Muscat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Plank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Representation of manipulable man-made objects in the dorsal stream</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discriminative models for multi-class object layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Question answering over freebase with multi-column convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Every picture tells a story: Generating sentences from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04623</idno>
		<title level="m">Draw: A recurrent neural network for image generation</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Beyond nouns: Exploiting prepositions and comparative adjectives for learning visual classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Observing human-object interactions: Using spatial and functional compatibility for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Revisiting visual question answering baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep variation-structured reinforcement learning for visual relationship and attribute detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning models for actions and personobject interactions with transfer to question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting regionto-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning semantic relationships for better action retrieval in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rossenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Viske: Visual knowledge extraction and question answering by visual verification of relation phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recognition using visual phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient object category recognition using classemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Show and tell: Lessons learned from the 2015 mscoco image captioning challenge. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fvqa: Factbased visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05433</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Modeling mutual context of object and human pose in human-object interaction activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning from collective intelligence: Feature learning using social images and tags</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TOMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Online collaborative learning for open-vocabulary visual classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
