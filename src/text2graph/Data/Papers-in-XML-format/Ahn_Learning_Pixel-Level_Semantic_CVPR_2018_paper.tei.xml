<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><forename type="middle">Ahn</forename><surname>Dgist</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korea</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>POSTECH</roleName><forename type="first">Suha</forename><surname>Kwak</surname></persName>
							<email>suha.kwak@postech.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korea</surname></persName>
						</author>
						<title level="a" type="main">Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent development of Deep Neural Networks (DNNs) has driven the remarkable improvements in semantic segmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39]</ref>. Despite the great success of DNNs, however, we still have a far way to go in achieving semantic segmentation in an uncontrolled and realistic environment. One of the main obstacles is lack of training data. Due to the prohibitively expensive annotation cost of pixel-level segmentation labels, existing datasets often suffer from lack of annotated examples and class diversity. This makes the conventional approaches limited to a small range of object categories predefined in the datasets.</p><p>Weakly supervised approaches have been studied to resolve the above issue and allow semantic segmentation models more scalable. Their common motivation is to utilize annotations like bounding boxes <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28]</ref> and scribbles <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">36]</ref> that are weaker than pixel-level labels but readily available in a large amount of visual data or easily obtainable thanks to their low annotation costs. Among various types of weak annotations for semantic segmentation, image-level class labels have been widely used <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37]</ref> since they are already given in existing largescale image datasets (e.g., ImageNet <ref type="bibr" target="#b6">[7]</ref>) or automatically annotated for image retrieval results by search keywords. However, learning semantic segmentation with the imagelevel label supervision is a significantly ill-posed problem since such supervision indicates only the existence of a certain object class, and does not inform object location and shape that are essential for learning segmentation.</p><p>Approaches in this line of research have incorporated additional evidences to simulate the location and shape information absent in the supervision. A popular choice for the localization cue is the Class Activation Map (CAM) <ref type="bibr" target="#b39">[40]</ref>, which highlights local discriminative parts of target object by investigating the contribution of hidden units to the output of a classification DNN. The discriminative areas highlighted by CAMs are in turn used as seeds that will be propagated to cover the entire object area. To recover the object area accurately from the seeds, previous approaches have utilized image segmentation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30]</ref>, motions in video <ref type="bibr" target="#b34">[35]</ref>, or both <ref type="bibr" target="#b10">[11]</ref>, all of which are useful to estimate object shape. For the same purpose, a class-agnostic salient region is estimated and incorporated with the seeds in <ref type="bibr" target="#b25">[26]</ref>. However, they demand extra data (i.e., videos) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">35]</ref>, additional supervision (i.e., object bounding box) <ref type="bibr" target="#b25">[26]</ref>, or off-the-shelf techniques (i.e., image segmentation) that cannot take advantage of representation learning in DNNs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>In this paper, we present a simple yet effective approach to compensate the missing information for object shape with no external data or additional supervision. The key component of our framework is AffinityNet, which is a DNN that takes an image as input and predicts semantic  <ref type="figure">Figure 1</ref>. Illustration of our approach. Salient areas for object classes and background are first localized in training images by CAMs <ref type="bibr" target="#b39">[40]</ref> (Section 3.1). From the salient regions, we sample pairs of adjacent coordinates and assign binary labels to them according to their class consistency. The labeled pairs are then used to train AffinityNet (Section 3.2). The trained AffinityNet in turn predicts semantic affinities within local image areas, which are incorporated with random walk to revise the CAMs (Section 3.3) and generate their segmentation labels (Section 3.4). Finally, the generated annotations are employed as supervision to train a semantic segmentation model.</p><p>affinities of pairs of adjacent image coordinates. Given an image and its CAMs, we first build a neighborhood graph where each pixel is connected to its neighbors within a certain radius, and estimate semantic affinities of pairs connected in the graph through AffinityNet. Sparse activations in CAMs are then diffused by random walk <ref type="bibr" target="#b22">[23]</ref> on the graph, for each class: The affinities on edges in the graph encourage random walk to propagate the activations to nearby and semantically identical areas, and penalize propagation to areas of the other classes. This semantic diffusion revises CAMs significantly so that fine object shapes are recovered. We apply this process to training images for synthesizing their segmentation labels by taking the class label associated to the maximum activation of the revised CAMs at each pixel. The generated segmentation labels are used to train a segmentation model for testing. The remaining issue is how to learn AffinityNet without extra data or additional supervision. To this end, the initial CAMs of training images are utilized as sources of supervision. Because CAMs often miss some object parts and exhibit false alarms, they are incomplete as a supervision for learning semantic segmentation whose goal is to predict the entire object masks accurately. However, we found that they are often locally correct and provide evidences to identify semantic affinities within a small image area, which are the objective of AffinityNet. To generate reliable labels of the local semantic affinities, we disregard areas with relatively low activation scores on the CAMs so that only confident object and background areas remain. A training example is then obtained by sampling a pair of adjacent image coordinates on the confident areas, and its binary label is 1 if its coordinates belong to the same class and 0 otherwise.</p><p>The overall pipeline of the proposed approach is illustrated in <ref type="figure">Figure 1</ref>. First, CAMs of training images are computed and utilized to generate semantic affinity labels, which are used as supervision to train AffinityNet. We then apply the trained AffinityNet to each training image to compute the semantic affinity matrix of its neighborhood graph, which is employed in random walk to revise its CAMs and obtain synthesized segmentation labels. Finally, the generated segmentation labels are used to train a semantic segmentation DNN, which is the only network that will be used at test time. Our contribution is three-fold:</p><p>• We propose a novel DNN named AffinityNet that predicts high-level semantic affinities in a pixel-level, but is trained with image-level class labels only.</p><p>• Unlike most previous weakly supervised methods, our approach does not rely heavily on off-the-shelf techniques, and takes advantage of representation learning through end-to-end training of AffinityNet.</p><p>• On the PASCAL VOC 2012 <ref type="bibr" target="#b7">[8]</ref>, ours achieves stateof-the-art performance among models trained under the same level of supervision, and is competitive with those relying on stronger supervision or external data. Surprisingly, it even outperforms FCN <ref type="bibr" target="#b21">[22]</ref>, the wellknown fully supervised model in the early days.</p><p>The rest of this paper is organized as follows. Section 2 reviews previous approaches closely related to ours, and Section 3 describes each step of our framework in details. Then we empirically evaluate the proposed framework on the public benchmark in Section 5, and conclude in Section 6 with brief remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Various types of weak supervision: Weakly supervised approaches have been extensively studied for semantic segmentation to address the data deficiency problem. Successful examples of weak supervision for semantic segmentation include bounding box <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28]</ref>, scribble <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">36]</ref>, point <ref type="bibr" target="#b0">[1]</ref>, and so on. However, these types of weak supervision still require a certain amount of human intervention during annotation procedure, so it is costly to annotate these weak labels for a large number of visual data.</p><p>Image-level labels as weak supervision: Image-level class labels have been widely used as weak supervision for semantic segmentation since they demand minimum or no human intervention to be annotated. Early approaches have tried to train a segmentation model directly from imagelevel labels <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, but their performance is not satisfactory since the labels are too coarse to teach segmentation. To address this issue, some of previous arts incorporate segmentation seeds given by discriminative localization techniques <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b39">40]</ref> with additional evidences like superpixels <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">30]</ref>, segmentation proposals <ref type="bibr" target="#b29">[30]</ref>, and motions in video <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">35]</ref>, which are useful to estimate object shapes and obtained by off-the-shelf unsupervised techniques.</p><p>Our framework based on AffinityNet has a clear advantage over the above approaches. AffinityNet learns from data how to propagate local activations to entire object area, while the previous methods cannot take such an advantage. Like ours, a few methods improve segmentation quality without off-the-shelf preprocessing. Wei et al. <ref type="bibr" target="#b36">[37]</ref> propose to progressively expand segmentation results by searching for new and complementary object regions sequentially. On the other hand, Kolesnikov and Lampert <ref type="bibr" target="#b13">[14]</ref> learn a segmentation model to approximate the output of dense Conditional Random Field (dCRF) <ref type="bibr" target="#b14">[15]</ref> applied to the segmentation seeds given by CAMs.</p><p>Learning pixel-level affinities: Our work is also closely related to the approaches that learn to predict affinity matrices in pixel-level <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b35">36]</ref>. Specifically, a pixel-centric affinity matrix of an image is estimated by a DNN trained with segmentation labels in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref>. Bertasius et al. <ref type="bibr" target="#b1">[2]</ref> incorporate the affinity matrix with random walk, whose role is to refine the output of a segmentation model like dCRF. Cheng et al. <ref type="bibr" target="#b4">[5]</ref> design a deconvolution network, in which unpooling layers leverage the affinity matrix to recover sharp boundaries during upsampling. Both of the above methods aim to refine outputs of fully supervised segmentation models in a pixel-level. On the contrary, our goal is to recover object shape from coarse and noisy responses of object parts with a high-level semantic affinity matrix, and AffinityNet has a totally different architecture accordingly. Vernaza and Chandraker <ref type="bibr" target="#b35">[36]</ref> employ scribbles as weak supervision, and propose to learn a segmentation network and the random walk affinity matrix simultaneously so that the output of the network and that of random walk propagation of the scribbles become identical. Our approach is different from this work in the following three aspects. First, our framework is trained with image-level labels, which are significantly weaker than the scribbles employed in <ref type="bibr" target="#b35">[36]</ref>. Second, in our approach random walk can jump to any other positions within a certain radius, but in <ref type="bibr" target="#b35">[36]</ref> it is allowed to move only to four nearest neighbors. Third, AffinityNet learns pairwise semantic affinity explicitly, but the model in <ref type="bibr" target="#b35">[36]</ref> learns it implicitly.</p><p>Learning with synthetic labels: We adopt the disjoint pipeline that first generates synthetic labels and train a segmentation model with the labels in a fully supervised manner. Such a pipeline has been studied for object detection <ref type="bibr" target="#b33">[34]</ref> as well as semantic segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref> in weakly supervised settings. A unique feature of our approach is the AffinityNet, an end-to-end trainable DNN improving the quality of synthetic labels significantly, when compared to previous methods that adopt existing optimization techniques (e.g., GraphCut, GrabCut, and dCRF) and/or off-the-shelf preprocessing steps aforementioned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Framework</head><p>Our approach to weakly supervised semantic segmentation is roughly divided into two parts: (1) Synthesizing pixel-level segmentation labels of training images given their image-level class labels, and (2) Learning a DNN for semantic segmentation with the generated segmentation labels. The entire framework is based on three DNNs: A network computing CAMs, AffinityNet, and a segmentation model. The first two are used to generate segmentation labels of training images, and the last one is the DNN that performs actual semantic segmentation and is trained with the synthesized segmentation annotations. The remainder of this section describes characteristics of the three networks and training schemes for them in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Computing CAMs</head><p>CAMs play an important role in our framework. As in many other weakly supervised approaches, they are considered as segmentation seeds, which typically highlight local salient parts of object and later propagate to cover the entire object area. Furthermore, in our framework they are employed as sources of supervision for training AffinityNet.</p><p>We follow the approach of <ref type="bibr" target="#b39">[40]</ref> to compute CAMs of training images. The architecture is a typical classification network with global average pooling (GAP) followed by a fully connected layer, and is trained by a classification criteria with image-level labels. Given the trained network, the CAM of a groundtruth class c, which is denoted by M c , is computed by</p><formula xml:id="formula_0">M c (x, y) = w ⊤ c f cam (x, y),<label>(1)</label></formula><p>where w c is the classification weights associated to the class c and f cam (x, y) indicates the feature vector located at (x, y) on the feature map before the GAP. M c is further normalized so that the maximum activation equals 1:</p><formula xml:id="formula_1">M c (x, y) → M c (x, y)/ max x,y M c (x, y).</formula><p>For any class c ′ irrelevant to the groundtruths, we disregard M c ′ by making its activation scores zero. We also estimate a background activation map, which is given by where C is the set of object classes and α ≥ 1 denotes a hyper-parameter that adjusts the background confidence scores. Qualitative examples of CAMs obtained by our approach are visualized in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><formula xml:id="formula_2">M bg (x, y) = 1 − max c∈C M c (x, y) α (2) (a) (b) (c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning AffinityNet</head><p>AffinityNet aims to predict class-agnostic semantic affinity between a pair of adjacent coordinates on a training image. The predicted affinities are used in random walk as transition probabilities so that random walk propagates activation scores of CAMs to nearby areas of the same semantic entity, which improves the quality of CAMs significantly.</p><p>For computational efficiency, AffinityNet is designed to predict a convolutional feature map f aff where the semantic affinity between a pair of feature vectors is defined in terms of their L 1 distance. Specifically, the semantic affinity between features i and j is denoted by W ij and defined as</p><formula xml:id="formula_3">W ij = exp − f aff (x i , y i ) − f aff (x j , y j ) 1 ,<label>(3)</label></formula><p>where (x i , y i ) indicates the coordinate of the i th feature on the feature map f aff . In this way, a large number of semantic affinities in a given image can be computed efficiently by a single forward pass of the network. <ref type="figure" target="#fig_1">Figure 3</ref> illustrates the AffinityNet architecture and the way it computes f aff . Training this architecture requires semantic affinity labels for pairs of feature map coordinates, i.e., labels for W ij in Eq. (3). However, such labels are not directly available in our setting where only image-level labels are given. In the remaining part of this section, we present how to generate the affinity labels and train AffinityNet with them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Generating Semantic Affinity Labels</head><p>To train AffinityNet with image-level labels, we exploit CAMs of training images as incomplete sources of supervision. Although CAMs are often inaccurate as shown in <ref type="figure" target="#fig_0">Figure 2</ref>, we found that by carefully manipulating them, reliable supervision for semantic affinity can be obtained.</p><p>Our basic idea is to identify confident areas of objects and background from the CAMs, and sample training examples only from these areas. By doing so, the semantic equivalence between a pair of sampled coordinates can be aff is obtained by aggregating feature maps from multiple levels of a backbone network so that f aff can take semantic information at various field-of-views. Specifically, we first apply 1×1 convolutions to the multi-level feature maps for dimensionality reduction, concatenate the results as a single feature map, and employ one more 1×1 convolution for adaptation to the target task. More details of the architecture is described in Section 4. determined reliably. To estimate confident areas of objects, we first amplify M bg by decreasing α in Eq. (2) so that background scores dominate insignificant activation scores of objects in the CAMs. After applying dCRF to the CAMs for their refinement, we identify confident areas for each object class by collecting coordinates whose scores for the target class are greater than those of any other classes including the amplified background. Also, in the opposite setting (i.e., increasing α to weaken M bg ), confident background areas can be identified in the same manner. Remaining areas in the image are then considered as neutral. A result of this procedure is visualized in <ref type="figure" target="#fig_2">Figure 4(a)</ref>. Now a binary affinity label can be assigned to every pair of coordinates according to their class labels determined by the confident areas. For two coordinates (x i , y i ) and (x j , y j ) that are not neutral, their affinity label W * ij is 1 if their classes are the same, and 0 otherwise. Also, if at least one of the coordinates is neutral, we simply ignore the pair during training. This scheme, which is illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>(b), enables us to collect a fairly large number of pairwise affinity labels which are also reliable enough.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">AffinityNet Training</head><p>AffinityNet is trained by approximating the binary affinity labels W * ij with the predicted semantic affinities W ij of Eq. (3) in a gradient descent fashion. Especially, affinities of only sufficiently adjacent coordinates are considered during training due to the following two reasons. First, it is difficult to predict semantic affinity between two coordinates too far from each other due to the lack of context. Second, by addressing pairs of adjacent coordinates only, we can reduce computational cost significantly. Thus the set of coordinate pairs used in training, denoted by P, is given by where d(·, ·) is the Euclidean distance and γ is a search radius that limits the distance between a selected pair. However, learning AffinityNet directly from P is not desirable due to the class imbalance issue. We observed that in P the class distribution is significantly biased to positive ones since negative pairs are sampled only around object boundaries. Also in the subset of positive pairs, the number of background pairs is notably larger than that of object pairs as background is larger than object areas in many photos. To address this issue, we divide P into three subsets, and aggregate losses obtained from individual subsets. Specifically, we first divide P into two subsets of positive and negative pairs:</p><formula xml:id="formula_4">P = (i, j) | d (x i , y i ), (x j , y j ) &lt; γ, ∀i = j (4)<label>(a)</label></formula><formula xml:id="formula_5">P + = (i, j) | (i, j) ∈ P, W * ij = 1 ,<label>(5)</label></formula><formula xml:id="formula_6">P − = (i, j) | (i, j) ∈ P, W * ij = 0 ,<label>(6)</label></formula><p>and further break P + into P + fg and P + bg for objects and background, respectively. Then the cross-entropy loss is computed per subset as follows:</p><formula xml:id="formula_7">L + fg = − 1 |P + fg | (i,j)∈P + fg log W ij ,<label>(7)</label></formula><formula xml:id="formula_8">L + bg = − 1 |P + bg | (i,j)∈P + bg log W ij ,<label>(8)</label></formula><formula xml:id="formula_9">L − = − 1 |P − | (i,j)∈P − log(1 − W ij ).<label>(9)</label></formula><p>Finally, the loss for training the AffinityNet is defined as</p><formula xml:id="formula_10">L = L + fg + L + bg + 2L − .<label>(10)</label></formula><p>Note that the loss in Eq. <ref type="formula" target="#formula_0">(10)</ref> is class-agnostic. Thus the trained AffinityNet decides the class consistency between two adjacent coordinates while not aware of their classes explicitly. This class-agnostic scheme allows AffinityNet to learn a more general representation that can be shared among multiple object classes and background, and enlarges the set of training samples per class significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Revising CAMs Using AffinityNet</head><p>The trained AffinityNet is used to revise CAMs of training images. Local semantic affinities predicted by AffinityNet are converted to a transition probability matrix, which enables random walk to be aware of semantic boundaries in image, and encourages it to diffuse activation scores within those boundaries. We empirically found that random walk with the semantic transition matrix significantly improves the quality of CAMs and allows us to generate accurate segmentation labels consequently.</p><p>For an input image, AffinityNet generates a convolutional feature map, and semantic affinities between features on the map are computed according to Eq. (3). Note that, as in training of AffinityNet, the affinities are computed between features within local circles of radius γ. The computed affinities form an affinity matrix W , whose diagonal elements are 1. The transition probability matrix T of random walk is derived from the affinity matrix as follows:</p><formula xml:id="formula_11">T = D −1 W •β , where D ii = j W β ij .<label>(11)</label></formula><p>In the above equation, the hyper-parameter β has a value greater than 1 so that W •β , the Hadamard power of the original affinity matrix, ignores immaterial affinities in W . Thus using W</p><p>•β instead of W makes our random walk propagation more conservative. The diagonal matrix D is computed for row-wise normalization of W</p><p>•β . Through random walk with T , a single operation of the semantic propagation is implemented by multiplying T to the CAMs. We perform this propagation iteratively until the predefined number of iterations is reached. Then M * c , the revised CAM of class c is given by</p><formula xml:id="formula_12">vec(M * c ) = T t · vec(M c ) ∀c ∈ C ∪ {bg},<label>(12)</label></formula><p>where vec(·) means vectorization of a matrix, and t is the number of iterations. Note that the value of t is set to a power of 2 so that Eq. (12) performs matrix multiplication only log 2 t + 1 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Learning a Semantic Segmentation Network</head><p>The revised CAMs of training images are then used to generate segmentation labels of the images. Since CAMs are smaller than their input image in size, we upsample them to the resolution of the image by bilinear interpolation, and refine them with dCRF. A segmentation label of a training image is then obtained simply by selecting the class label associated with the maximum activation score at every pixel in the revised and upsampled CAMs. Note that the background class can be also selected since we compute CAMs for background as well as object classes.</p><p>The segmentation labels obtained by the above procedure are used as supervision to train a segmentation network. Any fully supervised semantic segmentation model can be employed in our approach as we provide segmentation labels of training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Network Architectures</head><p>In this section, we present details of DNN architectures adopted in our framework. Note that our approach can be implemented with any existing DNNs serving the same purposes, although we carefully design the following models to enhance segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Backbone Network</head><p>The three DNNs in our framework are all built upon the same backbone network. The backbone is a modified version of Model A1 <ref type="bibr" target="#b37">[38]</ref>, which is also known as ResNet38 and has 38 convolution layers with wide channels. To obtain the backbone network, the ultimate GAP and fully connected layer of the original model are first removed. Then the convolution layers of the last three levels 1 are replaced by atrous convolutions with a common input stride of 1, and their dilation rates are adjusted so that the backbone network will return a feature map of stride 8. The atrous convolution has been known to enhance segmentation quality by enlarging receptive field without sacrificing feature map resolution <ref type="bibr" target="#b3">[4]</ref>. We empirically observed that it works also in our weakly supervised models, CAM and AffinityNet, as it enables the models to recover fine shapes of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Details of DNNs in Our Framework</head><p>Network computing CAMs: We obtain this model by adding the following three layers on the top of the backbone network in the order: a 3×3 convolution layer with 512 channels for a better adaptation to the target task, a global average pooling layer for feature map aggregation, and a fully connected layer for classification. AffinityNet: This network is designed to aggregate multilevel feature maps of the backbone network in order to leverage semantic information acquired at various field-ofviews when computing affinities. For the purpose, the feature maps output from the last three levels of the backbone network are selected. Before aggregation, their channel dimensionalities are reduced to 128, 256, and 512, for the first, second, and third feature maps, respectively, by individual 1×1 convolution layers. Then the feature maps are concatenated to be a single feature map with 896 channels. We finally add one more 1×1 convolution layer with 896 channels on the top for adaptation. <ref type="bibr" target="#b0">1</ref> A level is a group of residual units that share the same output stride. <ref type="bibr">Kwak et</ref>   Segmentation model: We strictly follow <ref type="bibr" target="#b37">[38]</ref> to build our segmentation network. Specifically, we put two more atrous convolution layers on the top of the backbone. They have the same dilation rate of 12, while the numbers of channels are 512 for the first one and 21 for the second. The resulting network is called "Ours-ResNet38" in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>This section demonstrates the effectiveness of our approach with comparisons to current state of the art in weakly supervised semantic segmentation on the PASCAL VOC 2012 segmentation benchmark <ref type="bibr" target="#b7">[8]</ref>. For a performance metric, we adopt Intersection-over-Union (IoU) between groundtruth and predicted segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>Dataset: All DNNs in our framework are trained and evaluated on the PASCAL VOC 2012 segmentation benchmark,  for a fair comparison to previous approaches. Following the common practice, we enlarge the set of training images by adopting segmentation annotations presented in <ref type="bibr" target="#b8">[9]</ref>. Consequently 10,582 images in total are used as training examples and 1,449 images are kept for validation. Network parameter optimization: The backbone network of our DNNs is pretrained on the ImageNet <ref type="bibr" target="#b6">[7]</ref>. The entire network parameters are then finetuned on the PASCAL VOC 2012 by Adam <ref type="bibr" target="#b12">[13]</ref>. The following data augmentation techniques are commonly used when training all the three DNNs: horizontal flip, random cropping, and color jittering <ref type="bibr" target="#b15">[16]</ref>. Also, for the networks except AffinityNet, we randomly scale input images during training, which is useful to impose scale invariance on the networks. Parameter setting: α in Eq. (2) is 16 by default, and changed to 4 and 24 to amplify and weaken background activations, respectively. We set γ in Eq. (4) to 5 and β in Eq. (11) to 8. Also, t in Eq. <ref type="formula" target="#formula_0">(12)</ref> is fixed by 256. For dCRF, we used the default parameters given in the original code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analysis of Synthesized Segmentation Labels</head><p>The performance of our label synthesis method is measured in mIoU between groundtruth and generated segmentation labels as summarized in <ref type="table">Table 1</ref>. For ablation study, our method is divided into three parts: CAM, RW (random walk with AffinityNet), and dCRF. To demonstrate the advantage of the proposed method, we also report the score of Superpixel Pooling Net (SPN) <ref type="bibr" target="#b16">[17]</ref> that incorporates CAM with superpixels as additional clues to generate segmentation labels with image-level label supervision. As shown in <ref type="table">Table 1</ref>, even our CAM outperforms SPN in terms of the quality of generated segmentation labels without using offthe-shelf techniques like superpixel. We believe this is because of the various data augmentation techniques and the more powerful backbone network with atrous convolution layers. Moreover, through random walk with the learned semantic affinity, the quality of segmentation annotations is improved remarkably, which demonstrates the effectiveness of AffinityNet. Finally, dCRF further improves the label quality slightly, and we employ this last version as the supervision for learning the segmentation network.</p><p>Examples of the synthesized segmentation labels are shown in <ref type="figure" target="#fig_3">Figure 5</ref>, where one can see that random walk with AffinityNet handles false positives and missing areas in CAMs effectively. To illustrate the role of AffinityNet in this process, we also visualized predicted semantic affinities of the images by detecting edges on the feature map f aff , and observed that AffinityNet has capability to detect semantic boundaries although it is trained with image-level labels. As such boundaries penalize random walk propagation between semantically different objects, the synthesized segmentation labels can recover accurate shapes of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparisons to Previous Work</head><p>We first quantitatively compare our approach with previous methods based only on image-level class labels. The results on the PASCAL VOC 2012 are summarized in Table 2 and 3. Note that we also evaluate DeepLab <ref type="bibr" target="#b3">[4]</ref> trained with our synthetic labels, called "Ours-DeepLab", for fair comparisons to other models whose backbones are VGG16 <ref type="bibr" target="#b32">[33]</ref>. Both of our models outperform the current state of the art <ref type="bibr" target="#b36">[37]</ref> by large margins in terms of mean accuracy on both val and test sets of the benchmark, while OursResNet38 is slightly better than Ours-DeepLab thanks to the more powerful representation of ResNet38. Our models are also compared to the approaches based on extra training data or stronger supervision in <ref type="table">Table 4</ref>. They substantially outperform the approaches based on the same level of supervision but with extra data and annotations like segmentation labels in MS-COCO <ref type="bibr" target="#b19">[20]</ref>, class-agnostic bounding boxes in MSRA Saliency <ref type="bibr" target="#b20">[21]</ref>, and YouTube videos <ref type="bibr" target="#b30">[31]</ref>. They are also competitive with previous arts relying on stronger supervision like scribble and bounding box. Surprisingly, Ours-ResNet38 outperforms even FCN <ref type="bibr" target="#b21">[22]</ref>, the well-known early work on fully supervised semantic segmentation. These results show that segmentation labels generated by our method are sufficiently strong, and can substitute for extra data or stronger supervision. We finally compare our models with their fully supervised versions, DeepLab <ref type="bibr" target="#b3">[4]</ref> and ResNet38 <ref type="bibr" target="#b37">[38]</ref>, which are the upper bounds we can achieve. Specifically, Ours-DeepLab recovers 86% of its bound, and Ours-ResNet38 achieves 77%. <ref type="figure" target="#fig_4">Figure 6</ref> presents qualitative results of Ours-ResNet38 and compares them to those of CrawlSeg <ref type="bibr" target="#b10">[11]</ref>, which is the current state of the art using image-level supervision. Our method relying only on image-level label supervision tends to produce more accurate results even though CrawlSeg exploits extra video data to synthesize segmentation labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>To alleviate the lack of annotated data issue in semantic segmentation, we have proposed a novel framework based on AffinityNet to generate accurate segmentation labels of training images given their image-level class labels only. The effectiveness of our approach has been demonstrated on the PASCAL VOC 2012 benchmark, where DNNs trained with the labels generated by our method substantially outperform the previous state of the art relying on the same level of supervision, and is competitive with those demanding stronger supervision or extra data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Visualization of CAMs obtained by our approach. (a) Input image. (b) CAMs of object classes: Brighter means more confident object region. (c) CAMs of background: Darker means more confident background region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Overall architecture of AffinityNet. The output feature map f aff is obtained by aggregating feature maps from multiple levels of a backbone network so that f aff can take semantic information at various field-of-views. Specifically, we first apply 1×1 convolutions to the multi-level feature maps for dimensionality reduction, concatenate the results as a single feature map, and employ one more 1×1 convolution for adaptation to the target task. More details of the architecture is described in Section 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Conceptual illustration of generating semantic affinity labels. (a) Confident areas of object classes and background: peach for person, green for plant, and black for background. The neutral area is color-coded in white. (b) Coordinate pairs sampled within a small radius for training AffinityNet. Each pair is assigned label 1 if its two coordinates come from the same class, and label 0 otherwise. When at least one of the two coordinates belongs to the neutral area, the pair is ignored during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Qualitative examples of synthesized segmentation labels of training images in the PASCAL VOC 2012 benchmark. (a) Input images. (b) Groundtruth segmentation labels. (c) CAMs of object classes. (d) Visualization of the predicted semantic affinities. (e) Synthesized segmentation annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Qualitative results on the PASCAL VOC 2012 val set. (a) Input images. (b) Groundtruth segmentation. (c) Results obtained by CrawlSeg [11]. (d) Results of Ours-ResNet38. Compared to CrawlSeg, which is the current state-of-the-art model based on image-level label supervision, our method better captures larger object areas and less prone to miss objects. The object boundaries of our results are smoother than those of CrawlSeg as we do not apply dCRF to the final results. More results can be found in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Method bkg aero bike bird boat bottle bus car cat chair cow table dog horse mbk person plant sheep sofa train tv meanTable 2. Performance on the PASCAL VOC 2012 val set, compared to weakly supervised approaches based only on image-level labels.</figDesc><table>EM-Adapt [28] 67.2 29.2 17.6 28.6 22.2 29.6 47.0 44.0 44.2 14.6 35.1 24.9 41.0 34.8 41.6 32.1 24.8 37.4 24.0 38.1 31.6 33.8 
CCNN [29] 
68.5 25.5 18.0 25.4 20.2 36.3 46.8 47.1 48.0 15.8 37.9 21.0 44.5 34.5 46.2 40.7 30.4 36.3 22.2 38.8 36.9 35.3 
MIL+seg [30] 
79.6 50.2 21.6 40.9 34.9 40.5 45.9 51.5 60.6 12.6 51.2 11.6 56.8 52.9 44.8 42.7 31.2 55.4 21.5 38.8 36.9 42.0 
SEC [14] 
82.4 62.9 26.4 61.6 27.6 38.1 66.6 62.7 75.2 22.1 53.5 28.3 65.8 57.8 62.3 52.5 32.5 62.6 32.1 45.4 45.3 50.7 
AdvErasing [37] 83.4 71.1 30.5 72.9 41.6 55.9 63.1 60.2 74.0 18.0 66.5 32.4 71.7 56.3 64.8 52.4 37.4 69.1 31.4 58.9 43.9 55.0 
Ours-DeepLab 87.2 57.4 25.6 69.8 45.7 53.3 76.6 70.4 74.1 28.3 63.2 44.8 75.6 66.1 65.1 71.1 40.5 66.7 37.2 58.4 49.1 58.4 
Ours-ResNet38 88.2 68.2 30.6 81.1 49.6 61.0 77.8 66.1 75.1 29.0 66.0 40.2 80.4 62.0 70.4 73.7 42.5 70.7 42.6 68.1 51.6 61.7 

Method 
bkg aero bike bird boat bottle bus car cat chair cow table dog horse mbk person plant sheep sofa train tv mean 
EM-Adapt [28] 76.3 37.1 21.9 41.6 26.1 38.5 50.8 44.9 48.9 16.7 40.8 29.4 47.1 45.8 54.8 28.2 30.0 44.0 29.2 34.3 46.0 39.6 
CCNN [29] 
70.1 24.2 19.9 26.3 18.6 38.1 51.7 42.9 48.2 15.6 37.2 18.3 43.0 38.2 52.2 40.0 33.8 36.0 21.6 33.4 38.3 35.6 
MIL+seg [30] 
78.7 48.0 21.2 31.1 28.4 35.1 51.4 55.5 52.8 7.8 56.2 19.9 53.8 50.3 40.0 38.6 27.8 51.8 24.7 33.3 46.3 40.6 
SEC [14] 
83.5 56.4 28.5 64.1 23.6 46.5 70.6 58.5 71.3 23.2 54.0 28.0 68.1 62.1 70.0 55.0 38.4 58.0 39.9 38.4 48.3 51.7 
AdvErasing [37] 
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
55.7 
Ours-DeepLab 88.0 61.1 29.2 73.0 40.5 54.1 75.2 70.4 75.1 27.8 62.5 51.4 78.4 68.3 76.2 71.8 40.7 74.9 49.2 55.0 48.3 60.5 
Ours-ResNet38 89.1 70.6 31.6 77.2 42.2 68.9 79.1 66.5 74.9 29.6 68.7 56.1 82.1 64.8 78.6 73.5 50.8 70.7 47.7 63.9 51.1 63.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Performance on the PASCAL VOC 2012 test set, compared to weakly supervised approaches based only on image-level labels.bounding box, I-image-level label, and F-segmentation label.</figDesc><table>Method 
Sup. 
Extra Data 
val 
test 
TransferNet [10] 
I 
MS-COCO [20] 
52.1 51.2 
Saliency [26] 
I 
MSRA [21], BSDS [24] 
55.7 56.7 
MCNN [35] 
I 
YouTube-Object [31] 
38.1 39.8 
CrawlSeg [11] 
I 
YouTube Videos 
58.1 58.7 
What'sPoint [1] 
P 
-
46.0 43.6 
RAWK [36] 
S 
-
61.4 
-
ScribbleSup [18] 
S 
-
63.1 
-
WSSL [28] 
B 
-
60.6 62.2 
BoxSup [6] 
B 
-
62.0 64.6 
SDI [12] 
B 
BSDS [24] 
65.7 67.5 
FCN [22] 
F 
-
-
62.2 
DeepLab [3] 
F 
-
67.6 70.3 
ResNet38 [38] 
F 
-
80.8 82.5 
Ours-DeepLab 
I 
-
58.4 60.5 
Ours-ResNet38 
I 
-
61.7 63.7 

Table 4. Performance on the PASCAL VOC 2012 val and test sets. 
The supervision types (Sup.) indicate: P-point, S-scribble, B-
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: This work was supported by Kakao, Korea Creative Content Agency (KOCCA), and Ministry of Culture, Sports, and Tourism (MCST) of Korea.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What&apos;s the Point: Semantic Segmentation with Point Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional random walk networks for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>PP</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Localitysensitive deconvolution networks with gated fusion for rgb-d indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BoxSup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ImageNet: a large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Pascal Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning transferrable knowledge for semantic segmentation with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3204" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation using web-crawled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7322" to="7330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="695" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems (NIPS)</title>
		<meeting>the Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems (NIPS)</title>
		<meeting>the Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation using superpixel pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4111" to="4117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Dan Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Random walks on graphs: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lovsz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int&apos;l Conf. Computer Vision</title>
		<meeting>8th Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001-07" />
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploiting saliency for object segmentation from image level labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4410" to="4419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a DCNN for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Constrained convolutional neural networks for weakly supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From image-level to pixellevel labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1713" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3282" to="3289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchically gated deep networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="3059" to="3067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation using motion cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="388" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning random-walk label propagation for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10080</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
