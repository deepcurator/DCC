<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Attention-guided Image-to-Image Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><forename type="middle">A</forename><surname>Mejjati</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Bath</orgName>
								<orgName type="institution" key="instit2">University of Bath</orgName>
								<orgName type="institution" key="instit3">Brown University</orgName>
								<orgName type="institution" key="instit4">University of Bath</orgName>
								<orgName type="institution" key="instit5">University of Bath</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Richardt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Bath</orgName>
								<orgName type="institution" key="instit2">University of Bath</orgName>
								<orgName type="institution" key="instit3">Brown University</orgName>
								<orgName type="institution" key="instit4">University of Bath</orgName>
								<orgName type="institution" key="instit5">University of Bath</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Tompkin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Bath</orgName>
								<orgName type="institution" key="instit2">University of Bath</orgName>
								<orgName type="institution" key="instit3">Brown University</orgName>
								<orgName type="institution" key="instit4">University of Bath</orgName>
								<orgName type="institution" key="instit5">University of Bath</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darren</forename><surname>Cosker</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Bath</orgName>
								<orgName type="institution" key="instit2">University of Bath</orgName>
								<orgName type="institution" key="instit3">Brown University</orgName>
								<orgName type="institution" key="instit4">University of Bath</orgName>
								<orgName type="institution" key="instit5">University of Bath</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang</forename><forename type="middle">In</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Bath</orgName>
								<orgName type="institution" key="instit2">University of Bath</orgName>
								<orgName type="institution" key="instit3">Brown University</orgName>
								<orgName type="institution" key="instit4">University of Bath</orgName>
								<orgName type="institution" key="instit5">University of Bath</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Attention-guided Image-to-Image Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Current unsupervised image-to-image translation techniques struggle to focus their attention on individual objects without altering the background or the way multiple objects interact within a scene. Motivated by the important role of attention in human perception, we tackle this limitation by introducing unsupervised attention mechanisms that are jointly adversarially trained with the generators and discriminators. We demonstrate qualitatively and quantitatively that our approach attends to relevant regions in the image without requiring supervision, which creates more realistic mappings when compared to those of recent approaches.</p><p>Figure 1: By explicitly modeling attention, our algorithm is able to better alter the object of interest in unsupervised image-to-image translation tasks, without changing the background at the same time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: By explicitly modeling attention, our algorithm is able to better alter the object of interest in unsupervised image-to-image translation tasks, without changing the background at the same time.</p><p>ial networks (GANs), the quality of the generated images improves as the generator and discriminator compete to reach the Nash equilibrium expressed by the minimax loss of the training procedure <ref type="bibr" target="#b11">[12]</ref>.</p><p>However, these approaches are limited by the system's inability to attend only to specific scene objects. In the unsupervised case, where images are not paired or aligned, the network must additionally learn which parts of the scene are intended to be translated. For instance, in <ref type="figure">Figure 1</ref>, a convincing translation between the horse and zebra domains requires the network to attend to each animal and change only those parts of the image. This is challenging for existing approaches, even if they use a localized loss like PatchGAN <ref type="bibr" target="#b12">[13]</ref>, as the network itself has no explicit attention mechanism. Instead, they typically aim to minimize the divergence between the underlying data-generating distribution for the entire image in the source and target domains. To overcome this limitation, we propose to minimize the divergence between only the relevant parts of the data-generating distributions for the source and target domains. For this, we find inspiration from attentional mechanisms in human perception <ref type="bibr" target="#b13">[14]</ref>, and their successful application in machine learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref>. We add an attention network to each generator in the CycleGAN setup. These are jointly trained to produce attention maps for regions that the discriminator 'considers' are the most discriminative between the source and target domains. Then, these maps are applied to the input of the generator to constrain it to relevant image regions. The whole network is trained end-to-end with no additional supervision. We qualitatively and quantitatively show that explicitly incorporating attention into image translation networks significantly improves the quality of translated images (see <ref type="figure">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Image-to-image translation. Contemporary image-to-image translation approaches leverage the powerful ability of deep neural networks to build meaningful representations. Specifically, GANs have proven to be the gold standard in achieving appealing image-to-image translation results. For instance, Isola et al.'s pix2pix algorithm <ref type="bibr" target="#b8">[9]</ref> uses a GAN conditioned on the source image and imposes an L 1 loss between the generated image and its ground-truth map. This requires the existence of ground-truth paired images from each of the source and target domains. Zhu et al.'s unpaired image-to-image translation network <ref type="bibr" target="#b0">[1]</ref> builds upon pix2pix and removes the paired input data burden by imposing that each image should be reconstructed correctly when translated twice, i.e., when mapped from source to target to source. These maps must conserve the overall structure and content of the image. DiscoGAN <ref type="bibr" target="#b2">[3]</ref> and DualGAN <ref type="bibr" target="#b4">[5]</ref> use the same principle, but with different losses, making them more or less robust to changes in shape.</p><p>Some unsupervised translation approaches assume the existence of a shared latent space between source and target domains. Liu and Tuzel's Coupled GAN (CoGAN) <ref type="bibr" target="#b15">[16]</ref> learns an estimate of the joint data-generating distribution using samples from the marginals, by enforcing source and target discriminators and generators to share parameters in low-level layers. Liu et al.'s unsupervised image-to-image translation networks (UNIT) <ref type="bibr" target="#b3">[4]</ref> build upon Coupled GAN by assuming the existence of a shared low-dimensional latent space between the source and target domains. Once the image is mapped to its latent representation, then a generator decodes it into its target domain version. Huang et al.'s multi-modal UNIT (MUNIT) <ref type="bibr" target="#b16">[17]</ref> framework extends this idea to multi-modal image-to-image translation by assuming two latent representations: one for 'style' and one for 'content'. Then, the cross-domain image translation is performed by combining different content and style representations.</p><p>Given input images depicting objects at multiple scales, the aforementioned approaches are sometimes able to translate the foreground. However, they generally also affect the background in unwanted ways, leading to unrealistic translations. We demonstrate that our algorithm is able to overcome this limitation by incorporating attention into the image translation framework.</p><p>Attending to specific regions within image translation has recently been explored by Ma et al. <ref type="bibr" target="#b17">[18]</ref>, who attempt to decouple local textures from holistic shapes by attending to local objects of interest (e.g., eyes, nose, and mouth in a face); this is manifested through attention maps as individual square image regions. This limits the approach, as <ref type="bibr" target="#b0">(1)</ref> it assumes that all objects are the same size, corresponding to the sizes of the square attention maps, and (2) it involves tuning hyper-parameters for the number and size of the square regions. As a consequence, this approach cannot straightforwardly deal with image translation without altering the background.</p><p>Attention learning. Attention learning has benefited from advances in deep learning. Contemporary approaches use convolution-deconvolution networks trained on ground-truth masks <ref type="bibr" target="#b18">[19]</ref>, and combine these architectures with recurrent attention models. Specifically, Kuen et al.'s saliency detection <ref type="bibr" target="#b19">[20]</ref>   <ref type="figure">Figure 2</ref>: Data-flow diagram from the source domain S to the target domain T during training. The roles of S and T are symmetric in our network, so that data also flows in the opposite direction T → S.</p><p>regions in the input image for saliency estimation. Then, these local estimates are combined into a global estimate. Such approaches cannot be applied in our setting, since they require supervision.</p><p>Unsupervised attention learning includes Mnih et al.'s recurrent model of visual attention <ref type="bibr" target="#b14">[15]</ref>, which uses only a few learned square regions of the image trained from classification labels. This approach is not differentiable and requires training with reinforcement learning, which is not straightforward to apply in our problem. More recently, attention has been enforced on activation functions to select only task-relevant features <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref>. However, we show in experiments that our approach of enforcing attention on the input image provides better results for image-to-image translation.</p><p>Learning attention also encourages the generation of more realistic images compared to classic vanilla GANs. For example, Zhang et al.'s self-attention GANs <ref type="bibr" target="#b21">[22]</ref> constrain the generator to gradually consider non-local relationships in the feature space by using unsupervised attention, which produces globally realistic images. Yang et al.'s recursive approach <ref type="bibr" target="#b22">[23]</ref> generates images by decoupling the generation of the foreground and background in a sequential manner; however, its extension to image-to-image translation is not straightforward as in that case we only care about modifying the foreground. Attention has also been used for video generation <ref type="bibr" target="#b23">[24]</ref>, where a binary mask is learned to distinguish between dynamic and static regions in each frame of a generated video. The generated masks are trained to detect unrealistic motions and patterns in the generated frames, whereas our attention network is trained to find the most discriminative regions which characterize a given image domain. Finally, Chen et al.'s contemporaneous work shares our goal of learning an attention map for image translation <ref type="bibr" target="#b24">[25]</ref>; we will discuss the differences between our methods after explaining our approach (see Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our approach</head><p>The goal of image translation is to estimate a map F S→T from a source image domain S to a target image domain T based on independently sampled data instances X S and X T , such that the distribution of the mapped instances F S→T (X S ) matches the probability distribution P T of the target. Our starting point is Zhu et al.'s CycleGAN approach <ref type="bibr" target="#b0">[1]</ref>, which also learns a domain inverse F T →S to enforce cycle consistency: F T →S (F S→T (X S )) ≈ X S . The training of the transfer network F S→T requires a discriminator D T to try to detect the translated outputs from the observed instances X T . For cycle consistency, the inverse map F T →S and the corresponding discriminator D S are simultaneously trained.</p><p>Solving this problem requires solving two equally important tasks: (1) locating the areas to translate in each image, and (2) applying the right translation to the located areas. We achieve this by adding two attention networks A S and A T , which select areas to translate by maximizing the probability that the discriminator makes a mistake. We denote A S : S → S a and A T : T → T a , where S a and T a are the attention maps induced from S and T , respectively. Each attention map contains per-pixel [0,1] estimates. After feeding the input image to the generator, we apply the learned mask to the generated image using an element-wise product ' ', and then add the background using the inverse of the mask applied to the input image. As such, A S and A T are trained in tandem with the generators; <ref type="figure">Figure 2</ref> visualizes this process.</p><p>Henceforth, we will describe only the map F S→T ; the inverse map F T →S is defined similarly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Attention-guided generator</head><p>First, we feed the input image s ∈ S into the generator F S→T , which maps s to the target domain T . Then, the same input is fed to the attention network A S , resulting in the attention map s a = A S (s). To create the 'foreground' object s f ∈ T , we apply s a to F S→T (s) via an element-wise product on each RGB channel: s f = s a F S→T (s) <ref type="figure">(Figure 2</ref> shows an example). Finally, we create the 'background' image s b = (1−s a ) s, and add it to the masked output of the generator F S→T . Thus, the mapped image s is obtained by:</p><formula xml:id="formula_0">s = s a F S→T (s) Foreground + (1−s a ) s Background .</formula><p>(1)</p><p>Attention map intuition. The attention network A S plays a key role in Equation <ref type="formula">1</ref>. If the attention map s a was replaced by all ones, to mark the entire image as relevant, then we obtain CycleGAN as a special case of our approach. If s a was all zeros, then the generated image would be identical to the input image due to the background term in Equation 1, and the discriminator would never be fooled by the generator. If s a attends to an image region without a relevant foreground instance to translate, then the result s will preserve its source domain class (i.e. a horse will remain a horse).</p><p>In other words, the image parts which most describe the domain will remain unchanged, which makes it straightforward for the discriminator D T to detect the image as a fake. Therefore, the only way to find an equilibrium between generator F S→T , attention map A S , and discriminator D T is for A S to focus on the objects or areas that the corresponding discriminator thinks are the most descriptive within its domain (i.e., the horses). The discriminator mechanism which makes GAN generators produce realistic images also makes our attention networks find the domain-descriptive objects in the images.</p><p>The attention map is continuous between [0,1], i.e., it is a matte rather than a segmentation mask. This is valuable for three reasons: (1) it makes estimating the attention maps differentiable, and so able to train at all, (2) it allows the network to be uncertain about attention during the training process, which allows convergence, and (3) it allows the network to learn how to compose edges, which otherwise might make the foreground object look 'stuck on' or produce fringing artifacts.</p><p>Loss function. This process is governed by the adversarial energy:</p><formula xml:id="formula_1">L s adv (F S→T ,A S ,D T ) = E t∼P T (t) log(D T (t)) +E s∼P S (s) log(1−D T (s )) .<label>(2)</label></formula><p>In addition, and similarly to CycleGAN, we add a cycle-consistency loss to the overall framework by enforcing a one-to-one mapping between s and the output of its inverse mapping s :</p><formula xml:id="formula_2">L s cyc (s,s ) = s−s 1 ,<label>(3)</label></formula><p>where s is obtained from s via F T →S and A T , similarly to Equation 1.</p><p>This added loss makes our framework more robust in two ways: (1) it enforces the attended regions in the generated image to conserve content (e.g., pose), and (2) it encourages the attention maps to be sharp (converging towards a binary map), as the cycle-consistency loss of unattended areas will always be zero. Further, when computing s , we use the attention map extracted from A T (s ). This adds another consistency requirement, as the generated attention maps produced by A S and A T for s and s , respectively, should match to minimize Equation 3.</p><p>We obtain the final energy to optimize by combining the adversarial and cycle-consistency losses for both source and target domains:</p><formula xml:id="formula_3">L(F S→T ,F T →S ,A S ,A T ,D S ,D T ) = L s adv +L t adv +λ cyc L s cyc +L t cyc ,<label>(4)</label></formula><p>where we use the loss hyper-parameter λ cyc = 10 throughout our experiments. The optimal parameters of L are obtained by solving the minimax optimization problem:</p><formula xml:id="formula_4">F * S→T ,F * T →S ,A * S ,A * T ,D * S ,D * T = argmin F S→T ,F T →S ,A S ,A T argmax D S ,D T L(FS→T ,FT →S ,AS,AT ,DS,DT ) . (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attention-guided discriminator</head><p>Equation 1 constrains the generators to act only on attended regions: as the attention networks train to become more accurate at finding the foreground, the generator improves in translating just the object of interest between domains, e.g., from horse to zebra. However, there is a tension: the whole-image discriminators look (implicitly) at the distribution of backgrounds with respect to the translated foregrounds. For instance, one observes that the translated horse now looks correctly like a zebra, but also that the overall scene is fake, because the background still shows where horses live-in meadows-and not where zebras live-in savannas. In this sense, we really are trying to make a 'fake' image which does not match either underlying probability distribution P S or P T .</p><p>This tension manifests itself in two behaviors: (1) the generator F S→T tries to 'paint' background directly into the attended regions, and (2) the attention map slowly includes more and more background, converging towards a fully attended map (all values in the map converge to 1). Our appendix provides example cases (last column in <ref type="figure">Figure 7</ref>; ablation studies Ours-D and Ours-D-A in <ref type="figure">Figure 10</ref>).</p><p>To overcome this, we train the discriminator such that it only considers attended regions. Simply using s a s is problematic, as real samples fed to the discriminator now depend on the initially-untrained attention map s a . This leads to mode collapse if all networks in the GAN are trained jointly. To overcome this issue, we first train the discriminators on full images for 30 epochs, and then switch to masked images once the attention networks A S and A T have developed.</p><p>Further, with a continuous attention map, the discriminator may receive 'fractional' pixel values, which may be close to zero early in training. While the generator benefits from being able to blend pixels at object boundaries, multiplying real images by these fractional values causes the discriminator to learn that mid gray is 'real' (i.e., we push the answer towards the midpoint 0 of the normalized [−1,1] pixel space). Thus, we threshold the learned attention map for the discriminator:</p><formula xml:id="formula_5">t new = t if A T (t) &gt; τ 0 otherwise and s new = F S→T (s) if A S (s) &gt; τ 0 otherwise<label>(6)</label></formula><p>where t new and s new are masked versions of target sample t and translated source sample s , which only contain pixels exceeding a user-defined attention threshold τ , which we set to 0.1 ( <ref type="figure" target="#fig_3">Figure 8</ref> in the appendix justifies such choice). Moreover, we find that removing instance normalization from the discriminator at that stage is helpful as we do not want its final prediction to be influenced by zero values coming from the background.</p><p>Thus, we update the adversarial energy L adv of Equation 2 to:</p><formula xml:id="formula_6">L s adv (F S→T ,A S ,D T ) = E t∼P T (t) log(D T (t new )) +E s∼P S (s) log(1−D T (s new )) ,<label>(7)</label></formula><p>Algorithm 1 summarizes the training procedure for learning F S→T ; training F T →S is similar. Our appendix provides details of the individual network configurations.</p><p>When optimizing the objective in Equation 7 beyond 30 epochs, real image inputs to the discriminator are now also dependent on the learned attention maps. This can lead to mode collapse if the training is not performed carefully. For instance, if the mask returned by the attention network is always zero, Algorithm 1 Training procedure for the source-to-target map F S→T .</p><p>Input: X S , X T , K (number of epochs), λ cyc (cycle-consistency weight), α (ADAM learning rate).</p><p>1: for c = 0 to K −1 do 2:</p><formula xml:id="formula_7">for i = 0 to |X S |−1 do 3:</formula><p>Sample a data point s from X S and a data point t from X T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>if c &lt; 30 then</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Compute s using Equation 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Update parameters of F S→T , D T , and A S using Equation 4 with learning rate α.</p><p>7:</p><formula xml:id="formula_8">else 8:</formula><p>Compute s new and t new using Equation 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Update parameters of F S→T and D T using Equations 4 and 7 with learning rate α.  then the generator will always create 'real' images from the point of view of the discriminator, as the masked sample t new in Equation 7 would be all black. We avoid this situation by stopping the training of both A S and A T after 30 epochs ( <ref type="figure">Figure 7</ref> in the appendix justifies such hyper-parameter choice).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Baselines. We compare to DiscoGAN <ref type="bibr" target="#b2">[3]</ref> and CycleGAN <ref type="bibr" target="#b0">[1]</ref>, which are similar, but which use different losses: DiscoGAN uses a standard GAN loss <ref type="bibr" target="#b11">[12]</ref>, and CycleGAN uses a least-squared GAN loss <ref type="bibr" target="#b25">[26]</ref>. We also compare with DualGAN <ref type="bibr" target="#b4">[5]</ref>, which is similar to CycleGAN but uses a Wasserstein GAN loss <ref type="bibr" target="#b26">[27]</ref>. Aditionally, we compare with Liu et al.'s UNIT algorithm <ref type="bibr" target="#b3">[4]</ref>, which leverages the latent space assumption between each pair of source/target images. Finally, we compare with Wang et al.'s attention module <ref type="bibr" target="#b1">[2]</ref> by incorporating it after the first layer of our generators; we refer to this implementation as "RA".</p><p>Datasets. We use the 'Apple to Orange' (A ↔ O) and 'Horse to Zebra' (H ↔ Z) datasets provided by Zhu et al. <ref type="bibr" target="#b0">[1]</ref>, and the 'Lion to Tiger' (L ↔ T ) dataset obtained from the corresponding classes in the Animals With Attributes (AWA) dataset <ref type="bibr" target="#b27">[28]</ref>. These datasets contain objects at different scales across different backgrounds, which make the image-to-image translation setting more challenging. Note that for the mapping Lion to Tiger we do not find it necessary to apply the attention-guided discriminator part.</p><p>Qualitative results. Observing our learned attention maps, we can see that our approach is able to learn relevant image regions and ignore the background <ref type="figure">(Figure 3</ref>). When an input image does not contain any elements of the source domain, our approach does not attend to it, and so successfully leaves the image unedited. Holistic image translation approaches, on the other hand, are mislead by irrelevant background content and so incorrectly hallucinate texture patterns of the target objects (last two rows of <ref type="figure" target="#fig_2">Figure 5</ref>).</p><p>Among competing approaches, DiscoGAN struggles to separate the background and foreground content (see <ref type="figure" target="#fig_2">Figures 1, 4 and 5)</ref>. We believe this is partly because their cycle-consistency energy is given the same weight as the GAN's adversarial energy. DualGAN produces slightly better results, although the background is still heavily altered. For example, the first row of <ref type="figure">Figure 1</ref>    undesirable zebra patterns in the background. CycleGAN produces more visually appealing results with its least-squares GAN and appropriate weighting between the adversarial and cycle-consistency losses, even though some elements of the background are still altered. For instance, CycleGAN alters the writing on the chalkboard in the last row of <ref type="figure">Figure 4</ref>, and generates a blue-grey lion in the first row of <ref type="figure" target="#fig_2">Figure 5</ref> when asked to translate the zebra pinned down by the lion. The UNIT algorithm uses the shared latent space assumption between source and target domains to be robust to changes in geometric shape. For example, in the 7th row of <ref type="figure" target="#fig_2">Figure 5</ref>, we can see that the face of the lion cub is mapped to a tiger; however, the overall image is not realistic. Finally, incorporating residual attention (RA) modules into the image translation framework does not improve the generated image quality, which validates our choice of incorporating attention into images instead of on activation functions. This is particularly noticeable when the input source image does not contain any relevant object, as in <ref type="figure" target="#fig_2">Figure 5</ref> (bottom). In this case, existing algorithms are mislead by irrelevant background content and incorrectly hallucinate texture patterns of the target objects. By learning attention maps, our algorithm successfully ignores background contents and reproduces the input images.</p><formula xml:id="formula_9">Algorithm A → O O → A Z → H H → Z L → T T → L</formula><p>One limitation of our approach is visible in the last third row of <ref type="figure" target="#fig_2">Figure 5</ref>, which contains an albino tiger. In this challenging case of an object with outlier appearance within its domain, our attention network fails to identify the tiger as foreground, and so our network changes the background image content, too. However, overall, our approach of learning attention maps within unsupervised image-to-image translation obtains more realistic results, particularly for datasets containing objects at multiple scales and with different backgrounds.</p><p>Quantitative results. We use the recently proposed Kernel Inception Distance (KID) <ref type="bibr" target="#b28">[29]</ref> to quantitatively evaluate our image translation framework. KID computes the squared maximum mean discrepancy (MMD) between feature representations of real and generated images. Such feature representations are extracted from the Inception network architecture <ref type="bibr" target="#b29">[30]</ref>. In contrast to the Fréchet Inception Distance <ref type="bibr" target="#b30">[31]</ref>, KID has an unbiased estimator, which makes it more reliable, especially when there are fewer test images than the dimensionality of the inception features. While KID is not bounded, the lower its value, the more shared visual similarities there are between real and generated images. As we wish the foreground of mapped images to be in the target domain T and the background to remain in the source domain S, a good mapping should have a low KID value when computed using both the target and the source domains. Therefore, we report the mean KID value computed between generated samples using both source and target domains in <ref type="table" target="#tab_1">Table 1</ref>. Further, to ensure consistency, the mean KID values reported are averaged over 10 different splits of size 50, randomly sampled from each domain.</p><p>Our approach achieves the lowest KID score in all the mappings, with CycleGAN as the next best performing approach. UNIT achieves the second-lowest KID score, which suggests that the latent space assumption is useful in our setting. Using Wasserstein GAN allows DualGAN to follow closely behind. The CycleGAN variant using residual attention modules (RA) produces worse results than regular CycleGAN but comparable to UNIT, which suggests that applying attention on the feature space does not considerably improve performance. Finally, by giving the same weight to the adversarial and cyclic energy, DiscoGAN achieves the worst performance in terms of mean KID values, which is consistent with our qualitative results.</p><p>Ablation Study. First, we evaluate the cycle-consistency loss governed by <ref type="bibr">Equation 3</ref>. This is motivated by using attention to constrain the mapping between only relevant instances, which can be considered as a weak form of cycle consistency. The cycle-consistency loss plays an important role in making attention maps sharp; without them, we notice an onset of mode collapse in GAN training. As a result, we obtain a model ('Ours-cycle') with very high KID ( <ref type="table" target="#tab_3">Table 2)</ref>.</p><p>Next, we test the effect of computing attention on the inverse mapping. Instead of computing a new attention map A T (s ), we use the formerly computed A S (s). This model ('Ours-cycleAtt') performs worse, because computing attention on both the mapping and its inverse indirectly enforces similarity between both attention maps A T (s ) and A S (s).</p><p>Further, we evaluate behavior with only a single attention network: 'Ours-As' and 'Ours-At' corresponding to A S and A T , respectively. These approaches are the best performing after our final implementation: A S acts on s, but also on t via the inverse mapping, which influences the generators to still only translate relevant regions. Moreover, we measure the importance of our attention-guided discriminator by replacing it with a whole-image discriminator while stopping the training of the attention networks ('Ours-D'). For this model, mean KID values are higher than our final formulation because the generator tries to paint elements of the background onto the foreground to compensate for the variance between foreground and background in the source and target domains.</p><p>Finally, we consider the contemporaneous Attention GAN of Chen et al. <ref type="bibr" target="#b24">[25]</ref>, which also learns an attention map for image translation through a cyclic loss. We compare their approach using an ablated version of our software implementation, as we await a code release from the authors for a direct results comparison. Our approach differs in two ways: first, we feed the holistic image to the discriminator for the first 30 epochs, and afterwards show it only the masked image; second, we stop the training of the attention networks after 30 epochs to prevent it from focusing on the background as well. These two differences reduce errors caused by spurious image additions from F , and remove the need for the optional supervision introduced by Chen et al. to help remove background artifacts and better 'focus' the attention map on the foreground. <ref type="table" target="#tab_3">Table 2</ref> demonstrates this quantitatively ('Ours-D-A'), with higher KID scores compared to our final implementation. Please see the appendix document for visual examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>While recent unsupervised image-to-image translation techniques are able to map relevant image regions, they also inadvertently map irrelevant regions, too. By doing so, the generated images fail to look realistic, as the background and foreground are generally not blended properly. By incorporating an attention mechanism into unsupervised image-to-image translation, we demonstrate significant improvements in the quality of generated images. Our simple algorithm leverages the discriminator to learn accurate attention maps with no additional supervision. This suggests that our learned attention maps reflect where the discriminator looks before deciding whether an image is real or fake, making it an appropriate tool for investigating the behavior of adversarial networks.</p><p>Future work. Although our approach can produce appealing translation results in the presence of multi-scale objects and varying backgrounds, the overall approach is still not robust to shape changes between domains, e.g., making Pegasus by translating a horse into a bird. Our transfer must happen within attended regions in the image, but shape change typically requires altering parts outside these regions. In the appendix, we provide an example of such limitation via the mapping zebra to lion, <ref type="figure">Figure 6</ref>. Our code is released in the following Github repository: https://github.com/AlamiMejjati/Unsupervised-Attention-guided-Image-to-Image-Translation.</p><p>Discriminators D S and D T . We adopt the CycleGAN discriminator architecture: We use instance normalization everywhere apart from the last layer. However, when we start feeding only the foreground to the discriminator (after 30 epochs), we remove instance normalization as the input is at this stage a masked image and we do not want zero values to influence the generation process. In addition, instead of ReLUs, we use Leaky-ReLUs (LR) with slope 0.2.</p><p>Our discriminator architecture is: c4s2-64-LR, c4s2-128-LR, c4s2-256-LR, c4s1-512-LR, c4s1-1.</p><p>Finally, similar to CycleGAN we adopt Least Square GAN (LSGAN), as we find that it helps producing sharper images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Limitation of our approach</head><p>Although our approach can produce appealing translation results in the presence of multi-scale objects and varying backgrounds, the overall approach is still not robust to shape changes between domains, e.g., mapping zebras to lions depicted in <ref type="figure">Figure 6</ref>. Our transfer must happen within attended regions in the image, but shape change typically requires altering parts outside these regions. Consequently, the attention maps end up covering areas in the background in order to allow for this geometric change, however similar to CycleGAN such changes are limited due to the cyclic consistency constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Our attention map Generated Input Our attention map Generated <ref type="figure">Figure 6</ref>: A limitation of our algorithm is its lack of robustness to significant geometric changes as illustrated by the Lion → Zebra Mapping (left), and Zebra → Lion Mapping (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hyper-parameter tuning</head><p>Our algorithm is characterized by two training stages. In the first stage we train F S , F T , A S , A T and both discriminators D S and D T . In addition, the discriminators are trained with the holistic images as input. In the second stage we interrupt the training of attention networks A S and A T and train the discriminators using the foregrounds only. We apply this strategy as we noticed that when carrying the training using only the first training stage, the attention maps are also focused on the background. Such behavior is explained by different background scenes covering horse and zebra images (the first lives in green meadows, while the former lives in dry Savannah landscapes). <ref type="figure">Figure 7</ref> depicts such behavior: as the switching epoch between the first and second stage increases, more and more of the background is included in the attention maps (last columns in <ref type="figure">Figure 7)</ref>; on the other hand, if the switching point is done too early then the attention map fail to cover the entire foreground (first column in <ref type="figure">Figure 7</ref>).  <ref type="figure">Figure 7</ref>: Effect of varying the number of epochs before stopping the training of the attention networks and replacing the input to the discriminator with the foreground only.</p><p>Before feeding the foreground to the discriminator in the second stage, we threshold the attention masks to make them binary. This avoids feeding fractional values to the discriminators which stops it from learning that mid gray values in the foreground are real, making the generation process more realistic. <ref type="figure" target="#fig_3">Figure 8</ref> shows the effect of varying the threshold τ on the generated images: Low values of τ give equivalent results as the background in the learned attention images tend to be close to zero; however the higher τ gets, the less realistic are the generated images as foreground areas with lower uncertainty (e.g. due to unusual illumination or pose) are not taken into account in such scenario. This is specially the case for the mapping horse → zebra. <ref type="figure">Figure 9</ref> shows mapping results when the image requires holistic changes, here summer to winter and winter to summer. Even though our algorithm is not initially designed for such use case, we found that it is able to create attention maps focusing on the entire image. Note that since there is no clear distinction between foreground and background in this scenario, we do not apply Eq. 7 in this particular mapping. Further, this scenario required a longer training time (200 vs. 100 epochs). <ref type="figure">Figure 10</ref> shows qualitative transfer results for our ablation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional results</head><p>Figures 11 to 17 show example translation results for qualitative evaluation across six datasets, plus an example on domains which do not contain the object of interest ( <ref type="figure">Figure 17</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Our attention map Generated Input Our attention map Generated <ref type="figure">Figure 9</ref>: Even in presence of images requiring holistic changes, our algorithm is able to producing attention maps focusing on the entire image, which results on good quality generated images. Left: Summer to Winter mapping, Right: Winter to summer mapping.  <ref type="figure">Figure 10</ref>: Qualitative results for our ablation experiments. The images produced by our final formulation are sharper and more realistic compared to other approaches. Specifically, removing the cycle-consistency loss ('Ours-cycle') leads to the collapse of the GAN training, confirming its essential role in the image translation framework. By only adopting the holistic image discriminator ('Ours-D') and not stopping the training of the attention networks (Ours-D-A), we notice artifacts on the foreground and background as shown in the second and bottom row respectively. Furthermore, removing one attention network from our algorithm ('Ours-As', 'Ours-At') degrades the visibly quality of the generated images. Even though the quality decreases in this case, only the foreground in the generated images gets altered, which is an interesting observation even though one of the cycles (S → T or T → S) lack an attention estimator. Finally, reusing attention on the cycle pass of our algorithm ('Ours-cycleAtt') results on less sharp images compared to our final implementation. Figure 17: Image translation of Horse → Zebra on images without horses or zebras. By explicitly estimating attention maps, our algorithm successfully ignores irrelevant backgrounds and correctly reproduces the input images. Existing algorithms are mislead by these backgrounds and incorrectly hallucinate zebra stripe patterns.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>12: end for Output: Trained networks F * S→T , A * S and D * T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Input source images (top row) and their corresponding estimated attention maps (below). These reflect the discriminative areas between the source and target domains. The right side of the figure shows source and target attention maps, trained on horses and zebras, respectively, when applied to images without horse or zebra. The lack of attention suggests appropriate attention network behavior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Translation results. From top to bottom: Z → H, Z → H, H → Z, H → Z, A → O, O → A, L → T , and T → L. Below line: image translation in the absence of the source domain class (Z → H).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Effect of varying the threshold parameter τ . Low threshold values give similar results while higher values results on less realistic mappings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>uses Recurrent Neural Networks (RNN) to adaptively select a sequence of local</figDesc><table>s 

s' 

s'' 
 

+ 

s 

s a 
1-s a 

s f 
s' 

s b 

D T 

A S 

F ST 
A T 

F TS 

A S 

F ST 

⊙ 

⊙ 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Kernel Inception Distance×100 ± std.×100 for different image translation algorithms.</figDesc><table>Lower is better. Abbreviations: (A)pple, (O)range, (H)orse, (Z)ebra, (T )iger, (L)ion. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Kernel Inception Distance×100 ± std.×100 for ablations of our algorithm. Lower is better.</figDesc><table>Abbreviations: (H)orse, (Z)ebra. 
Algorithm 
Z → H 
H → Z 

Ours-cycle 
64.55 ± 0.34 41.48 ± 0.34 
Ours-cycleAtt 
9.46 ± 0.38 
7.79 ± 0.23 
Ours-As 
10.90 ± 0.25 
7.62 ± 0.25 
Ours-At 
9.30 ± 0.45 
7.80 ± 0.21 
Ours-D 
9.26 ± 0.22 
7.77 ± 0.35 
Ours-D-A 
9.86 ± 0.32 
8.28 ± 0.34 
Ours 
8.87 ± 0.26 
6.93 ± 0.27 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>Figure 11: Zebra → Horse translation results.Figure 12: Horse → Zebra translation results.Figure 13: Apple → Orange translation results.Figure 14: Orange → Apple translation results.Figure 15: Lion → Tiger translation results.Figure 16: Tiger → Lion translation results.</figDesc><table>Input 

Ours 
CycleGAN [1] 
RA [2] 
DiscoGAN [3] UNIT [4] 
DualGAN [5] 

Input 

Ours 
CycleGAN [1] 
RA [2] 
DiscoGAN [3] UNIT [4] 
DualGAN [5] 

Input 

Ours 
CycleGAN [1] 
RA [2] 
DiscoGAN [3] UNIT [4] 
DualGAN [5] 

Input 

Ours 
CycleGAN [1] 
RA [2] 
DiscoGAN [3] UNIT [4] 
DualGAN [5] 

Input 

Ours 
CycleGAN [1] 
RA [2] 
DiscoGAN [3] UNIT [4] 
DualGAN [5] 

Input 

Ours 
CycleGAN [1] 
RA [2] 
DiscoGAN [3] UNIT [4] 
DualGAN [5] 

Input 

Ours 
CycleGAN [1] 
RA [2] 
DiscoGAN [3] UNIT [4] 
DualGAN [5] 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Network architecture</head><p>Generators F S→T and F T →S . Our generator architecture is similar to the CycleGAN generator <ref type="bibr" target="#b0">[1]</ref>. Adopting CycleGAN's notation, "c7s1-k-R" denotes a 7×7 convolution with stride 1 and k filters, followed by a ReLU activation ('R'). "tcks2" denotes a 3×3 transpose convolution operation (sometimes called 'deconvolution') with k filters and stride 2, followed by a ReLU activation. "rk" denotes a residual block formed by two 3×3 convolutions with k filters, stride 1 and a ReLU activation. Sigmoid activation is indicated by 'S' and 'tanh' by 'T'. We apply Instance normalization after all layers apart from the last layer.</p><p>Our generator architecture is: c7s1-32-R, c3s2-64-R, c3s2-128-R, r128, r128, r128, r128, r128, r128, r128, r128, r128, tc64s2, tc32s2, c3s1-3-T.</p><p>Attention networks A S and A T . In our attention networks, we use instance normalization in all layers apart from the last layer. Further, instead of using transpose convolutions, we use nearest-neighbor upsampling layers "up2" that doubles the height and width of its input. We follow the upsampling layers with 3×3 convolutions of stride 1 with ReLU activations, apart from the last layer, which uses a sigmoid.</p><p>Our attention network architecture is: c7s1-32-R, c3s2-64-R, r64, up2, c3s1-64-R, up2, c3s1-32-R, c7s1-1-S.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">DualGAN: Unsupervised dual learning for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised diverse colorization via generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML-PKDD</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">SRPGAN: Perceptual generative adversarial network for single image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05927</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mariani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Istrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bagan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09655</idno>
		<title level="m">Data augmentation with balancing GAN</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Precomputed real-time texture synthesis with Markovian generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The dynamic representation of scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Cognition</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="17" to="42" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DA-GAN: Instance-level image translation by deep attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Wen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">PiCANet: Learning pixel-wise contextual attention for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent attentional networks for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learn to pay attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jetley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Self-attention generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">LR-GAN: Layered recursive generative adversarial networks for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention-GAN for object transfiguration in wild images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bińkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Demystifying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gans</surname></persName>
		</author>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking the Inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Klambauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
