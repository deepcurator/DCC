<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Disentangled Representations with Semi-Supervised Deep Generative Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Siddharth</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">Alan Turing Institute University of Cambridge</orgName>
								<orgName type="institution" key="instit3">Northeastern University</orgName>
								<orgName type="institution" key="instit4">University of Oxford</orgName>
								<orgName type="institution" key="instit5">Stanford University</orgName>
								<orgName type="institution" key="instit6">University of Oxford</orgName>
								<orgName type="institution" key="instit7">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooks</forename><surname>Paige</surname></persName>
							<email>bpaige@turing.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">Alan Turing Institute University of Cambridge</orgName>
								<orgName type="institution" key="instit3">Northeastern University</orgName>
								<orgName type="institution" key="instit4">University of Oxford</orgName>
								<orgName type="institution" key="instit5">Stanford University</orgName>
								<orgName type="institution" key="instit6">University of Oxford</orgName>
								<orgName type="institution" key="instit7">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Willem</forename><surname>Van De Meent</surname></persName>
							<email>j.vandemeent@northeastern.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">Alan Turing Institute University of Cambridge</orgName>
								<orgName type="institution" key="instit3">Northeastern University</orgName>
								<orgName type="institution" key="instit4">University of Oxford</orgName>
								<orgName type="institution" key="instit5">Stanford University</orgName>
								<orgName type="institution" key="instit6">University of Oxford</orgName>
								<orgName type="institution" key="instit7">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">Alan Turing Institute University of Cambridge</orgName>
								<orgName type="institution" key="instit3">Northeastern University</orgName>
								<orgName type="institution" key="instit4">University of Oxford</orgName>
								<orgName type="institution" key="instit5">Stanford University</orgName>
								<orgName type="institution" key="instit6">University of Oxford</orgName>
								<orgName type="institution" key="instit7">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
							<email>ngoodman@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">Alan Turing Institute University of Cambridge</orgName>
								<orgName type="institution" key="instit3">Northeastern University</orgName>
								<orgName type="institution" key="instit4">University of Oxford</orgName>
								<orgName type="institution" key="instit5">Stanford University</orgName>
								<orgName type="institution" key="instit6">University of Oxford</orgName>
								<orgName type="institution" key="instit7">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
							<email>pushmeet@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">Alan Turing Institute University of Cambridge</orgName>
								<orgName type="institution" key="instit3">Northeastern University</orgName>
								<orgName type="institution" key="instit4">University of Oxford</orgName>
								<orgName type="institution" key="instit5">Stanford University</orgName>
								<orgName type="institution" key="instit6">University of Oxford</orgName>
								<orgName type="institution" key="instit7">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
							<email>fwood@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">Alan Turing Institute University of Cambridge</orgName>
								<orgName type="institution" key="instit3">Northeastern University</orgName>
								<orgName type="institution" key="instit4">University of Oxford</orgName>
								<orgName type="institution" key="instit5">Stanford University</orgName>
								<orgName type="institution" key="instit6">University of Oxford</orgName>
								<orgName type="institution" key="instit7">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
							<email>philip.torr@eng.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">Alan Turing Institute University of Cambridge</orgName>
								<orgName type="institution" key="instit3">Northeastern University</orgName>
								<orgName type="institution" key="instit4">University of Oxford</orgName>
								<orgName type="institution" key="instit5">Stanford University</orgName>
								<orgName type="institution" key="instit6">University of Oxford</orgName>
								<orgName type="institution" key="instit7">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Disentangled Representations with Semi-Supervised Deep Generative Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Variational autoencoders (VAEs) learn representations of data by jointly training a probabilistic encoder and decoder network. Typically these models encode all features of the data into a single variable. Here we are interested in learning disentangled representations that encode distinct aspects of the data into separate variables. We propose to learn such representations using model architectures that generalise from standard VAEs, employing a general graphical model structure in the encoder and decoder. This allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables. We further define a general objective for semi-supervised learning in this model class, which can be approximated using an importance sampling procedure. We evaluate our framework's ability to learn disentangled representations, both by qualitative exploration of its generative capacity, and quantitative evaluation of its discriminative ability on a variety of models and datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning representations from data is one of the fundamental challenges in machine learning and artificial intelligence. Characteristics of learned representations can depend on their intended use. For the purposes of solving a single task, the primary characteristic required is suitability for that task. However, learning separate representations for each and every such task involves a large amount of wasteful repetitive effort. A representation that has some factorisable structure, and consistent semantics associated to different parts, is more likely to generalise to a new task.</p><p>Probabilistic generative models provide a general framework for learning representations: a model is specified by a joint probability distribution both over the data and over latent random variables, and a representation can be found by considering the posterior on latent variables given specific data. The learned representation -that is, inferred values of latent variables -depends then not just on the data, but also on the generative model in its choice of latent variables and the relationships between the latent variables and the data. There are two extremes of approaches to constructing generative models. At one end are fully-specified probabilistic graphical models <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref>, in which a practitioner decides on all latent variables present in the joint distribution, the relationships between them, and the functional form of the conditional distributions which define the model. At the other end are deep generative models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>, which impose very few assumptions on the structure of the model, instead employing neural networks as flexible function approximators that can be used to train a conditional distribution on the data, rather than specify it by hand.</p><p>The tradeoffs are clear. In an explicitly constructed graphical model, the structure and form of the joint distribution ensures that latent variables will have particular semantics, yielding a disentangled representation. Unfortunately, defining a good probabilistic model is hard: in complex perceptual domains such as vision, extensive feature engineering (e.g. Berant et al. <ref type="bibr" target="#b0">[1]</ref>, Siddharth et al. <ref type="bibr" target="#b30">[31]</ref>) may be necessary to define a suitable likelihood function. Deep generative models completely sidestep the difficulties of feature engineering. Although they address learning representations which then enable them to better reconstruct data, the representations themselves do not always exhibit consistent meaning along axes of variation: they produce entangled representations. While such approaches have considerable merit, particularly when faced with the absence of any side information about data, there are often situations when aspects of variation in data can be, or are desired to be characterised.</p><p>Bridging this gap is challenging. One way to enforce a disentangled representation is to hold different axes of variation fixed during training <ref type="bibr" target="#b20">[21]</ref>. Johnson et al. <ref type="bibr" target="#b13">[14]</ref> combine a neural net likelihood with a conjugate exponential family model for the latent variables. In this class of models, efficient marginalisation over the latent variables can be performed by learning a projection onto the same conjugate exponential family in the encoder. Here we propose a more general class of partiallyspecified graphical models: probabilistic graphical models in which the modeller only needs specify the exact relationship for some subset of the random variables in the model. Factors left undefined in the model definition are then learned, parametrised by flexible neural networks. This provides the ability to situate oneself at a particular point on a spectrum, by specifying precisely those axes of variations (and their dependencies) we have information about or would like to extract, and learning disentangled representations for them, while leaving the rest to be learned in an entangled manner.</p><p>A subclass of partially-specified models that is particularly common is that where we can obtain supervision data for some subset of the variables. In practice, there is often variation in the data which is (at least conceptually) easy to explain, and therefore annotate, whereas other variation is less clear. For example, consider the MNIST dataset of handwritten digits: the images vary both in terms of content (which digit is present), and style (how the digit is written), as is visible in the right-hand side of <ref type="figure" target="#fig_0">Fig. 1</ref>. Having an explicit "digit" latent variable captures a meaningful and consistent axis of variation, independent of style; using a partially-specified graphical model means we can define a "digit" variable even while leaving unspecified the semantics of the different styles, and the process of rendering a digit to an image. With unsupervised learning there is no guarantee that inference on a model with 10 classes will induce factored latent representations with factors corresponding to the the 10 digits. However, given a small amount of labelled examples, this task becomes significantly easier. Fundamentally, our approach conforms to the idea that well-defined notions of disentanglement require specification of a task under which to measure it <ref type="bibr" target="#b3">[4]</ref>. For example, when considering images of people's faces, we might wish to capture the person's identity in one context, and the lighting conditions on the faces in another, facial features in another, or combinations of these in yet other contexts. Partially-specified models and weak supervision can be seen as a way to operationalise this task-dependence directly into the learning objective.</p><p>In this paper we introduce a recipe for learning and inference in partially-specified models, a flexible framework that learns disentangled representations of data by using graphical model structures to encode constraints to interpret the data. We present this framework in the context of variational autoencoders (VAEs), developing a generalised formulation of semi-supervised learning with DGMs that enables our framework to automatically employ the correct factorisation of the objective for any given choice of model and set of latents taken to be observed. In this respect our work extends previous efforts to introduce supervision into variational autoencoders <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32]</ref>. We introduce a variational objective which is applicable to a more general class of models, allowing us to consider graphical-model structures with arbitrary dependencies between latents, continuous-domain latents, and those with dynamically changing dependencies. We provide a characterisation of how to compile partially-supervised generative models into stochastic computation graphs, suitable for end-to-end training. This approach allows us also amortise inference <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref>, simultaneously learning a network that performs approximate inference over representations at the same time we learn the unknown factors of the model itself. We demonstrate the efficacy of our framework on a variety of tasks, involving classification, regression, and predictive synthesis, including its ability to encode latents of variable dimensionality. </p><formula xml:id="formula_0">z (handwriting style) y (digit label) Disentangled Representation Stochastic Computation Graph for VAE ε z p η θ z p η θ (a) (b) (c) (d)</formula><formula xml:id="formula_1">distribution q z, y | x i . 129 L ✓, ; x i = E q (z,y|x i ) " log p ✓ x i | z, y p(z, y) q (z, y | x i ) # .<label>(2)</label></formula><p>By contrast, in the fully supervised setting the values y are treated as observed and become fixed 130 inputs into the computation graph, instead of being sampled from q . When the label y is observed 131 along with the data, for fixed (x i , y i ) pairs, the lower bound on the conditional log-marginal likelihood</p><formula xml:id="formula_2">132 log p ✓ (x | y) is 133 L x|y ✓, z ; x i , y i = E q z (z|x i ,y i ) " log p ✓ x i | z, y i p z | y i q z (z | x i , y i ) # .<label>(3)</label></formula><p>This quantity can be optimized directly to learn model parameters ✓ and z simultaneously via SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>134</head><p>However, it does not contain the encoder parameters y . This difficulty was also encountered in a 135 related context by Kingma et al. <ref type="bibr" target="#b16">[17]</ref>. Their solution was to augment the loss function by including 136 an explicit additional term for learning a classifier directly on the supervised points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>137</head><p>Here we propose an alternative approach. We extend the model with an auxiliary variableỹ with</p><formula xml:id="formula_3">138 likelihood p(ỹ | y) = ỹ (y) to define densities 139 p(ỹ, y, z, x) = p(ỹ | y)p ✓ (x | y, z)p(y, z) q(ỹ, y, z | x) = p(ỹ | y)q(y, z | x)</formula><p>. When we marginalize the ELBO for this model overỹ, we recover the expression in Equation (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>140</head><p>Treatingỹ = y i as observed results in the supervised objective</p><formula xml:id="formula_4">141 L ✓, ; x i ỹ=y i = E q (z,y|x i ) " y i (y) log p ✓ x i | z, y p(z, y) q (z, y | x i ) # .<label>(4)</label></formula><p>Integration over an observed y is then replaced with evaluation of the ELBO and the density q y at 142 y i . A Monte Carlo estimator of Equation <ref type="formula" target="#formula_4">(4)</ref> can be constructed automatically for any factorization 143 of q by sampling latent variables z and weighting the resulting ELBO estimate by the conditional 144 density terms q y (y|·).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>145</head><p>Note that the exact functional form of the Monte Carlo estimator will vary depending on the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Framework and Formulation</head><p>VAEs <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref> are a class of deep generative models that simultaneously train both a probabilistic encoder and decoder for a elements of a data set</p><formula xml:id="formula_5">D = {x 1 , . . . x N }.</formula><p>The central analogy is that an encoding z can be considered a latent variable, casting the decoder as a conditional probability density p θ (x|z). The parameters η θ (z) of this distribution are the output of a deterministic neural network with parameters θ (most commonly MLPs or CNNs) which takes z as input. By placing a weak prior over z, the decoder defines a posterior and joint distribution</p><formula xml:id="formula_6">p θ (z | x) ∝ p θ (x | z)p(z). x n z n θ φ N</formula><p>Inference in VAEs can be performed using a variational method that approximates the posterior distribution p θ (z | x) using an encoder q φ (z | x), whose parameters λ φ (x) are the output of a network (with parameters φ) that is referred to as an "inference network" or a "recognition network". The generative and inference networks, denoted by solid and dashed lines respectively in the graphical model, are trained jointly by performing stochastic gradient ascent on the evidence lower bound</p><formula xml:id="formula_7">(ELBO) L(φ, θ; D) ≤ log p θ (D), L(φ, θ; D) = N n=1 L(φ, θ; x n ) = N n=1 E q φ (z|x n ) [log p θ (x n | z) + log p(z) − log q φ (z|x n )]. (1)</formula><p>Typically, the first term</p><formula xml:id="formula_8">E q φ (z|x n ) [log p θ (x n | z)]</formula><p>is approximated by a Monte Carlo estimate and the remaining two terms are expressed as a divergence −KL(q φ (z|x n ) p(z)), which can be computed analytically when the encoder model and prior are Gaussian.</p><p>In this paper, we will consider models in which both the generative model p θ (x, y, z) and the approximate posterior q φ (y, z | x) can have arbitrary conditional dependency structures involving random variables defined over a number of different distribution types. We are interested in defining VAE architectures in which a subset of variables y are interpretable. For these variables, we assume that supervision labels are available for some fraction of the data. The VAE will additionally retain some set of variables z for which inference is performed in a fully unsupervised manner. This is in keeping with our central goal of defining and learning in partially-specified models. In the running example for MNIST, y corresponds to the classification label, whereas z captures all other implicit features, such as the pen type and handwriting style.</p><p>This class of models is more general than the models in the work by Kingma et al. <ref type="bibr" target="#b17">[18]</ref>, who consider three model designs with a specific conditional dependence structure. We also do not require p(y, z) to be a conjugate exponential family model, as in the work by Johnson et al. <ref type="bibr" target="#b14">[15]</ref>. To perform semi-supervised learning in this class of models, we need to i) define an objective that is suitable to general dependency graphs, and ii) define a method for constructing a stochastic computation graph <ref type="bibr" target="#b29">[30]</ref> that incorporates both the conditional dependence structure in the generative model and that of the recognition model into this objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Objective Function</head><formula xml:id="formula_9">x n y n z n φ θ x m y m z m N M</formula><p>Previous work on semi-supervised learning for deep generative models <ref type="bibr" target="#b17">[18]</ref> defines an objective over N unsupervised data points</p><formula xml:id="formula_10">D = {x 1 , . . . , x N } and M supervised data points D sup = {(x 1 , y 1 ), . . . , (x M , y M )}, L(θ, φ; D, D sup ) = N n=1 L(θ, φ; x n ) + γ M m=1 L sup (θ, φ; x m , y m ).<label>(2)</label></formula><p>Our model's joint distribution factorises into unsupervised and supervised collections of terms over D and D sup as shown in the graphical model. The standard variational bound on the joint evidence of all observed data (including supervision) also factorises as shown in Eq. <ref type="bibr" target="#b1">(2)</ref>. As the factor corresponding to the unsupervised part of the graphical model is exactly that as Eq. <ref type="formula">(1)</ref>, we focus on the supervised term in Eq. (2), expanded below, incorporating an additional weighted component as in Kingma et al. <ref type="bibr" target="#b17">[18]</ref>.</p><formula xml:id="formula_11">L sup (θ, φ; x m , y m ) = E q φ (z|x m ,y m ) log p θ (x m , y m , z) q φ (z | x m , y m ) + α log q φ (y m | x m ).<label>(3)</label></formula><p>Note that the formulation in Eq. <ref type="formula" target="#formula_1">(2)</ref> introduces an constant γ that controls the relative strength of the supervised term. While the joint distribution in our model implicitly weights the two terms, in situations where the relative sizes of D and D sup are vastly different, having control over the relative weights of the terms can help ameliorate such discrepancies.</p><p>This definition in Eq. (3) implicitly assumes that we can evaluate the conditional probability q φ (z|x, y) and the marginal q φ (y|x) = dz q φ (y, z|x). This was indeed the case for the models considered by Kingma et al. <ref type="bibr" target="#b17">[18]</ref>, which have a factorisation q φ (y, z|x) = q φ (z|x, y)q φ (y|x).</p><p>Here we will derive an estimator for L sup that generalises to models in which q φ (y, z | x) can have an arbitrary conditional dependence structure. For purposes of exposition, we will for the moment consider the case where</p><formula xml:id="formula_12">q φ (y, z | x) = q φ (y | x, z)q φ (z | x). For this factorisation, generating samples z m,s ∼ q φ (z | x m , y m )</formula><p>requires inference, which means we can no longer compute a simple Monte Carlo estimator by sampling from the unconditioned distribution q φ (z | x m ). Moreover, we also cannot evaluate the density q φ (z | x m , y m ).</p><p>In order to address these difficulties, we re-express the supervised terms in the objective as</p><formula xml:id="formula_13">L sup (θ, φ; x m , y m ) = E q φ (z|x m ,y m ) log p(x m , y m , z) q φ (y m , z | x m ) + (1 + α) log q φ (y m | x m ),<label>(4)</label></formula><p>which removes the need to evaluate q φ (z | x m , y m ). We can then use (self-normalised) importance sampling to approximate the expectation. To do so, we sample proposals z m,s ∼ q φ (z | x m ) from the unconditioned encoder distribution, and define the estimator</p><formula xml:id="formula_14">E q φ (z|x m ,y m ) log p θ (x m , y m , z) q φ (y m , z | x m ) 1 S S s=1 w m,s Z m log p θ (x m , y m , z m,s ) q φ (y m , z m,s | x m ) ,<label>(5)</label></formula><p>where the unnormalised importance weights w </p><formula xml:id="formula_15">w m,s := q φ (y m , z m,s | x m ) q φ (z m,s | x m ) , Z m = 1 S S s=1 w m,s .<label>(6)</label></formula><p>To approximate log q φ (y m | x m ), we use a Monte Carlo estimator of the lower bound that is normally used in maximum likelihood estimation,</p><formula xml:id="formula_16">log q φ (y m | x m ) ≥ E q φ (z|x m ) log q φ (y m , z | x m ) q φ (z | x m ) 1 S S s=1 log w m,s ,<label>(7)</label></formula><p>using the same samples z  <ref type="formula" target="#formula_16">(7)</ref>, we obtain the estimator</p><formula xml:id="formula_17">L sup (θ, φ; x m , y m ) := 1 S S s=1 w m,s Z m log p θ (x m , y m , z m,s ) q φ (y m , z m,s | x m ) + (1 + α) log w m,s .<label>(8)</label></formula><p>We note that this estimator applies to any conditional dependence structure. Suppose that we were to define an encoder q φ (z 2 , y 1 , </p><formula xml:id="formula_18">z 1 | x) with factorisation q φ (z 2 | y 1 , z 1 , x)q φ (y 1 | z 1 , x)q φ (z 1 | x). If we propose z 2 ∼ q φ (z 2 | y 1 , z 1 , x) and z 1 ∼ q φ (z 1 | x),</formula><formula xml:id="formula_19">w m,s := q φ (z m,s 2 , y m 1 , z m,s 1 | x m ) q φ (z m,s 2 | y m 1 , z m,s 1 , x m )q φ (z m,s 1 | x m ) = q φ (y m 1 | z m,s 1 , x m ).</formula><p>In general, the importance weights are simply the product of conditional probabilities of the supervised variables y in the model. Note that this also applies to the models in Kingma et al. <ref type="bibr" target="#b17">[18]</ref>, whose objective we can recover by taking the weights to be constants w m,s = q φ (y m | x m ).</p><p>We can also define an objective analogous to the one used in importance-weighted autoencoders <ref type="bibr" target="#b1">[2]</ref>, in which we compute the logarithm of a Monte Carlo estimate, rather than the Monte Carlo estimate of a logarithm. This objective takes the form</p><formula xml:id="formula_20">L sup,iw (θ, φ; x m , y m ) := log 1 S S s=1 p θ (x m , y m , z m,s ) q φ (z m,s | x m ) + α log 1 S S s=1 w m,s ,<label>(9)</label></formula><p>which can be derived by moving the sums in Eq. <ref type="formula" target="#formula_17">(8)</ref> into the logarithms and applying the substitution</p><formula xml:id="formula_21">w m,s /q φ (y m , z m,s | x m ) = 1/q φ (z m,s | x m ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Construction of the Stochastic Computation Graph</head><p>To perform gradient ascent on the objective in Eq. <ref type="formula" target="#formula_17">(8)</ref>, we map the graphical models for p θ (x, y, z) and q φ (y, z|x) onto a stochastic computation graph in which each stochastic node forms a sub-graph. <ref type="figure" target="#fig_0">Figure 1</ref> shows this expansion for the simple VAE for MNIST digits from <ref type="bibr" target="#b16">[17]</ref>. In this model, y is a discrete variable that represents the underlying digit, our latent variable of interest, for which we have partial supervision data. An unobserved Gaussian-distributed variable z captures the remainder of the latent information. This includes features such as the hand-writing style and stroke thickness. In the generative model ( <ref type="figure" target="#fig_0">Fig. 1 top-left)</ref>, we assume a factorisation p θ (x, y, z) = p θ (x | y, z)p(y)p(z) in which y and z are independent under the prior. In the recognition model ( <ref type="figure" target="#fig_0">Fig. 1 bottom-left)</ref>, we use a conditional dependency structure q φ (y, z | x) = q φz (z | y, x)q φy (y|x) to disentangle the digit label y from the handwriting style z <ref type="figure" target="#fig_0">(Fig. 1 right)</ref>.</p><p>The generative and recognition model are jointly form a stochastic computation graph ( <ref type="figure" target="#fig_0">Fig. 1 centre)</ref> containing a sub-graph for each stochastic variable. These can correspond to fully supervised, partially supervised and unsupervised variables. This example graph contains three types of subgraphs, corresponding to the three possibilities for supervision and gradient estimation:</p><p>• For the fully supervised variable x, we compute the likelihood p under the generative model, that is p θ (x | y, z) = N (x ; η θ (y, z)). Here η θ (y, z) is a neural net with parameters θ that returns the parameters of a normal distribution (i.e. a mean vector and a diagonal covariance).</p><p>• For the unobserved variable z, we compute both the prior probability p(z) = N (z ; η z ), and the conditional probability q φ (z | x, y) = N (z ; λ φz (x, y)). Here the usual reparametrisation is used to sample z from q φ (z | x, y) by first sampling ∼ N (0, I) using the usual reparametrisation trick z = g( , λ φ (x, y)).</p><p>• For the partially observed variable y, we also compute probabilities p(y) = Discrete(y; η y ) and q φy (y|x) = Discrete(y; λ φz (x)). The value y is treated as observed when available, and sampled otherwise. In this particular example, we sample y from a q φy (y|x) using a Gumbel-softmax <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> relaxation of the discrete distribution.</p><p>The example in <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates a general framework for defining VAEs with arbitrary dependency structures. We begin by defining a node for each random variable. For each node we then specify a distribution type and parameter function η, which determines how the probability under the generative model depends on the other variables in the network. This function can be a constant, fully deterministic, or a neural network whose parameters are learned from the data. For each unsupervised and semi-supervised variable we must additionally specify a function λ that returns the parameter values in the recognition model, along with a (reparametrised) sampling procedure.</p><p>Given this specification of a computation graph, we can now compute the importance sampling estimate in Eq. <ref type="formula" target="#formula_17">(8)</ref> by simply running the network forward repeatedly to obtain samples from q φ (·|λ) for all unobserved variables. We then calculate p θ (x, y, z), q φ (y|x), q φ (y, z|x), and the importance weight w, which is the joint probability of all semi-supervised variable for which labels are available. This estimate can then be optimised with respect to the variables θ and φ to train the autoencoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We evaluate our framework along a number of different axes pertaining to its ability to learn disentangled representations through the provision of partial graphical-model structures for the latents and weak supervision. In particular, we evaluate its ability to (i) function as a classifier/regressor for particular latents under the given dataset, (ii) learn the generative model in a manner that preserves the semantics of the latents with respect to the data generated, and (iii) perform these tasks, in a flexible manner, for a variety of different models and data.</p><p>For all the experiments run, we choose architecture and parameters that are considered standard for the type and size of the respective datasets. Where images are concerned (with the exception of MNIST), we employ (de)convolutional architectures, and employ a standard GRU recurrence in the Multi-MNIST case. For learning, we used AdaM <ref type="bibr" target="#b15">[16]</ref> with a learning rate and momentumcorrection terms set to their default values. As for the mini batch sizes, they varied from 100-700 depending on the dataset being used and the sizes of the labelled subset D sup . All of the above, including further details of precise parameter values and the source code, including our PyTorchbased library for specifying arbitrary graphical models in the VAE framework, is available athttps://github.com/probtorch/probtorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MNIST and SVHN</head><p>We begin with an experiment involving a simple dependency structure, in fact the very same as that in Kingma et al. <ref type="bibr" target="#b17">[18]</ref>, to validate the performance of our importance-sampled objective in the special case where the recognition network and generative models factorise as indicated in <ref type="figure" target="#fig_0">Fig. 1(left)</ref>, giving us importance weights that are constant w m,s = q φ (y m |x m ). The model is tested on it's ability to classify digits and perform conditional generation on the MNIST and Google Street-View House Numbers (SVHN) datasets. As <ref type="figure" target="#fig_0">Fig. 1(left)</ref> shows, the generative and recognition models have the "digit" label, denoted y, partially specified (and partially supervised) and the "style" factor, denoted z, assumed to be an unobserved (and unsupervised) variable. <ref type="figure" target="#fig_3">Figure 2</ref>(a) and (c) illustrate the conditional generation capabilities of the learned model, where we show the effect of first transforming a given input (leftmost column) into the disentangled latent space, and with the style latent variable fixed, manipulating the digit through the generative model to generate data with expected visual characteristics. Note that both these results were obtained with partial supervision -100 (out of 50000) labelled data points in the case of MNIST and 1000 (out of 70000) labelled data points in the case of SVHN. The style latent variable z was taken to be a diagonal-covariance Gaussian of 10 and 15 dimensions respectively. <ref type="figure" target="#fig_3">Figure 2(d)</ref> shows the same for SVHN with full supervision. <ref type="figure" target="#fig_3">Figure 2(b)</ref> illustrates the alternate mode of conditional generation, where the style latent, here taken to be a 2D Gaussian, is varied with the digit held fixed.</p><p>Next, we evaluate our model's ability to effectively learn a classifier from partial supervision. We compute the classification error on the label-prediction task on both datasets, and the results are reported in the table in <ref type="figure" target="#fig_4">Fig. 3</ref>. Note that there are a few minor points of difference in the setup between our method and those we compare against <ref type="bibr" target="#b17">[18]</ref>. We always run our models directly on the data, with no pre-processing or pre-learning on the data. Thus, for MNIST, we compare against model M2 from the baseline which does just the same. However, for SVHN, the baseline method does not report errors for the M2 model; only the two-stage M1+M2 model which involves a separate feature-extraction step on the data before learning a semi-supervised classifier.</p><p>As the results indicate, our model and objective does indeed perform on par with the setup considered in Kingma et al. <ref type="bibr" target="#b17">[18]</ref>, serving as basic validation of our framework. We note however, that from the perspective of achieving the lowest possible classification error, one could adopt any number of alternate factorisations <ref type="bibr" target="#b23">[24]</ref> and innovations in neural-network architectures <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Supervision rate: As discussed in Section 2.1, we formulate our objective to provide a handle on the relative weight between the supervised and unsupervised terms. For a given unsupervised set size N , supervised set size M , and scaling term γ, the relative weight is ρ = γM/(N + γM ). <ref type="figure" target="#fig_4">Figure 3</ref> shows exploration of this relative weight parameter over the MNIST and SVHN datasets and over different supervised set sizes M . Each line in the graph measures the classification error for a given M , over ρ, starting at γ = 1, i.e. ρ = M/(N + M ). In line with Kingma et al. <ref type="bibr" target="#b17">[18]</ref>, we use α = 0.1/ρ. When the labelled data is very sparse (M N ), over-representing the labelled examples during training can help aid generalisation by improving performance on the test data. In our experiments, for the most part, choosing this factor to be ρ = M/(N + M ) provides good results. However, as is to be expected, over-fitting occurs when ρ is increased beyond a certain point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Intrinsic Faces</head><p>We next move to a more complex domain involving generative models of faces. As can be seen in the graphical models for this experiment in <ref type="figure">Fig. 4</ref>, the dependency structures employed here are more complex in comparison to those from the previous experiment. Here, we use the "Yale B" dataset <ref type="bibr" target="#b5">[6]</ref> as processed by Jampani et al. <ref type="bibr" target="#b11">[12]</ref> for the results in <ref type="figure">Fig. 5</ref>. We are interested in showing that our model can learn disentangled representations of identity and lighting and evaluate it's performance on the tasks of (i) classification of person identity, and (ii) regression for lighting direction.</p><p>Note that our generative model assumes no special structure -we simply specify a model where all latent variables are independent under the prior. Previous work <ref type="bibr" target="#b11">[12]</ref> assumed a generative model with latent variables identity i, lighting l, shading s, and reflectance r, following the relationship (n · l) × r + for the pixel data. Here, we wish to demonstrate that our generative model still learns the correct relationship over these latent variables, by virtue of the structure in the recognition model and given (partial) supervision.</p><p>Note that in the recognition model <ref type="figure">(Fig. 4)</ref>, the lighting l is a latent variable with continuous domain, and one that we partially supervise. Further, we encode identity i as a categorical random variable, instead of constructing a pixel-wise surface-normal map (each assumed to be independent Gaussian) as is customary. This formulation allows us to address the task of predicting identity directly, instead of applying surrogate evaluation methods (e.g. nearest-neighbour classification based on inferred reflectance). <ref type="figure">Figure 5</ref> presents both qualitative and quantitative evaluation of the framework to jointly learn both the structured recognition model, and the generative model parameters.</p><formula xml:id="formula_22">Intrinsic Faces Multi-MNIST x i s r x i r s x a k x k z k y k K K x K a k x k hk z k y k hk−1 K Generative Model Recognition Model Generative Model Recognition Model</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-MNIST</head><p>Finally, we conduct an experiment that extends the complexity from the prior models even further. Particularly, we explore the capacity of our framework to handle models with stochastic dimensionality -having the number of latent variables itself determined by a random variable, and models that can be composed of other smaller (sub-)models. We conduct this experiment in the domain of multi-MNIST. This is an apposite choice as it satisfies both the requirements above -each image can have a varying number of individual digits, which essentially dictates that the model must learn to count, and as each image is itself composed of (scaled and translated) exemplars from the MNIST data, we can employ the MNIST model itself within the multi-MNIST model.</p><p>The model structure that we assume for the generative and recognition networks is shown in <ref type="figure">Fig. 4</ref>. We extend the models from the MNIST experiment by composing it with a stochastic sequence generator, in which the loop length K is a random variable. For each loop iteration k = 1, . . . , K, the generative model iteratively samples a digit y k , style z k , and uses these to generate a digit image x k in the same manner as in the earlier MNIST example. Additionally, an affine tranformation is also sampled for each digit in each iteration to transform the digit images x k into a common, combined canvas that represents the final generated image x, using a spatial transformer network <ref type="bibr" target="#b10">[11]</ref>.</p><p>In the recognition model, we predict the number of digits K from the pixels in the image. For each loop iteration k = 1, . . . , K, we define a Bernoulli-distributed digit image x k . When supervision is available, we compute the probability of x k from the binary cross-entropy in the same manner as in the likelihood term for the MNIST model. When no supervision is available, we deterministically set x k to the mean of the distribution. This can be seen akin to providing bounding-boxes around the constituent digits as supervision, which must be taken into account when learning the affine transformations that decompose a multi-MNIST image into its constituent MNIST-like images. This model design is similar to the one used in DRAW <ref type="bibr" target="#b9">[10]</ref>, recurrent VAEs <ref type="bibr" target="#b2">[3]</ref>, and AIR <ref type="bibr" target="#b4">[5]</ref>. In the absence of a canonical multi-MNIST dataset, we created our own from the MNIST dataset by manipulating the scale and positioning of the standard digits into a combined canvas, evenly balanced across the counts (1-3) and digits. We then conducted two experiments within this domain. In the first experiment, we seek to measure how well the stochastic sequence generator learns to count on its own, with no heed paid to disentangling the latent representations for the underlying digits.</p><p>Here, the generative model presumes the availability of individual MNIST-digit images, generating combinations under sampled affine transformations. In the second experiment, we extend the above model to now also incorporate the same pre-trained MNIST model from the previous section, which allows the generative model to sample MNIST-digit images, while also being able to predict the underlying digits. This also demonstrates how we can leverage compositionality of models: when a complex model has a known simpler model as a substructure, the simpler model and its learned weights can be dropped in directly.</p><p>The count accuracy errors across different supervised set sizes, reconstructions for a random set of inputs, and the decomposition of a given set of inputs into their constituent individual digits, are shown in <ref type="figure" target="#fig_6">Fig. 6</ref>. All reconstructions and image decompositions shown correspond to the nested-model configuration. We observe that not only are we able to reliably infer the counts of the digits in the given images, we are able to simultaneously reconstruct the inputs as well as its constituent parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>In this paper we introduce a framework for learning disentangled representations of data using partially-specified graphical model structures and semi-supervised learning schemes in the domain of variational autoencoders (VAEs). This is accomplished by defining hybrid generative models which incorporate both structured graphical models and unstructured random variables in the same latent space. We demonstrate the flexibility of this approach by applying it to a variety of different tasks in the visual domain, and evaluate its efficacy at learning disentangled representations in a semisupervised manner, showing strong performance. Such partially-specified models yield recognition networks that make predictions in an interpretable and disentangled space, constrained by the structure provided by the graphical model and the weak supervision.</p><p>The framework is implemented as a PyTorch library <ref type="bibr" target="#b25">[26]</ref>, enabling the construction of stochastic computation graphs which encode the requisite structure and computation. This provides another direction to explore in the future -the extension of the stochastic computation graph framework to probabilistic programming <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. Probabilistic programs go beyond the presented framework to permit more expressive models, incorporating recursive structures and higher-order functions. The combination of such frameworks with neural networks has recently been studied in Le et al. <ref type="bibr" target="#b22">[23]</ref> and Ritchie et al. <ref type="bibr" target="#b28">[29]</ref>, indicating a promising avenue for further exploration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Semi-supervised learning in structured variational autoencoders, illustrated on MNIST digits. Top-Left: Generative model. Bottom-Left: Recognition model. Middle: Stochastic computation graph, showing expansion of each node to its corresponding sub-graph. Generative-model dependencies are shown in blue and recognition-model dependencies are shown in orange. See Section 2.2 for a detailed explanation. Right: learned representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Eq. (5). When we combine the terms in Eqs. (5) and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Visual analogies for the MNIST data, partially supervised with just 100 labels (out of 50000). We infer the style variable z and then vary the label y. (b) Exploration in style space with label y held fixed and (2D) style z varied. Visual analogies for the SVHN data when (c) partially supervised with just 1000 labels, and (d) fully supervised.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Right: Classification error rates for different labelled-set sizes M over multiple runs, with supervision rate ρ = γM N +γM , γ = 1. For SVHN, we compare against a multi-stage process (M1+M2) [18], where our model only uses a single stage. Left: Classification error over different labelled set sizes and supervision rates for MNIST (top) and SVHN (bottom). Here, scaling of the classification objective is held fixed at α = 50 (MNIST) and α = 70 (SVHN). Note that for sparsely labelled data (M N ), a modicum of over-representation (γ &gt; 1) helps improve generalisation with better performance on the test set. Conversely, too much over-representation leads to overfitting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Generative and recognition models for the intrinsic-faces and multi-MNIST experiments. Input Recon. Varying Identity</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Left: Example input multi-MNIST images and reconstructions. Top-Right: Decomposition of Multi-MNIST images into constituent MNIST digits. Bottom-Right: Count accuracy over different supervised set sizes M for given dataset size M + N = 82000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>then the importance weights w</figDesc><table>m,s 

for 
the estimator in Eq. (8) are defined as 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling biological processes for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Vander Linden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brittany</forename><surname>Harding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00519</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">Importance weighted autoencoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A framework for the quantitaive evaluation of disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams Cian Eastwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Learning Disentangled Representaions: from Perception to Control</title>
		<meeting>the Workshop on Learning Disentangled Representaions: from Perception to Control</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Attend, infer, repeat: Fast scene understanding with generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theophane</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08575</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From few to many: Illumination cone models for face recognition under variable lighting and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Georghiades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="643" to="660" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Amortized inference in probabilistic reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CogSci</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Church: A language for generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nd Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vk Mansinghka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="220" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1462" to="1471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Consensus message passing for layered graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Varun Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Winn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="425" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Composing graphical models with neural networks for structured representations and fast inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiltschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sandeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Datta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2946" to="2954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Composing graphical models with neural networks for structured representations and fast inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">B</forename><surname>Wiltschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sandeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Learning Representations</title>
		<meeting>the 2nd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semisupervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Picture: A probabilistic programming language for scene perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tejas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikash</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mansinghka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4390" to="4399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep convolutional inverse graphics network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tejas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2530" to="2538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Local computations with probabilities on graphical structures and their application to expert systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Steffen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Lauritzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spiegelhalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="157" to="224" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Inference compilation and universal probabilistic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atilim</forename><surname>Tuan Anh Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Gunes Baydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wood</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09900</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.05473</idno>
		<title level="m">Auxiliary deep generative models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00712</idno>
		<title level="m">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<ptr target="http://pytorch.org" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2017" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raiko</forename><forename type="middle">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3532" to="3540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 31st International Conference on Machine Learning</title>
		<meeting>The 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep amortized inference for probabilistic programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Horsfall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah D</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05735</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gradient estimation using stochastic computation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theophane</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3510" to="3522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Seeing what you&apos;re told: Sentence-guided activity recognition in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="732" to="771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3465" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ladder variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning stochastic inverses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stuhlmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3048" to="3056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lightweight implementations of probabilistic programming languages via transformational compilation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wingate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stuhlmueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A new approach to probabilistic programming inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Willem Van De Meent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikash</forename><surname>Mansinghka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1024" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
