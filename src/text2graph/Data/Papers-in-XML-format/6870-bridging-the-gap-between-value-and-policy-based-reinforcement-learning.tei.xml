<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bridging the Gap Between Value and Policy Based Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dale Schuurmans</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dale Schuurmans</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
							<email>kelvinxx@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Dale Schuurmans</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dale Schuurmans</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bridging the Gap Between Value and Policy Based Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We establish a new connection between value and policy based reinforcement learning (RL) based on a relationship between softmax temporal value consistency and policy optimality under entropy regularization. Specifically, we show that softmax consistent action values correspond to optimal entropy regularized policy probabilities along any action sequence, regardless of provenance. From this observation, we develop a new RL algorithm, Path Consistency Learning (PCL), that minimizes a notion of soft consistency error along multi-step action sequences extracted from both on-and off-policy traces. We examine the behavior of PCL in different scenarios and show that PCL can be interpreted as generalizing both actor-critic and Q-learning algorithms. We subsequently deepen the relationship by showing how a single model can be used to represent both a policy and the corresponding softmax state values, eliminating the need for a separate critic. The experimental evaluation demonstrates that PCL significantly outperforms strong actor-critic and Q-learning baselines across several benchmarks. </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>By contrast, value based methods, such as Q-learning <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b20">21]</ref>, can learn from any trajectory sampled from the same environment. Such "off-policy" methods are able to exploit data from other sources, such as experts, making them inherently more sample efficient than on-policy methods <ref type="bibr" target="#b9">[10]</ref>. Their key drawback is that off-policy learning does not stably interact with function approximation <ref type="bibr" target="#b34">[35,</ref><ref type="bibr">Chap.11]</ref>. The practical consequence is that extensive hyperparameter tuning can be required to obtain stable behavior. Despite practical success <ref type="bibr" target="#b21">[22]</ref>, there is also little theoretical understanding of how deep Q-learning might obtain near-optimal objective values.</p><p>Ideally, one would like to combine the unbiasedness and stability of on-policy training with the data efficiency of off-policy approaches. This desire has motivated substantial recent work on off-policy actor-critic methods, where the data efficiency of policy gradient is improved by training an offpolicy critic <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b9">10]</ref>. Although such methods have demonstrated improvements over on-policy actor-critic approaches, they have not resolved the theoretical difficulty associated with off-policy learning under function approximation. Hence, current methods remain potentially unstable and require specialized algorithmic and theoretical development as well as delicate tuning to be effective in practice <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>In this paper, we exploit a relationship between policy optimization under entropy regularization and softmax value consistency to obtain a new form of stable off-policy learning. Even though entropy regularized policy optimization is a well studied topic in RL <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>-in fact, one that has been attracting renewed interest from concurrent work <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b10">11]</ref>-we contribute new observations to this study that are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Notation &amp; Background</head><p>We model an agent's behavior by a parametric distribution π θ (a | s) defined by a neural network over a finite set of actions. At iteration t, the agent encounters a state s t and performs an action a t sampled from π θ (a | s t ). The environment then returns a scalar reward r t and transitions to the next state s t+1 .</p><p>Note: Our main results identify specific properties that hold for arbitrary action sequences. To keep the presentation clear and focus attention on the key properties, we provide a simplified presentation in the main body of this paper by assuming deterministic state dynamics. This restriction is not necessary, and in the Supplementary Material we provide a full treatment of the same concepts generalized to stochastic state dynamics. All of the desired properties continue to hold in the general case and the algorithms proposed remain unaffected.</p><p>For simplicity, we assume the per-step reward r t and the next state s t+1 are given by functions r t = r(s t , a t ) and s t+1 = f (s t , a t ) specified by the environment. We begin the formulation by reviewing the key elements of Q-learning <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>, which uses a notion of hard-max Bellman backup to enable off-policy TD control. First, observe that the expected discounted reward objective, O ER (s, π), can be recursively expressed as,</p><formula xml:id="formula_0">O ER (s, π) = a π(a | s) [r(s, a) + γO ER (s , π)] ,</formula><p>where s = f (s, a) .</p><p>Let V • (s) denote the optimal state value at a state s given by the maximum value of O ER (s, π) over policies, i.e., V</p><p>• (s) = max π O ER (s, π). Accordingly, let π • denote the optimal policy that results in V</p><p>• (s) (for simplicity, assume there is one unique optimal policy), i.e., π • = argmax π O ER (s, π). Such an optimal policy is a one-hot distribution that assigns a probability of 1 to an action with maximal return and 0 elsewhere. Thus we have</p><formula xml:id="formula_2">V • (s) = O ER (s, π • ) = max a (r(s, a) + γV • (s )).<label>(2)</label></formula><p>This is the well-known hard-max Bellman temporal consistency. Instead of state values, one can equivalently (and more commonly) express this consistency in terms of optimal action values, Q • :</p><formula xml:id="formula_3">Q • (s, a) = r(s, a) + γ max a Q • (s , a ) .<label>(3)</label></formula><p>Q-learning relies on a value iteration algorithm based on (3), where Q(s, a) is bootstrapped based on successor action values Q(s , a ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Softmax Temporal Consistency</head><p>In this paper, we study the optimal state and action values for a softmax form of temporal consistency <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b6">7]</ref>, which arises by augmenting the standard expected reward objective with a discounted entropy regularizer. Entropy regularization <ref type="bibr" target="#b45">[46]</ref> encourages exploration and helps prevent early convergence to sub-optimal policies, as has been confirmed in practice (e.g., <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref>). In this case, one can express regularized expected reward as a sum of the expected reward and a discounted entropy term,</p><formula xml:id="formula_4">O ENT (s, π) = O ER (s, π) + τ H(s, π) ,<label>(4)</label></formula><p>where τ ≥ 0 is a user-specified temperature parameter that controls the degree of entropy regularization, and the discounted entropy H(s, π) is recursively defined as</p><formula xml:id="formula_5">H(s, π) = a π(a | s) [− log π(a | s) + γ H(s , π)] .<label>(5)</label></formula><p>The objective O ENT (s, π) can then be re-expressed recursively as,</p><formula xml:id="formula_6">O ENT (s, π) = a π(a | s) [r(s, a) − τ log π(a | s) + γO ENT (s , π)] .<label>(6)</label></formula><p>Note that when γ = 1 this is equivalent to the entropy regularized objective proposed in <ref type="bibr" target="#b45">[46]</ref>.</p><p>Let V * (s) = max π O ENT (s, π) denote the soft optimal state value at a state s and let π * (a | s) denote the optimal policy at s that attains the maximum of O ENT (s, π). When τ &gt; 0, the optimal policy is no longer a one-hot distribution, since the entropy term prefers the use of policies with more uncertainty. We characterize the optimal policy π * (a | s) in terms of the O ENT -optimal state values of successor states V * (s ) as a Boltzmann distribution of the form,</p><formula xml:id="formula_7">π * (a | s) ∝ exp{(r(s, a) + γV * (s ))/τ } .<label>(7)</label></formula><p>It can be verified that this is the solution by noting that the O ENT (s, π) objective is simply a τ -scaled constant-shifted KL-divergence between π and π * , hence the optimum is achieved when π = π * .</p><p>To derive V * (s) in terms of V * (s ), the policy π * (a | s) can be substituted into (6), which after some manipulation yields the intuitive definition of optimal state value in terms of a softmax (i.e., log-sum-exp) backup,</p><formula xml:id="formula_8">V * (s) = O ENT (s, π * ) = τ log a exp{(r(s, a) + γV * (s ))/τ } .<label>(8)</label></formula><p>Note that in the τ → 0 limit one recovers the hard-max state values defined in (2). Therefore we can equivalently state softmax temporal consistency in terms of optimal action values Q * (s, a) as,</p><formula xml:id="formula_9">Q * (s, a) = r(s, a) + γV * (s ) = r(s, a) + γτ log a exp(Q * (s , a )/τ ) .<label>(9)</label></formula><p>Now, much like Q-learning, the consistency equation <ref type="formula" target="#formula_9">(9)</ref> can be used to perform one-step backups to asynchronously bootstrap Q * (s, a) based on Q * (s , a ). In the Supplementary Material we prove that such a procedure, in the tabular case, converges to a unique fixed point representing the optimal values.</p><p>We point out that the notion of softmax Q-values has been studied in previous work (e.g., <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7]</ref>). Concurrently to our work, <ref type="bibr" target="#b10">[11]</ref> has also proposed a soft Q-learning algorithm for continuous control that is based on a similar notion of softmax temporal consistency. However, we contribute new observations below that lead to the novel training principles we explore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Consistency Between Optimal Value &amp; Policy</head><p>We now describe the main technical contributions of this paper, which lead to the development of two novel off-policy RL algorithms in Section 5. The first key observation is that, for the softmax value function V * in (8), the quantity exp{V * (s)/τ } also serves as the normalization factor of the optimal policy π * (a | s) in <ref type="formula" target="#formula_7">(7)</ref>; that is,</p><formula xml:id="formula_10">π * (a | s) = exp{(r(s, a) + γV * (s ))/τ } exp{V * (s)/τ } .<label>(10)</label></formula><p>Manipulation of <ref type="formula" target="#formula_1">(10)</ref> by taking the log of both sides then reveals an important connection between the optimal state value V * (s), the value V * (s ) of the successor state s reached from any action a taken in s, and the corresponding action probability under the optimal log-policy, log π * (a | s).</p><p>Theorem 1. For τ &gt; 0, the policy π * that maximizes O ENT and state values V * (s) = max π O ENT (s, π) satisfy the following temporal consistency property for any state s and action a (where s = f (s, a)),</p><formula xml:id="formula_11">V * (s) − γV * (s ) = r(s, a) − τ log π * (a | s) .<label>(11)</label></formula><p>Proof. All theorems are established for the general case of a stochastic environment and discounted infinite horizon problems in the Supplementary Material. Theorem 1 follows as a special case.</p><p>Note that one can also characterize π * in terms of Q * as</p><formula xml:id="formula_12">π * (a | s) = exp{(Q * (s, a) − V * (s))/τ } .<label>(12)</label></formula><p>An important property of the one-step softmax consistency established in <ref type="formula" target="#formula_1">(11)</ref> is that it can be extended to a multi-step consistency defined on any action sequence from any given state. That is, the softmax optimal state values at the beginning and end of any action sequence can be related to the rewards and optimal log-probabilities observed along the trajectory. Corollary 2. For τ &gt; 0, the optimal policy π * and optimal state values V * satisfy the following extended temporal consistency property, for any state s 1 and any action sequence a 1 , ..., a t−1 (where</p><formula xml:id="formula_13">s i+1 = f (s i , a i )): V * (s 1 ) − γ t−1 V * (s t ) = t−1 i=1 γ i−1 [r(s i , a i ) − τ log π * (a i | s i )] .<label>(13)</label></formula><p>Proof. The proof in the Supplementary Material applies (the generalized version of) Theorem 1 to any s 1 and sequence a 1 , ..., a t−1 , summing the left and right hand sides of (the generalized version of) (11) to induce telescopic cancellation of intermediate state values. Corollary 2 follows as a special case.</p><p>Importantly, the converse of Theorem 1 (and Corollary 2) also holds: Theorem 3. If a policy π(a | s) and state value function V (s) satisfy the consistency property (11) for all states s and actions a (where s = f (s, a)), then π = π * and V = V * . (See the Supplementary Material.)</p><p>Theorem 3 motivates the use of one-step and multi-step path-wise consistencies as the foundation of RL algorithms that aim to learn parameterized policy and value estimates by minimizing the discrepancy between the left and right hand sides of (11) and (13).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Path Consistency Learning (PCL)</head><p>The temporal consistency properties between the optimal policy and optimal state values developed above lead to a natural path-wise objective for training a policy π θ , parameterized by θ, and a state value function V φ , parameterized by φ, via the minimization of a soft consistency error. Based on (13), we first define a notion of soft consistency for a d-length sub-trajectory s i:i+d ≡ (s i , a i , . . . , s i+d−1 , a i+d−1 , s i+d ) as a function of θ and φ:</p><formula xml:id="formula_14">C(s i:i+d , θ, φ) = −V φ (s i ) + γ d V φ (s i+d ) + d−1 j=0 γ j [r(s i+j , a i+j ) − τ log π θ (a i+j | s i+j )] .<label>(14)</label></formula><p>The goal of a learning algorithm can then be to find V φ and π θ such that C(s i:i+d , θ, φ) is as close to 0 as possible for all sub-trajectories s i:i+d . Accordingly, we propose a new learning algorithm, called Path Consistency Learning (PCL), that attempts to minimize the squared soft consistency error over a set of sub-trajectories E,</p><formula xml:id="formula_15">O PCL (θ, φ) = s i:i+d ∈E 1 2 C(s i:i+d , θ, φ) 2 .<label>(15)</label></formula><p>The PCL update rules for θ and φ are derived by calculating the gradient of <ref type="bibr" target="#b14">(15)</ref>. For a given trajectory s i:i+d these take the form,</p><formula xml:id="formula_16">∆θ = η π C(s i:i+d , θ, φ) d−1 j=0 γ j ∇ θ log π θ (a i+j | s i+j ) ,<label>(16)</label></formula><formula xml:id="formula_17">∆φ = η v C(s i:i+d , θ, φ) ∇ φ V φ (s i ) − γ d ∇ φ V φ (s i+d ) ,<label>(17)</label></formula><p>where η v and η π denote the value and policy learning rates respectively. Given that the consistency property must hold on any path, the PCL algorithm applies the updates <ref type="formula" target="#formula_1">(16)</ref> and <ref type="formula" target="#formula_1">(17)</ref> both to trajectories sampled on-policy from π θ as well as trajectories sampled from a replay buffer. The union of these trajectories comprise the set E used in (15) to define O PCL .</p><p>Specifically, given a fixed rollout parameter d, at each iteration, PCL samples a batch of on-policy trajectories and computes the corresponding parameter updates for each sub-trajectory of length d. Then PCL exploits off-policy trajectories by maintaining a replay buffer and applying additional updates based on a batch of episodes sampled from the buffer at each iteration. We have found it beneficial to sample replay episodes proportionally to exponentiated reward, mixed with a uniform distribution, although we did not exhaustively experiment with this sampling procedure. In particular, we sample a full episode s 0:T from the replay buffer of size B with probability 0.1/B + 0.9 · exp(α</p><formula xml:id="formula_18">T −1 i=0 r(s i , a i ))/Z,</formula><p>where we use no discounting on the sum of rewards, Z is a normalization factor, and α is a hyper-parameter. Pseudocode of PCL is provided in the Appendix.</p><p>We note that in stochastic settings, our squared inconsistency objective approximated by Monte Carlo samples is a biased estimate of the true squared inconsistency (in which an expectation over stochastic dynamics occurs inside rather than outside the square). This issue arises in Q-learning as well, and others have proposed possible remedies which can also be applied to PCL <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Unified Path Consistency Learning (Unified PCL)</head><p>The PCL algorithm maintains a separate model for the policy and the state value approximation. However, given the soft consistency between the state and action value functions (e.g.,in (9)), one can express the soft consistency errors strictly in terms of Q-values. Let Q ρ denote a model of action values parameterized by ρ, based on which one can estimate both the state values and the policy as,</p><formula xml:id="formula_19">V ρ (s) = τ log a exp{Q ρ (s, a)/τ } ,<label>(18)</label></formula><formula xml:id="formula_20">π ρ (a | s) = exp{(Q ρ (s, a) − V ρ (s))/τ } .<label>(19)</label></formula><p>Given this unified parameterization of policy and value, we can formulate an alternative algorithm, called Unified Path Consistency Learning (Unified PCL), which optimizes the same objective (i.e., (15)) as PCL but differs by combining the policy and value function into a single model. Merging the policy and value function models in this way is significant because it presents a new actor-critic paradigm where the policy (actor) is not distinct from the values (critic). We note that in practice, we have found it beneficial to apply updates to ρ from V ρ and π ρ using different learning rates, very much like PCL. Accordingly, the update rule for ρ takes the form,</p><formula xml:id="formula_21">∆ρ = η π C(s i:i+d , ρ) d−1 j=0 γ j ∇ ρ log π ρ (a i+j | s i+j ) + (20) η v C(s i:i+d , ρ) ∇ ρ V ρ (s i ) − γ d ∇ ρ V ρ (s i+d ) .<label>(21)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Connections to Actor-Critic and Q-learning</head><p>To those familiar with advantage-actor-critic methods <ref type="bibr" target="#b20">[21]</ref> (A2C and its asynchronous analogue A3C) PCL's update rules might appear to be similar. In particular, advantage-actor-critic is an on-policy method that exploits the expected value function,</p><formula xml:id="formula_22">V π (s) = a π(a | s) [r(s, a) + γV π (s )] ,<label>(22)</label></formula><p>to reduce the variance of policy gradient, in service of maximizing the expected reward. As in PCL, two models are trained concurrently: an actor π θ that determines the policy, and a critic V φ that is trained to estimate V π θ . A fixed rollout parameter d is chosen, and the advantage of an on-policy trajectory s i:i+d is estimated by</p><formula xml:id="formula_23">A(s i:i+d , φ) = −V φ (s i ) + γ d V φ (s i+d ) + d−1 j=0 γ j r(s i+j , a i+j ) .<label>(23)</label></formula><p>The advantage-actor-critic updates for θ and φ can then be written as,</p><formula xml:id="formula_24">∆θ = η π E s i:i+d |θ [A(s i:i+d , φ)∇ θ log π θ (a i |s i )] ,<label>(24)</label></formula><formula xml:id="formula_25">∆φ = η v E s i:i+d |θ [A(s i:i+d , φ)∇ φ V φ (s i )] ,<label>(25)</label></formula><p>where the expectation E s i:i+d |θ denotes sampling from the current policy π θ . These updates exhibit a striking similarity to the updates expressed in <ref type="formula" target="#formula_1">(16)</ref> and <ref type="bibr" target="#b16">(17)</ref>. In fact, if one takes PCL with τ → 0 and omits the replay buffer, a slight variation of A2C is recovered. In this sense, one can interpret PCL as a generalization of A2C. Moreover, while A2C is restricted to on-policy samples, PCL minimizes an inconsistency measure that is defined on any path, hence it can exploit replay data to enhance its efficiency via off-policy learning.</p><p>It is also important to note that for A2C, it is essential that V φ tracks the non-stationary target V π θ to ensure suitable variance reduction. In PCL, no such tracking is required. This difference is more dramatic in Unified PCL, where a single model is trained both as an actor and a critic. That is, it is not necessary to have a separate actor and critic; the actor itself can serve as its own critic.</p><p>One can also compare PCL to hard-max temporal consistency RL algorithms, such as Q-learning <ref type="bibr" target="#b42">[43]</ref>. In fact, setting the rollout to d = 1 in Unified PCL leads to a form of soft Q-learning, with the degree of softness determined by τ . We therefore conclude that the path consistency-based algorithms developed in this paper also generalize Q-learning. Importantly, PCL and Unified PCL are not restricted to single step consistencies, which is a major limitation of Q-learning. While some have proposed using multi-step backups for hard-max Q-learning <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b20">21]</ref>, such an approach is not theoretically sound, since the rewards received after a non-optimal action do not relate to the hard-max Q-values Q • . Therefore, one can interpret the notion of temporal consistency proposed in this paper as a sound generalization of the one-step temporal consistency given by hard-max Q-values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Connections between softmax Q-values and optimal entropy-regularized policies have been previously noted. In some cases entropy regularization is expressed in the form of relative entropy <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31]</ref>, and in other cases it is the standard entropy <ref type="bibr" target="#b46">[47]</ref>. While these papers derive similar relationships to <ref type="formula" target="#formula_7">(7)</ref> and <ref type="formula" target="#formula_8">(8)</ref>, they stop short of stating the single-and multi-step consistencies over all action choices we highlight. Moreover, the algorithms proposed in those works are essentially single-step Q-learning variants, which suffer from the limitation of using single-step backups. Another recent work <ref type="bibr" target="#b24">[25]</ref> uses the softmax relationship in the limit of τ → 0 and proposes to augment an actor-critic algorithm with offline updates that minimize a set of single-step hard-max Bellman errors. Again, the methods we propose are differentiated by the multi-step path-wise consistencies which allow the resulting algorithms to utilize multi-step trajectories from off-policy samples in addition to on-policy samples.</p><p>The proposed PCL and Unified PCL algorithms bear some similarity to multi-step Q-learning <ref type="bibr" target="#b25">[26]</ref>, which rather than minimizing one-step hard-max Bellman error, optimizes a Q-value function approximator by unrolling the trajectory for some number of steps before using a hard-max backup. While this method has shown some empirical success <ref type="bibr" target="#b20">[21]</ref>, its theoretical justification is lacking, since rewards received after a non-optimal action no longer relate to the hard-max Q-values Q</p><p>• . In contrast, the algorithms we propose incorporate the log-probabilities of the actions on a multi-step rollout, which is crucial for the version of softmax consistency we consider.</p><p>Other notions of temporal consistency similar to softmax consistency have been discussed in the RL literature. Previous work has used a Boltzmann weighted average operator <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b4">5]</ref>. In particular, this operator has been used by <ref type="bibr" target="#b4">[5]</ref> to propose an iterative algorithm converging to the optimal maximum reward policy inspired by the work of <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b38">39]</ref>. While they use the Boltzmann weighted average, they briefly mention that a softmax (log-sum-exp) operator would have similar theoretical properties. More recently <ref type="bibr" target="#b2">[3]</ref> proposed a mellowmax operator, defined as log-average-exp. These log-averageexp operators share a similar non-expansion property, and the proofs of non-expansion are related. The results of PCL against A3C and DQN baselines. Each plot shows average reward across 5 random training runs (10 for Synthetic Tree) after choosing best hyperparameters. We also show a single standard deviation bar clipped at the min and max. The x-axis is number of training iterations. PCL exhibits comparable performance to A3C in some tasks, but clearly outperforms A3C on the more challenging tasks. Across all tasks, the performance of DQN is worse than PCL.</p><p>Additionally it is possible to show that when restricted to an infinite horizon setting, the fixed point of the mellowmax operator is a constant shift of the Q * investigated here. In all these cases, the suggested training algorithm optimizes a single-step consistency unlike PCL and Unified PCL, which optimizes a multi-step consistency. Moreover, these papers do not present a clear relationship between the action values at the fixed point and the entropy regularized expected reward objective, which was key to the formulation and algorithmic development in this paper.</p><p>Finally, there has been a considerable amount of work in reinforcement learning using off-policy data to design more sample efficient algorithms. Broadly speaking, these methods can be understood as trading off bias <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b8">9]</ref> and variance <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b22">23]</ref>. Previous work that has considered multi-step off-policy learning has typically used a correction (e.g., via importance-sampling <ref type="bibr" target="#b28">[29]</ref> or truncated importance sampling with bias correction <ref type="bibr" target="#b22">[23]</ref>, or eligibility traces <ref type="bibr" target="#b27">[28]</ref>). By contrast, our method defines an unbiased consistency for an entire trajectory applicable to on-and off-policy data. An empirical comparison with all these methods remains however an interesting avenue for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>We evaluate the proposed algorithms, namely PCL &amp; Unified PCL, across several different tasks and compare them to an A3C implementation, based on <ref type="bibr" target="#b20">[21]</ref>, and an implementation of double Q-learning with prioritized experience replay, based on <ref type="bibr" target="#b29">[30]</ref>. We find that PCL can consistently match or beat the performance of these baselines. We also provide a comparison between PCL and Unified PCL and find that the use of a single unified model for both values and policy can be competitive with PCL.</p><p>These new algorithms are easily amenable to incorporate expert trajectories. Thus, for the more difficult tasks we also experiment with seeding the replay buffer with 10 randomly sampled expert trajectories. During training we ensure that these trajectories are not removed from the replay buffer and always have a maximal priority.</p><p>The details of the tasks and the experimental setup are provided in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Results</head><p>We present the results of each of the variants PCL, A3C, and DQN in <ref type="figure" target="#fig_0">Figure 1</ref>. After finding the best hyperparameters (see the Supplementary Material), we plot the average reward over training iterations for five randomly seeded runs. For the Synthetic Tree environment, the same protocol is performed but with ten seeds instead. The results of PCL vs. PCL augmented with a small number of expert trajectories on the hardest algorithmic tasks. We find that incorporating expert trajectories greatly improves performance.</p><p>The gap between PCL and A3C is hard to discern in some of the more simple tasks such as Copy, Reverse, and RepeatCopy. However, a noticeable gap is observed in the Synthetic Tree and DuplicatedInput results and more significant gaps are clear in the harder tasks, including ReversedAddition, ReversedAddition3, and Hard ReversedAddition. Across all of the experiments, it is clear that the prioritized DQN performs worse than PCL. These results suggest that PCL is a competitive RL algorithm, which in some cases significantly outperforms strong baselines.</p><p>We compare PCL to Unified PCL in <ref type="figure">Figure 2</ref>. The same protocol is performed to find the best hyperparameters and plot the average reward over several training iterations. We find that using a single model for both values and policy in Unified PCL is slightly detrimental on the simpler tasks, but on the more difficult tasks Unified PCL is competitive or even better than PCL.</p><p>We present the results of PCL along with PCL augmented with expert trajectories in <ref type="figure">Figure 3</ref>. We observe that the incorporation of expert trajectories helps a considerable amount. Despite only using a small number of expert trajectories (i.e., 10) as opposed to the mini-batch size of 400, the inclusion of expert trajectories in the training process significantly improves the agent's performance. We performed similar experiments with Unified PCL and observed a similar lift from using expert trajectories. Incorporating expert trajectories in PCL is relatively trivial compared to the specialized methods developed for other policy based algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref>. While we did not compare to other algorithms that take advantage of expert trajectories, this success shows the promise of using pathwise consistencies. Importantly, the ability of PCL to incorporate expert trajectories without requiring adjustment or correction is a desirable property in real-world applications such as robotics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We study the characteristics of the optimal policy and state values for a maximum expected reward objective in the presence of discounted entropy regularization. The introduction of an entropy regularizer induces an interesting softmax consistency between the optimal policy and optimal state values, which may be expressed as either a single-step or multi-step consistency. This softmax consistency leads to the development of Path Consistency Learning (PCL), an RL algorithm that resembles actor-critic in that it maintains and jointly learns a model of the state values and a model of the policy, and is similar to Q-learning in that it minimizes a measure of temporal consistency error. We also propose the variant Unified PCL which maintains a single model for both the policy and the values, thus upending the actor-critic paradigm of separating the actor from the critic. Unlike standard policy based RL algorithms, PCL and Unified PCL apply to both on-policy and off-policy trajectory samples. Further, unlike value based RL algorithms, PCL and Unified PCL can take advantage of multi-step consistencies. Empirically, PCL and Unified PCL exhibit a significant improvement over baseline methods across several algorithmic benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The results of PCL against A3C and DQN baselines. Each plot shows average reward across 5 random training runs (10 for Synthetic Tree) after choosing best hyperparameters. We also show a single standard deviation bar clipped at the min and max. The x-axis is number of training iterations. PCL exhibits comparable performance to A3C in some tasks, but clearly outperforms A3C on the more challenging tasks. Across all tasks, the performance of DQN is worse than PCL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: The results of PCL vs. Unified PCL. Overall we find that using a single model for both values and policy is not detrimental to training. Although in some of the simpler tasks PCL has an edge over Unified PCL, on the more difficult tasks, Unified PCL preforms better.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgment</head><p>We thank Rafael Cosman, Brendan O'Donoghue, Volodymyr Mnih, George Tucker, Irwan Bello, and the Google Brain team for insightful comments and discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Apprenticeship learning via inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning near-optimal policies with bellman-residual minimization based fitted policy iteration and a single sample path</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="89" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A new softmax operator for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05628</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Dynamic policy programming with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kappen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic policy programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2012-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimal control as a graphical model inference problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn. J</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">G-learning: Taming the noise in reinforcement learning via soft updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pakman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>UAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The reactor: A sample-efficient actor-critic architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gruslys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04651</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Holly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Q-Prop: Sample-efficient policy gradient with an off-policy critic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08165</idno>
		<title level="m">Reinforcement learning with deep energy-based policies</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4565" to="4573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Approximate maxent inverse optimal control and its application for mental simulation of human interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farahmand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A natural policy gradient. NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Path integrals and symmetry breaking for optimal control theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical mechanics: theory and experiment</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">11011</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Reinforcement learning in robotics: A survey. IJRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end training of deep visuomotor policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">39</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A contextual-bandit approach to personalized news article recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<title level="m">Continuous control with deep reinforcement learning. ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Algorithms for sequential decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
		<respStmt>
			<orgName>Brown University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Asynchronous methods for deep reinforcement learning. ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Safe and efficient off-policy reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stepleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improving policy gradient by exploring underappreciated rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">PGQ: Combining policy gradient and Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>O&amp;apos;donoghue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Incremental multi-step Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="283" to="290" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Relative entropy policy search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Müling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Eligibility traces for off-policy policy evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science Department Faculty Publication Series</title>
		<imprint>
			<biblScope unit="page">80</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Off-policy temporal-difference learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<title level="m">Prioritized experience replay. ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06440</idno>
		<title level="m">Equivalence between policy gradients and soft Q-learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<title level="m">Trust region policy optimization. ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">High-dimensional continuous control using generalized advantage estimation. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<title level="m">Deterministic policy gradient algorithms. ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Introduction to Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>2nd edition. Preliminary Draft</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Temporal difference learning and TD-gammon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>CACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Personalized ad recommendation systems for life-time value optimization with guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Theocharous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Linearly-solvable Markov decision problems. NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
		<title level="m">Policy gradients in linearly-solvable MDPs. NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Sample efficient actor-critic with experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning from delayed rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge England</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn. J</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Function optimization using connectionist reinforcement learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Modeling purposeful adaptive behavior with the principle of maximum causal entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Maximum entropy inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
