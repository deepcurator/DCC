<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spherical convolutions and their application in molecular modelling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Boomsma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science IT</orgName>
								<orgName type="institution" key="instit1">University of Copenhagen</orgName>
								<orgName type="institution" key="instit2">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jes</forename><surname>Frellsen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science IT</orgName>
								<orgName type="institution" key="instit1">University of Copenhagen</orgName>
								<orgName type="institution" key="instit2">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Spherical convolutions and their application in molecular modelling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Convolutional neural networks are increasingly used outside the domain of image analysis, in particular in various areas of the natural sciences concerned with spatial data. Such networks often work out-of-the box, and in some cases entire model architectures from image analysis can be carried over to other problem domains almost unaltered. Unfortunately, this convenience does not trivially extend to data in non-euclidean spaces, such as spherical data. In this paper, we introduce two strategies for conducting convolutions on the sphere, using either a spherical-polar grid or a grid based on the cubed-sphere representation. We investigate the challenges that arise in this setting, and extend our discussion to include scenarios of spherical volumes, with several strategies for parameterizing the radial dimension. As a proof of concept, we conclude with an assessment of the performance of spherical convolutions in the context of molecular modelling, by considering structural environments within proteins. We show that the models are capable of learning non-trivial functions in these molecular environments, and that our spherical convolutions generally outperform standard 3D convolutions in this setting. In particular, despite the lack of any domain specific feature-engineering, we demonstrate performance comparable to state-of-the-art methods in the field, which build on decades of domain-specific knowledge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Given the transformational role that convolutional neural networks (CNNs) have had in the area of image analysis, it is natural to consider whether such networks can be efficiently applied in other contexts. In particular spatially embedded data can often be interpreted as images, allowing for direct transfer of neural network architectures to these domains. Recent years have demonstrated interesting examples in a broad selection of the natural sciences, ranging from physics <ref type="bibr" target="#b0">(Aurisano et al., 2016;</ref><ref type="bibr" target="#b17">Mills et al., 2017)</ref> to biology <ref type="bibr" target="#b26">(Wang et al., 2016;</ref><ref type="bibr" target="#b18">Min et al., 2017)</ref>, in many cases showing convolutional neural networks to substantially outperform existing methods.</p><p>The standard convolutional neural network can be applied naturally to data embedded in a Euclidean space, where uniformly spaced grids can be trivially defined. For other manifolds, such as the sphere, it is less obvious, and to our knowledge, convolutional neural networks for such manifolds have not been systematically investigated. In particular for the sphere, the topic has direct applications in a range of scientific disciplines, such as the earth sciences, astronomy, and modelling of molecular structure.</p><p>This paper presents two strategies for creating spherical convolutions, as understood in the context of convolutional neural networks (i.e., discrete, and efficiently implementable as tensor operations). The first is a straightforward periodically wrapped convolution on a spherical-polar grid. The second builds on the concept of a cubed-sphere <ref type="bibr" target="#b19">(Ronchi et al., 1996)</ref>. We proceed with extending these strategies to include the radial component, using concentric grids, which allows us to conduct convolutions in spherical volumes.</p><p>Our hypothesis is that these concentric spherical convolutions should outperform standard 3D convolutions in cases where data is naturally parameterized in terms of a radial component. We test this hypothesis in the context of molecular modelling. We will consider structural environments in a molecule as being defined from the viewpoint of a single amino acid or nucleotide: how does such an entity experience its environment in terms of the mass and charge of surrounding atoms? We show that a standard convolutional neural network architectures can be used to learn various features of molecular structure, and that our spherical convolutions indeed outperform standard 3D convolutions for this purpose. We conclude by demonstrating state-of-the art performance in predicting mutation induced changes in protein stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Spherical convolutions</head><p>Conventional CNNs work on discretized input data on a grid in R n , such as time series data in R and image data in R 2 . At each convolutional layer l a CNN performs discrete convolutions (or a correlation)</p><formula xml:id="formula_0">[f * k i ](x) = x ∈Z n C l c=1 f c (x )k i c (x − x )<label>(1)</label></formula><p>of the input feature map f : <ref type="bibr" target="#b4">Cohen and Welling, 2016;</ref><ref type="bibr" target="#b9">Goodfellow et al., 2016)</ref>. While such convolutions are equivariant to translation on the grid, they are not equivariant to scaling <ref type="bibr" target="#b4">(Cohen and Welling, 2016)</ref>. This means that in order to preserve the translation equivariance in R n , conventional CNNs rely on the grid being uniformly spaced within each dimension of R n . Constructing such a grid is straightforward in R n . However, for convolutions on other manifolds such as the 2D sphere, S 2 = {v ∈ R 3 |vv = 1}, no such planar uniform grid is available, due to the non-linearity of the space <ref type="bibr" target="#b16">(Mardia and Jupp, 2009)</ref>. In this section, we briefly discuss the consequences of using convolutions in the standard non-uniform spherical-polar grid, and present an alternative grid for which the non-uniformity is expected to be less severe.</p><formula xml:id="formula_1">Z n → R C l and a set of C l+1 filters k i : Z n → R C l (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Convolutions of features on S</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>A natural approach to a discretization on the sphere is to represent points v on the sphere by their spherical-polar coordinates (θ, φ) and construct uniformly spaced grid in these coordinates, where the spherical coordinates are defined by v = (cos θ, sin θ cos φ, sin θ sin φ) . Convolutions on such a grid can be implemented efficiently using standard 2D convolutions when taking care of using periodic padding at the φ boundaries. The problem with a spherical-polar coordinate grid is that it is highly non-equidistant when projected onto the sphere: the distance between grid points becomes increasingly small as we move from the equator to the poles ( <ref type="figure">figure 1, left)</ref>. This reduces the ability to share filters between different areas of the sphere.</p><p>Figure 1: Two realizations of a grid on the sphere. Left: a grid using equiangular spacing in a standard spherical-polar coordinate system, and Right: An equiangular cubed-sphere representation, as described in <ref type="bibr" target="#b19">Ronchi et al. (1996)</ref>. As a potential improvement, we will investigate a spherical convolution based on the cubed-sphere transformation <ref type="bibr" target="#b19">(Ronchi et al., 1996)</ref>. The transformation is constructed by decomposing the sphere into six patches defined by projecting the circumscribed cube onto the sphere (figure 1, right). In this transformation a point on the sphere v ∈ S 2 is mapped to a patch b ∈ {1, 2, 3, 4, 5, 6} and two coordinates (ξ, η) ∈ [−π/4, π/4[ 2 on that patch. The coordinate are given by the angles between the axis pointing to the patch and v measured in the two coordinate planes perpendicular to the patch. For instance the vectors {v ∈ S 2 |v x &gt; v y and v x &gt; v z } map to patch b = 1 and we have tan ξ = v y /v x and tan η = v z /v x . The remaining mappings are described by <ref type="bibr" target="#b19">Ronchi et al. (1996)</ref>.</p><p>If we grid the two angles (ξ, η) uniformly in the cubed-sphere transformation and project this grid onto the sphere, we obtain a grid that is more regular <ref type="bibr" target="#b19">(Ronchi et al., 1996)</ref>, although it has artefacts in the 8 corners of the circumscribed cube ( <ref type="figure">figure 1, right)</ref>. The cubed-sphere convolution is then constructed by applying the conventional convolution in equation (1) to a uniformly spaced grid on each of the six cubed shape patches. This construction has two main advances: 1) within each patch, the convolution is almost equivariant to translation in ξ and η and 2) features on the cubed-sphere grid can naturally be expressed using tensors, which means that the spherical convolution can be efficiently implemented on a GPU. When implementing convolutions and pooling operations for the cubed-sphere grid, one has to be careful in padding each patch with the contents of the four neighbouring patches, in order to preserve the wrapped topology of the sphere (figure 2, right).</p><p>Both of these two approaches to spherical convolutions are hampered by a lack of rotational equivariance, which restricts the degree with which filters can be shared over the surface of the sphere, leading to suboptimal efficiency in the learning of the parameters. Despite this limitation, for capturing patterns in spherical volumes, we expect that the ability to express patterns naturally in terms of radial and angular dimensions has advantages over standard 3D convolutions. We test this hypothesis in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Convolutions of features on B</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>The two representations from figure 1 generalize to the ball B 3 by considering concentric shells at uniformly separated radii. In the case of the cubed-sphere, this means that a vector v ∈ B 3 is mapped to the unique coordinates (r, b, ξ, η), where r = √ vv is the radius and (b, ξ, η) are the cubed-sphere coordinates at r, and we construct a uniform grid in r, ξ and η. Likewise, in the spherical-polar case, we construct a uniform grid in r, θ and φ. We will refer to these grids as concentric cubed-sphere grid and concentric spherical-polar grid respectively (figure 3). As is the case for their S 2 counterparts, features on these grids can be naturally expressed using tensors.</p><p>We can apply the conventional 3D convolutions in equation <ref type="formula" target="#formula_0">(1)</ref> to features on the concentric cubedsphere and the concentric spherical-polar grids, and denote these as concentric cubed-sphere convolution (CCSconv) and concentric spherical-polar convolution (CSPconv). For fixed r, the convolutions will thus have the same properties as in the S 2 case. In these concentric variants, the convolutions will not be equivariant to translations in r, which again reduces the potential to share filter parameters. <ref type="figure">Figure 3</ref>: Three realizations of a grid on the ball. Left: a grid using equiangular spacing in a standard spherical-polar coordinate system (concentric spherical-polar grid). Center: An equiangular cubed-sphere representation, as described in <ref type="bibr" target="#b19">Ronchi et al. (1996)</ref> (concentric cubed-sphere grid). Right: a Cartesian grid.</p><p>We propose to address this issue in three ways. First, we can simply apply the convolution over the full range of r with a large number of filters C l+1 and hope that the network will automatically allocate different filters at different radii. Secondly, we can make the filters k i (x − x , x r ) depend on r, which corresponds to using different (possibly overlapping) filters on each spherical shell (conv-banded-disjoint). Thirdly, we can divide the r-grid into segments and apply the same filter within each segment (conv-banded), potentially with overlapping regions (depending on the stride). The three approaches are illustrated in figure 4.</p><p>In the experiments below, we will be comparing the performance of our concentric spherical convolution methods to that of a simple 3D convolution in a Cartesian grid (figure 3, right). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Modelling structural environments in molecules</head><p>In the last decades, substantial progress has been made in the ability to simulate and analyse molecular structures on a computer. Much of this progress can be ascribed to the molecular force fields used to capture the physical interactions between atoms. The basic functional forms of these models were established in the late 1960s, and through gradual refinements they have become a success story of molecular modelling. Despite these positive developments, the accuracy of molecular force fields is known to still be a limiting factor for many biological and pharmaceutical applications, and further improvements are necessary in this area to increase the robustness of methods for e.g. protein prediction and design. There are indications that Machine Learning could provide solutions to such challenges. While, traditionally, most of the attention in the Machine Learning community has been dedicated <ref type="figure">Figure 5</ref>: Example of the environment surrounding an amino acid in a protein, in this case the phenylalanine at position 30 in protein GB1 (PDB ID: 2GB1). Left: a cartoon representation of GB1, where the helix is red, the sheets are yellow and the coils are grey. The phenylalanine is shown using an atomic representation in green. Right: an atomic representation of GB1, where carbon atoms are green, oxygen atoms are red, nitrogen atoms are blue and hydrogen atoms are grey. A sphere centered at the C α atom of the phenylalanine with a radius of 12Å is shown in grey.</p><p>to predicting structural features from amino acid sequences (e.g. secondary structure, disorder, and contact prediction), there are increasingly applications taking three dimensional molecular structure as input <ref type="bibr" target="#b1">(Behler and Parrinello, 2007;</ref><ref type="bibr" target="#b13">Jasrasaria et al., 2016;</ref><ref type="bibr" target="#b20">Schütt et al., 2017;</ref><ref type="bibr" target="#b22">Smith et al., 2017)</ref>.</p><p>In particular in the field of quantum chemistry, a number of studies have demonstrated the ability of deep learning techniques to accurately predict energies of molecular systems. Common to many of these methods is a focus on manually engineered features, where the molecular input structure is encoded based on prior domain-specific knowledge, such as specific functional relationships between atoms and their environments <ref type="bibr" target="#b1">(Behler and Parrinello, 2007;</ref><ref type="bibr" target="#b22">Smith et al., 2017)</ref>. Recently, a few studies have demonstrated the potential of automatically learning such features, by encoding the molecular structural input in a more domain-agnostic manner, for instance considering only pairwise distance matrices <ref type="bibr" target="#b20">(Schütt et al., 2017)</ref>, space filling curves <ref type="bibr" target="#b13">(Jasrasaria et al., 2016)</ref>, or basic structural features <ref type="bibr" target="#b24">(Wallach et al., 2015)</ref>.</p><p>The fact that atomic forces are predominantly distance-based suggests that molecular environments are most naturally represented with a radial-based parameterization, which makes it an obvious test case for the convolutions presented in the previous section. If successful, such convolutions could allow us to make inferences directly from the raw molecular structure of a molecule, avoiding the need of manual feature engineering. We will consider the environments that each amino acids experience within its globular protein structure as images in the 3-ball. <ref type="figure">Figure 5</ref> shows an example of the environment experienced by an arbitrarily chosen amino acid in the GB1 protein (PDB ID: 2GB1). Although distorted by the fish-eye perspective, the local environment (right) displays several key features of the data: we see clear patterns among neighboring atoms, depending on their local structure, and we can imagine the model learning to recognize hydrogen bonds and charge interactions between an amino acid and its surroundings.</p><p>Our representation of the molecular environment includes all atoms within a 12 Å radius of the C α atom of the amino acid in question. Each atom is represented by three fundamental properties: 1) its position relative to the amino acid in question (i.e., the position in the grid), 2) its mass, and 3) its partial charge, as defined by the amber99sb force field <ref type="bibr" target="#b11">(Hornak et al., 2006)</ref>. We construct two types of models, which are identical except for their output. The first outputs the propensity for different secondary structure labels at a given position (i.e., helix, extended, coil), while the second outputs the propensity for different amino acid types. Each of these models will be implemented with both the Cartesian, the concentric spherical and the concentric cubed-sphere convolutions. Furthermore, for the concentric cubed-sphere convolutions, we compare the three strategies for dealing with the radial component illustrated in figure 4. </p><note type="other">1 CCSconv + ReLU 3 × 5 × 5 × 2 × 16 6 × 22 × 19 × 19 × 16 1 CCSsumpool 1 × 3 × 3 6 × 22 × 10 × 10 × 16 2 CCSconv + ReLU 3 × 3 × 3 × 16 × 32 6 × 20 × 10 × 10 × 32 2 CCSsumpool 3 × 3 × 3 6 × 9 × 5 × 5 × 32 3 CCSconv + ReLU 3 × 3 × 3 × 32 × 64 6 × 7 × 5 × 5 × 64 3 CCSsumpool 1 × 3 × 3 6 × 7 × 3 × 3 × 64 4 CCSconv + ReLU 3 × 3 × 3 × 64 × 128 6 × 5 × 3 × 3 × 128 4 CCSsumpool 1 × 3 × 3 6 × 5 × 3 × 3 × 128 5 Dense + ReLU 34 560 × 2 048 2 048 6 Dense + ReLU 2 048 × 2 048 2 048 7 Dense + Softmax 2 048 × o o</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model architecture</head><p>The input to the network is a grid (concentric cubed-sphere, concentric spherical polar or Cartesian). Each voxel has two input channels: the mass of the atom that lies in the given bin and the atom's partial charge (or zeros if no atom is found). The resolution of the grids are chosen so that the maximum distance within a bin is 0.5Å, which ensures that bins are occupied by at most one atom. The radius of the ball is set to 12Å, since most physical interactions between atoms occur within this distance <ref type="bibr" target="#b12">(Irbäck and Mohanty, 2006)</ref>. This gives us an input tensor of shape (b = 6, r = 24, ξ = 38, η = 38, C 1 = 2) for the concentric cubed-sphere case, (r = 24, θ = 76, φ = 151, C 1 = 2) for the concentric spherical polar case, and (x = 60, y = 60, z = 60, C 1 = 2) for the Cartesian case.</p><p>We use a deep model architecture that is loosely inspired by the VGG models <ref type="bibr" target="#b21">(Simonyan and Zisserman, 2015)</ref>, but employs the convolution operators described above. Our models have four convolutional layers followed by three dense layers, as illustrated in table 1. Each convolutional layer is followed by rectified linear unit (ReLU) activation function <ref type="bibr" target="#b10">(Hahnloser et al., 2000;</ref><ref type="bibr" target="#b8">Glorot et al., 2011</ref>) and a sum pooling operation which is appropriately wrapped in the case of the concentric cubed-sphere and the concentric spherical polar grid. We use sum pooling since the input features, mass and partial charge, are both physical quantities that are naturally additive. The total number of parameters is the models (with the amino acid output) are 75 313 253 (concentric cubed-sphere), 69 996 645 (concentric spherical polar), and 61 159 077 (Cartesian). Furthermore, for the concentric cubed-sphere case, we include a comparison of the two alternative strategies for the radial component: the conv b and the conv bd , which have 75 745 333 and 76 844 661 parameters respectively. Finally, to see the effect of convolutions over a purely dense model, we include a baseline model where the convolutional layers are replaced with dense layers, but otherwise following the same architecture, and roughly the same number of parameters (66 670 613).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>We minimized the cross-entropy loss using Adam <ref type="bibr" target="#b15">(Kingma and Ba, 2015)</ref>, regularized by penalizing the loss with the sum of the L 2 of all weights, using a multiplicative factor of 0.001. All dense layers also used dropout regularization with a probability of 0.5 of keeping a neuron. The models were trained on NVIDIA Titan X (Pascal) GPUs, using a batch size of 100 and a learning rate of 0.0001.</p><p>The models were trained on data set of high resolution crystal structures. A large initial (nonhomology-reduced) data set was constructed using the PISCES server <ref type="bibr" target="#b25">(Wang and Dunbrack, 2003)</ref>. For all structures, hydrogen atoms were added using the Reduce program <ref type="bibr" target="#b27">(Word et al., 1999)</ref>, after which partial charges were assigned using the OpenMM framework <ref type="bibr" target="#b7">(Eastman et al., 2012)</ref>, using the amber99sb force field <ref type="bibr" target="#b11">(Hornak et al., 2006)</ref>. During these stages strict filters were applied to remove structures that 1) were incomplete (missing chains or missing residues compared to the seqres entry), 2) had chain breaks, 3) failed to parse in OpenMM, or 4) led the Reduce program to crash. Finally, the remaining set was resubmitted to the PISCES server, where homology-reduction was done at the 30% level. This left us with 2336 proteins, out of which 1742 were used for training, 10 for validation, and the remainder was set aside for testing. The homology-reduction ensures that any pair of sequences in the data set are at most 30% identical at the amino-acid-level, which allows us to safely split the data into non-overlapping sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We now discuss results obtained with the secondary structure and amino acid models, respectively. Despite the apparent similarity of the two models, the two tasks have substantially different biological implications: secondary structure is related to the 3D structure locally at a given position in a protein, i.e. whether the protein assumes a helical or a more extended shape. In contrast, amino acid propensities describe allowed mutations in a protein, which is related to the fundamental biochemistry of the molecule, and is relevant for understanding genetic disease and for design of new proteins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning the DSSP secondary structure function</head><p>Predicting the secondary structure of a protein conditioned on knowledge of the three dimensional structure is not considered a hard problem. We include it here because we are interested in the ability of the neural network to learn the function that is typically used to annotate three dimensional structures with secondary structure, in our case DSSP <ref type="bibr" target="#b14">(Kabsch and Sander, 1983)</ref>. Interestingly, the different concentric convolutional models are seen to perform about equally well on this problem (table 2, Q3), marginally outperforming the Cartesian convolution and substantially outperforming the dense baseline model.</p><p>To get a sense of the absolute performance, we would ideally compare to existing methods on the same problem. However, rediscovering the DSSP function is not a common task in bioinformatics, and not many tools are available that would constitute a meaningful comparison, in particular because secondary structure annotation algorithms use different definitions of secondary structure. We here use the TORUSDBN model <ref type="bibr" target="#b2">(Boomsma et al., 2008</ref><ref type="bibr" target="#b3">(Boomsma et al., , 2014</ref>) to provide such a baseline. The model is sequential in the sequence of a protein, and thus captures local structural information only. While the model is originally designed to sample backbone dihedral angles conditioned on an amino acid sequence or secondary structure sequence, it is generative, and can thus be used in reverse and provide the most probable secondary structure or amino acid sequence given using viterbi decoding. Most importantly, it is trained on DSSP, making it useful as a comparison for this study. Included as the last row in table 2, TORUSDBN demonstrates slightly lower performance compared to our convolutional approaches, illustrating that most of the secondary structure signal is encoded in the local angular preferences. It is encouraging to see that the convolutional networks capture all these local signals, but obtain additional performance through more non-local interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Learning amino acid propensities</head><p>Compared to secondary structure, predicting the amino acid propensity is substantially harder-partly because of the larger sample space, but also because we expect such preferences to be defined by more global interaction patterns. Interestingly, the two concentric convolutions perform about equally well, suggesting that the added regularity of the cubed-sphere representation does not provide a substantial benefit for this case (table 2, Q20). However, both methods substantially outperform the standard 3D convolution, which again outperforms the dense baseline model. We also note that there is now a significant difference between the three radial strategies, with conv-banded-disjoint (bd) and conv-banded (b) both performing worse than the simpler case of using a single convolution over the entire r-range. Again, we include TorusDBN as an external reference. The substantially lower performance of this model confirms that the amino acid label prediction task depends predominantly on non-local features not captured by this model. Finally, we include another baseline: the most frequent amino acid observed at this position among homologous (evolutionarily related) proteins. It is remarkable that the concentric models (which are trained on a homology-reduced protein set), are capable of learning the structural preferences of amino acids to the same extent as the information that is encoded as genetic variation in the sequence databases. This strongly suggests the ability of our models to learn general relationships between structure and sequence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Predicting change-of-stability</head><p>The models in the previous section not only predict the most likely amino acid, but also the entire distribution. A natural question is whether the ratio of probabilities of two amino acids according to this distribution is related to the change of stability induced by the corresponding mutation. We briefly explore this question here.</p><p>The stability of a protein is the difference in free energy ∆G between the folded and unfolded conformation of a protein. The change in stability that occurs as a consequence of a mutation is thus frequently referred to as ∆∆G. These values can be measured experimentally, and several data sets with these values are publicly available. As a simple approximation, we can interpret the sum of negative log-probabilities of each amino acid along the sequence as a free energy of the folded state G f . To account for the free energy of the unfolded state, G u , we could consider the negative log-probability that the amino acid in question occurs in the given amino acid sequence (without conditioning on the environment). Again, assuming independence between sites in the chain, this could be modelled by simply calculating the log-frequencies of the different amino acids across the data set, and summing over all sites of the specific protein to get the total free energy. Subtracting these two pairs of values for the wild type (W) and mutant (M) would give us a rough estimate of the ∆∆G, which due to our assumption of independence between sites simplifies to just the difference in values at the given site:</p><formula xml:id="formula_2">∆∆G(W ,M ) = (G f (M n ) − G u (M n )) − (G f (W n ) − G u (W n )),<label>(2)</label></formula><p>whereW andM denote the full wild type and mutant sequence respectively, and W n and M n denote the amino acids of wild type and mutant at the site n at which they differ. Given the extensive set of simplifying assumptions in the argument above, we do not use the expression in equation <ref type="formula" target="#formula_2">(2)</ref> directly but rather use the four log-</p><formula xml:id="formula_3">probabilities (G f (M n ), G u (M n ), G f (W n ), G u (W n ))</formula><p>as input to a simple regression model (a single hidden layer neural network with 10 hidden nodes and a ReLU activation function), trained on experimentally observed ∆∆G data. We calculate the performance on several standard experimental data sets on mutation-induced change-of-stability, in each case using 5-fold cross validation, and reporting the correlation between experimentally measured and our calculated ∆∆G. As a baseline, we compare our performance to two of the best known programs for calculating ∆∆G: Rosetta and FoldX. The former were taken from a recent publication <ref type="bibr" target="#b6">(Conchúir et al., 2015)</ref>, while the latter were calculated using the FoldX program (version 4). The comparison shows that even a very simple approach based on our convolutional models produces results that are comparable to the state-of-the-art in the field <ref type="table" target="#tab_2">(table 3)</ref>. This is despite the fact that we use a rather crude approximation of free energy, and that our approach disregards the fact that a mutation at a given site modifies the environment grids of all amino acids within the 12 Å range. Although these initial results should therefore not be considered conclusive, they suggest that models like the ones we propose could play a future role in ∆∆G predictions.</p><p>Apart from the overall levels of performance, the most remarkable feature of table 3 is that it shows equal performance for the Cartesian and concentric cubed-sphere convolutions, despite the fact that the former displayed substantially lower Q20 scores. This peculiar result points to an interesting caveat in the interpretation of the predicted distribution over amino acids for a given environment. At sufficiently high resolution of the structural environment, a perfect model would be able to reliably predict the identity of the wild type amino acid by the specific shape of the hole it left behind. This means that as models improve, the entropy of the predicted amino acid distributions is expected to decrease, with increasingly peaked distributions centered at the wild type. An increased sensitivity towards the exact molecular environment will therefore eventually decrease the models ability to consider other amino acids at that position, leading to lower ∆∆G performance. The missing ingredient in our approach is the structural rearrangement in the environments that occurs as a consequence of the mutation. A full treatment of the problem should average the predictions over the available structural variation, and structural resampling is indeed part of both Rosetta and FoldX. For these reasons, it is difficult to make clear interpretations of the relative differences in performance of the three convolution procedures in table 3. The overall performance of all three, however, indicates that convolutions might be useful as part of a more comprehensive modelling strategy such as those used in Rosetta and FoldX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Convolutional neural networks are a powerful tool for analyzing spatial data. In this paper, we investigated the possibility of extending the applicability of the technique to data in the 3-ball, presenting two strategies for conducting convolutions in these spherical volumes. We assessed the performance of the two strategies (and variants thereof) on various tasks in molecular modelling, and demonstrate a substantial potential of these such concentric convolutional approaches to outperform standard 3D convolutions for such data.</p><p>We expect that further improvements to the concentric convolution approach can be obtained by improving the spherical convolutions themselves. In particular, a convolution operation that is rotationally equivariant would provide greater data efficiency than the approach used here. Very recently, a procedure for conducting convolutions in SO(3) was proposed, which seems to provide an elegant solution to this problem <ref type="bibr" target="#b5">(Cohen et al., 2018)</ref>.</p><p>Finally, we note that while this manuscript was in review, another paper on the application of convolutional neural networks for predicting amino acid preferences conditioned on structural environments was published, by Torng and Altman <ref type="bibr" target="#b23">(Torng and Altman, 2017)</ref>. Their study is conceptually similar to one of the applications described in this paper, but uses a Cartesian grid and standard 3D convolution (in addition to other minor differences, such as a one-hot atom type encoding). While Torng and Altman present a more thorough biological analysis in their paper than we do here, the accuracy they report is considerably lower than what we obtained. Based on the comparisons reported here, we anticipate that models such as theirs could be improved by switching to a concentric representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Availability</head><p>The spherical convolution Tensorflow code and the datasets used in this paper are available at https://github.com/deepfold.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: A cubed-sphere grid and a curve on the sphere. Right: The six planes of a cubed-sphere representation and the transformation of the curve to this representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Three strategies for the radial component of concentric cubed-sphere or concentric spherical convolutions. (a) conv: The same convolution-filter is applied to all values of r, (b) conv-bandeddisjoint (conv bd ): convolution-filters are only applied in the angular directions, using different filters for each block in r, (c) conv-banded (conv b ): convolutions are applied within radial segments, Note that for visual clarity, we use a stride of 3 in this figure, although we use a stride of 1 in practice.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>The architecture of the CNN where o represent the output size, which is 3 for secondary structure output and 20 for amino acid output. As an example, we use the convolutional filter sizes from the concentric cubed-sphere (CCS) case. Similar sizes are used for the other representations.</figDesc><table>Layer 
Operation 
Filter / weight size 
Layer output size 
0 
Input 
6 × 24 × 38 × 38 × 2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Performance of various models in the prediction of (a) DSSP-style secondary structure conditioned and (b) amino acid propensity conditioned on the structure. The Q3 score is defined as the percentage of correct predictions for the three possible labels: helix, extended and coil. The Q20 score is defined as the percentage of correct predictions for the 20 possible amino acid labels.</figDesc><table>Model 
Q3 (secondary structure) Q20 (amino acid) 

CCSconv 
0.933 
0.564 
CCSconv bd 
0.931 
0.515 
CCSconv b 
0.932 
0.548 
CSPconv 
0.932 
0.560 
Cartesian 
0.922 
0.500 
CCSdense 
0.888 
0.348 
PSSM 
-
0.547 
TORUSDBN 
0.894 
0.183 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Pearson correlation coefficients between experimentally measured and predicted changes of stability for several sets of published stability measurements.</figDesc><table>Rosetta FoldX CCSconv CSPconv Cartesian 

Kellogg 
0.65 
0.70 
0.66 
0.64 
0.66 
Guerois 
0.65 
0.73 
0.66 
0.64 
0.66 
Potapov 
0.52 
0.59 
0.52 
0.51 
0.52 
ProTherm* 
0.44 
0.53 
0.49 
0.48 
0.49 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Villum Foundation (W.B., grant number VKR023445).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A convolutional neural network neutrino event classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aurisano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Himmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Messier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pawloski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Psihas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vahle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Instrumentation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">9001</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generalized neural-network representation of high-dimensional potential-energy surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Parrinello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">146401</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A generative, probabilistic model of local protein structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Boomsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Mardia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ferkinghoff-Borg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hamelryck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="8932" to="8937" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Equilibrium simulations of proteins using molecular fragment replacement and NMR chemical shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Boomsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frellsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ferkinghoff-Borg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hamelryck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lindorff-Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vendruscolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">38</biblScope>
			<biblScope unit="page" from="13852" to="13857" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<editor>M. F. Balcan and K. Q. Weinberger</editor>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">Spherical CNNs. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A web resource for standardized benchmark datasets, metrics, and Rosetta protocols for macromolecular modeling and design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Ó</forename><surname>Conchúir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Barlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Pache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ollikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>O&amp;apos;meara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kortemme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">130433</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">OpenMM 4: a reusable, extensible, hardware independent library for high performance molecular simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eastman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Friedrichs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Chodera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Radmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bruns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Beauchamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Shirts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Theory and Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="461" to="469" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>G. Gordon, D. Dunson, and M. Dudík</editor>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Fort Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H R</forename><surname>Hahnloser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sarpeshkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">405</biblScope>
			<biblScope unit="page" from="947" to="951" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Comparison of multiple Amber force fields and development of improved protein backbone parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hornak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Okur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Strockbine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roitberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Simmerling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="712" to="725" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PROFASI: a Monte Carlo simulation package for protein folding and aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Irbäck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohanty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Chemistry</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1548" to="1555" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jasrasaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">O</forename><surname>Pyzer-Knapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rappoport</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<idno>arXiv, 1608.05747</idno>
		<title level="m">Space-filling curves as a novel crystal structure representation for machine learning models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dictionary of protein secondary structure: pattern recognition of hydrogen-bonded and geometrical features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kabsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biopolymers</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2577" to="2637" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Mardia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Jupp</surname></persName>
		</author>
		<title level="m">Directional statistics</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning and the Schrödinger equation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tamblyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review A</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">42113</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning in bioinformatics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="851" to="869" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The &quot;cubed sphere&quot;: A new method for the solution of partial differential equations in spherical geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Iacono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paolucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="114" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Quantum-chemical insights from deep tensor neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13890</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ANI-1: an extensible neural network potential with DFT accuracy at force field computational cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Isayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roitberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3192" to="3203" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3D deep convolutional neural networks for amino acid environment similarity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Torng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">302</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dzamba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heifets</surname></persName>
		</author>
		<idno>arXiv, 1510.02855</idno>
		<title level="m">AtomNet: a deep convolutional neural network for bioactivity prediction in structure-based drug discovery</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PISCES: a protein sequence culling server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Dunbrack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1589" to="1591" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Protein secondary structure prediction using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Asparagine and glutamine: using hydrogen atom contacts in the choice of side-chain amide orientation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Word</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">285</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1735" to="1747" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
