<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalisation in humans and deep neural networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
							<email>robert.geirhos@bethgelab.org</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Neural Information Processing Group</orgName>
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">International Max Planck Research School for Intelligent Systems</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">R</forename><surname>Medina Temme</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Neural Information Processing Group</orgName>
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Rauber</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centre for Integrative Neuroscience</orgName>
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">International Max Planck Research School for Intelligent Systems</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><forename type="middle">H</forename><surname>Schütt</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Neural Information Processing Group</orgName>
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Graduate School of Neural and Behavioural Sciences</orgName>
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Potsdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centre for Integrative Neuroscience</orgName>
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Bernstein Center for Computational Neuroscience Tübingen</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Max Planck Institute for Biological Cybernetics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Neural Information Processing Group</orgName>
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Centre for Integrative Neuroscience</orgName>
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Bernstein Center for Computational Neuroscience Tübingen</orgName>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generalisation in humans and deep neural networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Joint first / joint senior authors § To whom correspondence should be addressed:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We compare the robustness of humans and current convolutional deep neural networks (DNNs) on object recognition under twelve different types of image degradations. First, using three well known DNNs  GoogLeNet)   we find the human visual system to be more robust to nearly all of the tested image manipulations, and we observe progressively diverging classification error-patterns between humans and DNNs when the signal gets weaker. Secondly, we show that DNNs trained directly on distorted images consistently surpass human performance on the exact distortion types they were trained on, yet they display extremely poor generalisation abilities when tested on other distortion types. For example, training on salt-and-pepper noise does not imply robustness on uniform white noise and vice versa. Thus, changes in the noise distribution between training and testing constitutes a crucial challenge to deep learning vision systems that can be systematically addressed in a lifelong machine learning approach. Our new dataset consisting of 83K carefully measured human psychophysical trials provide a useful reference for lifelong robustness against image degradations set by the human visual system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Deep neural networks as models of human object recognition</head><p>The visual recognition of objects by humans in everyday life is rapid and seemingly effortless, as well as largely independent of viewpoint and object orientation <ref type="bibr" target="#b0">[1]</ref>. The rapid and primarily foveal recognition during a single fixation has been termed core object recognition (see <ref type="bibr" target="#b1">[2]</ref> for a review). We know, for example, that it is possible to reliably identify objects in the central visual field within a single fixation in less than 200 ms when viewing "standard" images <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. Based on the rapidness of object recognition, core object recognition is often thought to be achieved with mainly feedforward processing although feedback connections are ubiquitous in the primate brain. Object recognition Likewise, when trained and tested on images with additive uniform noise, performance is super-human. (c) Striking generalisation failure: When trained on images with salt-and-pepper noise and tested on images with uniform noise, performance is at chance level-even though both noise types do not seem much different to human observers.</p><p>in the primate brain is believed to be realised by the ventral visual pathway, a hierarchical structure consisting of areas V1-V2-V4-IT, with information from the retina reaching the cortex in V1 (e.g. <ref type="bibr" target="#b4">[5]</ref>).</p><p>Until a few years ago, animate visual systems were the only ones known to be capable of broadranging visual object recognition. This has changed, however, with the advent of brain-inspired deep neural networks (DNNs) which, after having been trained on millions of labeled images, achieve human-level performance when classifying objects in images of natural scenes <ref type="bibr" target="#b5">[6]</ref>. DNNs are now employed on a variety of tasks and set the new state-of-the-art, sometimes even surpassing human performance on tasks which only a few years ago were thought to be beyond an algorithmic solution for decades to come <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Since DNNs and humans achieve similar accuracy, a number of studies have started investigating similarities and differences between DNNs and human vision <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>. On the one hand, the network units are an enormous simplification given the sophisticated nature and diversity of neurons in the brain <ref type="bibr" target="#b24">[25]</ref>. On the other hand, often the strength of a model lies not in replicating the original system but rather in its ability to capture the important aspects while abstracting from details of the implementation (e.g. <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>).</p><p>One of the most remarkable properties of the human visual system is its ability to generalise robustly. Humans generalise across a wide variety of changes in the input distribution, such as across different illumination conditions and weather types. For instance, human object recognition is largely unimpaired even if there are rain drops or snow flakes in front of an object. While humans are certainly exposed to a large number of such changes during their preceding lifetime (i.e., at "training time", as we would say for DNNs), there seems to be something very generic about the way the human visual system is able to generalise that is not limited to the same distribution one was exposed to previously. Otherwise we would not be able to make sense of a scene if there was some sort of "new", previously unseen noise. Even if one never had a shower of confetti before, one is still able to effortlessly recognise objects at a carnival parade. Naturally, such generic, robust mechanisms are not only desirable for animate visual systems but also for solving virtually any visual task that goes beyond a well-confined setting where one knows the exact test distribution already at training time.</p><p>Deep learning for autonomous driving may be one prominent example: one would like to achieve robust classification performance in the presence of confetti, despite not having had any confetti exposure during training time. Thus, from a machine learning perspective, general noise robustness can be used as a highly relevant example of lifelong machine learning <ref type="bibr" target="#b27">[28]</ref> requiring generalisation that does not rely on the standard assumption of independent, identically distributed (i.i.d.) samples at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Comparing generalisation abilities</head><p>Generalisation in DNNs usually works surprisingly well: First of all, DNNs are able to learn sufficiently general features on the training distribution to achieve a high accuracy on the i.i.d. test distribution despite having sufficient capacity to completely memorise the training data <ref type="bibr" target="#b28">[29]</ref>, and considerable effort has been devoted to understand this phenomenon (e.g. <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>). <ref type="bibr" target="#b0">1</ref> Secondly, features learned on one task often transfer to only loosely related tasks, such as from classification to saliency prediction <ref type="bibr" target="#b32">[33]</ref>, emotion recognition <ref type="bibr" target="#b33">[34]</ref>, medical imaging <ref type="bibr" target="#b34">[35]</ref> and a large number of other transfer learning tasks <ref type="bibr" target="#b35">[36]</ref>. However, transfer learning still requires a substantial amount of training before it works on the new task. Here, we focus on a third setting that adopts the lifelong machine learning point of view of generalisation <ref type="bibr" target="#b36">[37]</ref>: How well can a visual learning system cope with a new image degradation after it has learned to cope with a certain set of image distortions before. As a measure of object recognition robustness we can test the ability of a classifier or visual system to tolerate changes in the input distribution up to a certain degree, i.e., to achieve high recognition performance despite being evaluated on a test distribution that differs to some degree from the training distribution (testing under realistic, non-i.i.d. conditions). Using this approach we measure how well DNNs and human observers cope with parametric image manipulations that gradually distort the original image.</p><p>First, we assess how top-performing DNNs that are trained on ImageNet, GoogLeNet <ref type="bibr" target="#b37">[38]</ref>, VGG-19 <ref type="bibr" target="#b38">[39]</ref> and ResNet-152 <ref type="bibr" target="#b39">[40]</ref>, compare against human observers when tested on twelve different distortions such as additive noise or phase noise (see <ref type="figure">Figure 2</ref> for an overview)-in other words, how well do they generalise towards previously unseen distortions. <ref type="bibr" target="#b1">2</ref> In a second set of experiments, we train networks directly on distorted images to see how well they can in general cope with noisy input, and how much training on distortions as a form of data augmentation helps in dealing with other distortions. Psychophysical investigations of human behaviour on object recognition tasks, measuring accuracies depending on image colour (greyscale vs. colour), image contrast and the amount of additive visual noise have been powerful means of exploring the human visual system, revealing much about the internal computations and mechanisms at work <ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref>. As a consequence, similar experiments might yield equally interesting insights into the functioning of DNNs, especially as a comparison to high-quality measurements of human behaviour. In particular, human data for our experiments were obtained using a controlled lab environment (instead of e.g. Amazon Mechanical Turk without sufficient control about presentation times, display calibration, viewing angles, and sustained attention of participants). Our carefully measured behavioural datasets-twelve experiments encompassing a total number of 82,880 psychophysical trials-as well as materials and code are available online at https://github.com/rgeirhos/generalisation-humans-DNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>We here report the core elements of employed paradigm, procedure, image manipulations, observers and DNNs; this is aimed at giving the reader just enough information to understand experiments and results. For in-depth explanations we kindly refer to the comprehensive supplementary material, which seeks to provide exhaustive and reproducible experimental details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Paradigm, procedure &amp; 16-class-ImageNet</head><p>For this study, we developed an experimental paradigm aimed at comparing human observers and DNNs as fair as possible by using a forced-choice image categorisation task. <ref type="bibr" target="#b2">3</ref> Achieving a fair psychophysical comparison comes with a number of challenges: First of all, many highperforming DNNs are trained on the ILSRVR 2012 database <ref type="bibr" target="#b49">[50]</ref> with 1,000 fine-grained categories (e.g., over a hundred different dog breeds). If humans are asked to name objects, however, they most naturally categorise them into so-called entry-level categories (e.g. dog rather than German shepherd). We thus developed a mapping from 16 entry-level categories such as dog, car or chair to their corresponding ImageNet categories using the WordNet hierarchy <ref type="bibr" target="#b50">[51]</ref>. We term this dataset "16-class-ImageNet" since it groups a subset of ImageNet classes into 16 entrylevel categories (airplane, bicycle, boat, car, chair, dog, keyboard, oven, bear, bird, bottle, cat, clock, elephant, knife, truck). In every experiment, then, an image was presented on a computer screen and observers had to choose the correct category by clicking on one of these 16 categories. For pre-trained DNNs, the sum of all softmax values mapping to <ref type="figure">Figure 2</ref>: Example stimulus image of class bird across all distortion types. From left to right, image manipulations are: colour (undistorted), greyscale, low contrast, high-pass, low-pass (blurring), phase noise, power equalisation. Bottom row: opponent colour, rotation, Eidolon I, II and III, additive uniform noise, salt-and-pepper noise. Example stimulus images across all used distortion levels are available in the supplementary material. a certain entry-level category was computed. The entry-level category with the highest sum was then taken as the network's decision. A second challenge is the fact that standard DNNs only use feedforward computations at inference time, while recurrent connections are ubiquitous in the human brain <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>. <ref type="bibr" target="#b3">4</ref> In order to prevent this discrepancy from playing a major confounding role in our experimental comparison, presentation time for human observers was limited to 200 ms. An image was immediately followed by a 200 ms presentation of a noise mask with 1/f spectrum, known to minimise, as much as psychophysically possible, feedback influence in the brain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Observers &amp; pre-trained deep neural networks</head><p>Data from human observers were compared against classification performance of three pre-trained DNNs: VGG-19 <ref type="bibr" target="#b38">[39]</ref>, GoogLeNet <ref type="bibr" target="#b37">[38]</ref> and ResNet-152 <ref type="bibr" target="#b39">[40]</ref>. For each of the twelve experiments that were conducted, either five or six observers participated (with the exception of the colour experiment, for which only three observers participated since similar experiments had already been performed by a number of studies <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>). Observers reported normal or corrected-to-normal vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Image manipulations</head><p>A total of twelve experiments were performed in a well-controlled psychophysical lab setting. In every experiment, a (possibly parametric) distortion was applied to a large number of images, such that the signal strength ranged from 'no distortion / full signal' to 'distorted / weak(er) signal'. We then measured how classification accuracy changed as a function of signal strength. Three of the employed image manipulations were dichotomous (colour vs. greyscale, true vs. opponent colour, original vs. equalised power spectrum); one manipulation had four different levels (0, 90, 180 and 270 degrees of rotation); one had seven levels (0, 30, ..., 180 degrees of phase noise) and the other distortions had eight different levels. Those manipulations were: uniform noise, controlled by the 'width' parameter indicating the bounds of pixel-wise additive uniform noise; low-pass filtering and high-pass filtering (with different standard deviations of a Gaussian filter); contrast reduction (contrast levels from 100% to 1%) as well as three different manipulations from the eidolon toolbox <ref type="bibr" target="#b56">[57]</ref>). The three eidolon experiments correspond to different versions of a parametric image manipulation, with the 'reach' parameter controlling the strength of the distortion. Additionally, for experiments with training on distortions, we also evaluated performance on stimuli with salt-and-pepper noise (controlled by parameter p indicating probability of setting a pixel to either black or white; p ∈ [0, <ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr">80</ref>, 95]%). More information about the different image manipulations is provided in the supplementary material (Section Image preprocessing and distortions), where we also show example images across all manipulations and distortion levels ( <ref type="bibr">Figures 10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14)</ref>. For a brief overview, <ref type="figure">Figure 2</ref> depicts one exemplary manipulation per distortion. Overall, the manipulations we used were chosen to reflect a large variety of possible distortions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training on distortions</head><p>Beyond evaluating standard pre-trained DNNs on distortions (results reported in <ref type="figure" target="#fig_1">Figure 3</ref>), we also trained networks directly on distortions ( <ref type="figure" target="#fig_2">Figure 4</ref>). These networks were trained on 16-classImageNet, a subset of the standard ImageNet dataset as described in Section 2.1. This reduced the size of the unperturbed training set to approximately one fifth. To correct for the highly imbalanced number of samples per class, we weighted each sample in the loss function with a weight proportional to one over the number of samples of the corresponding class. All networks trained in these experiments had a ResNet-like architecture that differed from a standard ResNet-50 only in the number of output neurons that we reduced from 1000 to 16 to match the 16 entry-level classes of the dataset. Weights were initialised with a truncated normal distribution with zero mean and a standard deviation of</p><formula xml:id="formula_0">1 √ n</formula><p>where n is the number of output neurons in a layer. While training from scratch, we performed on-the-fly data augmentation using different combinations of the image manipulations. When training a network on multiple types of image manipulations (models B1 to B9 as well as C1 and C2 of <ref type="figure" target="#fig_2">Figure 4</ref>), the type of manipulation (including unperturbed, i.e. standard colour images if applicable) was drawn uniformly and we only applied one manipulation at a time (i.e., the network never saw a single image perturbed with multiple image manipulations simultaneously, except that some image manipulations did include other manipulations per construction: uniform noise, for example, was always added after conversion to greyscale and contrast reduction to 30%). For a given image manipulation, the amount of perturbation was drawn uniformly from the levels used during test time (cf. <ref type="figure" target="#fig_1">Figure 3</ref>). The remaining aspects of the training followed standard training procedures for training a ResNet on ImageNet: we used SGD with a momentum of 0.997, a batch size of 64, and an initial learning rate of 0.025. The learning rate was multiplied with 0.1 after 30, 60, 80 and 90 epochs (when training for 100 epochs) or 60, 120, 160 and 180 epochs (when training for 200 epochs). Training was done using TensorFlow 1.6.0 <ref type="bibr" target="#b57">[58]</ref>. In the training experiments, all manipulations with more than two levels were included except for the eidolon stimuli, since the generation of those stimuli is computationally too slow for ImageNet training. For comparison purposes, we additionally included colour vs. greyscale as well as salt-and-pepper noise (for which there is no human data, but informal comparisons between uniform noise and salt-and-pepper noise strongly suggest that human performance will be similar, see <ref type="figure" target="#fig_0">Figure 1c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Generalisation of humans and pre-trained DNNs towards distortions</head><p>In order to assess generalisation performance when the signal gets weaker, we tested twelve different ways of degrading images. These images at various levels of signal strength were then shown to both human observers in a lab and to pre-trained DNNs (ResNet-152, GoogLeNet and VGG-19) for classification. The results of this comparison are visualised in <ref type="figure" target="#fig_1">Figure 3</ref>. While human and DNN performance was similar for comparatively minor colour-related distortions such as conversion to greyscale or opponent colours, we find human observers to be more robust for all of the other distortions: by a small margin for low contrast, power equalisation and phase noise images and by a larger margin for uniform noise, low-pass, high-pass, rotation and all three eidolon experiments. Furthermore, there are strong differences in the error patterns as measured by the response distribution entropy (indicating biases towards certain categories). Human participants' responses were distributed more or less equally amongst the 16 classes, whereas all three DNNs show increasing biases towards certain categories when the signal gets weaker. These biases are not completely explained by the prior class probabilities, and deviate from distortion to distortion. For instance, ResNet-152 almost solely predicts class bottle for images with strong uniform noise (irrespective of the ground truth category), <ref type="bibr" target="#b4">5</ref> and classes dog or bird for images distorted by phase noise. One might think of simple tricks to reduce the discrepancy between the response distribution entropy of DNNs and humans. One possible way would be increasing the softmax temperature parameter and assuming that model decisions are sampled from the softmax distribution rather than taking the argmax. However, increasing the response DNN distribution entropy in this way dramatically decreases classification accuracy and thus comes with a trade-off (cf. <ref type="figure">Figure 8</ref> in the supplementary material).</p><p>These results are in line with previous findings reporting human-like processing of chromatic information in DNNs <ref type="bibr" target="#b18">[19]</ref>   noise and blur <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref>. Overall, DNNs seem to have much more problems generalising to weaker signals than humans, across a wide variety of image distortions. While the human visual system has been exposed to a number of distortions during evolution and lifetime, we clearly had no exposure whatsoever to many of the exact image manipulations that we tested here. Thus, our human data show that a high level of generalisation is, in principle, possible. There may be many different reasons for the discrepancy between human and DNN generalisation performance that we find: Are there limitations in terms of the currently used network architectures (as hypothesised by <ref type="bibr" target="#b59">[60]</ref>), which may be inferior to the human brain's intricate computations? Is it a problem of the training data (as suggested by e.g. <ref type="bibr" target="#b60">[61]</ref>), or are today's training methods / optimisers not sufficient to solve robust and general object recognition? In order to shed light on the dissimilarities we found, we performed a second batch of experiments by training networks directly on distorted images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training DNNs directly on distorted images</head><p>We trained one network per distortion directly and from scratch on (potentially manipulated) 16-classImageNet images. The results of this training are visualised in <ref type="figure" target="#fig_2">Figure 4</ref> (models A1 to A9). We find that these specialised networks consistently outperformed human observers, by a large margin, on the image manipulation they were trained on (as indicated by strong network performance on the diagonal). This is a strong indication that currently employed architectures (such as ResNet-50) and training methods (standard optimiser and training procedure) are sufficient to 'solve' distortions under i.i.d. train/test conditions. We were able to not only close the human-DNN performance gap that was observed by <ref type="bibr" target="#b12">[13]</ref> (who fine-tuned networks on distortions, reporting improved but not human-level DNN performance) but to surpass human performance in this respect. While the human visual system certainly has a much more complicated structure <ref type="bibr" target="#b23">[24]</ref>, this does not seem to be necessary to deal with even strong image manipulations of the type employed here.</p><p>However, as noted earlier, robust generalisation is primarily not about solving a specific problem known exactly in advance. We therefore tested how networks trained on a certain distortion type perform when tested on other distortions. These results are visualised in <ref type="figure" target="#fig_2">Figure 4</ref> by the off-diagonal cells of models A1 to A9. Overall, we find that training on a certain distortion slightly improves performance on other distortions in a few instances, but is detrimental in other cases (when compared to a vanilla ResNet-50 trained on colour images, model A1 in the figure). <ref type="bibr" target="#b5">6</ref> Performance on salt-andpepper noise as well as uniform noise was close to chance level for all networks, even for a network trained directly on the respectively other noise model. This may be surprising given that these two types of noise do not seem very different to a human eye (as indicated in <ref type="figure" target="#fig_0">Figure 1c)</ref>. Hence, training a network on one distortion does not generally lead to improvements on other distortions.</p><p>Since training on a single distortion alone does not seem to be sufficient to evoke robust generalisation performance in DNNs, we also trained the same architecture (ResNet-50) on two additional settings. Models B1 to B9 in <ref type="figure" target="#fig_2">Figure 4</ref> show performance for training on one particular distortion in combination with uniform noise (training consisted of 50% images from each manipulation). Uniform noise was chosen since it seemed to be one of the hardest distortions for all networks, and hence they might benefit from including this particular distortion in the training data. Furthermore, we trained models C1 and C2 on all but one distortion (either uniform or salt-and-pepper noise was left out).</p><p>We find that object recognition performance of models B1 to B9 is improved compared to models A1 to A9, both on the distortions they were actually trained on (diagonal entries with red rectangles in <ref type="figure" target="#fig_2">Figure 4</ref>) as well as on a few of the distortions that were not part of the training data. However, this improvement may be largely due to the fact that models B1 to B9 were trained on 200 epochs instead of 100 epochs as for models A1 to A9, since the accuracy of model B9 (trained &amp; tested on uniform noise, 200 epochs) also shows an improvement towards model A9 (trained &amp; tested on uniform noise, 100 epochs). Hence, in the presence of heavy distortions, training longer may go a long way but incorporating other distortions in the training does not seem to be generally beneficial to model performance. Furthermore, we find that it is possible even for a single model to reach high accuracies on all of the eight distortions it was trained on (models C1 and C2), however for both left-out uniform and salt-and-pepper noise, object recognition accuracy stayed around 11 to 14%, which is by far closer to chance level (approx. 6%) than to the accuracy reached by a specialised network trained on this exact distortion (above 70%, which serves as a lower bound on the achievable performance).</p><p>Taken together, these findings indicate that data augmentation with distortions alone may be insufficient to overcome the generalisation problem that we find. It may be necessary to move from asking "why are DNNs generalising so well (under i.i.d. settings)?" <ref type="bibr" target="#b28">[29]</ref> to "why are DNNs generalising so poorly (under non-i.i.d. settings)?". It is up to future investigations to determine how DNNs that are currently being handled as computational models of human object recognition can solve this challenge. At the exciting interface between cognitive science / visual perception and deep learning, inspiration and ideas may come from both fields: While the computer vision sub-area of domain adaptation (see <ref type="bibr" target="#b62">[63]</ref> for a review) is working on robust machine inference in spite of shifts in the input distribution, the human vision community is accumulating strong evidence for the benefits of local gain control mechanisms. These normalisation processes seem to be crucial for many aspects of robust animal and human vision <ref type="bibr" target="#b45">[46]</ref>, are predictive for human vision data <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b63">64]</ref> and have proven useful in the context of computer vision <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b65">66]</ref>. It could be an interesting avenue for future research to determine whether there is a connection between neural normalisation processes and DNN generalisation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We conducted a behavioural comparison of human and DNN object recognition robustness against twelve different image distortions. In comparison to human observers, we find the classification performance of three well-known DNNs trained on ImageNet-ResNet-152, GoogLeNet and VGG-19-to decline rapidly with decreasing signal-to-noise ratio under image distortions. Additionally, we find progressively diverging patterns of classification errors between humans and DNNs with weaker signals. Our results, based on 82,880 psychophysical trials under well-controlled lab conditions, demonstrate that there are still marked differences in the way humans and current DNNs process <ref type="bibr" target="#b5">6</ref> The no free lunch theorem <ref type="bibr" target="#b61">[62]</ref> states that better performance on some input is necessarily accompanied by worse performance on other input; however we here are only interested in a very narrow subset of the possible input space-namely, natural images corrupted by distortions. The high accuracies of human observers across distortions indicate that it is, in principle, possible to achieve good performance on many distortions simultaneously. object information. These differences, in our setting, cannot be overcome by training on distorted images (i.e., data augmentation): While DNNs cope perfectly well with the exact distortion they were trained on, they still show a strong generalisation failure towards previously unseen distortions. Since the space of possible distortions is literally unlimited (both theoretically and in real-world applications), it is not feasible to train on all of them. DNNs have a generalisation problem when it comes to settings that go beyond the usual (yet often unrealistic) i.i.d. assumption. We believe that solving this generalisation problem will be crucial both for robust machine inference and towards better models of human object recognition, and we envision that our findings as well as our carefully measured and freely available behavioural data 7 may provide a new useful benchmark for improving DNN robustness and a motivation for neuroscientists to identify mechanisms in the brain that may be responsible for this remarkable robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author contributions</head><p>The initial project idea of comparing humans against DNNs was developed by F.A.W. and R.G. All authors jointly contributed towards designing the study and interpreting the data. R.G. and C.R.M.T. We would like to thank David Janssen for his invaluable contributions in shaping the early stage of this project. Furthermore, we are very grateful to Tom Wallis for providing the MATLAB source code of one of his experiments, and for allowing us to use and modify it; Silke Gramer for administrative and Uli Wannek for technical support, as well as Britta Lewke for the method of creating response icons and Patricia Rubisch for help with testing human observers. Moreover, we would like to thank Nikolaus Kriegeskorte, Jakob Macke and Tom Wallis for helpful feedback, and three anonymous reviewers for constructive suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material</head><p>While the main aspects of employed paradigm, procedure, observers and DNNs were already mentioned earlier, this section aims at providing exhaustive and reproducible experimental details. Furthermore, <ref type="figure" target="#fig_8">Figure 6</ref> examines how network uncertainty develops as a function of signal strength, and <ref type="figure" target="#fig_9">Figure 7</ref> shows the classification accuracy of networks trained on distortions across all conditions. All data, if not stated otherwise, were analysed using R version 3.2.3 <ref type="bibr" target="#b66">[67]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paradigm &amp; procedure</head><p>A schematic of a typical trial is shown in <ref type="figure" target="#fig_4">Figure 5</ref>. Prior to starting the experiment, all participants were shown the response screen and asked to name all categories to ensure that the task was fully clear. They were instructed to click on the category that they thought resembles the image best, and to guess if they were unsure. They were allowed to change their choice within the 1500 ms response interval; the last click on a category icon of the response screen was counted as the answer. The experiment was not self-paced, i.e. the response screen was always visible for 1500 ms and thus, each experimental trial lasted exactly 2200 ms (300 ms + 200 ms + 200 ms + 1500 ms). During the whole experiment, the screen background was set to a grey value of 0.454 in the [0, 1] range, corresponding to the mean greyscale value of all images in the dataset (41.17 cd/m 2 ).</p><p>On separate days we conducted twelve different experiments. The number of trials per experiment is reported in <ref type="table" target="#tab_2">Table 1</ref>. For each experiment, we randomly chose between 70 and 80 images per category from the pool of images without replacement (i.e., no observer ever saw an image more than once throughout the entire experiment). Within each category, all conditions were counterbalanced. Random stimulus selection was done individually for each participant to reduce the influence of any accidental bias in the image selection. Images within the experiments were presented in randomised order. After 256 trials (colour, uniform noise and eidolon experiments), 128 trials (contrast experiment) and 160 trials (remaining experiments), the mean performance of the last block was displayed on the screen, and observers were free to take a short break. Ahead of each experiment, all observers conducted approximately 10 minutes of practice trials to gain familiarity with the task and the position of the categories on the response screen. Trials in which human observers failed to click on any category were recorded as an incorrect answer in the data analysis, and are shown as a separate category (top row) in the confusion matrices (DNNs, obviously, never fail to respond). Such a failure to respond occurred, on average, in only 1.91% of trials per experiment-one of the advantages of controlled laboratory studies (SD = 0.69%). Then, a response screen appeared for 1500 ms, during which the observer clicked on a category. Note that we increased the contrast of the noise mask in this figure for better visibility when printed. Categories rowwise from top to bottom: knife, bicycle, bear, truck, airplane, clock, boat, car, keyboard, oven, cat, bird, elephant, chair, bottle, dog. The icons are a modified version of the ones from the MS COCO website (http://mscoco.org/explore/).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apparatus</head><p>All stimuli were presented on a VIEWPixx LCD monitor (VPixx Technologies, Saint-Bruno, Canada) in a dark chamber. The 22" monitor (484 × 302 mm) had a spatial resolution of 1920 × 1200 pixels at a refresh rate of 120 Hz. Stimuli were presented at the center of the screen with 256 × 256 pixels, corresponding, at a viewing distance of 123 cm, to 3 × 3 degrees of visual angle. A chin rest was used in order to keep the position of the head constant over the course of an experiment. Stimulus presentation and response recording were controlled using MATLAB (Release 2016a, The MathWorks, Inc., Natick, Massachusetts, U.S.) and the Psychophysics Toolbox extensions version 3.0.12 <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69]</ref> along with the iShow library 8 on a desktop computer (12 core CPU i7-3930K, AMD HD7970 graphics card "Tahiti" by AMD, Sunnyvale, California, United States) running Kubuntu 14.04 LTS. Responses were collected with a standard computer mouse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observers</head><p>Three observers participated in the colour experiment (all male; 22 to 28 years; mean: 25 years) and in the contrast experiment. Six observers participated in the opponent colour, high-pass filter, low-pass filter, phase noise and power equalisation experiments (three female, three male; 20 to 25 years; mean: 22 years). In the other two experiments, five observers took part (uniform noise experiment: one female, four male; 20 to 28 years; mean: 23 years; eidolon experiments: three female, two male; 19 to 28 years; mean: 22 years). Subject-01 is an author and participated in all but the eidolon experiments. All other participants were either paid e 10 per hour for their participation or gained course credit. All observers were students and reported normal or corrected-to-normal vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-trained deep neural networks</head><p>We used GoogLeNet <ref type="bibr" target="#b37">[38]</ref>, VGG-19 <ref type="bibr" target="#b38">[39]</ref> and ResNet-152 <ref type="bibr" target="#b39">[40]</ref> for our analyses. For all three networks, we used the pretrained implementations as provided by the TensorFlow-Slim framework <ref type="bibr" target="#b8">9</ref> and programmed in the TensorFlow library for machine learning <ref type="bibr" target="#b57">[58]</ref>. The individual pretrained weights were also downloaded from the latter GitHub repository. We validated that our installation reproduced the classification accuracies provided on the website. The networks' input were 224 × 224 pixel RGB images. For greyscale images, we set all three channels to be equal to the greyscale image's single channel. Images were fed through the networks using a single feedforward pass. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Categories and image database</head><p>The images serving as psychophysical stimuli were images extracted from the training set of the ImageNet Large Scale Visual Recognition Challenge 2012 database <ref type="bibr" target="#b49">[50]</ref>. This database contains millions of labeled images grouped into 1,000 very fine-grained categories (e.g., over a hundred different dog breeds). If human observers are asked to name objects, however, they most naturally categorise them into so-called basic or entry-level categories, e.g. dog rather than German shepherd <ref type="bibr" target="#b69">[70]</ref>. The Microsoft COCO (MS COCO) database <ref type="bibr" target="#b70">[71]</ref> is an image database structured according to 91 such entry-level categories, making it an excellent source of categories for an object recognition task. Thus for our experiments we fused the carefully selected entry-level categories in the MS COCO database with the large quantity of images in ImageNet. Using WordNet's hypernym relationship (x is a hypernym of y if y is a "kind of" x, e.g., dog is a hypernym of German shepherd), we mapped every ImageNet label to an entry-level category of MS COCO in case such a relationship exists, retaining 16 clearly non-ambiguous categories with sufficiently many images within each category (see <ref type="figure" target="#fig_4">Figure 5</ref> for a iconic representation of the 16 categories). A complete list of ImageNet labels used for the experiments can be found in our online repository. <ref type="bibr" target="#b9">10</ref> Since all investigated DNNs, when shown an image, output classification predictions for all 1,000 ImageNet categories, we disregarded all predictions for categories that were not mapped to any of the 16 entry-level categories. For each of those 16 categories we summed over the predictions of all ImageNet categories mapping to that particular entry-level category. Then the entry-level category with the highest summed prediction was selected as the network's response. This way, the DNN response selection corresponds directly to the forced-choice paradigm for our human observers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image preprocessing and distortions</head><p>We used Python for all image preprocessing (Version 2.7.11) and for running experiments through pre-trained networks (Version 3.5). From the pool of ImageNet images of the 16 entry-level categories, we excluded all greyscale images (1%) as well as all images not at least 256 × 256 pixels in size (11% of non-greyscale images). We then cropped all images to a center patch of 256 × 256 pixels as follows: First, every image was cropped to the largest possible center square. This center square was then downsampled to the desired size with PIL.Image.thumbnail((256, 256), Image.ANTIALIAS). Human observers get adapted to the mean luminance of the display during experiments, and thus images which are either very bright or very dark may be harder to recognise due to their very different perceived brightness. We therefore excluded all images which had a mean deviating more than two standard deviations from that of other images (5% of correct-sized colour-images excluded). In total we retained 213,555 images from ImageNet.</p><p>For the experiments using greyscale images the stimuli were converted using the rgb2gray method <ref type="bibr" target="#b71">[72]</ref> in Python. This was the case for all experiments and conditions except for the 'colour' condition of the colour experiment, as well as for the opponent colour experiment. For the contrast experiment, we employed eight different contrast levels c ∈ {1,  <ref type="figure" target="#fig_0">Figure 10</ref> for example stimuli. For the salt and pepper noise experiment, used in DNN training experiments, we also scaled the greyscale image to a contrast level of 30% prior to adding noise in order to ensure maximal comparability with the uniform noise experiment. Salt and pepper noise, i.e. setting pixels to either black or white, was drawn pixelwise with a certain probability p, p ∈ {0, 10, 20, 35, 50, 65, 80, 95}%. See <ref type="figure" target="#fig_0">Figure 14</ref> for example salt-and-pepper stimuli at all conditions. For the opponent colours experiment, our aim was to produce images that would be perceived by human observers as having exactly the opposite colours of the original, while retaining the same luminance. Therefore, we converted images to a colour space in which we could invert the colours without affecting luminance values. One such colour space is the Derrington-Krauskopf-Lennie (DKL) colour space <ref type="bibr" target="#b72">[73]</ref>. In order to account for the nonlinearity of our experimental display monitor, we measured the emitted luminance for RGB grey values between 0 and 255. From this we built a lookup table from RGB grey values to actual emitted luminance values fmonitor. To evaluate how much the human retina's long-, middle-, and short-wave receptors would be excited by the colours presented on the monitor, we measured the intensity of all emitted wave lengths between 390-780 nm for the RGB values (255 0 0), (0 255 0), (0 0 255), respectively. We then multiplied the respective emitted spectra between 390-780 nm with the corresponding cone sensitivities taken from the 2-deg LMS fundamentals proposed by <ref type="bibr" target="#b73">[74]</ref> and summed over them. This resulted in a matrix C from RGB to cone activities (LMS space). Then we calculated a conversion matrix D of cone activities into the DKL colour space following the conversion example in <ref type="bibr" target="#b74">[75]</ref>. An image was, consequently, converted from RGB to DKL by applying fmonitor to it and subsequent multiplication with first C and then D. The DKL space has three channels reminiscent of the opponent colour process of the human visual system <ref type="bibr" target="#b74">[75]</ref>. They are DKL lum , a luminance channel, DKLL−M , a channel representing the difference between long-and middle-wave receptor activation, as well as DKL S−lum , a channel representing the difference between the activation of the short-wave receptor and the luminance. Since we wanted to keep the luminance unchanged, we multiplied the DKLL−M and DKL S−lum channels with the value '-1'. Subsequently, we converted the manipulated images back to RGB using the inverse matrices of D and C and then applied the inverse of fmonitor to them. All resulting pixel values outside the range [0, 1] were clipped to 0 or 1. This only happened for 0.34% of pixels with a mean clipped away value of 0.004. This corresponds to the minimal colour intensity step as 0.004 ≈ For the low-pass and high-pass experiments we used the scipy.ndimage.filters.gaussian_filter() function. The low-pass experiment's eight conditions differed in the standard deviation of the Gaussian filter. Standard deviations were 0 (original image), 1, 3, 7, 10, 15 and 40 pixels <ref type="figure" target="#fig_0">(Figure 11</ref>. We used constant padding with the mean pixel value over the testing images (0.4423) and truncation at four standard deviations. The high-pass experiment also had eight conditions. Standard deviations were 0.4, 0.45, 0.55, 0.7, 1, 1.5, 3 pixels and inf (original image) <ref type="figure" target="#fig_0">(Figure 11</ref>). The high-pass filtered images were produced by subtracting a low-pass filtered image as described above from the original image. However, many of the high-pass filtered images' pixels fell outside the [0, 1] range. To resolve this, we calculated the difference between the mean pixel value over all test images (0.4423) and the mean pixel value of the high-pass filtered image. That difference was added back to the image. This had the effect that images approached a uniform mean grey image of value 0.4423 for low standard deviations. For both experiments pixel values were clipped to the [0, 1] range, if lying outside after the filtering. This only happened for &lt;0.001% of pixels with a mean clipped away value of &lt;0.001 for the both filtering experiments.</p><p>We implemented the equalisation of the power spectra and phase noise in the Fourier domain. Conversion to frequency domain was accomplished by a fast Fourier transform through the application of the fft2() and then fftshift() functions of the Python package scipy.fftpack. This results in a matrix of complex numbers F , which represents both the phases and amplitudes of the individual frequencies in one complex number. F is organised in symmetric pairs of complex numbers with just their imaginary part differing in its sign and cancelling each other out when reversing the Fourier transform again. When transforming F to polar coordinates, the angle represents the respective frequency's phase and the distance from the origin represents its amplitude. Hence, we extracted the phases and amplitudes of the individual frequencies with the functions numpy.angle(F) and numpy.abs(F), respectively. The power equalisation experiment had two conditions: original and power-equalised ( <ref type="figure" target="#fig_0">Figure 13</ref>). For the power-equalised images, we first calculated the mean amplitude spectrum over all test images, which showed the typical 1 f shape [e.g. <ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77]</ref>. Thereafter, we set all images amplitudes to the mean amplitude spectrum. Since the power spectrum is the square of the amplitude spectrum, the images were essentially power-equalised. There were seven conditions in the phase noise experiment. These were 0, 30, 60, 90, 120, 150 and 180 degrees noise width w <ref type="figure" target="#fig_0">(Figure 13</ref>). To each frequency's phase a phase shift randomly drawn from a continuous uniform distribution over the interval [−w, w] was added. To ensure that the imaginary parts would later cancel out again, we added the same phase noise to both frequencies of each symmetric pair. After performing the respective manipulations, a Fnew was calculated by recombining the new phases and amplitudes. Then we did an inverse Fourier transform using ifftshift() and then ifft2(). Finally we clipped all pixel values to the [0, 1] range. This was the case for 0.038% of pixels with a mean clipped value of about 0.003 for the phase noise experiment and for 0.013% of pixels with a mean clipped value of 0.005 for the power-equalisation experiment.</p><p>There were four conditions for the rotation experiment: 0 (original), 90, 180 and 270 degrees rotation angle. Rotation by 90 degrees was accomplished by first transposing the image matrix and then reversing the column order. Rotation by 180 degrees was done by reversing both, row and column ordering. Rotation by 270 degrees was implemented by first reversing the images columns and then transposing it.</p><p>For the eidolon experiments, all stimuli were generated using the eidolon toolbox for Python <ref type="bibr" target="#b10">11</ref> , more specifically its PartiallyCoherentDisarray(image, reach, coherence, grain) function. Using a combination of the three parameters reach, coherence and grain, one obtains a distorted version of the original image (a so-called eidolon). The parameters reach and coherence were varied in the experiment; grain was held constant with a value of 10.0 throughout the experiment (grain indicates how fine-grained the distortion is; a value of 10.0 corresponds to a medium-grainy distortion). Reach ∈ {1.0, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental modifications</head><p>Our psychophysical experiments were conducted in two batches and over an extended period of time. After completing the first batch of experiments (all experiments on the left half of <ref type="figure" target="#fig_1">Figure 3</ref>, i.e. a, c, e, g, i and k), we performed a number of modifications for the second batch of experiments. We here briefly list all the changes in which the second batch of experiments differed from the previously reported methods.</p><p>Noise mask: In the human experiments, each experimental image was immediately followed by a 1 f pink noise mask (cf. <ref type="figure" target="#fig_4">Figure 5</ref>). In the second batch of experiments this noise mask was enhanced to improve its masking effect. This was done by multiplying each pixel value by four. Values greater than 1 or smaller than 0 due to the multiplication were then clipped to 1 or 0.</p><p>Cropping vs. downsampling: In the first experimental batch, humans saw 256 × 256 images. However, DNN classification was based on those images 224 × 224 centre crop. Thus, humans and DNNs saw slightly different images. Therefore, we used downsampling to 224 × 224 for the second batch of both, human and DNN experiments. As a consequence, the mean grey pixel value over all experimental images and hence the background grey value for presenting those images changed slightly from 0.454 to 0.442 in the [0, 1] range. JPEG vs. PNG: All images of the first batch of experiments, prior to showing them to human observers or DNNs, were saved in the JPEG format using the default settings of the skimage.io.imsave function. The JPEG format was chosen because the image training database for all three networks, ImageNet <ref type="bibr" target="#b49">[50]</ref>, consists of JPEG images. However, as the lossy compression of JPEG may introduce artefacts, we also examined the difference in DNN results between saving to JPEG and to PNG, which is lossless up to rounding issues. We therefore ran all those DNN experiments additionally saving them in the (up to rounding issues) lossless PNG format. We did not find any noteworthy differences for the colour, noise, and eidolon experiments. However, for the contrast experiment, the networks achieved on average better results for PNG images. We therefore tested three human observers additionally on the same stimuli (PNG instead of JPEG images). In this experiment, three of the JPEG experiment's five observers participated for maximal comparability. <ref type="bibr" target="#b11">12</ref> We found human observers to be better for PNG images as well. In absolute terms, participants were 2.68% better on average. In order to disentangle the influence of JPEG compression and image manipulations, we used PNG images for all other experiments, that is for the false colour, phase noise, power equalisation, rotation, high-pass and low-pass experiments as well as for the DNN training experiments.</p><p>Python Version: The second batch used Python Version 3.5 instead of Python 2.7 for image preprocessing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error bars &amp; entropy</head><p>When showing accuracy in any of the plots, the error bars provided report the range of the data observed for different observers (not the often shown S.E. of the means, which would be much smaller). To produce a comparable measure of uncertainty for the DNNs, we computed seven runs with different subsets of the data, with each run consisting of the same number of images per category and condition that a single human observer was exposed to and report the range of accuracies observed in these runs. Seven runs are the maximum possible number of runs without ever showing an image to a DNN more than once per experiment.</p><p>For all response distribution entropy results <ref type="figure" target="#fig_1">(Figures 3 and 7)</ref>, we calculated the entropy as the average of individual participants' entropies: otherwise, if the entropy was calculated over the aggregated human trials, individual differences might cancel each other out, which would lead to a higher human response distribution entropy. <ref type="figure" target="#fig_8">Figure 6</ref> shows the entropy of the networks' predictions over the 1000 ILSVRC12 classes as a measure of the networks' 'uncertainty'. In principle, the more uncertain a network is in its predictions the more evenly it will distribute its softmax activations between classes and thus the higher the entropy will be. For all experiments, uncertainty roughly increases with distortion strength as reported by previous studies <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref>. However, for uniform noise and the eidolon distortions all networks become more certain again for some higher distortion levels. Furthermore, ResNet-152 also becomes more certain for stronger distortions in the low-pass experiment and is consistently more confident in its predictions than GoogleNet and VGG-19. Thus there are distortions for which all networks and especially ResNet-152 fail to capture that the input signal is becoming worse. Instead they limit their predictions to only a few classes (cf. <ref type="figure" target="#fig_1">Figure 3</ref>) with high certainty. This result is in line with previous reports stating that uncertainty in standard discriminative deep neural networks is not well represented [e.g. 78].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction uncertainty</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network training results across all distortion conditions</head><p>While <ref type="figure" target="#fig_2">Figure 4</ref> shows performance of networks trained on distortions for a single distortion level per manipulation, we here report the performance across all stimulus levels. In <ref type="figure" target="#fig_9">Figure 7</ref>, the performance of a vanilla ResNet-50 : Classification accuracy and response distribution entropy for human observers, ResNet-50 as well as an All-Distortions-Net and a Specialised-Net. All networks are trained from scratch; the Specialised-Net in every plot is trained on a single distortion (models A1 to A9 in <ref type="figure" target="#fig_2">Figure 4</ref>) whereas the All-Distortions-Net is trained on a number of distortions simultaneously. This corresponds to models C1 and C2 in <ref type="figure" target="#fig_2">Figure 4</ref>: for subplot 7d, salt-and-pepper noise, performance of model C1 is shown. For subplot 7b, uniform noise, performance of C2 is shown. For all other plots, performance of the All-Distortions-Net is shown as the mean of performance for models C1 and C2.</p><p>is compared against a network with the same architecture that is trained on a distortion directly (refered to as 'Specialised-Net'), as well as to a network that is trained on all distortions simultaneously (named 'AllDistortions-Net'). Object recognition accuracy shows a relatively consistent pattern across experiments: human performance is better than the performance of a vanilla ResNet-50. However, both an All-Distortions-Net and a Specialised-Net reach extremely high accuracies, with the Specialised-Net being either on par with or slightly better than the All-Distortions-Net. Interestingly, the response distribution entropy of those two networks is largely human-like, i.e. close to 4 bits of entropy (or no bias towards a certain category), even for conditions where the overall accuracy is low (e.g. for the difficult conditions of uniform and salt-and-pepper noise). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Classification performance of ResNet-50 trained from scratch on (potentially distorted) ImageNet images. (a) Classification performance when trained on standard colour images and tested on colour images is close to perfect (better than human observers). (b) Likewise, when trained and tested on images with additive uniform noise, performance is super-human. (c) Striking generalisation failure: When trained on images with salt-and-pepper noise and tested on images with uniform noise, performance is at chance level-even though both noise types do not seem much different to human observers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Classification accuracy and response distribution entropy for GoogLeNet, VGG-19 and ResNet-152 as well as for human observers. 'Entropy' indicates the Shannon entropy of the response/decision distribution (16 classes). It here is a measure of bias towards certain categories: using a test dataset that is balanced with respect to the number of images per category, responding equally frequently with all 16 categories elicits the maximum possible entropy of four bits. If a network or observer responds prefers some categories over others, entropy decreases (down to zero bits in the extreme case of responding with one particular category all the time, irrespective of the ground truth category). Human 'error bars' indicate the full range of results across participants. Image manipulations are explained in Section 2.3 and visualised in Figures 10, 11, 12, 13 and 14.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Classification accuracy (in percent) for networks with potentially distorted training data. Rows show different test conditions at an intermediate difficulty (exact condition indicated in brackets, units as in Figure 3). Columns correspond to differently trained networks (leftmost column: human observers for comparison; no human data available for salt-and-pepper noise). All of the networks were trained from scratch on (a potentially manipulated version of) 16-class-ImageNet. Manipulations included in the training data are indicated by a red rectangle; additionally 'greyscale' is underlined if it was part of the training data because a certain distortion encompasses greyscale images at full contrast. Models A1 to A9: ResNet-50 trained on a single distortion (100 epochs). Models B1 to B9: ResNet-50 trained on uniform noise plus one other distortion (200 epochs). Models C1 &amp; C2: ResNet-50 trained on all but one distortion (200 epochs). Chance performance is at 1 16 = 6.25% accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>developed the image manipulations and acquired the behavioural data with input from H.H.S. and F.A.W.; J.R. trained networks on distortions; experimental data and networks were evaluated by C.R.M.T., R.G. and J.R. with input from H.H.S, M.B. and F.A.W.; R.G. and C.R.M.T. worked on making our work reproducible (data, code and materials openly accessible; writing supplementary material); R.G. wrote the paper with significant input from all other authors. Acknowledgments This work has been funded, in part, by the German Federal Ministry of Education and Research (BMBF) through the Bernstein Computational Neuroscience Program Tübingen (FKZ: 01GQ1002) as well as the German Research Foundation (DFG; Sachbeihilfe Wi 2103/4-1 and SFB 1233 on "Robust Vision"). The authors thank the International Max Planck Research School for Intelli- gent Systems (IMPRS-IS) for supporting R.G. and J.R.; J.R. acknowledges support by the Bosch Forschungsstiftung (Stifterverband, T113/30057/17); M.B. acknowledges support by the Centre for Integrative Neuroscience Tübingen (EXC 307) and by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PC00003.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Schematic of a trial. After the presentation of a central fixation square (300 ms), the image was visible for 200 ms, followed immediately by a noise mask with 1/f spectrum (200 ms). Then, a response screen appeared for 1500 ms, during which the observer clicked on a category. Note that we increased the contrast of the noise mask in this figure for better visibility when printed. Categories rowwise from top to bottom: knife, bicycle, bear, truck, airplane, clock, boat, car, keyboard, oven, cat, bird, elephant, chair, bottle, dog. The icons are a modified version of the ones from the MS COCO website (http://mscoco.org/explore/).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>3, 5, 10, 15, 30, 50, 100%}. For an image in the [0, 1] range, scaling the image to a new contrast level c was achieved by computing new_value = c 100% · original_value + 1 − c 100% 2 10 https://github.com/rgeirhos/generalisation-humans-DNNs for each pixel. For the uniform noise experiment, we first scaled all images to a contrast level of c = 30%. Subsequently, white uniform noise of range [−w, w] was added pixelwise,In case this resulted in a value out of the [0, 1] range, this value was clipped to either 0 or 1. By design, this never occurred for a noise range less or equal to 0.35 due to the reduced contrast (see above). For w = 0.6, clipping occurred in 17.2% of all pixels and for w = 0.9 in 44.4% of all pixels. See</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>128.0} is an amplitude-like parameter indicating the strength of the distortion, coherence ∈ {0.0, 0.3, 1.0} defines the relationship between local and global image structure. Those two parameters were fully crossed, resulting in 8 · 3 = 24 different eidolon conditions. A high coherence value "retains the local image structure even when the global image structure is destroyed" [57, p. 10]. A coherence value of 0.0 corresponds to 'completely incoherent', a value of 1.0 to 'fully coherent'. The third value 0.3 was chosen because it produces images that perceptually lie-as informally determined by the authors-in the middle between those two extremes. See Figures 12 and 13 for example eidolon stimuli. The coherence levels of 1.0, 0.3 and 0.0 are refered to as eidolon experiment I, II and III throughout the paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Mean entropy of the probabilities for the 1000 ILSVRC classes for GoogLeNet, VGG-19 and ResNet-152. Dotted line indicates the maximum possible entropy. This is a measure of network 'uncertainty'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7</head><label>7</label><figDesc>Figure 7: Classification accuracy and response distribution entropy for human observers, ResNet-50 as well as an All-Distortions-Net and a Specialised-Net. All networks are trained from scratch; the Specialised-Net in every plot is trained on a single distortion (models A1 to A9 in Figure 4) whereas the All-Distortions-Net is trained on a number of distortions simultaneously. This corresponds to models C1 and C2 in Figure 4: for subplot 7d, salt-and-pepper noise, performance of model C1 is shown. For subplot 7b, uniform noise, performance of C2 is shown. For all other plots, performance of the All-Distortions-Net is shown as the mean of performance for models C1 and C2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :Figure 9 :Figure 10 :Figure 11 :Figure 12 :Figure 13 :Figure 14 :</head><label>891011121314</label><figDesc>Figure 8: Classifcation accuracy (a) and response distribution entropy (b) as well as the trade-off between accuracy and entropy (c, d, e) for different softmax temperatures when the decision of a ResNet-50 model is sampled from its distribution over classes (softmax output) rather than taking the argmax of the distribution (which is equivalent to sampling with temperature → 0). While increasing the temperature does increase the response distribution entropy of ResNet-50, it simultaneously decreases the classification accuracy. For uniform noise with a width of 0.1 (d), increasing the temperature to match the response distribution entropy of humans reduces the accuracy of ResNet-50 below 0.1 whereas human accuracy is at 0.75.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>but strong decreases in DNN recognition accuracy for image degradations like</figDesc><table>88.5 96.7 

8.1 
50.0 83.1 
90.8 
86.1 84.2 
95.9 
95.5 
10.4 
10.2 
90.6 
11.2 
97.9 95.4 72.3 93.0 91.1 92.4 94.9 

86.6 87.8 
9.8 
94.1 86.2 
90.5 
93.2 87.8 
95.1 
94.8 
10.3 
11.4 
95.6 
12.8 
94.0 96.8 96.2 93.3 95.7 94.3 90.9 

47.6 13.1 
29.0 
89.4 19.6 
10.2 
39.8 17.1 
88.2 
90.9 
28.6 
34.6 
14.2 
37.9 
46.3 51.7 95.1 50.5 79.1 59.4 45.2 

49.8 21.1 
20.6 
29.9 11.7 
8.3 
92.6 27.7 
90.7 
91.4 
10.4 
18.9 
24.7 
19.8 
25.1 22.8 29.2 25.0 94.3 27.5 28.3 

48.5 18.9 
6.6 
16.4 78.4 
9.8 
11.9 16.0 
74.9 
74.7 
6.9 
7.1 
16.1 
9.3 
16.0 18.6 14.4 87.2 20.5 13.8 13.5 

45.6 6.2 
80.3 
6.9 9.0 
6.0 
7.3 6.2 
71.5 
11.0 
10.2 
85.4 
7.3 
89.8 
84.6 83.3 85.0 84.6 83.7 82.5 83.8 

57.4 23.3 
8.9 
31.2 27.0 
24.4 
46.6 81.4 
82.6 
82.9 
7.4 
7.8 
28.3 
7.6 
30.8 31.4 30.6 31.4 43.4 87.4 24.1 

78.5 36.5 
8.0 
39.9 31.8 
89.0 
40.4 37.7 
80.5 
80.1 
8.5 
8.3 
43.3 
8.8 
38.5 41.9 40.3 35.2 40.1 40.5 89.0 

NA 6.1 
6.2 
5.8 7.9 
6.4 
6.2 6.2 
13.6 
78.6 
79.4 
89.6 
6.4 
6.2 
6.2 6.1 6.3 5.4 5.8 5.7 6.2 

= manipulation included in training data 

uniform noise (0.35) 

salt−and−pepper noise (0.2) 

rotation (90°) 

phase noise (90°) 

high−pass (std=0.7) 

low−pass (std=7) 

contrast (5%) 

greyscale 

colour 

h u m 

a n 
o b s e r v e r s 

A 1 
A 2 
A 3 
A 4 
A 5 
A 6 
A 7 
A 8 
A 9 
B 1 
B 2 
B 3 
B 4 
B 5 
B 6 
B 7 
B 8 
B 9 
C 
1 
C 
2 

Model 

Evaluation condition 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Numbers of trials in the respective experiments. C. = conditions; P. = practice trials &amp; blocks; M.= main experiment trials and blocks. The per condition column reports the number of trials per category and distortion level. The duration is reported without breaks.</figDesc><table>Distortion type 
C. P. blocks P. total M. blocks M. total Per C. Duration 

Colour 
2 
2 
320 
5 
1280 
40 
47 min 
Uniform noise 
8 
2 
256 
5 
1280 
10 
47 min 
Contrast 
8 
2 
256 
10 
1280 
10 
47 min 
Eidolon I 
8 
4 
384 
5 
1280 
10 
47 min 
Eidolon II 
8 
4 
384 
5 
1280 
10 
47 min 
Eidolon III 
8 
4 
384 
5 
1280 
10 
47 min 
Opponent colours 
2 
2 
224 
7 
1120 
35 
41 min 
Low-pass filtering 
8 
2 
256 
8 
1280 
10 
47 min 
High-pass filtering 8 
2 
256 
8 
1280 
10 
47 min 
Phase noise 
7 
2 
224 
7 
1120 
10 
41 min 
Power-equalisation 2 
2 
224 
7 
1120 
35 
41 min 
Rotation 
4 
2 
256 
8 
1280 
20 
47 min 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Still, DNNs usually need orders of magnitude more training data in comparison to humans, as explored by the literature on one-shot or few-shot learning (see e.g. [23] for an overview). 2 We have reported a subset of these experiments on arXiv in an earlier version of this paper [41]. 3 This is the same paradigm as reported in [49].</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">But see e.g. [54] for a critical assessment of this argument.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">A category-level analysis of decision biases for the uniform noise experiment is provided in the supplementary material, Figure 9.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">http://dx.doi.org/10.5281/zenodo.34217 9 https://github.com/tensorflow/models/tree/master/research/slim, cloned on May 2, 2017</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">https://github.com/gestaltrevision/Eidolon 12 A time gap of approximately six months between both experiments should minimise memory effects; furthermore, human participants were not shown any feedback (correct / incorrect classification choice) during the experiments.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recognition-by-components: a theory of human image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irving</forename><surname>Biederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="147" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How does the brain solve visual object recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Dicarlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><forename type="middle">C</forename><surname>Zoccolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="434" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Short-term conceptual memory for pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: human learning and memory</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">509</biblScope>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speed of processing in the human visual system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Fize</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Marlot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="issue">6582</biblScope>
			<biblScope unit="page" from="520" to="522" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Separate visual pathways for perception and action</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvyn</forename><forename type="middle">A</forename><surname>Goodale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">David</forename><surname>Milner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Neurosciences</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="25" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veda</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Nham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madeleine</forename><surname>Leach</surname></persName>
		</author>
		<idno>0028-0836</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep neural networks rival the representation of primate IT cortex for core visual object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">F</forename><surname>Cadieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L K</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ardila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Majaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Performance-optimized hierarchical models predict neural responses in higher visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ha</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">A</forename><surname>Cadieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darren</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="8619" to="8624" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamics of scene representations in the human brain revealed by magnetoencephalography and deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Radoslaw Martin Cichy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Pantazis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep networks resemble human feed-forward vision in invariant object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masoud</forename><surname>Saeed Reza Kheradpisheh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothée</forename><surname>Ganjtabesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Masquelier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.03929</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A study and comparison of human and deep learning recognition performance under visual distortions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Karam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02498</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Karam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04744</idno>
		<title level="m">Can the early human visual system compete with deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Human perception in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Dekel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.04674</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Do computational models differ systematically from human object perception?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rt Pramod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sp Arun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1601" to="1609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Invariant object recognition is a personalized selection of invariant features in humans, not simply explained by hierarchical feed-forward vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Karimi-Rouzbahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasour</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Ebrahimpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14402</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Totally looks like-how humans compare, compared to machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">D</forename><surname>Solbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01485</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Processing of chromatic information in a deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Flachot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gegenfurtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA A</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="334" to="346" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A parametric texture model based on deep convolutional features closely matches texture appearance for humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><forename type="middle">M</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Funke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><forename type="middle">A</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Felix A Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5" to="5" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Eigen-distortions of hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Berardino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valero</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Ballé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3533" to="3542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks outperform feature-based but not categorical models in explaining object similarity judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kamila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Jozwik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><forename type="middle">R</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marieke</forename><surname>Storrs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1726</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tomer D Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep neural networks in computational neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Tim C Kietzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mcclure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegeskorte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Opening the grey box</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevan A C</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Neurosciences</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="286" to="293" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Science and statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E P</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Box</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">356</biblScope>
			<biblScope unit="page" from="791" to="799" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep neural networks: A new framework for modeling biological vision and brain information processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Kriegeskorte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Vision Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="417" to="446" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Lifelong Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<idno>1627055010</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
			<biblScope unit="page">9781627055017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">Pack</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05468</idno>
		<title level="m">Generalization in deep learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploring generalization in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nati</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5949" to="5958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Opening the black box of deep neural networks via information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravid</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Ziv</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00810</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Kümmerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01563</idno>
		<title level="m">Deepgaze II: Reading fixations from deep features trained on object recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep learning for emotion recognition on small datasets using transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Wei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet</forename><forename type="middle">Dung</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilios</forename><surname>Vonikakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on international conference on multimodal interaction</title>
		<meeting>the 2015 ACM on international conference on multimodal interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="443" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Guest editorial deep learning in medical imaging: Overview and future promise of an exciting new technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hayit Greenspan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald M</forename><surname>Bram Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1153" to="1159" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Is learning the n-th thing any easier than learning the first?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="640" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Janssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Heiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wichmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06969</idno>
		<title level="m">Comparing deep neural networks against humans: object recognition when the signal gets weaker</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Grating contrast: Discrimination may be better than detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Nachmias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R V</forename><surname>Sansbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1039" to="1042" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Why use noise?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Pelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America A</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="647" to="653" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Some Aspects of Modelling Human Spatial Vision: Contrast Discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felix A Wichmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>The University of Oxford</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Contrast discrimination with pulse trains in pink noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C M</forename><surname>G Bruce Henning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America A</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1259" to="1266" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Normalization as a canonical neural computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Carandini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="62" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Linearity and normalization in simple cells of the macaque primary visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Carandini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anthony Movshon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="8621" to="8644" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ultra-rapid categorisation of natural scenes does not rely on colour cues: a study in monkeys and humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Delorme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Fabre-Thorpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="2187" to="2200" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Methods and measurements to compare men against machines. Electronic Imaging, Human Vision and Electronic Imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felix A Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Janssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aguilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Heiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianne</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Maertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="36" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Feedforward, horizontal, and feedback processing in the visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Lamme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henk</forename><surname>Super</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spekreijse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current opinion in neurobiology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="529" to="535" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The small world of the cerebral cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Sporns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">D</forename><surname>Zwi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="162" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">How can the brain be so fast</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wulfram</forename><surname>Gerstner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23 Problems in Systems Neuroscience</title>
		<editor>J. Leo van Hemmen and Terrence J Sejnowski</editor>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="135" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep neural networks as a computational model for human shape sensitivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kubilius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefania</forename><surname>Bracci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans P Op De</forename><surname>Beeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1004896</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Phase noise and the classification of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doris</forename><forename type="middle">I</forename><surname>Felix A Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">R</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gegenfurtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1520" to="1529" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Eidolons: Novel stimuli for vision research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Valsecchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Van Doorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Wagemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Gegenfurtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="7" to="7" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vasiljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05760</idno>
		<title level="m">Examining the Impact of Blur on Recognition by Convolutional Networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Understanding how image quality affects deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Quality of Multimedia Experience (QoMEX)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>Eighth International Conference on</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">On classification of distorted images with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sibo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1213" to="1217" />
		</imprint>
	</monogr>
	<note>2017 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">No free lunch theorems for optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">G</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macready</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="82" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Visual domain adaptation: A survey of recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghuraman</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruonan</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">An image-computable psychophysical spatial vision model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Heiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felix A Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="12" to="12" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition? In Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2146" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Normalizing the normalizers: Comparing and extending network normalization schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04520</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>R Core Team</surname></persName>
		</author>
		<ptr target="https://www.R-project.org/" />
		<title level="m">R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brainard</surname></persName>
		</author>
		<title level="m">The psychophysics toolbox. Spatial Vision</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="433" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">What&apos;s new in Psychtoolbox-3. Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Brainard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Pelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Ingling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Broussard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Principles of categorization. Concepts: core readings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleanor</forename><surname>Rosch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">189</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">scikit-image: image processing in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Nunez-Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Boulogne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">D</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Yager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuelle</forename><surname>Gouillart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">453</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Chromatic mechanisms in lateral geniculate nucleus of macaque</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Derrington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krauskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lennie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">357</biblScope>
			<biblScope unit="page" from="241" to="265" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">The spectral sensitivities of the middle-and long-wavelengthsensitive cones derived from measurements in observers of known genotype</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Stockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindsay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sharpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1711" to="1737" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Human color vision, chapter Cone Contrast and Opponent Modulation Color Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brainard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Optical Society of America</publisher>
			<pubPlace>Washington, DC, 2 edition</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Modelling the power spectra of natural images: Statistics and information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Der Schaaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Van Hateren</surname></persName>
		</author>
		<idno>1016/ 0042-6989</idno>
		<ptr target="http://dx.doi.org/10" />
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="2" to="8" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Animal detection in natural scenes: Critical features revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Felix A Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Drewes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">R</forename><surname>Rosas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gegenfurtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
