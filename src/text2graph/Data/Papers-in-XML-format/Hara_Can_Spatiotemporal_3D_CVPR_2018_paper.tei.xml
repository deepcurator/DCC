<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
							<email>kensho.hara@aist.go.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Advanced Industrial Science and Technology (AIST) Tsukuba</orgName>
								<address>
									<settlement>Ibaraki</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
							<email>hirokatsu.kataoka@aist.go.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Advanced Industrial Science and Technology (AIST) Tsukuba</orgName>
								<address>
									<settlement>Ibaraki</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
							<email>yu.satou@aist.go.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Advanced Industrial Science and Technology (AIST) Tsukuba</orgName>
								<address>
									<settlement>Ibaraki</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The use of large-scale datasets is extremely important when using deep convolutional neural networks (CNNs), which have massive parameter numbers, and the use of CNNs in the field of computer vision has expanded significantly in recent years. ImageNet <ref type="bibr" target="#b3">[4]</ref>, which includes more 1https://github.com/kenshohara/3D-ResNets-PyTorch than a million images, has contributed substantially to the creation of successful vision-based algorithms. In addition to such large-scale datasets, a large number of algorithms, such as residual learning <ref type="bibr" target="#b9">[10]</ref>, have been used to improve image classification performance by adding increased depth to CNNs, and the use of very deep CNNs trained on ImageNet have facilitated the acquisition of generic feature representation. Using such feature representation, in turn, has significantly improved the performance of several other tasks including object detection, semantic segmentation, and image captioning (see top row in <ref type="figure" target="#fig_0">Figure 1</ref>). To date, the video datasets available for action recognition have been relatively small when compared with image recognition datasets. Representative video datasets, such as UCF-101 <ref type="bibr" target="#b20">[21]</ref> and HMDB-51 <ref type="bibr" target="#b16">[17]</ref>, can be used to provide realistic videos with sizes around 10 K, but even though they are still used as standard benchmarks, such datasets are obviously too small to be used for optimizing CNN representations from scratch. In the last couple of years, ActivityNet <ref type="bibr" target="#b4">[5]</ref>, which is a somewhat larger video dataset, has become available, and its use has make it possible to accomplish additional tasks such as untrimmed action classification and detection, but the number of action instances it contains is still limited. More recently, the Kinetics dataset <ref type="bibr" target="#b15">[16]</ref> was created with the aim of being positioned as a de facto video dataset standard that is roughly equivalent to the position held by ImageNet in relation to image datasets. More than 300 K videos have been collected for the Kinetics dataset, which means that the scale of video datasets has begun to approach that of image datasets.</p><p>For action recognition, CNNs with spatio-temporal threedimensional (3D) convolutional kernels (3D CNNs) are recently more effective than CNNs with two-dimensional (2D) kernels <ref type="bibr" target="#b1">[2]</ref>. From several years ago <ref type="bibr" target="#b13">[14]</ref>, 3D CNNs are explored to provide an effective tool for accurate action recognition. However, even the usage of well-organized models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref> has failed to overcome the advantages of 2D-based CNNs that combine both stacked flow and RGB images <ref type="bibr" target="#b19">[20]</ref>. The primary reason for this failure has been the relatively small data-scale of video datasets that are available for optimizing the immense number of parameters in 3D CNNs, which are much larger than those of 2D CNNs. In addition, basically, 3D CNNs can only be trained on video datasets whereas 2D CNNs can be pretrained on ImageNet. Recently, however, Carreira and Zisserman achieved a significant breakthrough using the Kinetics dataset as well as the inflation of 2D kernels pretrained on ImageNet into 3D ones <ref type="bibr" target="#b1">[2]</ref>. Thus, we now have the benefit of a sophisticated 3D convolution that can be engaged by the Kinetics dataset.</p><p>However, can 3D CNNs retrace the successful history of 2D CNNs and ImageNet? More specifically, can the use of 3D CNNs trained on Kinetics produces significant progress in action recognition and other various tasks? (See bottom row in <ref type="figure" target="#fig_0">Figure 1</ref>.) To achieve such progress, we consider that Kinetics for 3D CNNs should be as large-scale as ImageNet for 2D CNNs, though no previous work has examined enough about the scale of Kinetics. Conventional 3D CNN architectures trained on Kinetics are still relatively shallow <ref type="bibr">(10 [16]</ref>, 22 <ref type="bibr" target="#b1">[2]</ref>, and 34 <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref> layers). If using the Kinetics dataset enables very deep 3D CNNs at a level similar to ImageNet, which can train 152-layer 2D CNNs <ref type="bibr" target="#b9">[10]</ref>, that question could be answered in the affirmative.</p><p>In this study, we examine various 3D CNN architectures from relatively shallow to very deep ones using the Kinetics and other popular video datasets (UCF-101, HMDB-51, and ActivityNet) in order to provide us insights for answering the above question. The 3D CNN architectures tested in this study are based on residual networks (ResNets) <ref type="bibr" target="#b9">[10]</ref> and their extended versions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref> because they have simple and effective structures. Accordingly, using those datasets, we performed several experiments aimed at training and testing those architectures from scratch, as well as their fine-tuning. The results of those experiments (see Section 4 for details) show the Kinetics dataset can train 3D ResNet-152 from scratch to a level that is similar to the training accomplished by 2D ResNets on ImageNet, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Based on those results, we will discuss the possibilities of future progress in action recognition and other video tasks.</p><p>To our best knowledge, this is the first work to focus on the training of very deep 3D CNNs from scratch for action recognition. Previous studies showed deeper 2D CNNs trained on ImageNet achieved better performance <ref type="bibr" target="#b9">[10]</ref>. However, it is not trivial to show deeper 3D CNNs are better based on the previous studies because the data-scale of image datasets differs from that of video ones. The results of this study, which indicate deeper 3D CNNs are more effective, can be expected to facilitate further progress in computer vision for videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Video Datasets</head><p>The HMDB-51 <ref type="bibr" target="#b16">[17]</ref> and UCF-101 <ref type="bibr" target="#b20">[21]</ref> datasets are currently the most successful in the field of action recognition. These datasets gained significant popularity in the early years of the field, and are still used as popular benchmarks. However, a recent consensus has emerged that indicates that they are simply not large enough for training deep CNNs from scratch <ref type="bibr" target="#b15">[16]</ref>.</p><p>A couple of years after the abovementioned datasets were introduced, larger video datasets were produced. These include ActivityNet <ref type="bibr" target="#b4">[5]</ref>, which contains 849 hours of video, including 28,000 action instances. ActivityNet also provides some additional tasks, such as untrimmed classification and detection, but the number of action instances is still on the order of tens of thousands. This year (2017), in an effort to create a successful pretrained model, Kay et al. released the Kinetics dataset <ref type="bibr" target="#b15">[16]</ref>. The Kinetics dataset includes more than 300,000 trimmed videos covering 400 categories. In order to determine whether it can train deeper 3D CNNs, we performed a number of experiments using these recent datasets, as well as the UCF-101 and HMDB-51 datasets.</p><p>Other huge datasets such as Sports-1M <ref type="bibr" target="#b14">[15]</ref> and YouTube-8M <ref type="bibr" target="#b0">[1]</ref> have been proposed. Although these databases are larger than Kinetics, their annotations are slightly noisy and only video-level labels have been assigned. (In other words, they include frames that do not relate to target actions.) Such noise and the presence of unrelated frames have the potential to prevent these models from providing good training. In addition, with file sizes in excess of 10 TB, their scales are simply too large to allow them to be utilized easily. Because of these issues, we will refrain from discussing these datasets in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Action Recognition Approaches</head><p>One of the popular approaches to CNN-based action recognition is the use of two-stream CNNs with 2D convolutional kernels. In their study, Simonyan et al. proposed a method that uses RGB and stacked optical flow frames as appearance and motion information, respectively <ref type="bibr" target="#b19">[20]</ref>, and showed that combining the two-streams has the ability to improve action recognition accuracy. Since that study, numerous methods based on the two-stream CNNs have been proposed to improve action recognition performance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Unlike the abovementioned approaches, we focused on CNNs with 3D convolutional kernels, which have recently begun to outperform 2D CNNs through the use of large-scale video datasets. These 3D CNNs are intuitively effective because such 3D convolution can be used to directly extract spatio-temporal features from raw videos. For example, Ji et al. proposed applying 3D convolution to extract spatiotemporal features from videos, while Tran et al. trained 3D CNNs, which they referred to as C3D, using the Sports-1M dataset <ref type="bibr" target="#b14">[15]</ref>. Since that study, C3D has been seen as a de facto standard for 3D CNNs. They also experimentally found that a 3 × 3 × 3 convolutional kernel achieved the best performance level. In another study, Varol et al. showed that expanding the temporal length of inputs for C3D improves recognition performance <ref type="bibr" target="#b24">[25]</ref>. Those authors also found that using optical flows as inputs to 3D CNNs resulted in a higher level of performance than can be obtained from RGB inputs, but that the best performance could be achieved by combining RGB and optical flows. Meanwhile, <ref type="bibr">Kay et al.</ref> showed that the results of 3D CNNs trained from scratch on their Kinetics dataset were comparable with the results of 2D CNNs pretrained on ImageNet, even though the results of 3D CNNs trained on the UCF101 and HMDB51 datasets were inferior to the 2D CNNs results. In still another study, Carreira et al. proposed inception <ref type="bibr" target="#b21">[22]</ref> based 3D CNNs, which they referred to as I3D, and achieved state-of-the-art performance <ref type="bibr" target="#b1">[2]</ref>. More recently, some works introduced ResNet architectures into 3D CNNs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref>, though they examined only relatively shallow ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental configuration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Summary</head><p>In this study, in order to determine whether current video datasets have sufficient data for training of deep 3D CNNs, we conducted the three experiments described below using UCF-101 <ref type="bibr" target="#b20">[21]</ref>, HMDB-51 <ref type="bibr" target="#b16">[17]</ref>, ActivityNet <ref type="bibr" target="#b4">[5]</ref>, and Kinetics <ref type="bibr" target="#b15">[16]</ref>. We first examined the training of relatively shallow 3D CNNs from scratch on each video dataset. According to previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref>, 3D CNNs trained on UCF-101, HMDB-51, and ActivityNet do not achieve high accuracy whereas ones trained on Kinetics work well. We try to reproduce such results to ascertain whether the datasets have sufficient data for deep 3D CNNs. Specifically, we used ResNet-18, which is the shallowest ResNet architecture, based on the assumption that if the ResNet-18 overfits when being trained on a dataset, that dataset is too small to be used for training deep 3D CNNs from scratch. See Section 4.1 for details.</p><p>We then conducted a separate experiment to determine whether the Kinetics dataset could train deeper 3D CNNs. A main point of this trial was to determine how deeply the datasets could train 3D CNNs. Therefore, we trained 3D ResNets on Kinetics while varying the model depth from 18 to 200. If Kinetics can train very deep CNNs, such as ResNet-152, which achieved the best performance in ResNets on ImageNet <ref type="bibr" target="#b9">[10]</ref>, we can be confident that they have sufficient data to train 3D CNNs. Therefore, the results of this experiment are expected to be very important for the future progress in action recognition and other video tasks. See Section 4.2 for details.</p><p>In the final experiment, we examined the fine-tuning of Kinetics pretrained 3D CNNs on UCF-101 and HMDB-51. Since pretraining on large-scale datasets is an effective way to achieve good performance levels on small datasets, we expect that the deep 3D ResNets pretrained on Kinetics would perform well on relatively small UCF-101 and HMDB-51. This experiment examines whether the transfer visual representations by deep 3D CNNs from one domain to another domain works effectively. See Section 4.3 for details. , F as the kernel size, and the number of feature maps of the convolutional filter are x × x × x and F, respectively, and group as the number of groups of group convolutions, which divide the feature maps into small groups. BN refers to batch normalization <ref type="bibr" target="#b12">[13]</ref>. Shortcut connections of the architectures are summation except for those of DenseNet, which are concatenation. <ref type="table">Table 1</ref>: Network Architectures. Each convolutional layer is followed by batch normalization <ref type="bibr" target="#b12">[13]</ref> and a ReLU <ref type="bibr" target="#b17">[18]</ref>. Spatio-temporal down-sampling is performed by conv3_1, conv4_1, and conv5_1 with a stride of two, except for DenseNet. F is the number of feature channels corresponding in <ref type="figure" target="#fig_2">Figure 3</ref>, and N is the number of blocks in each layer. DenseNet down-samples inputs using the transition layer, that consists of a 3 × 3 × 3 convolutional layer and a 2 × 2 × 2 average pooling layer with a stride of two, after conv2_x, conv3_x, and conv4_x. F of DenseNet is the number of input feature channels of first block in each layer, and N is the same as that of the other networks. A 3 × 3 × 3 max-pooling layer (stride 2) is also located before conv2_x of all networks for down-sampling. In addition, conv1 spatially down-samples inputs with a spatial stride of two. C of the fully-connected layer is the number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Block conv1 conv2_x conv3_x conv4_x conv5_x </p><formula xml:id="formula_0">F N F N F N F N ResNet- {18, 34} Basic conv, 7 × 7 × 7,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network architectures</head><p>Next, we explain the various ResNet-based architectures with 3D convolutions used in this study. ResNet, which is one of the most successful architectures in image classification, provides shortcut connections that allow a signal to bypass one layer and move to the next layer in the sequence. Since these connections pass through the networks' gradient flows from the later layers to the early layers, they can facilitate the training of very deep networks. Unlike previous studies that examined only limited 3D ResNet architectures <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref>, we examine not only deeper architectures but also some extended versions of ResNet. In particular, we explore the following architectures: ResNet (basic and bottleneck blocks) <ref type="bibr" target="#b9">[10]</ref>, pre-activation ResNet <ref type="bibr" target="#b10">[11]</ref>, wide ResNet (WRN) <ref type="bibr" target="#b30">[31]</ref>, ResNeXt <ref type="bibr" target="#b29">[30]</ref>, and DenseNet <ref type="bibr" target="#b11">[12]</ref>. The architectures are summarized in <ref type="figure" target="#fig_2">Figure 3</ref> and <ref type="table">Table 1</ref>. In the following paragraphs, we will briefly introduce each architecture.</p><p>A basic ResNets block consists of two convolutional layers, and each convolutional layer is followed by batch normalization and a ReLU. A shortcut pass connects the top of the block to the layer just before the last ReLU in the block. ResNet-18 and 34 adopt the basic blocks. We use identity connections and zero padding for the shortcuts of the basic blocks (type A in <ref type="bibr" target="#b9">[10]</ref>) to avoid increasing the number of parameters of these relatively shallow networks.</p><p>A ResNets bottleneck block consists of three convolutional layers. The kernel sizes of the first and third convolutional layers are 1 × 1 × 1, whereas those of the second are 3 × 3 × 3. The shortcut pass of this block is the same as that of the basic block. ResNet-50, 101, 152, and 200 adopt the bottleneck. We use identity connections except for those that are used for increasing dimensions (type B in <ref type="bibr" target="#b9">[10]</ref>).</p><p>The pre-activation ResNet is similar to bottleneck ResNet architectures, but there are differences in the convolution, batch normalization, and ReLU order. In ResNet, each convolutional layer is followed by batch normalization and a ReLU, whereas each batch normalization of the preactivation ResNet is followed by the ReLU and a convolutional layer. A shortcut pass connects the top of the block to the layer just after the last convolutional layer in the block. In their study, He et al. showed that such pre-activation facilitates optimization in the training and reduces overfitting <ref type="bibr" target="#b10">[11]</ref>. In this study, pre-activation ResNet-200 was evaluated.</p><p>The WRN architecture is the same as the ResNet (bottleneck), but there are differences in the number of feature maps for each convolutional layer. WRN increases the number of feature maps rather than the number of layers. Such wide architectures are efficient in parallel computing using GPUs <ref type="bibr" target="#b30">[31]</ref>. In this study, we evaluate the WRN-50 using a widening factor of two.</p><p>ResNeXt introduces cardinality, which is a different dimension from deeper and wider. Unlike the original bottleneck block, the ResNeXt block introduces group convolutions, which divide the feature maps into small groups. Cardinality refers to the number of middle convolutional layer groups in the bottleneck block. In their study, Xie et al. showed that increasing the cardinality of 2D architectures is more effective than using wider or deeper ones <ref type="bibr" target="#b29">[30]</ref>. In this study, we evaluate ResNeXt-101 using the cardinality of 32.</p><p>DenseNet makes connections from early layers to later layers by the use of a concatenation that is different from the ResNets summation. This concatenation connects each layer densely in a feed-forward fashion. DenseNets also adopt the pre-activation used in pre-activation ResNets. In their study, Huang et al. showed that it achieves better accuracy with fewer parameters than ResNets <ref type="bibr" target="#b11">[12]</ref>. In this study, we evaluate DenseNet-121 and 201 using a growth rate of 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation</head><p>Training. We use stochastic gradient descent with momentum to train the networks and randomly generate training samples from videos in training data in order to perform data augmentation. First, we select a temporal position in a video by uniform sampling in order to generate a training sample. A 16-frame clip is then generated around the selected temporal position. If the video is shorter than 16 frames, then we loop it as many times as necessary. Next, we randomly select a spatial position from the 4 corners or the center. In addition to the spatial position, we also select a spatial scale of the sample in order to perform multi-scale cropping. The procedure used is the same as <ref type="bibr" target="#b27">[28]</ref>. The scale is selected from 1, . Scale 1 means that the sample width and height are the same as the short side length of the frame, and scale 0.5 means that the sample is half the size of the short side length. The sample aspect ratio is 1 and the sample is spatio-temporally cropped at the positions, scale, and aspect ratio. We spatially resize the sample at 112 × 112 pixels. The size of each sample is 3 channels × 16 frames × 112 pixels × 112 pixels, and each sample is horizontally flipped with 50% probability. We also perform mean subtraction, which means that we subtract the mean values of ActivityNet from the sample for each color channel. All generated samples retain the same class labels as their original videos.</p><p>In our training, we use cross-entropy losses and backpropagate their gradients. The training parameters include a weight decay of 0.001 and 0.9 for momentum. When training the networks from scratch, we start from learning rate 0.1, and divide it by 10 after the validation loss saturates. When performing fine tuning, we start from a learning rate of 0.001, and assign a weight decay of 1e-5.</p><p>Recognition. We adopt the sliding window manner to generate input clips, (i.e., each video is split into non-overlapped 16-frame clips), and recognize actions in videos using the trained networks. Each clip is spatially cropped around a center position at scale 1. We then input each clip into the networks and estimate the clip class scores, which are averaged over all the clips of the video. The class that has the maximum score indicates the recognized class label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Datasets</head><p>As stated above, this study focuses on four datasets: UCF-101 <ref type="bibr" target="#b20">[21]</ref>, HMDB-51 <ref type="bibr" target="#b16">[17]</ref>, ActivityNet <ref type="bibr" target="#b4">[5]</ref>, and Kinetics <ref type="bibr" target="#b15">[16]</ref>.</p><p>UCF-101 includes 13,320 action instances from 101 human action classes. The videos were temporally trimmed to remove non-action frames. The average duration of each video is about 7 seconds. Three train/test splits (70% training and 30% testing) are provided in the dataset.</p><p>HMDB-51 includes 6,766 videos from 51 human action classes. Similar to UCF-101, the videos were temporally  trimmed. The average duration of each video is about 3 seconds. Three train/test splits (70% training and 30% testing) are provided in this dataset.</p><p>ActivityNet (v1.3) provides samples from 200 human action classes with an average of 137 untrimmed videos per class and 1.41 activity instances per video. Unlike the other datasets, ActivityNet consists of untrimmed videos, which include non-action frames. The total video length is 849 hours, and the total number of action instances is 28,108. This dataset is randomly split into three different subsets: training, validation, and testing. More specifically, 50% is used for training, 25% is used for validation, and 25% is used for testing.</p><p>The Kinetics dataset has 400 human action classes, and consists of more than 400 videos for each class. The videos were temporally trimmed and last around 10 seconds. The total number of the videos is in excess of 300,000. The number of training, validation, and testing sets are about 240,000, 20,000, and 40,000, respectively.</p><p>The video properties of all these datasets are similar. Most videos were extracted from YouTube, except for HMDB-51, which includes videos extracted from movies. The videos include dynamic background and camera motions and the main differences among them are the numbers of action classes and instances.</p><p>We resized the videos to heights of 240 pixels without changing their aspect ratios and then stored them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Analyses of training on each dataset</head><p>We began by training ResNet-18 on each dataset. According to previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref>, 3D CNNs trained on UCF-101, HMDB-51, and ActivityNet do not achieve high accuracy whereas ones trained on Kinetics work well. We tried to reproduce such results in this experiment. In this process, we used split 1 of UCF-101 and HMDB-51, and the training and validation sets of ActivityNet and Kinetics. <ref type="figure" target="#fig_5">Figure 4</ref> shows the training and validation losses of ResNet-18 on each dataset. As can be seen in the figure, the validation losses on UCF-101, HMDB-51, and ActivityNet quickly converged to high values and were clearly higher than their corresponding training losses. These results indicate that overfitting resulted when the training on those three datasets. In addition to those losses, we confirmed per-clip accuracies, which are evaluated for each clip rather than for each video. The validation accuracies of UCF-101, HMDB-51, and ActivityNet are 40.1, 16.2, and 26.8%, respectively. It should be noted that direct comparisons between our results and those of previous studies would be unfair because the accuracies reported in most papers were per-video accuracies. However, since these accuracies are very low even compared with earlier methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26]</ref>, our results indicate that it is difficult to train deep 3D CNNs from scratch on UCF-101, HMDB-51, and ActivityNet.</p><p>In contrast, the training and validation losses on Kinetics are significantly different than those on other datasets. Since the validation losses were slightly higher than the training losses, we could conclude that training ResNet-18 on Kinetics did not result in overfitting, and that it is possible for Kinetics to train deep 3D CNNs. In the next section, we will further investigate deeper 3D CNNs on Kinetics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analyses of deeper networks</head><p>Since the abovementioned experiment showed Kinetics could be used to train ResNet-18 without overfitting, we next examined deeper ResNets using the Kinetics training and validation sets.</p><p>Here, we will show ResNets accuracies changes based on model depths. <ref type="figure" target="#fig_1">Figure 2</ref> shows the averaged accuracies over top-1 and top-5 ones. We can see that, essentially, as the depth increased, accuracies improved, and that the improvements continued until reaching the depth of 152. We can also see that deeper ResNet-152 achieved significant improvement of accuracies compared with ResNet-18, which was the previously examined architecture <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref>. In con-trast, the accuracy of ResNet-200 was almost the same as that of ResNet-152. This result indicate that the training of ResNet-200 started to overfit. Interestingly, the results are similar to those for 2D ResNets on ImageNet <ref type="bibr" target="#b10">[11]</ref>. More specifically, the accuracies of both 2D and 3D ResNets improved as the depth increased until reaching the depth of 152, and then the accuracies did not increase when increasing the depths of 200. These results indicate that the Kinetics dataset has sufficient data to train 3D CNNs in a manner similar to ImageNet.</p><p>Comparisons with other architectures are shown in Table 2. Here, it can be seen that the accuracies of preactivation ResNet-200 are slightly low when compared with the standard ResNet-200 though He et al. reported that the pre-activation reduces overfitting and improves 2D ResNet-200 on ImageNet <ref type="bibr" target="#b10">[11]</ref>. We can also see that the WRN-50 achieved higher accuracies when compared with the ResNet-152, which is similar to the results on ImageNet <ref type="bibr" target="#b30">[31]</ref>. This result also supports that Kinetics is sufficient large for the training of 3D CNNs because the number of parameters of WRN-50 is larger than the ResNet-152. Furthermore, we can see that ResNeXt-101 achieved the best accuracies among the architectures tested. This result is also similar to that seen for ImageNet <ref type="bibr" target="#b29">[30]</ref>, and means that introducing the cardinality works well for the 3D ResNets on Kinetics. In contrast, the accuracies of the DenseNet-121 and 201 were slightly lower than the other architectures. The major advantage provide by dense connections is parameter efficiency, which contributes to reducing overfitting <ref type="bibr" target="#b11">[12]</ref>. However, Kinetics did not need such techniques to train deep 3D CNNs. <ref type="table" target="#tab_2">Table 3</ref> shows the results of the Kinetics test set used to compare ResNeXt-101, which achieved the highest accuracies, with the state-of-the-art methods. Here, it can be seen that the accuracies of ResNeXt-101 are clearly high compared with C3D with batch normalization <ref type="bibr" target="#b15">[16]</ref>, which is 10-layer network, as well as CNN+LSTM and two-stream CNN <ref type="bibr" target="#b15">[16]</ref>. This result also indicates the effectiveness of deeper 3D networks trained on Kinetics. In contrast, RGB-I3D trained on Kinetics from scratch <ref type="bibr" target="#b2">[3]</ref> were found to outperform ResNeXt-101 even though ResNeXt-101 is a deeper architecture than I3D. One of the reasons for this is the size differences of the network inputs. Specifically, the size of I3D is 3 × 64 × 224 × 224, whereas that of ResNeXt-101 is 3 × 16 × 112 × 112. Thus, I3D is 64 times larger than ResNeXt-101. To confirm the accuracies when using larger inputs, we also evaluated the ResNeXt-101 that used 3×64×112×112 inputs, which are the largest available sizes in our environment (NVIDIA TITAN X × 8). We can see that the network, referred as ResNeXt-101 (64f) in <ref type="table" target="#tab_2">Table 3</ref>, outperformed RGB-I3D even though the input size is still four times smaller than that of I3D. We can conclude that deeper 3D architectures trained on Kinetics are effective. In addition, it is felt that combining two-stream architec-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analyses of fine-tuning</head><p>Finally, in this section we confirm the performance of fine-tuning. In the experiments above, we showed that Kinetics can train deep 3D CNNs from scratch, but that it is difficult to train such networks on other datasets. In this section, we fine-tuned the Kinetics pretrained 3D CNNs on UCF-101 and HMDB-51. The results of this experiment are important for determining whether the 3D CNNs are effective for other datasets. It should be noted that, in this experiment, fine-tuning was only performed to train conv5_x and the fully connected layer because it achieved the best performance during the preliminary experiments. <ref type="table" target="#tab_3">Table 4</ref> shows the accuracies of Kinetics pretrained 3D CNNs, as well as ResNet-18 trained from scratch, in UCF-  <ref type="table" target="#tab_3">Table 4</ref> also shows that ResNeXt-101, which achieved the best performance on Kinetics, achieved the highest levels of performance on both datasets when compared with the other networks. The performance difference, however, is smaller than that of Kinetics. It is considered likely that this result also relates to the sizes of datasets. We then compared the results with DenseNet-121 because it is a parameter-efficient network, and thus might achieve better performance on small datasets. However, the DenseNet-121 results were lower than those of ResNet-50, thereby indicating that its greater efficiency did not contribute on fine-tuning of 3D CNNs.</p><p>We shows the results of our comparison with state-ofthe-art methods in <ref type="table" target="#tab_4">Table 5</ref>. Here, we can see that ResNeXt-101 achieved higher accuracies compared with C3D <ref type="bibr" target="#b22">[23]</ref>, P3D <ref type="bibr" target="#b18">[19]</ref>, two-stream CNN <ref type="bibr" target="#b19">[20]</ref>, and TDD <ref type="bibr" target="#b26">[27]</ref>. Furthermore, we can also see that ResNeXt-101 (64f), which utilize larger inputs described in previous section, slightly outperformed ST Multiplier Net <ref type="bibr" target="#b6">[7]</ref> and TSN <ref type="bibr" target="#b28">[29]</ref>, which utilize more complex two-stream architectures. We can also see that two-stream I3D <ref type="bibr" target="#b2">[3]</ref>, which utilizes simple two-stream 3D architectures pretrained on Kinetics, achieved the best accuracies. Based on these results, we can conclude that simple 3D architectures pretrained on Kinetics outperform complex 2D architectures. We believe that development of 3D CNNs rapidly grows and contributes to significant advances in video recognition and its related tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this study, we examined the architectures of various CNNs with spatio-temporal 3D convolutional kernels on current video datasets. Based on the results of those experiments, the following conclusions could be obtained: (i) ResNet-18 training resulted in significant overfitting for UCF-101, HMDB-51, and ActivityNet but not for Kinetics.</p><p>(ii) The Kinetics dataset has sufficient data for training of deep 3D CNNs, and enables training of up to 152 ResNets layers, interestingly similar to 2D ResNets on ImageNet. (iii) Kinetics pretrained simple 3D architectures outperforms complex 2D architectures on UCF-101 and HMDB-51, and the pretrained ResNeXt-101 achieved 94.5% and 70.2% on UCF-101 and HMDB-51, respectively.</p><p>We believe that the results of this study will facilitate further advances in video recognition and its related tasks. Following the significant advances in image recognition made by 2D CNNs and ImageNet, pretrained 2D CNNs on ImageNet experienced significant progress in various tasks such as object detection, semantic segmentation, and image captioning. It is felt that, similar to these, 3D CNNs and Kinetics have the potential to contribute to significant progress in fields related to various video tasks such as action detection, video summarization, and optical flow estimation. In our future work, we will investigate transfer learning not only for action recognition but also for other such tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Recent advances in computer vision for images (top) and videos (bottom). The use of very deep 2D CNNs trained on ImageNet generates outstanding progress in image recognition as well as in various other tasks. Can the use of 3D CNNs trained on Kinetics generates similar progress in computer vision for videos?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Averaged accuracies of 3D ResNets over top-1 and top-5 on the Kinetics validation set. Accuracy levels improve as network depths increase. The improvements continued until reaching the depth of 152. The accuracy of ResNet-200 is almost the same as that of ResNet-152. These results are similar to 2D ResNets on ImageNet [10].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Block of each architecture. We represent conv, x 3 , F as the kernel size, and the number of feature maps of the convolutional filter are x × x × x and F, respectively, and group as the number of groups of group convolutions, which divide the feature maps into small groups. BN refers to batch normalization [13]. Shortcut connections of the architectures are summation except for those of DenseNet, which are concatenation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: ResNet-18 training and validation losses. The validation losses on UCF-101, HMDB-51, and ActivityNet quickly converged to high values and were clearly higher than their corresponding training losses. The validation losses on Kinetics were slightly higher than the corresponding training losses, significantly different than those on the other datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Accuracies on the Kinetics validation set. Average is averaged accuracy over Top-1 and Top-5.</figDesc><table>Method 
Top-1 Top-5 Average 

ResNet-18 
54.2 
78.1 
66.1 
ResNet-34 
60.1 
81.9 
71.0 
ResNet-50 
61.3 
83.1 
72.2 
ResNet-101 
62.8 
83.9 
73.3 
ResNet-152 
63.0 
84.4 
73.7 
ResNet-200 
63.1 
84.4 
73.7 

ResNet-200 (pre-act) 
63.0 
83.7 
73.4 
Wide ResNet-50 
64.1 
85.3 
74.7 
ResNeXt-101 
65.1 
85.7 
75.4 
DenseNet-121 
59.7 
81.9 
70.8 
DenseNet-201 
61.3 
83.3 
72.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Accuracies on the Kinetics test set. Average is averaged accuracy over Top-1 and Top-5. Here, we refer the results of RGB- and Two-stream I3D trained from scratch [3] for fair comparison.tures with ResNeXt-101 make further improvements based on higher accuracies of two-stream I3D.</figDesc><table>Method 
Top-1 Top-5 Average 

ResNeXt-101 
-
-
74.5 

ResNeXt-101 (64f) 
-
-
78.4 

CNN+LSTM [16] 
57.0 
79.0 
68.0 

Two-stream CNN [16] 
61.0 
81.3 
71.2 

C3D w/ BN [16] 
56.1 
79.5 
67.8 

RGB-I3D [3] 
68.4 
88.0 
78.2 

Two-stream I3D [3] 
71.6 
90.0 
80.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Top-1 accuracies on UCF-101 and HMDB-51. All accu- racies are averaged over three splits.101 and HMDB-51. Here, it can be seen that Kinetics pre- trained ResNet-18 clearly outperformed one trained from scratch. This result indicate that pretraining on Kinetics is effective on UCF-101 and HMDB-51. We can also see that the accuracies basically improved as the depth increased, similar to the results obtained on Kinetics. However, un- like the results on Kinetics, ResNet-200 also improved the accuracies in HMDB-51. Because, as described above, the fine-tuning in this experiment was only performed to train conv5_x and the fully connected layer, the numbers of trained parameters were the same from ResNet-50 to ResNet-200. Therefore, the pretrained early layers, which work as feature extractors, relate to the differences of perfor- mance. These results indicate that feature representations of ResNet-200 would be suitable for HMDB-51 even though the 200-layer network might start to overfit on Kinetics.</figDesc><table>Method 
UCF-101 HMDB-51 

ResNet-18 (scratch) 
42.4 
17.1 

ResNet-18 
84.4 
56.4 
ResNet-34 
87.7 
59.1 
ResNet-50 
89.3 
61.0 
ResNet-101 
88.9 
61.7 
ResNet-152 
89.6 
62.4 
ResNet-200 
89.6 
63.5 

DenseNet-121 
87.6 
59.6 
ResNeXt-101 
90.7 
63.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 :</head><label>5</label><figDesc>Top-1 accuracies on UCF-101 and HMDB-51 comared with the state-of-the-art methods. All accuracies are averaged over three splits. Dim indicate the dimension of convolution kernel.</figDesc><table>Method 
Dim UCF-101 HMDB-51 

ResNeXt-101 
3D 
90.7 
63.8 
ResNeXt-101 (64f) 
3D 
94.5 
70.2 

C3D [23] 
3D 
82.3 
-
P3D [19] 
3D 
88.6 
-
Two-stream I3D [3] 
3D 
98.0 
80.7 

Two-stream CNN [20] 2D 
88.0 
59.4 
TDD [27] 
2D 
90.3 
63.2 
ST Multiplier Net [7] 
2D 
94.2 
68.9 
TSN [29] 
2D 
94.2 
69.4 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">conv, 3 3 , F BN ReLU conv, 3 3 , F BN ReLU ResNet (basic) conv, 1 3 , F BN ReLU conv, 3 3 , F BN ReLU ReLU conv, 1 3 , 4F BN ResNet (bottleneck) conv, 1 3 , F BN ReLU conv, 3 3 , F BN ReLU ReLU conv, 1 3 , 4F BN ResNet (pre-act) conv, 1 3 , F BN ReLU conv, 3 3 , F, group=32 BN ReLU ReLU BN</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<title level="m">YouTube-8M: A large-scale video classification benchmark. arXiv preprint</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the Kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07750</idno>
	</analytic>
	<monogr>
		<title level="m">A new model and the Kinetics dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ActivityNet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>the Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatiotemporal multiplier networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal features with 3D residual networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICCV Workshop on Action, Gesture, and Emotion Recognition</title>
		<meeting>the ICCV Workshop on Action, Gesture, and Emotion Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The Kinetics human action video dataset. arXiv preprint</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>the Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human action classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Roshan</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Convnet architecture search for spatiotemporal feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05038</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Long-term temporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04494</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="79" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Towards good practices for very deep two-stream convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02159</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
