<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting Video Saliency: A Large-scale Benchmark and a New Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
							<email>wenguanwang.ai@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Lab of Intelligent Information Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Lab of Intelligent Information Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Guo</surname></persName>
							<email>guofang@bit.edu.cncmm@nankai.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Lab of Intelligent Information Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">CCCE</orgName>
								<orgName type="institution">Nankai University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
							<email>aborji@crcv.ucf.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting Video Saliency: A Large-scale Benchmark and a New Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Corresponding author: Jianbing Shen. This work was supported in part by the Beijing Natural Science Foundation under Grant 4182056, the National Basic Research Program of China under Grant 2013CB328805, the Fok Ying Tung Education Foundation under Grant 141067, and the Specialized Fund for Joint Building Program of Beijing Municipal Educa-tion Commission.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In this work, we contribute to video saliency research in two ways. First </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human visual system (HVS) has an astonishing ability to quickly select visually important regions in its visual field. This cognitive process enables humans to easily interpret complex scenes in real time. Over the last few decades, several computational models have been proposed for imitating attentional mechanisms of HVS during static scene viewing. Significant advances have been achieved recently with the rapid spread of deep learning techniques and the availability of large-scale static gaze datasets (e.g., SALI-CON <ref type="bibr" target="#b30">[31]</ref>). In stark contrast, predicting observers' fixations during dynamic scene free-viewing has less been explored. This task, referred to as dynamic fixation prediction or video saliency detection is very useful for understanding human attentional behaviors and has several practical real-word applications (e.g., video captioning, compression, question answering, object segmentation, etc). It is thus highly desired to have a standard, high-quality dataset composed of diverse and representative video stimuli. Exiting datasets are severely limited in their coverage and scalability, and they only include special scenarios such as limited human activities in constrained situations. None of them includes general, representative, and diverse instances in unconstrained, task-independent scenarios. As a consequence, existing datasets often fail to offer a rich set of fixations for learning video saliency and to assess models. Moreover, the existing datasets did not provide an evaluation server with standalone held out test set to avoid potential dataset overfitting, which hinders further development on this topic.</p><p>While saliency benchmarks (e.g., MIT300 <ref type="bibr" target="#b31">[32]</ref> and LSUN <ref type="bibr" target="#b67">[68]</ref>) have been very instrumental in progressing the static saliency field, such standard widespread benchmarks are missing for video saliency modeling. We believe such benchmarks are highly needed to move the field forward. To this end, we propose a new benchmark "DHF1K (Dynamic Human Fixation 1K)" with a public server for report-ing evaluation results on a preserved test set. Our benchmark contains a dataset that is unique in terms of generality, diversity and difficulty. It includes 1K videos with more than 600K frames and per-frame fixation annotations from 17 observers. The sequences have been carefully collected to include diverse scenes, motion patterns, object categories, and activities. DHF1K is accompanied with a comprehensive evaluation of several state-of-the-art approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr">20,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b46">47]</ref>. Moreover, each video is annotated with a main category label (e.g., daily activities, animals) and rich attributes (e.g., camera/content movement, scene lighting, presence of humans), which would enable a deeper understanding of gaze guidance in free viewing of dynamic scenes.</p><p>Further, we propose a novel CNN-LSTM architecture <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b45">46]</ref> based video saliency model with a supervised attention mechanism. CNN layers are utilized for extracting static features within input frames, while convolutional LSTM (convLSTM) <ref type="bibr" target="#b65">[66]</ref> is utilized for sequential fixation prediction over successive frames. An attention module, learned from existing large-scale image saliency datasets, is used to enhance spatially informative features of the CNN. Such a design helps disentangle underlying spatial and temporal factors of dynamic attention and allows convLSTM to learn temporal saliency representations efficiently.</p><p>Our contributions are three-fold. First, we introduce a standard benchmark of 1K videos covering a wide range of scenes, motions, activities, etc. To the best of our knowledge, the proposed dataset is the largest eye-tracking dataset for dynamic, free-viewing fixation prediction. Second, we present a novel attentive CNN-LSTM architecture for predicting human gaze in dynamic scenes, which explicitly encodes static attention into dynamic saliency representation learning by leveraging both static and dynamic fixation data. Third, we present a comprehensive analysis of video saliency models (the first one, to the best of our knowledge) on existing datasets (Hollywood-2, UCF sports), and our new DHF1K dataset. Results show that our model significantly outperforms previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Video Eye-Tracking Datasets</head><p>There exist several datasets <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b16">17]</ref> for dynamic visual saliency prediction, but they are limited and often lack variety, generality and scalability of instances. Some statistics of these datasets are summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>The Hollywood-2 dataset <ref type="bibr" target="#b42">[43]</ref> comprises all the 1, 707 videos from Hollywood-2 action recognition dataset <ref type="bibr" target="#b41">[42]</ref>. The videos are collected from 69 Hollywood movies with 12 action categories, such as eating, kissing and running. The human fixation data were tracked from 19 observers belonging to 3 groups for free viewing (3 observers), action recognition (12 observers), and context recognition (4 ob- servers). Although this dataset is large, its content is limited to human actions and movie scenes. It mainly focuses on task-driven viewing mode, rather than free viewing. With 1, 000 frames randomly sampled from Hollywood-2, we found 84.5% fixations are located around the faces. The UCF sports fixation dataset <ref type="bibr" target="#b42">[43]</ref> contains 150 videos taken from the UCF sports action dataset <ref type="bibr" target="#b48">[49]</ref>. The videos cover 9 common sports action classes, such as diving, swinging and walking. Similar to Hollywood-2, the viewers have been biased towards task-aware observation by being instructed to "identify the actions occurring in the video sequence". From the statistics of 1, 000 frames randomly selected from UCF sports, we found 82.3% fixations fall inside the human body area.</p><p>The DIEM dataset <ref type="bibr" target="#b43">[44]</ref> is a public video eye-tracking dataset that has 84 videos collected from publicly accessible video resources (e.g., advertisements, documentaries, sport events, and movie trailers, etc). For each video, freeviewing fixations of around 50 observers were collected. This dataset is mainly limited in its coverage and scale.</p><p>Other datasets are either limited in terms of variety and scale of video stimuli <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b16">17]</ref>, or collected for a special purpose (e.g., salient objects in videos <ref type="bibr" target="#b58">[59]</ref>). More importantly, none of the aforementioned datasets includes a preserved test set for avoiding potential data overfitting, which has seriously hampered the research process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Computational Models for Fixation Prediction</head><p>The study of human gaze patterns in static scenes has received significant interests, which can be dated back to <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27]</ref>. Early static saliency models <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b62">63]</ref> are mostly based on the contrast assumption that conspicuous visual features "pop-out" and involuntarily capture attention (see <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> for review). Computational models compute multiple visual features such as color, edge, and orientation at multiple spatial scales to produce a "saliency map": an image distribution predicting the conspicuity of specific locations and their likelihood in attracting attention <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b43">44]</ref>. The locations with more distinct feature responses over surroundings usually gain higher saliency values. Deep learning based static saliency models <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref> have achieved astonishing improvements, relying on the powerful end-to-end learning ability of neural network and the availability of large-scale static saliency datasets <ref type="bibr" target="#b30">[31]</ref>.</p><p>Previous investigations of dynamic human fixation  To date, only a few deep learning based video saliency models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30]</ref> exist in this field. They are mainly based on two-stream network architecture <ref type="bibr" target="#b1">[2]</ref> that accounts for color images and motion fields separately, or two-layer LSTM with object information <ref type="bibr" target="#b29">[30]</ref>. These works show a better performance and demonstrate the potential advantages in applying neural networks to this problem. However, they do not 1) consider attentive mechanisms; 2) utilize existing large-scale static fixation datasets; and 3) exhaustively assess their performance over large amount of data. There are some salient object detection models <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b20">21]</ref> that attempt to uniformly highlight salient object regions in images or videos. Those models are often task-driven and focus on inferring the main object, instead of investigating the behavior of the HVS during scene free viewing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Attention Mechanisms in Neural Networks</head><p>Recently, incorporating attention mechanisms into network architectures has shown great success in several computer vision <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b54">55]</ref> and natural language processing tasks <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b47">48]</ref>. In such studies, attention is learned in an automatic, top-down, and task-specific manner, allowing the network to focus on the most relevant parts in images or sentences. In this paper, we use attention for enhancing intra-frame salient features, thus allowing the LSTM to model dynamic representations more easily. In contrast to previous models learning attentions implicitly, our attention module encodes strong static saliency information and can be learned from existing static saliency dataset in a supervised manner. This design leads to improved generality and prediction performance. It is the first attempt to incorporate a supervised attention mechanism into the network structure to achieve state-of-art results in dynamic fixation prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DHF1K Dataset</head><p>We introduce DHF1K, a large-scale dataset of gaze in free-viewing of videos. Our dataset includes 1K videos with   diverse content and length, with eye-tracking annotations from 17 observers. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the center bias of DHF1K, compared to Hollywood-2, and UCF sports datasets.</p><p>Stimuli. The collection of dynamic stimuli mainly follows the following 4 principles.</p><p>• Large scale and high quality. Both scale and quality are necessary to ensure the content diversity of a dataset and is crucial to guarantee a longer lifespan for a benchmark. To this end, we searched the Youtube engine with about 200 key terms (e.g., dog, walking, car, etc) and carefully selected 1, 000 video sequences from the retrieval results. All videos were converted from their original sources to a 30 fps Xvid MPEG-4 video file in an AVI container and were resized uniformly into 640 × 360 spatial resolution. Thus, DHF1K comprises a total 1, 000 video sequences with 582, 605 frames with total duration of 19, 420 seconds.</p><p>• Diverse content. Stimulus diversity is essential for avoiding overfitting and to delay performance saturation. It offers evenly distributed exogenous control for studying personexternal stimulus factors during scene free-viewing. In DHF1K, each video is manually annotated with a category label (totally 150 classes). Those labels are further classified into 7 main categories (see <ref type="table" target="#tab_3">Table 2</ref>). Those semantic annotations would enable a deeper understanding of the high-level stimuli factors guiding human gaze in dynamic scenes and be indicative for potential research. In <ref type="figure" target="#fig_1">Fig. 2</ref>, we show example frames from each category.</p><p>• Varied motion patterns. Previous investigations <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b43">44]</ref> suggested that motion is one of the key factors that directs attention allocation in dynamic viewing. For this, DHF1K is designed to span varied motion patterns (stable-/slow-/fast-motion of content and camera). Please see <ref type="table">Table  3</ref> for the information regarding motion patterns.</p><p>• Various objects. Previous studies <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b2">3]</ref> in cognitive and computer vision confirmed that object information is indicative to human fixations. The objects in the dataset vary in their type (e.g., human, animal, in <ref type="table" target="#tab_3">Table 2</ref>) and frequency (see <ref type="table">Table 3</ref>). For each video, five subjects were instructed to count the number of the main objects. The majority vote of their counts was considered as the final count.</p><p>For completeness, in <ref type="table" target="#tab_4">Table 4</ref>, we offer the information of the scene illumination and the amount of humans in the dataset. As demonstrated in <ref type="bibr" target="#b44">[45]</ref>, luminance is an important exogenous factor for attentive selection. Further, human beings are important high-level stimuli <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b7">8]</ref> in free-viewing.</p><p>Apparatus and technical specifications. Participants' eye movements were monitored binocularly using a Senso Motoric Instruments (SMI) RED 250 system at a sampling rate of 250 Hz. The dynamic stimuli were displayed on a 19" display (resolution 1440 × 900). A headrest was used to help participants' heads still at a distance of around 68 cm, as advised by the product manual.</p><p>Participants. 17 participants (10 males and 7 females, aging between 20 and 28) who passed the calibration of the eye tracker and had less than 10% fixation dropping rate, were quantified for our eye tracking experiment. All participants had normal or corrected-to-normal vision. All subjects had not seen the stimuli in DHF1K before. All provided informed consent and were naïve to the underlying purposes of the experiment.</p><p>Data capturing. The stimuli were equally partitioned into 10 non-overlapping sessions. Participants were required to freeview 10 sessions of videos in random order. In each session, the videos were also displayed at random. Before the experiments, every participant was calibrated using the standard routine in product manual with recommended settings for the best results. To avoid eye fatigue, each video presentation was followed by a 5-second waiting interval with black screen. After undergoing a session of videos, the participant can take a rest until she was ready for viewing the next session. Finally, 51, 038, 600 fixations were recorded from 17 subjects on 1, 000 videos.</p><p>Training/testing split. We split 1, 000 dynamic stimuli into separate training, validation and test sets. Following random selection, we arrive at a unique split consisting of 600 training and 100 validation videos with publicly available fixation records, as well as 300 test videos with annotations held-out for benchmarking purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our Approach</head><p>Overview. <ref type="figure" target="#fig_2">Fig. 3</ref> presents the overall architecture of our video saliency model. It is based on a CNN-LSTM architecture that combines convolutional network and recurrent model to exploit both spatial and temporal information for predicting video saliency. The CNN-LSTM network is extended with a supervised attention mechanism, which explicitly captures static saliency information and allows the LSTM to focus on learning dynamic information. The attention module is trained from rich static eye-tracking data. Thus our model is able to produce accurate, spatiotemporal saliency with improved generalization ability. Next, we explain each component of our model in detail.</p><p>CNN-LSTM architecture. Formally, given an input video {I t } t , we first obtain a sequence of convolutional features {X t } t from CNN. Then the features {X t } t are fed into a convLSTM <ref type="bibr" target="#b65">[66]</ref> as input. Here, the convLSTM is used for modeling the temporal dynamic nature of this sequential problem, which is achieved by incorporating memory units with gated operations. Additionally, through replacing dot products with convolutional operations, the convLSTM is able to preserve spatial information, which is essential for making spatially-variant pixel-level prediction.</p><p>More specifically, the convLSTM utilizes three convolution gates (input, output and forget) to control the flow of signal within the cell. With the input feature X t at time step t, the convLSTM outputs a hidden state H t and maintains a memory cell C t for controlling state update and output:</p><formula xml:id="formula_0">it = σ(W X i * Xt +W H i * Ht−1 +W C i • Ct−1 +bi),<label>(1)</label></formula><formula xml:id="formula_1">ft = σ(W X f * Xt +W H f * Ht−1 +W C f • Ct−1 +b f ),<label>(2)</label></formula><formula xml:id="formula_2">ot = σ(W X o * Xt +W H o * Ht−1 +W C o • Ct +bo),<label>(3)</label></formula><formula xml:id="formula_3">Ct = ft • Ct−1 +it •tanh(W X c * Xt +W H c * Ht−1 +bc), (4) Ht = ot • tanh(Ct),<label>(5)</label></formula><p>i t , f t , o t are the gates. σ and tanh are the activation functions of logistic sigmoid and hyperbolic tangent, respectively. ' * ' denotes the convolution operator and '•' represents Hadamard product. The dynamic fixation map can be obtained via convolving the hidden states H with a 1 × 1 kernel (see <ref type="figure" target="#fig_2">Fig. 3 (c)</ref>). In our implementation, the first five conv blocks of VGG-16 <ref type="bibr" target="#b52">[53]</ref> are used. For preserving more spatial details, we remove pool4 and pool5 layers, which results in ×8 instead of ×32 downsampling. At time step t, with an input frame I t with 224×224 resolution, we have X t ∈ R 28×28×512 and a 28×28 dynamic saliency map from the convLSTM. The kernel size of the conv layer in convLSTM is set as 3.</p><p>Attention module. We extend above CNN-LSTM architecture with an attention mechanism, which is learned from existing static fixation data in a supervised manner. Such design is mainly driven by the following three motivations: • Previous studies <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b63">64]</ref> shown that human attention is guided by both static and dynamic factors. Through the additional attention module, the CNN is enforced to generate a more explicit spatial saliency representation. This helps disentangle underlying spatial and temporal factors of dynamic attention, and allows convLSTM better capture temporal dynamics.</p><p>• CNN-LSTM architecture introduces a large number of parameters for modeling spatial and temporal patterns. However, for sequential data such as videos, obtaining labelled data is costly. Even though there are large-scale datasets like DHF1K that have 1K videos, the amount of training data is still insufficient, considering the high correlation among frames within same video. The supervised attentive module is able to leverage existing rich static fixation data to improve the generalization power of our model.</p><p>• In VGG-16, we remove the last two pooling layers to obtain a large feature map. This dramatically decreases the receptive field (212×212→140×140), which cannot cover the whole frame (224×224). To remedy this, we insert a set of down-and up-sampling operations into the attention module, which would enhance the intra-frame saliency information with an enlarged receptive field. By this, our model is able to make more accurate predictions from a global view.</p><p>As demonstrated in <ref type="figure" target="#fig_2">Fig. 3 (b)</ref>, our attentive module is built upon the conv5-3 layer, as an additional branch of several conv layers interleaved with pooling, and upsampling operations. Given the input feature X , with pooling layers, the attention module generates a downsampled attention map (7×7) with an enlarged receptive field (260×260). Then the small attention map is ×4 upsampled as the same spatial dimensions of X . Let M ∈ [0, 1]</p><p>28×28 be the upsampled attention map, the feature X ∈ R 28×28×512 from conv5-3 layer can be further enhanced by:</p><formula xml:id="formula_4">X c = M • X c ,<label>(6)</label></formula><p>where c ∈ {1, . . . , 512} is the index of the channel. Here, the attention module work as a feature selector to enhance the feature representation.</p><p>The above attention module may lose useful information for learning a dynamic saliency representation, as the attention module only considers static saliency information in still video frames. For this, inspired by the recent advances of attention mechanism and residual connection <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b54">55]</ref>, we improve Eq. 6 in residual form:</p><formula xml:id="formula_5">X c = (1 + M ) • X c .<label>(7)</label></formula><p>With the residual connection, both the original CNN features and the enhanced features are combined and fed to the LSTM model. In §5.2 and §5.4, more detailed explorations for the attention module are offered. Different from previous attention mechanisms that learn task-related attention in an implicit way, our attention module can learn from existing large-scale static fixation data in an explicit and supervised manner (detailed in next part).</p><p>Loss function. We use the following loss function <ref type="bibr" target="#b23">[24]</ref> that considers three different saliency evaluation metrics instead of one. The rationale here is that no single metric can fully capture how satisfactory a saliency map is.</p><p>We denote the predicted saliency map as Y ∈ [0, 1] 28×28 , the map of fixation locations as P ∈ {0, 1} 28×28 and the continuous saliency map (distribution) as Q ∈ [0, 1] 28×28 . Here the fixation map P is discrete, that records whether a pixel receives human fixation. The continuous saliency map is obtained via blurring each fixation location with a small Gaussian kernel. Our loss functions is defined as follows:</p><formula xml:id="formula_6">L(Y,P,Q) =LKL(Y,Q)+α1LCC (Y,Q)+α2LNSS(Y,P ),<label>(8)</label></formula><p>where L KL , L CC and L N SS are the Kullback-Leibler (KL) divergence, the Linear Correlation Coefficient (CC), and the Normalized Scanpath Saliency (NSS), respectively, which are derived from commonly used metrics to evaluate saliency prediction models. αs are balance parameters and are empirically set to α 1 = α 2 = 0.1. L KL is widely adopted for training saliency models and is chosen as the primary loss in our work:</p><formula xml:id="formula_7">LKL(Y, Q) = x Q(x) log Q(x) Y (x) .<label>(9)</label></formula><p>L CC measures the linear relationship between Y and Q:</p><formula xml:id="formula_8">LCC (Y, Q) = − cov(Y, Q) ρ(Y )ρ(Q) ,<label>(10)</label></formula><p>where cov(Y, Q) is the covariance of Y and Q, and ρ(·) stands for standard deviation. L N SS is derived from NSS metric:</p><formula xml:id="formula_9">LNSS(Y, P ) = − 1 N x Y (x) × P (x),<label>(11)</label></formula><p>where</p><formula xml:id="formula_10">Y = Y −µ(Y )</formula><p>ρ(Y ) and N = x P (x). It is calculated by taking the mean of scores from the normalized saliency map Y (with zero mean and unit standard deviation) at human eye fixations P . Since CC and N SS are similarity metrics, their negatives are adopted for minimization.</p><p>Training protocol. Our model is iteratively trained with sequential fixation and image data. In training, a video training batch is cascaded with an image training batch. More specifically, in a video training batch, we apply a loss defined over the final dynamic saliency prediction from LSTM. Let {Y denote the dynamic saliency predictions, the dynamic fixation sequence and the continuous ground-truth saliency maps, we minimize the following loss:</p><formula xml:id="formula_11">L d = T t=1 L(Y d t , P d t , Q d t ).<label>(12)</label></formula><p>In this process, the attention module is trained in an implicit way, since we do not have the groundtruth fixation of each frame in static scene.</p><p>In an image training batch, we only train our attention module via minimizing:</p><formula xml:id="formula_12">L s = L(M, P s , Q s ),<label>(13)</label></formula><p>where the M , P s , Q s indicate the attention map for our static attention module, the ground-truth static fixation map, and the ground-truth static saliency map. In this process, the training of attention module is supervised by the groundtruth static fixation. Note that, in image training batch, we do not train our LSTM module, as it is used for learning the dynamic representation.</p><p>For each video training batch, 20 consecutive frames from the same video are used. Both the video and the start frame are randomly selected. For each image training batch, we set the batch size as 20, and the images are randomly sampled from existing static fixation dataset. More implementation details can be found in § 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>Training/testing protocols. We use the static stimuli (10, 000 images) from the training set of SALICON <ref type="bibr" target="#b30">[31]</ref> dataset for training our attention module. For dynamic stimuli, we consider 4 settings: using the training set(s) from (i) DHF1K, (ii) Hollywood-2, (iii) UCF sports, and (iv) DHF1K+Hollywood-2+UCF sports. For DHF1K, we use the original training/validation/testing splitting (600/100/300). For Hollywood-2, following <ref type="bibr" target="#b41">[42]</ref>, 823 videos for training and 884 videos for testing. For UCF sports, the training and testing sets include 103 and 47 videos, respectively, as suggested by <ref type="bibr" target="#b48">[49]</ref>. We randomly sample 10% videos from the training sets of Hollywood-2, and UCF sports as their validation sets. We evaluate our model on the testing sets of DHF1K, Hollywood-2, and UCF sports dataset, in total 1, 231 video sequences with more than 400, 000 frames. Implementation details. Our model is implemented in Python on Keras, and trained with the Adam optimizer <ref type="bibr" target="#b33">[34]</ref>. During the training phase, the learning rate was set to 0.0001 and was decreased by a factor of 10 every 2 epochs. The network was trained for 10 epochs. We perform earlystopping on the validation set. Competitors. We compare our model with nine dynamic saliency models: PQFT <ref type="bibr" target="#b15">[16]</ref>, Seo et al. <ref type="bibr" target="#b51">[52]</ref>, Rudoy et al. <ref type="bibr" target="#b49">[50]</ref>, Hou et al. <ref type="bibr" target="#b22">[23]</ref>, Fang et al. <ref type="bibr" target="#b12">[13]</ref>, OBDL <ref type="bibr">[20]</ref>, AWS-D <ref type="bibr" target="#b36">[37]</ref>, OM-CNN <ref type="bibr" target="#b29">[30]</ref>, and Two-stream <ref type="bibr" target="#b1">[2]</ref> 1 . For the sake of complementary, we further compare with six state-of-theart static attention models: ITTI <ref type="bibr" target="#b27">[28]</ref>, GBVS <ref type="bibr" target="#b17">[18]</ref>, SAL-ICON <ref type="bibr" target="#b23">[24]</ref>, DVA <ref type="bibr" target="#b56">[57]</ref>, Shallow-Net <ref type="bibr" target="#b46">[47]</ref>, and Deep-Net <ref type="bibr" target="#b46">[47]</ref>. OM-CNN, Two-stream, SALICON, DVA, ShallowNet, and Deep-Net are deep learning models, and others are classical saliency models. Those models are selected due to: 1) their representability of the diversity of the state-ofthe-art; or 2) publicly available implementations. Baselines. We further derive 8 baselines. For each training setting, we derive two baselines: Our and Attention module, refer to our final dynamic saliency prediction and the intermediate output of our attention module, respectively. Evaluation metrics. Here, we employ five classic metrics, namely Normalized Scanpath Saliency (NSS), Similarity Metric (SIM), Linear Correlation Coefficient (CC), AUC-Judd (AUC-J), and shuffled AUC (s-AUC). Please refer to <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b56">57]</ref> for detailed descriptions of these metrics. Computation load. The whole model is trained in an endto-end manner. The entire training procedure takes about 30 hours with a single NVIDIA TITAN X GPU and a 4.0GHz Intel processor (in training setting (iv)). Since our model does not need any pre-or post-processing, it takes only about 0.08s to process an frame image of size 224 × 224.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Performance comparison</head><p>Performance on DHF1K. <ref type="table" target="#tab_5">Table 5</ref> reports the comparative results with the aforementioned saliency models, on the testing set (300 video sequences) of DHF1K dataset. In can be observed that the proposed model consistently and significantly outperforms other competitors, across all the metrics. This can be contributed to our specially designed atten-  tion module, which makes our model explicitly learn static and dynamic saliency representations in CNN and LSTM separately. Our model even does not use any optical flow algorithm (different with Fang et al. <ref type="bibr" target="#b12">[13]</ref>, Two-stream <ref type="bibr" target="#b1">[2]</ref>). This significantly improves the applicability of our model and demonstrates the effectiveness of our training protocol that leveraging both static and dynamic stimuli. Performance on Hollywood-2. We further test our model on Hollywood-2 dataset, where the testing sets comprises 884 video sequences. The results are summarized in <ref type="table" target="#tab_5">Table  5</ref>. Again, our model consistently significantly higher than other methods across various metrics. Besides, when we go insight into the performance with training settings, the performance would increase with increasing amount of training data. This suggests that the large-scale training data volume is important for the performance of neural network. Performance on UCF sports. With the test set (47 video sequences) of UCF sports dataset, we again observe the proposed model provides consistently good results, compared to related state-of-the-art (see <ref type="table" target="#tab_5">Table 5</ref>). Interestingly, we find that, with small amount of training data (training setting (iii), 103 video stimuli from UCF sports dataset), the proposed model achieves a very high performance, even better than the model (Our, training setting (iv)) trained with large-scale data (1.5K video stimuli). This could be explained by lack of diversity in the video training data, as the videos in UCF sports dataset are highly related (with similar scenes and actors) and small scale. This is also consistent with our research for UCF sports which shows that 82.3% fixations are located on the human body area (see § 2.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Analysis</head><p>Based on our extensive experiments, we provide more detailed analyses, which would give deeper insights of previous studies and suggest some hints for future research. Dynamic saliency models: deep vs non-deep learning. In dynamic scenes, previous deep learning based dynamic saliency models (i.e., OM-CNN, Two-stream) show significant improvements over classic dynamic models (e.g., PQFT, <ref type="bibr">Seo et al. et</ref>   <ref type="table">Table 6</ref>. Ablation study on DHF1K. See §5.4 for details.</p><p>underlying mechanisms of visual attention allocation during dynamic viewing are more complex and still not clear. Second, previous studies are more focused on computational models of static saliency, while less efforts were paid for modeling dynamic saliency. Deep learning models: static vs dynamic. Compared with state-of-the-art deep learning based static models (i.e., DVA, Deep-Net), previous deep learning based dynamic models (i.e., OM-CNN, Two-stream) only obtain slightly better performance (or only competitive). Although strong motion information (i.e., optical flow, motion network) have been encoded into OM-CNN and Two-stream, their performance are still limited. We attribute this into the inherent difficulties of video saliency prediction and previous models' neglect of utilizing existing rich static saliency data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation study</head><p>In this section, we offer a more detailed exploration of our proposed approach in several aspects with DHF1K dataset. We verify the effectiveness of the proposed mechanism, and examine the influence of different training protocols. The results are summarized in <ref type="table">Table 6</ref>. Effect of attention mechanism. By disabling the attention module, and only training with video stimuli we observe a performance drop (e.g., AUC-J: 0.890→0.847), verifying the effectiveness of attention module and showing that the leverage of static stimuli indeed improves the predication accuracy in dynamic scenes. For exploring the effect of the residual connection in attention module (Eq. 8), we train the model based on Eq. 5 (without residual connection). We observe a minor decrease; showing that employing residual connection could avoid distorting spatial features in frames. In our attention module, we apply down-sampling for enlarging the receptive field. We also study the influence of such design. We find that the attention module with enlarged receptive field would gain better performance, since the model could make prediction in global view.</p><p>Training. We assess different training protocols. By reducing the amount of static training stimuli from 10K to 5K, we observe a performance drop (e.g., AUC-J: 0.890→0.877). The baseline (w/o attention) can also be viewed as the model without any static training stimuli, which gains worse performance (e.g., AUC-J: 0.890→0.847).</p><p>Effect of convLSTM. To study the influence of convLSTM, we re-train our model without convLSTM (using training setting (iv)) and get a baseline: w/o convLSTM. We observe a drop of performance; showing that the dynamic information learnt in convLSTM could boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Conclusion</head><p>In this work, we presented "Dynamic Human Fixation (DHF1K)", a large-scale carefully designed and systematically collected benchmark dataset to facilitate research in video saliency modeling. To the best of our knowledge, our work is the most comprehensive performance evaluation of video saliency models. DHF1K contains 1K videos, which capture representative instances, diverse contents and various motions, with human eye-tracking annotations.</p><p>Further, we proposed a novel deep learning based video saliency model, which encodes a supervised attention mechanism to explicitly capture static saliency information and help LSTM better capture dynamic saliency representations over successive frames. We performed extensive experiments on DHF1K, Hollywood-2, and UCFsports datasets, and analyzed the performance of our model compared to previous attention models in dynamic scenes. Our experimental results demonstrate that our proposed model outperforms other competitors and is quite efficient.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Average annotation maps of three datasets used in benchmarking: (a) Hollywood-2, (b) UCF sports, (c) DHF1K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Example frames from DHF1K with fixations (red dots) and corresponding categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Network architecture of the proposed video saliency model. (a) Attentive CNN-LSTM architecture. (b) CNN layers with attention module are used for learning intra-frame static features, where the attention module is learned with the supervision from static saliency data. (c) ConvLSTM used for learning sequential saliency representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>*</head><label></label><figDesc>Non-deep learning model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Statistics of typical dynamic eye-tracking datasets.</figDesc><table>Dataset 
Year Videos Resolution Duration(s) Viewers Task 
CRCNS [25] 2004 50 
640 × 480 
6-94 
15 
task-goal 
Hollywood-2 [43] 2012 1,707 720 × 480 
2-120 
19 
task-goal 
UCF sports [43] 2012 150 720 × 480 
2-14 
19 
task-goal 
DIEM [44] 2011 84 1280 × 720 27-217 
∼50 free-view 
SFU [17] 2012 12 
352 × 288 
3-10 
15 
free-view 
DHF1K(Ours) 2017 1,000 640 × 360 
17-42 
17 
free-view 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Number of sub-classes in each category is reported. For example, Sports has sub-classes like swimming, jumping, etc.</figDesc><table>DHF1K 
Human 
Animal Artifact Scenery 
Daily ac. Sports Social ac. Art 
#sub-classes 

* 

20 
29 
13 
10 
36 
21 
21 
#videos 
134 
185 
116 
101 192 
162 
110 
 *  </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Statistics for video categories in DHF1K dataset.</figDesc><table>DHF1K 
Content motion 
Camera motion 
#Objects 
stable slow fast stable slow fast 0 
1 
2 ≥3 
#videos 
126 505 369 343 386 271 56 335 254 355 

Table 3. Statistics regarding motion patterns and number of 
main objects in DHF1K dataset. 

DHF1K 
Scene illumination 
#People 
day night indoor 0 
1 
2 ≥3 
#videos 577 37 
386 345 307 236 112 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Statistics regarding scene illumination and number of people in DHF1K dataset.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 .</head><label>5</label><figDesc>Quantitative results on DHF1K, Hollywood2, and UCF sports datasets. The best scores are marked in bold.</figDesc><table>Training settings 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>al., Rudoy et al., Hou et al., Fang et al.). This demonstrates the strong learning ability of neural net- work and the promise of developing neural network in this challenging area. Non-deep learning models: static vs dynamic. An inter- esting finding is classic dynamic methods (i.e., PQFT, Seo et al., Rudoy et al., Hou et al., Fang et al.) did not perform better than their static counterparts: ITTI, GBVS. This is probably due to two reasons. First, the perceptual cues andFigure 4. Qualitative results of our video saliency model on three datasets. Best viewed in color.</figDesc><table>DHF1K 

Hollywood-2 
UCF sports 

Aspects 
Variants 
AUC-J↑ SIM↑ s-AUC↑ CC↑ NSS↑ 

Baseline 
training setting (iv) 

(1.5K videos+10K images) 

0.890 0.315 0.601 0.434 2.354 

Attention 

w/o attention 

(1.5K videos) 

0.847 0.236 0.579 0.306 1.685 

module 
w/o residual connection 

(1.5K videos+10K images) 

0.874 0.303 0.594 0.401 2.174 

w/o downsampling 

(1.5K videos+10K images) 

0.870 0.298 0.583 0.389 2.085 

Training 
reduced training samples 

(1.5K videos+5K images) 

0.877 0.297 0.588 0.372 2.098 

convLSTM 
w/o convLSTM 

(1.5K videos+10K images) 

0.867 0.269 0.573 0.382 2.034 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We re-implemented [2] since the official codes cannot run correctly.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spatio-temporal saliency networks for dynamic saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kocak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What is a salient object? A dataset and a baseline model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="742" to="756" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">State-of-the-art in visual attention modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Sihite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="69" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Saliency based on information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Where should saliency models look next? In ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Look and think twice: Capturing top-down visual attention with feedback convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting human gaze using low-level saliency combined with face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cerf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Einhäuser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video saliency incorporating spatiotemporal cues and uncertainty weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The discriminant center-surround hypothesis for bottom-up saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminant saliency for visual recognition from cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Eye-tracking database for a set of standard video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hadizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Enriquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Bajic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="898" to="903" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How many bits does it take for a stimulus to be salient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khatoonabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Bajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2015</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Saliency detection: A spectral residual approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic visual attention: Searching for coding length increments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SALICON: Reducing the semantic gap in saliency prediction by adapting deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic foveation for video compression using a neurobiological model of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1304" to="1318" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Quantifying the contribution of low-level saliency to human eye movements in dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Cognition</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Computational modelling of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="194" to="203" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end saliency mapping via probability distribution prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jetley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Predicting video saliency with object-to-motion CNN and two-layer convolutional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06316</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SALICON: Saliency in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A benchmark of computational models of saliency to predict human fixations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIT Technical Report</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deepfix: A fully convolutional neural network for predicting human eye fixations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ayush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A coherent computational approach to model bottom-up visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thoreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="802" to="817" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dynamic whitening saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Leboran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">R</forename><surname>Fdez-Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">M</forename><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to predict eye fixations via multiresolution convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNNLS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spatiotemporal saliency in dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Actions in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Actions in the eye: dynamic gaze datasets and learnt saliency models for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1408" to="1424" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Clustering of gaze during dynamic scene viewing is predicted by motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Mital</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Neural correlates of attentive selection for color or luminance in extrastriate area V4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Motter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2178" to="2189" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image question answering using convolutional neural network with dynamic parameter prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Hongsuck</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Shallow and deep convolutional networks for saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Action mach a spatio-temporal maximum average correlation height filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning video saliency from human gaze using candidate selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rudoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnikmanor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Static and space-time visual saliency detection by self-resemblance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Large-scale optimization of hierarchical features for saliency prediction in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep cropping via attention box prediction and aesthetics assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep visual attention prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Saliency-aware geodesic video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Consistent video saliency using local gradient flow optimization and global refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4185" to="4196" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Video salient object detection via fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Correspondence driven saliency transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5025" to="5034" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Saliencyaware video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="33" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Stereoscopic thumbnail creation via efficient stereo saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2014" to="2027" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Where to look next? Combining static and dynamic proto-objects in a tva-based model of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wischnewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Belardinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">X</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Steil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognitive Computation</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="326" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Guided search 4.0. Integrated models of cognitive systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="99" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Cottrell. SUN: A bayesian framework for saliency using natural statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="32" to="32" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
