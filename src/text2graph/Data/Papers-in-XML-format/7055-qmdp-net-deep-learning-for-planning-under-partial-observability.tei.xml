<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">QMDP-Net: Deep Learning for Planning under Partial Observability</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Karkus</surname></persName>
							<email>karkus@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">NUS Graduate School for Integrative Sciences and Engineering</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hsu</surname></persName>
							<email>dyhsu@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">NUS Graduate School for Integrative Sciences and Engineering</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee</forename><forename type="middle">Sun</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">QMDP-Net: Deep Learning for Planning under Partial Observability</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>This paper introduces the QMDP-net, a neural network architecture for planning under partial observability. The QMDP-net combines the strengths of model-free learning and model-based planning. It is a recurrent policy network, but it represents a policy for a parameterized set of tasks by connecting a model with a planning algorithm that solves the model, thus embedding the solution structure of planning in a network learning architecture. The QMDP-net is fully differentiable and allows for end-to-end training. We train a QMDPnet on different tasks so that it can generalize to new ones in the parameterized task set and "transfer" to other similar tasks beyond the set. In preliminary experiments, QMDP-net showed strong performance on several robotic tasks in simulation. Interestingly, while QMDP-net encodes the QMDP algorithm, it sometimes outperforms the QMDP algorithm in the experiments, as a result of end-to-end learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Decision-making under uncertainty is of fundamental importance, but it is computationally hard, especially under partial observability <ref type="bibr" target="#b23">[24]</ref>. In a partially observable world, the agent cannot determine the state exactly based on the current observation; to plan optimal actions, it must integrate information over the past history of actions and observations. See <ref type="figure" target="#fig_0">Fig. 1</ref> for an example. In the model-based approach, we may formulate the problem as a partially observable Markov decision process (POMDP). Solving POMDPs exactly is computationally intractable in the worst case <ref type="bibr" target="#b23">[24]</ref>. Approximate POMDP algorithms have made dramatic progress on solving large-scale POMDPs <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref>; however, manually constructing POMDP models or learning them from data remains difficult. In the model-free approach, we directly search for an optimal solution within a policy class. If we do not restrict the policy class, the difficulty is data and computational efficiency. We may choose a parameterized policy class. The effectiveness of policy search is then constrained by this a priori choice.</p><p>Deep neural networks have brought unprecedented success in many domains <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30]</ref> and provide a distinct new approach to decision-making under uncertainty. The deep Q-network (DQN), which consists of a convolutional neural network (CNN) together with a fully connected layer, has successfully tackled many Atari games with complex visual input <ref type="bibr" target="#b20">[21]</ref>. Replacing the postconvolutional fully connected layer of DQN by a recurrent LSTM layer allows it to deal with partial observaiblity <ref type="bibr" target="#b9">[10]</ref>. However, compared with planning, this approach fails to exploit the underlying sequential nature of decision-making.</p><p>We introduce QMDP-net, a neural network architecture for planning under partial observability. QMDP-net combines the strengths of model-free learning and model-based planning. A QMDP-net is a recurrent policy network, but it represents a policy by connecting a POMDP model with an algorithm that solves the model, thus embedding the solution structure of planning in a network <ref type="bibr">31st</ref> Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. ) A policy trained on expert demonstrations in a set of randomly generated environments generalizes to a new environment. It also "transfers" to a much larger real-life environment, represented as a LIDAR map <ref type="bibr" target="#b11">[12]</ref>.</p><p>learning architecture. Specifically, our network uses QMDP <ref type="bibr" target="#b17">[18]</ref>, a simple, but fast approximate POMDP algorithm, though other more sophisticated POMDP algorithms could be used as well.</p><p>A QMDP-net consists of two main network modules <ref type="figure" target="#fig_1">(Fig. 2</ref>). One represents a Bayesian filter, which integrates the history of an agent's actions and observations into a belief, i.e. a probabilistic estimate of the agent's state. The other represents the QMDP algorithm, which chooses the action given the current belief. Both modules are differentiable, allowing the entire network to be trained end-to-end.</p><p>We train a QMDP-net on expert demonstrations in a set of randomly generated environments. The trained policy generalizes to new environments and also "transfers" to more complex environments ( <ref type="figure" target="#fig_0">Fig. 1c-d)</ref>. Preliminary experiments show that QMDP-net outperformed state-of-the-art network architectures on several robotic tasks in simulation. It successfully solved difficult POMDPs that require reasoning over many time steps, such as the well-known Hallway2 domain <ref type="bibr" target="#b17">[18]</ref>. Interestingly, while QMDP-net encodes the QMDP algorithm, it sometimes outperformed the QMDP algorithm in our experiments, as a result of end-to-end learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Planning under Uncertainty</head><p>A POMDP is formally defined as a tuple (S, A, O, T, Z, R), where S, A and O are the state, action, and observation space, respectively. The state-transition function T (s, a, s 0 ) = P (s 0 |s, a) defines the probability of the agent being in state s 0 after taking action a in state s. The observation function Z(s, a, o) = p(o|s, a) defines the probability of receiving observation o after taking action a in state s. The reward function R(s, a) defines the immediate reward for taking action a in state s.</p><p>In a partially observable world, the agent does not know its exact state. It maintains a belief, which is a probability distribution over S. The agent starts with an initial belief b 0 and updates the belief b t at each time step t with a Bayesian filter:</p><formula xml:id="formula_0">b t (s 0 ) = ⌧ (b t 1 , a t , o t ) = ⌘Z(s 0 , a t , o t ) P s2S T (s, a t , s 0 )b t 1 (s),<label>(1)</label></formula><p>where ⌘ is a normalizing constant. The belief b t recursively integrates information from the entire past history (a 1 , o 1 , a 2 , o 2 , . . . , a t , o t ) for decision making. POMDP planning seeks a policy ⇡ that maximizes the value, i.e., the expected total discounted reward:</p><formula xml:id="formula_1">V ⇡ (b 0 ) = E P 1 t=0 t R(s t , a t+1 ) b 0 , ⇡ ,<label>(2)</label></formula><p>where s t is the state at time t, a t+1 = ⇡(b t ) is the action that the policy ⇡ chooses at time t, and 2 (0, 1) is a discount factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Related Work</head><p>To learn policies for decision making in partially observable domains, one approach is to learn models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26]</ref> and solve the models through planning. An alternative is to learn policies directly <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref>. Model learning is usually not end-to-end. While policy learning can be end-to-end, it does not exploit model information for effective generalization. Our proposed approach combines model-based and model-free learning by embedding a model and a planning algorithm in a recurrent neural network (RNN) that represents a policy and then training the network end-to-end.</p><p>RNNs have been used earlier for learning in partially observable domains <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. In particular, Hausknecht and Stone extended DQN <ref type="bibr" target="#b20">[21]</ref>, a convolutional neural network (CNN), by replacing its post-convolutional fully connected layer with a recurrent LSTM layer <ref type="bibr" target="#b9">[10]</ref>. Similarly, Mirowski et al. <ref type="bibr" target="#b19">[20]</ref> considered learning to navigate in partially observable 3-D mazes. The learned policy generalizes over different goals, but in a fixed environment. Instead of using the generic LSTM, our approach embeds algorithmic structure specific to sequential decision making in the network architecture and aims to learn a policy that generalizes to new environments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview</head><p>We want to learn a policy that enables an agent to act effectively in a diverse set of partially observable stochastic environments. Consider, for example, the robot navigation domain in <ref type="figure" target="#fig_0">Fig. 1</ref>. The environments may correspond to different buildings. The robot agent does not observe its own location directly, but estimates it based on noisy readings from a laser range finder. It has access to building maps, but does not have models of its own dynamics and sensors. While the buildings may differ significantly in their layouts, the underlying reasoning required for effective navigation is similar in all buildings. After training the robot in a few buildings, we want to place the robot in a new building and have it navigate effectively to a specified goal.</p><p>Formally, the agent learns a policy for a parameterized set of tasks in partially observable stochastic environments:</p><formula xml:id="formula_2">W ⇥ = {W (✓) | ✓ 2 ⇥},</formula><p>where ⇥ is the set of all parameter values. The parameter value ✓ captures a wide variety of task characteristics that vary within the set, including environments, goals, and agents. In our robot navigation example, ✓ encodes a map of the environment, a goal, and a belief over the robot's initial state. We assume that all tasks in W ⇥ share the same state space, action space, and observation space. The agent does not have prior models of its own dynamics, sensors, or task objectives. After training on tasks for some subset of values in ⇥, the agent learns a policy that solves W (✓) for any given ✓ 2 ⇥.</p><p>A key issue is a general representation of a policy for W ⇥ , without knowing the specifics of W ⇥ or its parametrization. We introduce the QMDP-net, a recurrent policy network. A QMDP-net represents a policy by connecting a parameterized POMDP model with an approximate POMDP algorithm and embedding both in a single, differentiable neural network. Embedding the model allows the policy to generalize over W ⇥ effectively. Embedding the algorithm allows us to train the entire network end-to-end and learn a model that compensates for the limitations of the approximate algorithm.</p><formula xml:id="formula_3">Let M (✓) = (S, A, O, f T (·|✓), f Z (·|✓), f R (·|✓))</formula><p>be the embedded POMDP model, where S, A and O are the shared state space, action space, observation space designed manually for all tasks in W ⇥ and f T (·|·), f Z (·|·), f R (·|·) are the state-transition, observation, and reward functions to be learned from data. It may appear that a perfect answer to our learning problem would have f T (·|✓), f Z (·|✓), and f R (·|✓) represent the "true" underlying models of dynamics, observation, and reward for the task W (✓). This is true only if the embedded POMDP algorithm is exact, but not true in general. The agent may learn an alternative model to mitigate an approximate algorithm's limitations and obtain an overall better policy. In this sense, while QMDP-net embeds a POMDP model in the network architecture, it aims to learn a good policy rather than a "correct" model.</p><p>A QMDP-net consists of two modules <ref type="figure" target="#fig_1">(Fig. 2)</ref>. One encodes a Bayesian filter, which performs state estimation by integrating the past history of agent actions and observations into a belief. The other encodes QMDP, a simple, but fast approximate POMDP planner <ref type="bibr" target="#b17">[18]</ref>. QMDP chooses the agent's actions by solving the corresponding fully observable Markov decision process (MDP) and performing one-step look-ahead search on the MDP values weighted by the belief.</p><p>We evaluate the proposed network architecture in an imitation learning setting. We train on a set of expert trajectories with randomly chosen task parameter values in ⇥ and test with new parameter values. An expert trajectory consist of a sequence of demonstrated actions and observations</p><formula xml:id="formula_4">(a 1 , o 1 , a 2 , o 2 , . . .) for some ✓ 2 ⇥.</formula><p>The agent does not access the ground-truth states or beliefs along the trajectory during the training. We define loss as the cross entropy between predicted and demonstrated action sequences and use RMSProp <ref type="bibr" target="#b34">[35]</ref> for training. See Appendix C.7 for details. Our implementation in Tensorflow <ref type="bibr" target="#b0">[1]</ref> is available online at http://github.com/AdaCompNUS/qmdp-net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">QMDP-Net</head><p>We assume that all tasks in a parameterized set W ⇥ share the same underlying state space S, action space A, and observation space O. We want to learn a QMDP-net policy for W ⇥ , conditioned on the parameters ✓ 2 ⇥. A QMDP-net is a recurrent policy network. The inputs to a QMDP-net are the action a t 2 A and the observation o t 2 O at time step t, as well as the task parameter ✓ 2 ⇥. The output is the action a t+1 for time step t + 1.</p><formula xml:id="formula_5">A QMDP-net encodes a parameterized POMDP model M (✓) = (S, A, O, T = f T (·|✓), Z = f Z (·|✓), R = f R (·|✓)</formula><p>) and the QMDP algorithm, which selects actions by solving the model approximately. We choose S, A, and O of M (✓) manually, based on prior knowledge on W ⇥ , specifically, prior knowledge on S, A, and O. In general, S 6 = S, A 6 = A, and O 6 = O. The model states, actions, and observations may be abstractions of their real-world counterparts in the task. In our robot navigation example <ref type="figure" target="#fig_0">(Fig. 1)</ref>, while the robot moves in a continuous space, we choose S to be a grid of finite size. We can do the same for A and O, in order to reduce representational and computational complexity. The transition function T , observation function Z, and reward function R of M (✓) are conditioned on ✓, and are learned from data through end-to-end training. In this work, we assume that T is the same for all tasks in W ⇥ to simplify the network architecture. In other words, T does not depend on ✓.</p><p>End-to-end training is feasible, because a QMDP-net encodes both a model and the associated algorithm in a single, fully differentiable neural network. The main idea for embedding the algorithm in a neural network is to represent linear operations, such as matrix multiplication and summation, by convolutional layers and represent maximum operations by max-pooling layers. Below we provide some details on the QMDP-net's architecture, which consists of two modules, a filter and a planner. Filter module. The filter module <ref type="figure" target="#fig_2">(Fig. 3a)</ref> implements a Bayesian filter. It maps from a belief, action, and observation to a next belief,</p><formula xml:id="formula_6">b t+1 = f (b t |a t , o t ).</formula><p>The belief is updated in two steps. The first accounts for actions, the second for observations:</p><formula xml:id="formula_7">b 0 t (s) = P s 0 2S T (s 0 , a t , s)b t (s 0 ),<label>(3)</label></formula><formula xml:id="formula_8">b t+1 (s) = ⌘Z(s, o t )b 0 t (s),<label>(4)</label></formula><p>where o t 2 O is the observation received after taking action a t 2 A and ⌘ is a normalization factor.</p><p>We implement the Bayesian filter by transforming Eq. <ref type="formula" target="#formula_7">(3)</ref> and Eq. (4) to layers of a neural network. For ease of discussion consider our N ⇥N grid navigation task <ref type="figure" target="#fig_0">(Fig. 1a-c)</ref>. The agent does not know its own state and only observes neighboring cells. It has access to the task parameter ✓ that encodes the obstacles, goal, and a belief over initial states. Given the task, we choose M (✓) to have a N ⇥N state space. The belief, b t (s), is now an N ⇥N tensor. </p><p>Eq. (4) incorporates observations through an observation model Z(s, o). Now Z(s, o) is a N ⇥N ⇥|O| tensor that represents the probability of receiving observation o 2 O in state s 2 S. In our grid navigation task observations depend on the obstacle locations. We condition Z on the task parameter,</p><formula xml:id="formula_10">Z(s, o) = f Z (s, o|✓) for ✓ 2 ⇥.</formula><p>The function f Z is a neural network, mapping from ✓ to Z(s, o). In this paper f Z is a CNN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Z(s)</head><formula xml:id="formula_11">= P o2O Z(s, o)w o t .<label>(6)</label></formula><p>Finally, we obtain the updated belief, b t+1 (s), by multiplying b 0 t (s) and Z(s) element-wise, and normalizing over states. In our setting the initial belief for the task W (✓) is encoded in ✓. We initialize the belief in QMDP-net through an additional encoding function, b 0 = f B (✓).</p><p>Planner module. The QMDP planner <ref type="figure" target="#fig_2">(Fig. 3b)</ref> performs value iteration at its core. Q values are computed by iteratively applying Bellman updates,</p><formula xml:id="formula_12">Q k+1 (s, a) = R(s, a) + P s 0 2S T (s, a, s 0 )V k (s 0 ),<label>(7)</label></formula><formula xml:id="formula_13">V k (s) = max a Q k (s, a).<label>(8)</label></formula><p>Actions are then selected by weighting the Q values with the belief.</p><p>We can implement value iteration using convolutional and max pooling layers <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34]</ref>. In our grid navigation task Q(s, a) is a N ⇥N ⇥|A| tensor. Eq. <ref type="formula" target="#formula_13">(8)</ref> is expressed by a max pooling layer, where Q k (s, a) is the input and V k (s) is the output. Eq. <ref type="formula" target="#formula_12">(7)</ref> is a N ⇥N convolution with |A| convolutional filters, followed by an addition operation with R(s, a), the reward tensor. We denote the convolutional layer by f 0 T . The kernel weights of f 0 T encode the transition function T , similarly to f T in the filter. Rewards for a navigation task depend on the goal and obstacles. We condition rewards on the task parameter, R(s, a) = f R (s, a|✓). f R maps from ✓ to R(s, a). In this paper f R is a CNN.</p><p>We implement K iterations of Bellman updates by stacking the layers representing Eq. <ref type="formula" target="#formula_12">(7)</ref> and Eq. (8) K times with tied weights. After K iterations we get Q K (s, a), the approximate Q values for each state-action pair. We weight the Q values by the belief to obtain action values,</p><formula xml:id="formula_14">q(a) = P s2S Q K (s, a)b t (s).<label>(9)</label></formula><p>Finally, we choose the output action through a low-level policy function, f ⇡ , mapping from q(a) to the action output, a t+1 .</p><p>QMDP-net naturally extends to higher dimensional discrete state spaces (e.g. our maze navigation task) where n-dimensional convolutions can be used <ref type="bibr" target="#b13">[14]</ref>. While M (✓) is restricted to a discrete space, we can handle continuous tasks W ⇥ by simultaneously learning a discrete M (✓) for planning, and f A , f O , f B , f ⇡ to map between states, actions and observations in W ⇥ and M (✓).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>The main objective of the experiments is to understand the benefits of structure priors on learning neural-network policies. We create several alternative network architectures by gradually relaxing the structure priors and evaluate the architectures on simulated robot navigation and manipulation tasks. While these tasks are simpler than, for example, Atari games, in terms of visual perception, they are in fact very challenging, because of the sophisticated long-term reasoning required to handle partial observability and distant future rewards. Since the exact state of the robot is unknown, a successful policy must reason over many steps to gather information and improve state estimation through partial and noisy observations. It also must reason about the trade-off between the cost of information gathering and the reward in the distance future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>We compare the QMDP-net with a number of related alternative architectures. Two are QMDP-net variants. Untied QMDP-net relaxes the constraints on the planning module by untying the weights representing the state-transition function over the different CNN layers. LSTM QMDP-net replaces the filter module with a generic LSTM module. The other two architectures do not embed POMDP structure priors at all. CNN+LSTM is a state-of-the-art deep CNN connected to an LSTM. It is similar to the DRQN architecture proposed for reinforcement learning under partially observability <ref type="bibr" target="#b9">[10]</ref>. RNN is a basic recurrent neural network with a single fully-connected hidden layer. RNN contains no structure specific to planning under partial observability.</p><p>Each experimental domain contains a parameterized set of tasks W ⇥ . The parameters ✓ encode an environment, a goal, and a belief over the robot's initial state. To train a policy for W ⇥ , we generate random environments, goals, and initial beliefs. We construct ground-truth POMDP models for the generated data and apply the QMDP algorithm. If the QMDP algorithm successfully reaches the goal, we then retain the resulting sequence of action and observations (a 1 , o 1 , a 2 , o 2 , . . .) as an expert trajectory, together with the corresponding environment, goal, and initial belief. It is important to note that the ground-truth POMDPs are used only for generating expert trajectories and not for learning the QMDP-net.</p><p>For fair comparison, we train all networks using the same set of expert trajectories in each domain. We perform basic search over training parameters, the number of layers, and the number of hidden units for each network architecture. Below we briefly describe the experimental domains. See Appendix C for implementation details.</p><p>Grid-world navigation. A robot navigates in an unknown building given a floor map and a goal. The robot is uncertain of its own location. It is equipped with a LIDAR that detects obstacles in its direct neighborhood. The world is uncertain: the robot may fail to execute desired actions, possibly because of wheel slippage, and the LIDAR may produce false readings. We implemented a simplified version of this task in a discrete n⇥n grid world <ref type="figure" target="#fig_0">(Fig. 1c)</ref>. The task parameter ✓ is represented as an n⇥n image with three channels. The first channel encodes the obstacles in the environment, the second channel encodes the goal, and the last channel encodes the belief over the robot's initial state. The robot's state represents its position in the grid. It has five actions: moving in each of the four canonical directions or staying put. The LIDAR observations are compressed into four binary values corresponding to obstacles in the four neighboring cells. We consider both a deterministic and a stochastic variant of the domain. The stochastic variant adds action and observation uncertainties. The robot fails to execute the specified move action and stays in place with probability 0.2. The observations are faulty with probability 0.1 independently in each direction. We trained a policy using expert trajectories from 10, 000 random environments, 5 trajectories from each environment.</p><p>We then tested on a separate set of 500 random environments. Maze navigation. A differential-drive robot navigates in a maze with the help of a map, but it does not know its pose <ref type="figure" target="#fig_0">(Fig. 1d)</ref>. This domain is similar to the grid-world navigation, but it is significant more challenging. The robot's state contains both its position and orientation. The robot cannot move freely because of kinematic constraints. It has four actions: move forward, turn left, turn right and stay put. The observations are relative to the robot's current orientation, and the increased ambiguity makes it more difficult to localize the robot, especially when the initial state is highly uncertain. Finally, successful trajectories in mazes are typically much longer than those in randomly-generated grid worlds. Again we trained on expert trajectories in 10, 000 randomly generated mazes and tested them in 500 new ones. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2-D object grasping.</head><p>A robot gripper picks up novel objects from a table using a two-finger hand with noisy touch sensors at the finger tips. The gripper uses the fingers to perform compliant motions while maintaining contact with the object or to grasp the object. It knows the shape of the object to be grasped, maybe from an object database. However, it does not know its own pose relative to the object and relies on the touch sensors to localize itself. We implemented a simplified 2-D variant of this task, modeled as a POMDP <ref type="bibr" target="#b12">[13]</ref>. The task parameter ✓ is an image with three channels encoding the object shape, the grasp point, and a belief over the gripper's initial pose. The gripper has four actions, each moving in a canonical direction unless it touches the object or the environment boundary. Each finger has 3 binary touch sensors at the tip, resulting in 64 distinct observations. We trained on expert demonstration on 20 different objects with 500 randomly sampled poses for each object. We then tested on 10 previously unseen objects in random poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Choosing QMDP-Net Components for a Task</head><p>Given a new task W ⇥ , we need to choose an appropriate neural network representation for M (✓). More specifically, we need to choose S, A and O, and a representation for the functions</p><formula xml:id="formula_15">f R , f T , f 0 T , f Z , f O , f A , f B , f ⇡ .</formula><p>This provides an opportunity to incorporate domain knowledge in a principled way. For example, if W ⇥ has a local and spatially invariant connectivity structure, we can choose convolutions with small kernels to represent f T , f R and f Z .</p><p>In our experiments we use S = N ⇥N for N ⇥N grid navigation, and S = N ⇥N ⇥4 for N ⇥N maze navigation where the robot has 4 possible orientations. We use |A| = |A| and |O| = |O| for all tasks except for the object grasping task, where |O| = 64 and |O| = 16. We represent f T , f R and f Z by CNN components with 3⇥3 and 5⇥5 kernels depending on the task. We enforce that f T and f Z are proper probability distributions by using softmax and sigmoid activations on the convolutional kernels, respectively. Finally, f O is a small fully connected component, f A is a one-hot encoding function, f ⇡ is a single softmax layer, and f B is the identity function.</p><p>We can adjust the amount of planning in a QMDP-net by setting K. A large K allows propagating information to more distant states without affecting the number of parameters to learn. However, it results in deeper networks that are computationally expensive to evaluate and more difficult to train. We used K = 20 . . . 116 depending on the problem size. We were able to transfer policies to larger environments by increasing K up to 450 when executing the policy.</p><p>In our experiments the representation of the task parameter ✓ is isomorphic to the chosen state space S. While the architecture is not restricted to this setting, we rely on it to represent f T , f Z , f R by convolutions with small kernels. Experiments with a more general class of problems is an interesting direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results and Discussion</head><p>The main results are reported in <ref type="table" target="#tab_2">Table 1</ref>. Some additional results are reported in Appendix A. For each domain, we report the task success rate and the average number of time steps for task completion. Comparing the completion time is meaningful only when the success rates are similar.</p><p>QMDP-net successfully learns policies that generalize to new environments. When evaluated on new environments, the QMDP-net has higher success rate and faster completion time than the alternatives in nearly all domains. To understand better the performance difference, we specifically compared the architectures in a fixed environment for navigation. Here only the initial state and the goal vary across the task instances, while the environment remains the same. See the results in the last row of <ref type="table" target="#tab_2">Table 1</ref>. The QMDP-net and the alternatives have comparable performance. Even RNN performs very well. Why? In a fixed environment, a network may learn the features of an optimal policy directly, e.g., going straight towards the goal. In contrast, the QMDP-net learns a model for planning, i.e., generating a near-optimal policy for a given arbitrary environment.</p><p>POMDP structure priors improve the performance of learning complex policies. Moving across <ref type="table" target="#tab_2">Table 1</ref> from left to right, we gradually relax the POMDP structure priors on the network architecture. As the structure priors weaken, so does the overall performance. However, strong priors sometimes over-constrain the network and result in degraded performance. For example, we found that tying the weights of f T in the filter and f 0 T in the planner may lead to worse policies. While both f T and f 0 T represent the same underlying transition dynamics, using different weights allows each to choose its own approximation and thus greater flexibility. We shed some light on this issue and visualize the learned POMDP model in Appendix B.</p><p>QMDP-net learns "incorrect", but useful models. Planning under partial observability is intractable in general, and we must rely on approximation algorithms. A QMDP-net encodes both a POMDP model and QMDP, an approximate POMDP algorithm that solves the model. We then train the network end-to-end. This provides the opportunity to learn an "incorrect", but useful model that compensates the limitation of the approximation algorithm, in a way similar to reward shaping in reinforcement learning <ref type="bibr" target="#b21">[22]</ref>. Indeed, our results show that the QMDP-net achieves higher success rate than QMDP in nearly all tasks. In particular, QMDP-net performs well on the well-known Hallway2 domain, which is designed to expose the weakness of QMDP resulting from its myopic planning horizon. The planning algorithm is the same for both the QMDP-net and QMDP, but the QMDP-net learns a more effective model from expert demonstrations. This is true even though QMDP generates the expert data for training. We note that the expert data contain only successful QMDP demonstrations. When both successful and unsuccessful QMDP demonstrations were used for training, the QMDP-net did not perform better than QMDP, as one would expect.</p><p>QMDP-net policies learned in small environments transfer directly to larger environments. Learning a policy for large environments from scratch is often difficult. A more scalable approach would be to learn a policy in small environments and transfer it to large environments by repeating the reasoning process. To transfer a learned QMDP-net policy, we simply expand its planning module by adding more recurrent layers. Specifically, we trained a policy in randomly generated 30 ⇥ 30 grid worlds with K = 90. We then set K = 450 and applied the learned policy to several real-life environments, including Intel Lab (100⇥101) and Freiburg (139⇥57), using their LIDAR maps <ref type="figure" target="#fig_0">(Fig. 1c)</ref> from the Robotics Data Set Repository <ref type="bibr" target="#b11">[12]</ref>. See the results for these two environments in <ref type="table" target="#tab_2">Table 1</ref>. Additional results with different K settings and other buildings are available in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>A QMDP-net is a deep recurrent policy network that embeds POMDP structure priors for planning under partial observability. While generic neural networks learn a direct mapping from inputs to outputs, QMDP-net learns how to model and solve a planning task. The network is fully differentiable and allows for end-to-end training.</p><p>Experiments on several simulated robotic tasks show that learned QMDP-net policies successfully generalize to new environments and transfer to larger environments as well. The POMDP structure priors and end-to-end training substantially improve the performance of learned policies. Interestingly, while a QMDP-net encodes the QMDP algorithm for planning, learned QMDP-net policies sometimes outperform QMDP.</p><p>There are many exciting directions for future exploration. First, a major limitation of our current approach is the state space representation. The value iteration algorithm used in QMDP iterates through the entire state space and is well known to suffer from the "curse of dimensionality". To alleviate this difficulty, the QMDP-net, through end-to-end training, may learn a much smaller abstract state space representation for planning. One may also incorporate hierarchical planning <ref type="bibr" target="#b7">[8]</ref>. Second, QMDP makes strong approximations in order to reduce computational complexity. We want to explore the possibility of embedding more sophisticated POMDP algorithms in the network architecture. While these algorithms provide stronger planning performance, their algorithmic sophistication increases the difficulty of learning. Finally, we have so far restricted the work to imitation learning. It would be exciting to extend it to reinforcement learning. Based on earlier work <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34]</ref>, this is indeed promising.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: A robot learning to navigate in partially observable grid worlds. (a) The robot has a map. It has a belief over the initial state, but does not know the exact initial state. (b) Local observations are ambiguous and are insufficient to determine the exact state. (c, d) A policy trained on expert demonstrations in a set of randomly generated environments generalizes to a new environment. It also "transfers" to a much larger real-life environment, represented as a LIDAR map [12].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: QMDP-net architecture. (a) A policy maps a history of actions and observations to a new action. (b) A QMDP-net is an RNN that imposes structure priors for sequential decision making under partial observability. It embeds a Bayesian filter and the QMDP algorithm in the network. The hidden state of the RNN encodes the belief for POMDP planning. (c) A QMDP-net unfolded in time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: A QMDP-net consists of two modules. (a) The Bayesian filter module incorporates the current action a t and observation o t into the belief. (b) The QMDP planner module selects the action according to the current belief b t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Eq. ( 3 )</head><label>3</label><figDesc>is implemented as a convolutional layer with |A| convolutional filters. We denote the convolutional layer by f T . The kernel weights of f T encode the transition function T in M (✓). The output of the convolutional layer, b 0 t (s, a), is a N ⇥N ⇥|A| tensor. b 0 t (s, a) encodes the updated belief after taking each of the actions, a 2 A. We need to select the belief corresponding to the last action taken by the agent, a t . We can directly index b 0 t (s, a) by a t if A = A. In general A 6 = A, so we cannot use simple indexing. Instead, we will use "soft indexing". First we encode actions in A to actions in A through a learned function f A . f A maps from a t to an indexing vector w a t , a distribution over actions in A. We then weight b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Highly ambiguous observations in a maze. The four observations (in red) are the same, despite that the robot states are all different.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Object grasping using touch sensing. (a) An example [3]. (b) Simplified 2-D object grasping. Objects from the training set (top) and the test set (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Its network architecture contains a hierarchical extension of VIN for planning and thus does not deal with partial observability during planning. The QMDP-net extends the prior work on network architectures for MDP planning and for Bayesian filtering. It imposes the POMDP model and computation structure priors on the entire network architecture for planning under partial observability.</figDesc><table>The idea of embedding specific computation structures in the neural network architecture has been 
gaining attention recently. Tamar et al. implemented value iteration in a neural network, called 
Value Iteration Network (VIN), to solve Markov decision processes (MDPs) in fully observable 
domains, where an agent knows its exact state and does not require filtering [34]. Okada et al. 
addressed a related problem of path integral optimal control, which allows for continuous states 
and actions [23]. Neither addresses the issue of partial observability, which drastically increases 
the computational complexity of decision making [24]. Haarnoja et al. [9] and Jonschkowski and 
Brock [15] developed end-to-end trainable Bayesian filters for probabilistic state estimation. Silver 
et al. introduced Predictron for value estimation in Markov reward processes [31]. They do not 
deal with decision making or planning. Both Shankar et al. [28] and Gupta et al. [8] addressed 
planning under partial observability. The former focuses on learning a model rather than a policy. 
The learned model is trained on a fixed environment and does not generalize to new ones. The 
latter proposes a network learning approach to robot navigation in an unknown environment, with a 
focus on mapping. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Z(s, o) encodes observation probabilities for each of the observations, o 2 O. We need the ob- servation probabilities for the last observation o t . In general O 6 = O and we cannot index Z(s, o) directly. Instead, we will use soft indexing again. We encode observations in O to observations in O through f O . f O is a function mapping from o t to an indexing vector, w</figDesc><table>o 

t , a distribution over O. We 
then weight Z(s, o) by w 

o 

t , i.e. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison of QMDP-net and alternative architectures for recurrent policy networks. SR is the success rate in percentage. Time is the average number of time steps for task completion. D-n and S-n denote deterministic and stochastic variants of a domain with environment size n⇥n.</figDesc><table>QMDP 
QMDP-net 
Untied 
LSTM 
CNN 
RNN 
QMDP-net 
QMDP-net 
+LSTM 

Domain 
SR Time 
SR Time 
SR Time 
SR Time 
SR Time 
SR Time 

Grid D-10 
99.8 
8.8 
99.6 
8.2 
98.6 
8.3 
84.4 12.8 
90.0 13.4 
87.8 13.4 
Grid D-18 
99.0 15.5 
99.0 14.6 
98.8 14.8 
43.8 27.9 
57.8 33.7 
35.8 24.5 
Grid D-30 
97.6 24.6 
98.6 25.0 
98.8 23.9 
22.2 51.1 
19.4 45.2 
16.4 39.3 

Grid S-18 
98.1 23.9 
98.8 23.9 
95.9 24.0 
23.8 55.6 
41.4 65.9 
34.0 64.1 

Maze D-29 
63.2 54.1 
98.0 56.5 
95.4 62.5 
9.8 57.2 
9.2 41.4 
9.8 47.0 
Maze S-19 
63.1 50.5 
93.9 60.4 
98.7 57.1 
18.9 79.0 
19.2 80.8 
19.6 82.1 

Hallway2 
37.3 28.2 
82.9 64.4 
69.6 104.4 
82.8 89.7 
77.8 99.5 
68.0 108.8 

Grasp 
98.3 14.6 
99.6 18.2 
98.9 20.4 
91.4 26.4 
92.8 22.1 
94.1 25.7 

Intel Lab 
90.2 85.4 
94.4 107.7 
20.0 55.3 
-
-
-
Freiburg 
88.4 66.9 
93.2 81.1 
37.4 51.7 
-
-
-

Fixed grid 
98.8 17.4 
98.6 17.6 
99.8 17.0 
97.0 19.7 
98.4 19.9 
98.0 19.8 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments We thank Leslie Kaelbling and Tomás Lozano-Pérez for insightful discussions that helped to improve our understanding of the problem. The work is supported in part by Singapore Ministry of Education AcRF grant MOE2016-T2-2-068 and National University of Singapore AcRF grant R-252-000-587-112.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<ptr target="http://tensorflow.org/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Policy search by dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="831" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monte carlo value iteration for continuous-state POMDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithmic Foundations of Robotics IX</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="175" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A robot that reinforcement-learns to identify and memorize important previous observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhumatiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gruener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="430" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Infinite-horizon policy-gradient estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="319" to="350" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Closing the learning-planning loop with predictive state representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boots</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Siddiqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="954" to="966" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Cognitive mapping and planning for visual navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03920</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Backprop kf: Learning discriminative deterministic state estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ajay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4376" to="4384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1507.06527" />
		<title level="m">Deep recurrent Q-learning for partially observable MDPs</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The robotics data set repository (radish)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
		<ptr target="http://radish.sourceforge.net/" />
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Grasping POMDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4685" to="4692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end learnable histogram filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Brock</surname></persName>
		</author>
		<ptr target="http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen_pdf/Jonschkowski-16-NIPS-WS.pdf" />
	</analytic>
	<monogr>
		<title level="m">Workshop on Deep Learning for Action and Interaction at NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sarsop: Efficient point-based POMDP planning by approximating optimally reachable belief spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kurniawati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning policies for partially observable environments: Scaling up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Cassandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="362" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Predictive representations of state</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1555" to="1562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning to navigate in complex environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03673</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Policy invariance under reward transformations: Theory and application to reward shaping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="278" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rigazio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aoshima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09597</idno>
		<title level="m">Path integral networks: End-to-end differentiable optimal control</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The complexity of Markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="441" to="450" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Applying metric-trees to belief-point POMDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">page None</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Model-based online learning of POMDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Shimony</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="353" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A survey of point-based POMDP solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kaplow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Agents and Multi-agent Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="51" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reinforcement learning via recurrent convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Dwivedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2592" to="2597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Monte-carlo planning in large POMDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2164" to="2172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The predictron: End-to-end learning and planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dulac-Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barreto</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1612.08810" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Perseus: Randomized point-based value iteration for POMDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Spaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vlassis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="195" to="220" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<ptr target="http://www.ipb.uni-bonn.de/datasets/" />
		<title level="m">Robotics 2D-laser dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Value iteration networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2146" to="2154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Lecture 6.5 -rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Despot: Online POMDP planning with regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Somani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="231" to="266" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
