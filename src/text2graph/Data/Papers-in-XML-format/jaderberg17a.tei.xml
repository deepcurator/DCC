<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Decoupled Neural Interfaces using Synthetic Gradients</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017">2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">Marian</forename><surname>Czarnecki</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
						</author>
						<title level="a" type="main">Decoupled Neural Interfaces using Synthetic Gradients</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 34 th International Conference on Machine Learning</title>
						<meeting>the 34 th International Conference on Machine Learning <address><addrLine>Sydney, Australia</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="2017">2017</date>
						</imprint>
					</monogr>
					<note>1 DeepMind, London, UK. Correspondence to: Max Jaderberg &lt;jaderberg@google.com&gt;.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass -amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>After receiving the message hA from A, B can use its model of A, MB, to send back synthetic gradientsˆ A which are trained to approximate real error gradients A. Note that A does not need to wait for any extra computation after itself to get the correct error gradients, hence decoupling the backward computation. The feedback model MB can also be conditioned on any privileged information or context, c, available during training such as a label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Each layer (or module) in a directed neural network can be considered a computation step, that transforms its incoming data. These modules are connected via directed edges, creating a forward processing graph which defines the flow of data from the network inputs, through each module, producing network outputs. Defining a loss on outputs allows errors to be generated, and propagated back through the network graph to provide a signal to update each module.</p><p>This process results in several forms of locking, namely: (i) Forward Locking -no module can process its incoming data before the previous nodes in the directed forward graph have executed; (ii) Update Locking -no module can be updated before all dependent modules have executed in forwards mode; also, in many credit-assignment algorithms (including backpropagation <ref type="bibr" target="#b11">(Rumelhart et al., 1986)</ref>) we have (iii) Backwards Locking -no module can be updated before all dependent modules have executed in both forwards mode and backwards mode.</p><p>Forwards, update, and backwards locking constrain us to running and updating neural networks in a sequential, synchronous manner. Though seemingly benign when training simple feed-forward nets, this poses problems when thinking about creating systems of networks acting in multiple environments at different and possibly irregular or asynchronous timescales. For example, in complex systems comprised of multiple asynchronous cooperative modules <ref type="bibr">(or agents)</ref>, it is undesirable and potentially unfeasible that all networks are update locked. Another example is a distributed model, where part of the model is shared and used by many downstream clients -all clients must be fully executed and pass error gradients back to the shared model before the model can update, meaning the system trains as fast as the slowest client. The possibility to parallelise training of currently sequential systems could hugely speed up computation time.</p><p>The goal of this work is to remove update locking for neural networks. This is achieved by removing backpropagation. To update weights ✓ i of module i we drastically approximate the function implied by backpropagation:</p><formula xml:id="formula_0">@L @✓ i = f Bprop ((h i , x i , y i , ✓ i ), . . .) @h i @✓ i 'f Bprop (h i ) @h i @✓ i</formula><p>where h are activations, x are inputs, y is supervision, and L is the overall loss to minimise. This leaves dependency only on h i -the information local to module i. The premise of this method is based on a simple protocol for learnt communication, allowing neural network modules to interact and be trained without update locking. While the communication protocol is general with respect to the means of generating a training signal, here we focus on a specific implementation for networks trained with gradient descent -we replace a standard neural interface (a connection between two modules in a neural network) with a Decoupled Neural Interface (DNI). Most simply, when a module (e.g. a layer) sends a message (activations) to another module, there is an associated model which produces a predicted error gradient with respect to the message immediately. The predicted gradient is a function of the message alone; there is no dependence on downstream events, states or losses. The sender can then immediately use these synthetic gradients to get an update, without incurring any delay. And by removing update-and backwards locking in this way, we can train networks without a synchronous backward pass. We also show preliminary results that extend this idea to also remove forward locking -resulting in networks whose modules can also be trained without a synchronous forward pass. When applied to RNNs we show that using synthetic gradients allows RNNs to model much greater time horizons than the limit imposed by truncating backpropagation through time (BPTT). We also show that using synthetic gradients to decouple a system of two RNNs running at different timescales can greatly increase training speed of the faster RNN.</p><p>Our synthetic gradient model is most analogous to a value function which is used for gradient ascent <ref type="bibr" target="#b1">(Baxter &amp; Bartlett, 2000)</ref> or critics for training neural networks <ref type="bibr" target="#b12">(Schmidhuber, 1990)</ref>. Most other works that aim to remove backpropagation do so with the goal of performing biologically plausible credit assignment, but this doesn't eliminate update locking between layers. E.g. target propagation <ref type="bibr" target="#b8">(Lee et al., 2015;</ref><ref type="bibr" target="#b2">Bengio, 2014)</ref> removes the reliance on passing gradients between layers, by instead generating target activations which should be fitted to. However these targets must still be generated sequentially, propagating backwards through the network and layers are therefore still update-and backwards-locked. Other algorithms remove the backwards locking by allowing loss or rewards to be broadcast directly to each layer -e.g. RE-INFORCE <ref type="bibr" target="#b17">(Williams, 1992)</ref> (considering all activations are actions), Kickback <ref type="bibr" target="#b0">(Balduzzi et al., 2014)</ref>, and Policy Gradient Coagent Networks <ref type="bibr" target="#b16">(Thomas, 2011</ref>) -but still remain update locked since they require rewards to be generated by an output (or a global critic). While Real-Time Recurrent Learning <ref type="bibr" target="#b18">(Williams &amp; Zipser, 1989)</ref> or approximations such as <ref type="bibr" target="#b10">(Ollivier &amp; Charpiat, 2015;</ref><ref type="bibr" target="#b14">Tallec &amp; Ollivier, 2017)</ref> may seem a promising way to remove update locking, these methods require maintaining the full (or approximate) gradient of the current state with respect to the parameters. This is inherently not scalable and also requires the optimiser to have global knowledge of the network state. In contrast, by framing the interaction between layers as a local communication problem with DNI, we remove the need for global knowledge of the learning system. Other works such as <ref type="bibr" target="#b15">(Taylor et al., 2016;</ref><ref type="bibr" target="#b3">Carreira-Perpinán &amp; Wang, 2014)</ref> allow training of layers in parallel without backpropagation, but in practice are not scalable to more complex and generic network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Decoupled Neural Interfaces</head><p>We begin by describing the high-level communication protocol that is used to allow asynchronously learning agents to communicate.</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>  (a) An RNN trained with truncated BPTT using DNI to communicate over time: Every timestep a recurrent core takes input and produces a hidden state ht and output yt which affects a loss Lt. The core is unrolled for T steps (in this figure T = 3). Gradients cannot propagate across the boundaries of BPTT, which limits the time dependency the RNN can learn to model. However, the recurrent core includes a synthetic gradient model which produces synthetic gradientsˆ t which can be used at the boundaries of BPTT to enable the last set of unrolled cores to communicate with the future ones. (b) In addition, as an auxiliary task, the network can also be asked to do future synthetic gradient prediction: an extra outputˆ t+T is computed every timestep, and is trained to minimise kˆ t+T ˆ t+T k.</p><p>This protocol allows A to send messages to B in a way that A and B are update decoupled -A does not have to wait for B to evaluate the true utility before it can be updatedand A can still learn to send messages of high utility to B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We can apply this protocol to neural networks communicating, resulting in what we call Decoupled Neural Interfaces (DNI). For neural networks, the feedback error signal</head><p>A can take different forms, e.g. gradients can be used as the error signal to work with backpropagation, target messages as the error signal to work with target propagation, or even a value (cumulative discounted future reward) to incorporate into a reinforcement learning framework. However, as a clear and easily analysable set of first steps into this important and mostly unexplored domain, we concentrate our empirical study on differentiable networks trained with backpropagation and gradient-based updates. Therefore, we focus on producing error gradients as the feedback A which we dub synthetic gradients.</p><p>Notation To facilitate our exposition, it's useful to introduce some notation. Without loss of generality, consider neural networks as a graph of function operations (a finite chain graph in the case of a feed-forward models, an infinite chain in the case of recurrent ones, and more generally a directed acyclic graph). The forward execution of the network graph has a natural ordering due to the input dependencies of each functional node. We denote the function corresponding to step i in a graph execution as f i and the composition of functions (i.e. the forward graph) from step i to step j inclusive as F j i . We denote the loss associated with layer, i, of the chain as L i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Synthetic Gradient for Recurrent Networks</head><p>We begin by describing how our method of using synthetic gradients applies in the case of recurrent networks; in some ways this is simpler to reason about than feed-forward networks or more general graphs.</p><p>An RNN applied to infinite stream prediction can be viewed as an infinitely unrolled recurrent core module f with parameters ✓, such that the forward graph is</p><formula xml:id="formula_1">F 1 1 = (f i ) 1 i=1</formula><p>where f i = f 8i and the core module propagates an output y i and state h i based on some input x i :</p><formula xml:id="formula_2">y i , h i = f i (x i , h i 1 )</formula><p>. At a particular point in time t we wish to minimise </p><formula xml:id="formula_3">P 1 ⌧ =t L ⌧ . Of</formula><formula xml:id="formula_4">T ✓ ↵ 1 X ⌧ =t @L ⌧ @✓ = ✓ ↵( t+T X ⌧ =t @L ⌧ @✓ + ( 1 X ⌧ =T +1 @L ⌧ @h T ) @h T @✓ ) = ✓ ↵( t+T X ⌧ =t @L ⌧ @✓ + T @h T @✓ )</formula><p>and as in truncated BPTT, calculates P t+T ⌧ =t @L⌧ @✓ with backpropagation and approximates the remaining terms, beyond t + T , by using T = 0. This limits the time horizon over which updates to ✓ can be learnt, effectively limiting the amount of temporal dependency an RNN can learn. The approximation that T = 0 is clearly naive, and by using an appropriately learned approximation we can hope to do better. Treating the connection between recurrent cores at time t + T as a Decoupled Neural Interface we can approximate</p><formula xml:id="formula_5">T , withˆ T = M T (h T )</formula><p>-a learned approximation of the future loss gradients -as shown and described in <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>. This amounts to taking the infinitely unrolled RNN as the full neural network F 1 1 , and chunking it into an infinite number of sub-networks where the recurrent core is unrolled for T steps, giving F t+T 1 t . Inserting DNI between two adjacent sub-networks F t+T 1 t and F t+2T 1 t+T allows the recurrent network to learn to communicate to its future self, without being update locked to its future self. From the view of the synthetic gradient model, the RNN is predicting its own error gradients.</p><p>The synthetic gradient modelˆ</p><formula xml:id="formula_6">T = M T (h T )</formula><p>is trained to predict the true gradients by minimising a distance</p><formula xml:id="formula_7">d(ˆ T , T )</formula><p>to the target gradient T -in practice we find L 2 distance to work well. The target gradient is ideally the true gradient of future loss,</p><formula xml:id="formula_8">P 1 ⌧ =T +1 @L⌧ @h T</formula><p>, but as this is not a tractable target to obtain, we can use a target gradient that is itself bootstrapped from a synthetic gradient and then backpropagated and mixed with a number of steps of true</p><formula xml:id="formula_9">A hA→B A hA→B c MB fi fi+1 fi+2 … … … … fi fi+1 fi+2 … … … … Mi+1 iˆ i i+1 (a) (b) fi fi+1 fi+2 … … … … Mi+1 i Mi+2 (c) F N i+1 F i 1 ˆ i+1</formula><p>Figure <ref type="formula">3</ref> gradient, e.g.</p><formula xml:id="formula_10">T = P 2T ⌧ =T +1 @L⌧ @h T +ˆ 2T +1 @h 2T @h T .</formula><p>This bootstrapping is exactly analogous to bootstrapping value functions in reinforcement learning and allows temporal credit assignment to propagate beyond the boundary of truncated BPTT.</p><p>This training scheme can be implemented very efficiently by exploiting the recurrent nature of the network, as shown in <ref type="figure" target="#fig_0">Fig. 10</ref> in the Supplementary Material. In Sect. 3.1 we show results on sequence-to-sequence tasks and language modelling, where using synthetic gradients extends the time dependency the RNN can learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Auxiliary Tasks</head><p>We also propose an extension to aid learning of synthetic gradient models for RNNs, which is to introduce another auxiliary task from the RNN, described in <ref type="figure" target="#fig_1">Fig. 2 (b)</ref>. This extra prediction problem is designed to promote coupling over the maximum time span possible, requiring the recurrent core to explicitly model short term and long term synthetic gradients, helping propagate gradient information backwards in time. This is also shown to further increase performance in Sect. 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Synthetic Gradient for Feed-Forward Networks</head><p>As another illustration of DNIs, we now consider feed-forward networks consisting of N layers f i , i 2 {1, . . . , N}, each taking an input h i 1 and producing an output</p><formula xml:id="formula_11">h i = f i (h i 1 )</formula><p>, where h 0 = x is the input data. The forward execution graph of the full network can be denoted as as F N 1 , a section of which is illustrated in <ref type="figure">Fig. 3 (a)</ref>. Define the loss imposed on the output of the network as</p><formula xml:id="formula_12">L = L N .</formula><p>Each layer f i has parameters ✓ i that can be trained jointly to minimise L(h N ) with a gradient-based update rule</p><formula xml:id="formula_13">✓ i ✓ i ↵ i @h i @✓ i ; i = @L @h i</formula><p>where ↵ is the learning rate and @L @hi is computed with backpropagation. The reliance on i means that the update to layer i can only occur after the remainder of the network, i.e. F N i+1 (the sub-network of layers between layer i + 1 and layer N inclusive) has executed a full forward pass, generated the loss L(h N ), then backpropagated the gradient through every successor layer in reverse order. Layer i is therefore update locked to F N i+1 . To remove the update locking of layer i to F N i+1 we can use the communication protocol described previously. Layer i sends h i to layer i + 1, which has a communication model M i+1 that produces a synthetic error gradientˆ i = M i+1 (h i ), as shown in <ref type="figure">Fig. 3 (b)</ref>, which can be used immediately to update layer i and all the other layers in</p><formula xml:id="formula_14">F i 1 ✓ n ✓ n ↵ˆ i @h i @✓ n , n 2 {1, . . . , i}.</formula><p>To train the parameters of the synthetic gradient model M i+1 , we simply wait for the true error gradient i to be computed (after a full forwards and backwards execution of F N i+1 ), and fit the synthetic gradient to the true gradients by minimising kˆ</p><formula xml:id="formula_15">i i k 2 2 .</formula><p>Furthermore, for a feed-forward network, we can use synthetic gradients as communication feedback to decouple every layer in the network, as shown in <ref type="figure">Fig. 3 (c)</ref>. The execution of this process is illustrated in <ref type="figure">Fig. 9</ref> in the Supplementary Material. In this case, since the target error gradient i is produced by backpropagatingˆ i+1 through layer i + 1, i is not the true error gradient, but an estimate bootstrapped from synthetic gradient models later in the network. Surprisingly, this does not cause errors to compound and learning remains stable even with many layers, as shown in Sect. 3.3.</p><p>Additionally, if any supervision or context c is available at the time of synthetic gradient computation, the synthetic gradient model can take this as an extra input,</p><formula xml:id="formula_16">ˆ i = M i+1 (h i , c)</formula><p>. This process allows a layer to be updated as soon as a forward pass of that layer has been executed. This paves the way for sub-parts or layers of networks to be trained in an asynchronous manner, something we show in Sect. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Arbitrary Network Graphs</head><p>Although we have explicitly described the application of DNIs for communication between layers in feed-forward networks, and between recurrent cores in recurrent networks, there is nothing to restrict the use of DNIs for arbitrary network graphs. The same procedure can be applied to any network or collection of networks, any number of times. An example is in Sect. 3.2 where we show communication between two RNNs, which tick at different rates, where the communication can be learnt by using synthetic gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Mixing Real &amp; Synthetic Gradients</head><p>In this paper we focus on the use of synthetic gradients to replace real backpropagated gradients in order to achieve update unlocking. However, synthetic gradients could also be used to augment real gradients. Mixing real and synthetic gradients results in BP ( ), an algorithm anolgous to T D( ) for reinforcement learning <ref type="bibr" target="#b13">(Sutton &amp; Barto, 1998)</ref>. This can be seen as a generalized view of synthetic gradients, with the algorithms given in this section for update unlocked RNNs and feed-forward networks being specific instantiations of BP ( ). This generalised view is discussed further in Sect. A in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>In this section we perform empirical expositions of the use of DNIs and synthetic gradients, first by applying them to RNNs in Sect. 3.1 showing that synthetic gradients extend the temporal correlations an RNN can learn. Secondly, in Sect. 3.2 we show how a hierarchical, two-timescale system of networks can be jointly trained using synthetic gradients to propagate error signals between networks. Finally, we demonstrate the ability of DNIs to allow asynchronous updating of layers a feed-forward network in Sect. 3.3. More experiments can be found in Sect. C in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Recurrent Neural Networks</head><p>Here we show the application of DNIs to recurrent neural networks as discussed in Sect. 2.1. We test our models on the Copy task, Repeat Copy task, as well as character-level language modelling.</p><p>For all experiments we use an LSTM <ref type="bibr" target="#b7">(Hochreiter &amp; Schmidhuber, 1997)</ref> of the form in <ref type="bibr" target="#b5">(Graves, 2013)</ref>, whose output is used for the task at hand, and additionally as input to the synthetic gradient model (which is shared over all timesteps). The LSTM is unrolled for T timesteps after which backpropagation through time (BPTT) is performed. We also look at incorporating an auxiliary task which predicts the output of the synthetic gradient model T steps in the future as explained in Sect. 2.1. The implementation details of the RNN models are given in Sect. D.2 in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Copy and Repeat Copy</head><p>We first look at two synthetic tasks -Copy and Repeat Copy tasks from <ref type="bibr" target="#b6">(Graves et al., 2014)</ref>. Copy involves reading in a sequence of N characters and after a stop character is encountered, must repeat the sequence of N characters in order and produce a final stop character. Repeat Copy must also read a sequence of N characters, but after the stop character, reads the number, R, which indicates the number of times it is required to copy the sequence, before outputting a final stop character. Each sequence of reading and copying is an episode, of length T task = N + 3 for Copy and T task = NR + 3 for Repeat Copy.</p><p>While normally the RNN would be unrolled for the length of the episode before BPTT is performed, T = T task , we wish to test the length of time the RNN is able to model with and without DNI bridging the BPTT limit. We therefore train the RNN with truncated BPTT: T 2 {2, 3, 4, 5} with and without DNI, where the RNN is applied continuously and across episode boundaries. For each problem, once the RNN has solved a task with a particular episode length (averaging below 0.15 bits error), the task is made harder by extending N for Copy and Repeat Copy, and also R for Repeat Copy. <ref type="table">Table 1</ref> gives the results by reporting the largest T task that is successfully solved by the model. The RNNs without DNI generally perform as expected, with longer BPTT resulting in being able to model longer time dependencies. However, by introducing DNI we can extend the time dependency that is able to be modelled by an RNN. The additional computational complexity is negligible but we require an additional recurrent core to be stored in memory (this is illustrated in <ref type="figure" target="#fig_0">Fig. 10</ref> in the Supplementary Material). Because we can model larger time dependencies with a smaller T , our models become more data-efficient, learning faster and having to see less data samples to solve a task. Furthermore, when we include the extra task of predicting the synthetic gradient that will be produced T steps in the future (DNI + Aux), the RNNs with DNI are able to model even larger time dependencies. For example with T = 3 (i.e. performing BPTT across only three timesteps) on the Repeat Copy task, the DNI enabled RNN goes from being able to model 33 timesteps to 59 timesteps when using future synthetic gradient prediction as well. This is in contrast to without using DNI at all, where the RNN can only model 5 timesteps.</p><p>Language Modelling We also applied our DNI-enabled RNNs to the task of character-level language modelling, using the Penn Treebank dataset <ref type="bibr" target="#b9">(Marcus et al., 1993)</ref>. We use an LSTM with 1024 units, which at every timestep reads a character and must predict the next character in the sequence. We train with BPTT with and without DNI, as well as when using future synthetic gradient prediction (DNI + Aux), with T 2 {2, 3, 4, 5, 8} as well as strong baselines with T = 20, 40. We measure error in bits per  <ref type="table">Table 1</ref>. Results for applying DNI to RNNs. Copy and Repeat Copy task performance is reported as the maximum sequence length that was successfully modelled (higher is better), and Penn Treebank results are reported in terms of test set bits per character (lower is better) at the point of lowest validation error. No learning rate decreases were performed during training. character (BPC) as in <ref type="bibr" target="#b5">(Graves, 2013)</ref>, perform early stopping based on validation set error, and for simplicity do not perform any learning rate decay. For full experimental details please refer to Sect. D.2 in the Supplementary Material.</p><p>The results are given in <ref type="table">Table 1</ref>. Interestingly, with BPTT over only two timesteps (T = 2) an LSTM can get surprisingly good accuracy at next character prediction. As expected, increasing T results in increased accuracy of prediction. When adding DNI, we see an increase in speed of learning (learning curves can be found in <ref type="figure" target="#fig_3">Fig. 4 (Right)</ref> and <ref type="figure" target="#fig_0">Fig. 16</ref> in the Supplementary Material), and models reaching greater accuracy (lower BPC) than their counterparts without DNI. As seen with the Copy and Repeat Copy task, future synthetic gradient prediction further increases the ability of the LSTM to model long range temporal dependencies -an LSTM unrolled 5 timesteps with DNI and future synthetic gradient prediction gives the same BPC as a vanilla LSTM unrolled 20 steps, only needs 58% of the data and is 2⇥ faster in wall clock time to reach 1.35BPC.</p><p>Although we report results only with LSTMs, we have found DNI to work similarly for vanilla RNNs and Leaky RNNs <ref type="bibr" target="#b10">(Ollivier &amp; Charpiat, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Network System</head><p>In this section, we explore the use of DNI for communication between arbitrary graphs of networks. As a simple proof-of-concept, we look at a system of two RNNs, Network A and Network B, where Network B is executed at a slower rate than Network A, and must use communication from Network A to complete its task. The experimental setup is illustrated and described in <ref type="figure">Fig. 5 (a)</ref>. Full experimental details can be found in Sect. D.3 in the Supplementary Material.</p><p>First, we test this system trained end-to-end, with full backpropagation through all connections, which requires the joint Network A-Network B system to be unrolled for T 2 timesteps before a single weight update to both Network A and Network B, as the communication between Network A to Network B causes Network A to be update locked to Network B. We the train the same system but using synthetic gradients to create a learnable bridge between Net-</p><formula xml:id="formula_17">count(odd) = 1 count(3s) = 2 M A B t=4 A B A A A B t=4 t=3 t=2 t=1 M count(odd)=2 count(odd)=1 count(3s)=2</formula><p>(a) (b) <ref type="figure">Figure 5</ref>. (a) System of two RNNs communicating with DNI. Network A sees a datastream of MNIST digits and every T steps must output the number of odd digits seen. Network B runs every T steps, takes a message from Network A as input and must output the number of 3s seen over the last T 2 timesteps. Here is a depiction where T = 2. (b) The test error over the course of training Network A and Network B with T = 4. Grey shows when the two-network system is treated as a single graph and trained with backpropagation end-to-end, with an update every T 2 timesteps. The blue curves are trained where Network A and Network B are decoupled, with DNI (blue) and without DNI (red). When not decoupled (grey), Network A can only be updated every T 2 steps as it is update locked to Network B, so trains slower than if the networks are decoupled (blue and red). Without using DNI (red), Network A receives no feedback from Network B as to how to process the data stream and send a message, so Network B performs poorly. Using synthetic gradient feedback allows Network A to learn to communicate with Network B, resulting in similar final performance to the end-to-end learnt system (results remain stable after 100k steps).</p><p>work A and Network B, thus decoupling Network A from Network B. This allows Network A to be updated T times more frequently, by using synthetic gradients in place of true gradients from Network B. <ref type="figure">Fig. 5 (b)</ref> shows the results for T = 4. Looking at the test error during learning of Network A <ref type="figure">(Fig. 5 (b) Top)</ref>, it is clear that being decoupled and therefore updated more frequently allows Network A to learn much quicker than when being locked to Network B, reaching final performance in under half the number of steps. Network B also trains faster with DNI (most likely due to the increased speed in learning of Network A), and reaches a similar final accuracy as with full backpropagation <ref type="figure">(Fig. 5 (b) Bottom)</ref>. When the networks are decoupled but DNI is not used (i.e. no gradient is received by Network A from Network B), Network A receives no feedback from Network B, so cannot shape its representations and send a suitable message, meaning Network B cannot solve the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feed-Forward Networks</head><p>In this section we apply DNIs to feed-forward networks in order to allow asynchronous or sporadic training of layers, as might be required in a distributed training setup. As explained in Sect. 2.2, making layers decoupled by introducing synthetic gradients allows the layers to communicate with each other without being update locked.</p><p>Asynchronous Updates To demonstrate the gains by decoupling layers given by DNI, we perform an experiment on a four layer FCN model on MNIST, where the backwards pass and update for every layer occurs in random order and only with some probability p update (i.e. a layer is only updated after its forward pass p update of the time). This completely breaks backpropagation, as for example the first layer would only receive error gradients with probability p 3 update and even then, the system would be constrained to be synchronous. However, with DNI bridging the communication gap between each layer, the stochasticity of a layer's update does not mean the layer below cannot update, as it uses synthetic gradients rather than backpropagated gradients. We ran 100 experiments with different values of p update uniformly sampled between 0 and 1. The results are shown in <ref type="figure">Fig. 7</ref> (Left) for DNI with and without conditioning on the labels. With p update = 0.2 the network can still train to 2% accuracy. Incredibly, when the DNI is conditioned on the labels of the data (a reasonable assumption if training in a distributed fashion), the network trains perfectly with only 5% chance of an update, albeit just slower.</p><p>Complete Unlock As a drastic extension, we look at making feed-forward networks completely asynchronous, by removing forward locking as well. In this scenario, every layer has a synthetic gradient model, but also a synthetic input model -given the data, the synthetic input Update Decoupled Forwards and Update Decoupled DNI cDNI cDNI DNI <ref type="figure">Figure 7</ref>. Left: Four layer FCNs trained on MNIST using DNI between every layer, however each layer is trained stochasticallyafter every forward pass, a layer only does a backwards pass with probability pupdate. Population test errors are shown after different numbers of iterations (turquoise is at the end of training after 500k iterations). The purple diamond shows the result when performing regular backpropagation, requiring a synchronous backwards pass and therefore pupdate = 1. When using cDNIs however, with only 5% probability of a layer being updated the network can train effectively. Right: The same setup as previously described however we also use a synthetic input model before every layer, which allows the network to also be forwards decoupled. Now every layer is trained completely asynchronously, where with probability 1 pupdate a layer does not do a forward pass or backwards pass -effectively the layer is "busy" and cannot be touched at all. model produces an approximation of what the input to the layer will be. This is illustrated in <ref type="figure" target="#fig_4">Fig. 6</ref>. Every layer can now be trained independently, with the synthetic gradient and input models trained to regress targets produced by neighbouring layers. The results on MNIST are shown in <ref type="figure">Fig. 7</ref> (Right), and at least in this simple scenario, the completely asynchronous collection of layers train independently, but co-learn to reach 2% accuracy, only slightly slower. More details are given in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion &amp; Conclusion</head><p>In this work we introduced a method, DNI using synthetic gradients, which allows decoupled communication between components, such that they can be independently updated. We demonstrated significant gains from the increased time horizon that DNI-enabled RNNs are able to model, as well as faster convergence. We also demonstrated the application to a multi-network system: a communicating pair of fast-and slow-ticking RNNs can be decoupled, greatly accelarating learning. Finally, we showed that the method can be used facilitate distributed training by enabling us to completely decouple all the layers of a feed-forward net -thus allowing them to be trained asynchronously, non-sequentially, and sporadically.</p><p>It should be noted that while this paper introduces and shows empirical justification for the efficacy of DNIs and synthetic gradients, the work of <ref type="bibr" target="#b4">Czarnecki et al. (2017)</ref> delves deeper into the analysis and theoretical understanding of DNIs and synthetic gradients, confirming the convergence properties of these methods and modelling impacts of using synthetic gradients.</p><p>To our knowledge this is the first time that neural net modules have been decoupled, and the update locking has been broken. This important result opens up exciting avenues of exploration -including improving the foundations laid out here, and application to modular, decoupled, and asynchronous model architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. General communication protocol between A and B. After receiving the message hA from A, B can use its model of A, MB, to send back synthetic gradientsˆ A which are trained to approximate real error gradients A. Note that A does not need to wait for any extra computation after itself to get the correct error gradients, hence decoupling the backward computation. The feedback model MB can also be conditioned on any privileged information or context, c, available during training such as a label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. (a) An RNN trained with truncated BPTT using DNI to communicate over time: Every timestep a recurrent core takes input and produces a hidden state ht and output yt which affects a loss Lt. The core is unrolled for T steps (in this figure T = 3). Gradients cannot propagate across the boundaries of BPTT, which limits the time dependency the RNN can learn to model. However, the recurrent core includes a synthetic gradient model which produces synthetic gradientsˆ t which can be used at the boundaries of BPTT to enable the last set of unrolled cores to communicate with the future ones. (b) In addition, as an auxiliary task, the network can also be asked to do future synthetic gradient</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3. (a) A section of a vanilla feed-forward neural network F N 1 . (b) Incorporating one synthetic gradient model for the output of layer i. This results in two sub-networks F i 1 and F N i+1 which can be updated independently. (c) Incorporating multiple synthetic gradient models after every layer results in N independently updated layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Left: The task progression during training for the Repeat Copy task. All models were trained for 2.5M iterations, but the varying unroll length T results in different quantities of data consumed. The x-axis shows the number of samples consumed by the model, and the y-axis the time dependency level solved by the model -step changes in the time dependency indicate that a particular time dependency is deemed solved. DNI+Aux refers to DNI with the additional future synthetic gradient prediction auxiliary task. Right: Test error in bits per character (BPC) for Penn Treebank character modelling. We train the RNNs with different BPTT unroll lengths with DNI (solid lines) and without DNI (dashed lines). Early stopping is performed based on the validation set. Bracketed numbers give final test set BPC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Completely unlocked feed-forward network training allowing forward and update decoupling of layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>that this module is privy to dur- ing training such as the label or context. The feedbackˆis sent back to A which allows A to be updated immedi- ately. In time, B can fully evaluate the true utility A of the message received from A, and so B's utility model can be updated to fit the true utility, reducing the disparity between</figDesc><table>, Sender A sends a message h 
A to Re-
ceiver B. B has a model M 
B of the utility of the mes-
sage h 
A . B's model of utility M 
B is used to predict the 
feedback: an error signalˆ 
A = M 
B (h 
A , s 
B , c) based on 
the message h 
A , the current state of B, s 
B , and potentially 
any other information, c, A 

A and 
A . </table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Kickback cuts backprop&apos;s red-tape: Biologically plausible credit assignment in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vanchinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.6191</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Direct gradient-based reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Circuits and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="271" to="274" />
		</imprint>
	</monogr>
	<note>Geneva. The</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">How auto-encoders could provide credit assignment in deep networks via target propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.7906</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distributed optimization of deeply nested systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carreira-Perpinán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="10" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Understanding synthetic gradients and decoupled neural interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Swirszcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Difference target propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="498" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Training recurrent networks online without backtracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ollivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Charpiat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.07680</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Networks adjusting networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings ofDistributed Adaptive Neural Information Processing</title>
		<meeting>Distributed Adaptive Neural Information Processing</meeting>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
		<respStmt>
			<orgName>St. Augustin. Citeseer</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unbiased online recurrent optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ollivier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05043</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Burmeister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<title level="m">Training neural networks without gradients: A scalable admm approach. ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Policy gradient coagent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
