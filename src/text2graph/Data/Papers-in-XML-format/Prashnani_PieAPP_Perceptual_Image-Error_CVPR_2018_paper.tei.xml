<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PieAPP: Perceptual Image-Error Assessment through Pairwise Preference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekta</forename><surname>Prashnani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Cai</surname></persName>
							<email>hcai@ece.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasamin</forename><surname>Mostofi</surname></persName>
							<email>ymostofi@ece.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Sen</surname></persName>
							<email>psen@ece.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PieAPP: Perceptual Image-Error Assessment through Pairwise Preference</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.7919/F4GX48M7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the major goals of computer vision is to enable computers to "see" like humans. To this end, a key problem is the automatic computation of the perceptual error (or "distance") of a distorted image with respect to a corresponding reference in a way that is consistent with human observers. A successful solution to this problem would have many applications, including image compression/coding, restoration, and adaptive reconstruction. * Joint first authors.</p><p>This project was supported in part by NSF grants IIS-1321168 and IIS-1619376, as well as a Fall 2017 AI Grant (awarded to Ekta Prashnani).</p><p>Because of its importance, this problem, also known as full-reference image-quality assessment (FR-IQA) <ref type="bibr" target="#b58">[58]</ref>, has received significant research attention in the past few decades <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b32">32]</ref>. The na√Øve approaches do this by simply computing mathematical distances between the images based on norms such as L 2 or L 1 , but these are well-known to be perceptually inaccurate <ref type="bibr" target="#b52">[52]</ref>. Others have proposed metrics that try to exploit known aspects of the human visual system (HVS) such as contrast sensitivity <ref type="bibr" target="#b22">[22]</ref>, high-level structural acuity <ref type="bibr" target="#b52">[52]</ref>, and masking <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b51">51]</ref>, or use other statistics/features <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b44">[44]</ref><ref type="bibr" target="#b45">[45]</ref><ref type="bibr" target="#b46">[46]</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b57">57]</ref>. However, such hand-coded models are fundamentally limited by the difficulty of accurately modeling the complexity of the HVS and therefore do not work well in practice.</p><p>To address these limitations, some have proposed IQA methods based on machine learning to learn more sophisticated models <ref type="bibr" target="#b18">[19]</ref>. Although many learning-based methods use hand-crafted image features <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b40">40]</ref>, recent methods (including ours) apply deep-learning to FR-IQA to learn features automatically <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">25]</ref>. However, the accuracy of all existing learning-based methods depends on the size and quality of the datasets they are trained on, and existing IQA datasets are small and noisy. For instance, many datasets <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b55">55]</ref> are labeled using a mean opinion score (MOS) where each user gives the distorted image a subjective quality rating (e.g., 0 = "bad", 10 = "excellent"). These individual scores are then averaged in an attempt to reduce noise. Unfortunately, creating a good IQA dataset in this fashion is difficult because humans cannot assign quality or error labels to a distorted image consistently, even when comparing to a reference (e.g., try rating the images in <ref type="figure">Fig.1</ref> from 0 to 10!).</p><p>Other datasets (e.g., TID2008 <ref type="bibr" target="#b43">[43]</ref> and TID2013 <ref type="bibr" target="#b42">[42]</ref>) leverage the fact that it is much easier for people to select which image from a distorted pair is closer to the reference than to assign them quality scores. To translate user preferences into quality scores, they then take a set of distorted images and use a Swiss tournament <ref type="bibr" target="#b0">[1]</ref> to assign scores to each. However, this approach has the fundamental problem that the same distorted image could have varying scores in different sets (see supplementary for examples in TID2008 and TID2013). Moreover, the number of images and distortion types in all of these datasets is very limited.  <ref type="bibr" target="#b35">[35]</ref> 3.213 1.504 Bosse et al. <ref type="bibr" target="#b6">[7]</ref> 31.360 36.192 Kim et al. <ref type="bibr" target="#b25">[25]</ref> 0 of people prefer image B. Despite this simple visual task, 13 image quality assessment (IQA) methods-including both popular and stateof-the-art approaches-fail to predict the image that is visually closer to the reference. On the other hand, our proposed PieAPP error metric correctly predicts that B is better with a preference probability of 88.3% (or equivalently, an error of 2.541 for A and 0.520 for B, with the reference having an error of 0). Note that neither the reference image nor the distortion types present were available in our training set.</p><p>largest dataset we know of (TID2013) has only 25 images and 24 distortions, which hardly qualifies as "big-data" for machine learning. Thus, methods trained on these datasets have limited generalizability to new distortions, as we will show later. Because of these limitations, no method currently exists that can predict perceptual error like human observers, even for easy examples such as the one in <ref type="figure">Fig. 1</ref>. Here, although the answer is obvious to most people, all existing FR-IQA methods give the wrong answer, confirming that this problem is clearly far from solved.</p><p>In this paper, we make critical strides towards solving this problem by proposing a novel framework for learning perceptual image error as well as a new, corresponding dataset that is larger and of higher quality than previous ones. We first describe the dataset, since it motivates our framework. Rather than asking people to label images with a subjective quality score, we exploit the fact that it is much easier for humans to select which of two images is closer to a reference. However, unlike the TID datasets <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b43">43]</ref>, we do not explicitly convert this preference into a quality score, since approaches such as Swiss tournaments introduce errors and do not scale. Instead, we simply label the pairs by the percentage of people who preferred image A over B (e.g., a value of 50% indicates that both images are equally "distant" from the reference). By using this pairwise probability of preference as ground-truth labels, our dataset can be larger and more robust than previous IQA datasets.</p><p>Next, our proposed pairwise-learning framework trains an error-estimation function using the probability labels in our dataset. To do this, we input the distorted images (A,B) and the corresponding reference, R, into a pair of identical error-estimation functions which output the perceptualerror scores for A and B. The choice for the error-estimation function is flexible, and in this paper we propose a new deep convolutional neural network (DCNN) for it. The errors of A and B are then used to compute the predicted probability of preference for the image pair. Once our system, which we call PieAPP, is trained using the pairwise probabilities, we can use the learned error-estimation function on a single image A and a reference R to compute the perceptual error of A with respect to R. This trick allows us to quantify the perceived error of a distorted image with respect to a reference, even though our system was never explicitly trained with hand-labeled, perceptual-error scores.</p><p>The combination of our novel, pairwise-learning framework and new dataset results in a significant improvement in perceptual image-error assessment, and can also be used to further improve existing learning-based IQA methods. Interested readers can find our code, trained models, and datasets at https://doi.org/10.7919/F4GX48M7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Pairwise learning of perceptual image error</head><p>Existing IQA datasets (e.g., LIVE <ref type="bibr" target="#b47">[47]</ref>, TID2008 <ref type="bibr" target="#b43">[43]</ref>, CSIQ <ref type="bibr" target="#b26">[26]</ref>, and TID2013 <ref type="bibr" target="#b42">[42]</ref>) suffer from either the unreliable human rating of image quality or the set-dependence of Swiss tournaments. Unlike these previous datasets, our proposed dataset focuses exclusively on the probability of pairwise preference. In other words, given two distorted versions (A and B) of reference image R, subjects are asked to select the one that looks more similar to R. We then store the percentage of people who selected image A over B as the ground-truth label for this pair, which we call the probability of preference of A over B (written as p AB ). This approach is more robust because it is easier to identify the closer image than to assign quality scores, and does not suffer from set-dependency or scalability issues like Swiss tournaments since we never label the images with quality scores.</p><p>The challenge is how to use these probabilistic preference labels to estimate the perceptual-error scores of individual images compared to the reference. To do this, we assume, as shown in <ref type="figure">Fig. 2</ref> reference image can be mapped to a 1-D "perceptual-error" axis (as is common in IQA), with the reference at the origin and distorted versions placed at varying distances from the origin based on their perceptual error (images that are more perceptually similar to the reference are closer, others farther away). Note that since each reference image has its own quality axis, comparing a distorted version of one reference to that of another does not make logical sense.</p><p>Given this axis, we assume there is a function h which takes the perceptual-error scores of A and B (denoted by s A and s B , respectively), and computes the probability of preferring A over B: p AB = h(s A , s B ). In this paper, we use the Bradley-Terry (BT) sigmoid model <ref type="bibr" target="#b8">[9]</ref> for h, since it has successfully modeled human responses for pairwise comparisons in other applications <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b39">39]</ref></p><formula xml:id="formula_0">: 1 p AB = h(s A , s B ) = 1 1 + e s A ‚àís B .<label>(1)</label></formula><p>Unlike the standard BT model, the exponent here is negated so that lower scores are assigned to images visually closer to the reference. Given this, our goal is then to learn a function f that maps a distorted image to its perceptual error with respect to the reference, constrained by the observed probabilities of preference. More specifically, we propose a general optimization framework to train f as follows:</p><formula xml:id="formula_1">Œ∏ = argmin Œ∏ 1 T T ‚àë i=1 h ( f (A i , R i ; Œ∏), f (B i , R i ; Œ∏)) ‚àí p AB,i 2 2 ,<label>(2)</label></formula><p>where Œ∏ denotes the parameters of the image errorestimation function f , p AB,i is the ground-truth probability of preference based on human responses, and T is the total number of training pairs. If the training data is fitted correctly and is sufficient in terms of images and distortions, Eq. 2 will train f to estimate the underlying perceptual-error scores for every image so that their relative spacing on the image quality scale will match their pairwise probabilities (enforced by Eq. 1), with images that are closer to the reference having smaller numbers. These underlying perceptual <ref type="bibr" target="#b0">1</ref> We empirically verify that BT is consistent with our collected human responses in Sec. 5.1. Figure 3: Our pairwise-learning framework consists of error-and probability-estimation blocks. In our implementation, the errorestimation function f has two weight-shared feature-extraction (FE) networks that take in reference R and a distorted input (A or B), and a score-computation (SC) network that uses the extracted features from each image to compute the perceptual-error score (see <ref type="figure" target="#fig_1">Fig. 4</ref> for more details). Note that the FE block for R is shared between f (A, R; Œ∏) and f (B, R; Œ∏). The computed perceptual-error scores for A and B (s A and s B ) are then passed to the probabilityestimation function h, which implements the Bradley-Terry (BT) model (Eq. 1) and outputs the probability of preferring A over B.</p><formula xml:id="formula_2">w reference image R distorted image A distorted image B w A s ) Œ∏ ; A, R ( f ) B , s A s ( h B s</formula><p>errors are estimated up to an additive constant, as only the relative distances between images are constrained by Eq. 1. We discuss how to account for this constant by setting the error of the reference with itself to 0 in Sec. 3.</p><p>To learn the error-estimation function f , we propose a novel pairwise-learning framework, shown in <ref type="figure">Fig. 3</ref>. The inputs to our system are sets of three images (A, B, and R), and the output is the probability of preferring A over B with respect to R. Our framework has two main learning blocks, f (A, R; Œ∏) and f (B, R; Œ∏), that compute the perceptual error of each image. The estimated errors s A and s B are then subtracted and fed through a sigmoid that implements the BT model in Eq. 1 (function h) to predict the probability of preferring A over B. The entire system can then be trained by backpropagating the squared L 2 error between the predicted probabilities and the ground-truth human preference labels to minimize Eq. 2.</p><p>At this point, we simply need to make sure we have an expressive computational model for f as well as a large dataset with a rich variety of images and distortion types. To model f , we propose a new DCNN-based architecture which we describe in Sec. 3. For the dataset, we propose a new large-scale image distortion dataset with probabilistic pairwise human comparison labels as discussed in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">New DCNN for image-error assessment</head><p>Before describing our implementation for function f , we note that our pairwise-learning framework is general and can by used to train any learning model for error computation by simply replacing f (A, R; Œ∏) and f (B, R; Œ∏). In fact, we show in Sec. 5.4 how the performance of Bosse et al. <ref type="bibr" target="#b6">[7]</ref>'s and Kim et al. <ref type="bibr" target="#b25">[25]</ref>'s architectures is considerably improved when integrated into our framework. Furthermore, once trained on our framework, the error-estimation function f can be used by itself to compute the perceptual error of individual images with respect to a reference. Indeed, this is how we get all the results in the paper.</p><p>In our implementation, the error-estimation block f consists of two kinds of subnetworks (subnets, for short). There are three identical, weight-shared feature-extraction (FE) subnets (one for each input image), and two weightshared score-computation (SC) subnets that compute the perceptual-error scores for A and B. Together, two FE and one SC subnets comprise the error-estimation function f . As is common in other IQA algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b57">57]</ref>, we compute errors on a patchwise basis by feeding corresponding patches from A, B, and R through the FE and SC subnets, and aggregate them to obtain the overall errors, s A and s B . <ref type="figure" target="#fig_1">Figure 4</ref> shows the details of our implementation of function f . The three (weight-shared) FE subnets each consist of 11 convolutional (CONV) layers <ref type="figure" target="#fig_1">(Fig. 4a)</ref>. For each set of input patches (A m , B m , and R m , where m is the patch index), the corresponding feature maps from the FE CONV layers at different depths are flattened and concatenated into feature vectors x m A , x m B , and x m R . Using features from multiple layers has two advantages: 1) multiple CONV layers contain features from different scales of the input image, thereby leveraging both high-level and low-level features for error score computation, and 2) skip connections enable better gradient backpropagation through the network.</p><p>Once these feature vectors are computed by the FE subnet, the differences between the corresponding feature vectors of the distorted and reference patches are fed into the SC subnet <ref type="figure" target="#fig_1">(Fig. 4b)</ref>. Each SC subnet consists of two fullyconnected (FC) networks. The first FC network takes in the multi-layer feature difference (i.e., x m R ‚àí x m A ), and predicts the patchwise error (s m A ). These are aggregated using weighted averaging to compute the overall image error (s A ), where the weight for each patch w m A is computed using the second FC network (similar to Bosse et al. <ref type="bibr" target="#b6">[7]</ref>). This network uses the feature difference from the last CONV layer of the FE subnet as input (denoted as y m R ‚àí y m A for A and R), since the weight for a patch is akin to the higher-level patch saliency <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b57">57]</ref> captured by deeper CONV layers.</p><p>Feeding the feature differences to the SC subnet ensures that when estimating the perceptual error for a reference image (i.e., A = R), the SC block would receive x m R ‚àí x m R = 0 as input. The system would therefore output a constant value which is invariant to the reference image, caused by the bias terms in the fully-connected networks in the SC subnet. By subtracting this constant from the predicted error, we ensure that the "origin" of the quality axis is always positioned at 0 for each reference image.</p><p>To train our proposed architecture, we adopt a random patch-sampling strategy <ref type="bibr" target="#b6">[7]</ref>, which prevents over-fitting and improves learning. At every training iteration, we randomly sample 36 patches of size 64 √ó 64 from our training images which are of size 256 √ó 256. The density of our patch sampling ensures that any pixel in the input image is included in at least one patch with a high probability (0.900). This is in contrast with earlier approaches <ref type="bibr" target="#b6">[7]</ref>, where patches are sampled sparsely and there is only a 0.154 probability that a specific pixel in an image will be in one of the sampled patches. This makes it harder to learn a good perceptual-error metric. <ref type="bibr" target="#b1">2</ref> At test time, we randomly sample 1, 024 patches for each image to compute the perceptual error. We now describe our dataset for training the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Large-scale image distortion dataset</head><p>As discussed earlier, existing IQA datasets <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b47">47]</ref> suffer from many problems, such as unreliable quality labels and a limited variety of image contents and distortions. For example, they do not contain many important distortions that appear in real-world computer vision and image processing applications, such as artifacts from deblurring or dehazing. As a result, training high-quality perceptual-error metrics with these datasets is difficult, if not impossible.</p><p>To address these problems (and train our proposed system), we have created our own large-scale dataset, labeled with pairwise probability of preference, that includes a wide variety of image distortions. Furthermore, we also built a test set with a large number of images and distortion types that do not overlap with the training set, allowing a rigorous evaluation of the generalizability of IQA algorithms.  terms of the number of reference images, the number of distortion types, and the total number of distorted images. We next discuss the composition of our dataset.</p><p>Reference images: The proposed dataset contains 200 unique reference images (160 reference images are used for training and 40 for testing), which are selected from the Waterloo Exploration Database <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref> because of its highquality images. The selected reference images are representative of a wide variety of real-world content. Currently, the image size in our dataset is 256 √ó 256, which is a popular size in computer vision and image processing applications. This size also enables crowdsourced workers to evaluate the images without scrolling the screen. However, we note that since our architecture samples patches from the images, it can work on input images of various sizes. Image distortions: In our proposed dataset, we have included a total of 75 distortions, with a total of 44 distortions in the training set, and 31 in the test set which are distinct from the training set. <ref type="bibr" target="#b3">4</ref> More specifically, our set of image distortions spans the following categories: 1) common image artifacts (e.g., additive Gaussian noise, speckle noise); 2) distortions that capture important aspects of the HVS (e.g., non-eccentricity, contrast sensitivity); and 3) complex artifacts from computer vision and image processing algorithms (e.g., deblurring, denoising, super-resolution, compression, geometric transformations, color transformations, and reconstruction). Although recent IQA datasets cover some of the distortions in categories 1 and 2, they do not contain many distortions from category 3 even though they are important to computer vision and image processing. We refer the readers to the supplementary file for a complete list of the training and test distortions in our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training set</head><p>We select 160 reference images and 44 distortions for training PieAPP. Each training example is a pairwise comparison consisting of a reference image R, two distorted versions A and B, along with a labelp AB , which is the estimated probabilistic human preference based on collected human data (see Sec.  <ref type="bibr" target="#b4">5</ref> On the other hand, in an intra-type comparison, A and B are generated by applying the same distortion to R with different parameters. For each reference image, there are 21 groups of intra-type comparisons, containing 3 distorted images generated using the same distortion with different parameter settings. The exhaustive pairwise comparisons within each group (both inter-type and intra-type) and the corresponding human labelsp AB are then used as the training data. Overall, there are a total of 77,280 pairwise comparisons for training (67,200 inter-type and 10,080 intra-type). Inter-distortion comparisons allow us to capture human preference across different distortion types and are more challenging than the intradistortion comparisons due to a larger variety of pairwise combinations and the difficulty in comparing images with different distortion types. We therefore devote a larger proportion of our dataset to inter-distortion comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Test set of unseen distortions and images</head><p>The test set contains 40 reference images and 31 distortions, which are representative of a variety of image contents and visual effects. None of these images and distortions are in the training set. For each reference image, there are 15 distorted images with randomly-sampled distortions (sampled to ensure that the test set has both inter and intra-type comparisons). Probabilistic labels are assigned to the exhaustive pairwise comparisons of the 15 distorted images for each reference. The test set then contains a total of 4,200 distorted image pairs (105 per reference image).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Amazon Mechanical Turk data collection</head><p>We use Amazon Mechanical Turk (MTurk) to collect human responses for both the training and test pairs. In each pairwise image comparison, the MTurk user is presented with distorted images (A and B) and the reference R. The user is asked to select the image that he/she considers more similar to the reference. <ref type="bibr" target="#b5">6</ref> However, we need to collect a sufficient number of responses per pair to accurately estimate p AB . Furthermore, we need to do this for 77, 280 training pairs and 4, 200 test pairs, resulting in a large number of MTurk inquiries and a prohibitive cost. In the next two sections, we show how to avoid this problem by analyzing the number of responses needed per image pair to statistically estimate its p AB accurately, and then showing how to use a maximum likelihood (ML) estimator to accurately label a larger set of pairs based on a smaller set of acquired labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Number of responses per comparison</head><p>We model the human response as a Bernoulli random variable ŒΩ with a success probability p AB , which is the probability of a person preferring A over B. Given n human responses ŒΩ i , i = 1, ..., n, we can estimate</p><formula xml:id="formula_3">p AB byp AB = 1 n ‚àë n i=1 ŒΩ i .</formula><p>We must choose n such that prob(|p AB ‚àí p AB | ‚â§ Œ∑) ‚â• P target , for a target P target and tolerance Œ∑. It can be easily confirmed (see supplementary) that by choosing n = 40 and Œ∑ = 0.15, we can achieve a reasonable P target ‚â• 0.94. Therefore, we collect 40 responses for each pairwise comparison in the training and test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Statistical estimation of human preference</head><p>Collecting 40 MTurk responses for 77,280 pairs of training images is expensive. Thus, we use statistical modeling to estimate the missing human labels based on a subset of the exhaustive pairwise comparison data <ref type="bibr" target="#b50">[50]</ref>. To see how, suppose we need to estimate p AB for all possible pairs of N images (e.g., N = 15 in each inter-type group) with perceptualerror scores s = [s 1 , ..., s N ]. We denote the human responses by a count matrix C = {c i, j }, where c i, j is the number of times image i is preferred over image j. The scores can then be obtained by solving an ML estimation problem <ref type="bibr" target="#b50">[50]</ref>:</p><formula xml:id="formula_4">s ‚ãÜ = argmax ‚àë i, j c i, j log S(s i ‚àí s j ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>where S(.) is the sigmoid function of the BT model for human responses (see Sec. 2).</head><p>We solve this optimization problem via gradient descent.</p><p>However, we must query a sufficient number of pairwise comparisons so that the optimal solution recovers the underlying true scores. It is sufficient to query a subset of all the possible comparisons as long as each image appears in at least k comparisons (k &lt; N ‚àí 1) presented to the humans, where k can be determined empirically <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b59">59]</ref>. Our empirical analysis reveals that k = 10 is sufficient as the binary error rate over the estimated part of the subset becomes 0.0006. More details are included in the supplementary file.</p><p>ML estimation reduces the number of pairs that need labeling considerably, from 81,480 to 62,280 (23.56% reduction). Note that we only do this for the training set and not for the test set (we query 40 human responses for each pairwise comparison in the test set), which ensures that no test errors are caused by possible ML estimation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We implemented the system presented in Sec. 3 in TensorFlow <ref type="bibr" target="#b1">[2]</ref>, and trained it on the training set described in Sec. 4 for 300K iterations (2 days) on an NVIDIA Titan X GPU. In this section, we evaluate the performance of PieAPP and compare it with popular and state-of-the-art IQA methods on both our proposed test set (Sec. 4.2) and two existing datasets (CSIQ <ref type="bibr" target="#b26">[26]</ref> and TID2013 <ref type="bibr" target="#b42">[42]</ref>). Since there are many recent learning-based algorithms (e.g., <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b28">28]</ref>), in this paper we only compare against the three that performed the best on established datasets based on their published results <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">25]</ref>. We show comparisons against other methods and on other datasets in the supplementary. We begin by validating the BT model for our data and showing that ML estimation can accurately fill in missing human labels (Sec. 5.1). Next, we compare the performance of PieAPP on our test set against that of popular and stateof-the-art IQA methods (Sec. 5.2), with our main results presented in <ref type="table" target="#tab_7">Table 2</ref>. We also test our learning model (given by the error-estimation block f ) on other datasets by training it on them (Sec. 5.3, <ref type="table">Table 3</ref>). Finally, we show that existing learning-based methods can be improved using our pairwise-learning framework (Sec. 5.4, Table. 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Consistency of the BT model and the estimation of probabilistic human preference</head><p>We begin by showing the validity of our assumptions that: 1) the Bradley-Terry (BT) model accurately accounts for human decision-making for pairwise image comparisons, and 2) ML estimation can be used to accurately fill in the human preference probability for pairs without human labels. To do these experiments, we first collect exhaustive pairwise labels for 6 sets of 15 distorted images (total 630 image pairs), with 100 human responses for each comparison, which ensures that the probability labels are highly reliable (P target = 0.972 and Œ∑ = 0.11; see Sec. 4.3.1).</p><p>To validate the BT model, we estimate the scores for all images in a set with ML estimation, using the ground-truth labels of all the pairs. <ref type="bibr" target="#b6">7</ref> Using the estimated scores, we compute the preference probability for each pair based on BT (Eq. 1), which effectively tests whether the BT scores can "fit" the measured pairwise probabilities. Indeed, <ref type="figure" target="#fig_2">Fig. 5a</ref> shows that the relationship between the ground-truth and the estimated probabilities is close to identity, which indicates that BT is a good fit for human decision-making.</p><p>We then validate the ML estimation when not all pair-  wise comparisons are labeled. We estimate the scores using 10 ground-truth comparisons per image in each set (k = 10; see Sec. 4.3.2), instead of using all the pairwise comparisons like before. <ref type="figure" target="#fig_2">Fig. 5b</ref> shows that we have a close-toidentity relationship with a negligible increase of noise, indicating the good accuracy of the ML-estimation process. Finally, we reduce the number of human responses per comparison: we again use 10 comparisons per image in each set, but this time with only 40 responses per comparison (as we did for our entire training set), instead of the 100 responses we used previously. <ref type="figure" target="#fig_2">Fig. 5c</ref> shows that the noise has increased slightly but the fit to the ground-truth labels is still quite good. Hence, this validates the way we supplemented our hand-labeled data using ML estimation.</p><formula xml:id="formula_5">KRCC METHODp AB ‚àà [0, 1]p AB / ‚àà [0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance on our unseen test set</head><p>We now compare the performance of our proposed PieAPP metric to that of other IQA methods on our test set, where the images and distortion types are completely disjoint from the training set. This tests the generalizability of the various approaches to new distortions and image content. We compare the methods using the following evaluation criteria: 1. Accuracy of predicted quality (or perceptual error): As discussed in Sec. 5.1, we obtain the ground-truth scores through ML estimation, using the ground-truth preference labels for all the pairs in the test set. As is typically done in IQA papers, we compute the Pearson's linear correlation coefficient (PLCC) to assess the correlation between the magnitudes of the scores predicted by the IQA method and the ground-truth scores. <ref type="bibr" target="#b7">8</ref> We also use the Spearman's rank correlation coefficient (SRCC) to assess the agreement of ranking of images based on the predicted scores. 2. Accuracy of predicted pairwise preference: IQA methods are often used to tell which distorted image, A or  <ref type="table">Table 3</ref>: Comparison on two standard IQA datasets (CSIQ <ref type="bibr" target="#b26">[26]</ref> and TID2013 <ref type="bibr" target="#b42">[42]</ref>). For all the learning methods, we used the numbers directly provided by the authors (dashes "-" indicate numbers were not provided). For a fair comparison, we used only one error-estimation block of our pairwise-learning framework (function f ) trained directly on the MOS labels of each dataset. The performance of PieAPP on these datasets, when trained with our pairwise-learning framework, is shown in <ref type="table">Table 4</ref>.</p><p>B, is closer to a reference. Therefore, we want to know the binary error rate (BER), the percentage of test set pairs predicted incorrectly. We report the Kendall's rank correlation coefficient (KRCC), which is related to the BER by  <ref type="bibr" target="#b52">[52]</ref>, MS-SSIM <ref type="bibr" target="#b53">[53]</ref>, GMSD <ref type="bibr" target="#b54">[54]</ref>, VSI <ref type="bibr" target="#b56">[56]</ref>, PSNR-HMA <ref type="bibr" target="#b41">[41]</ref>, FSIMc <ref type="bibr" target="#b57">[57]</ref>, SFF <ref type="bibr" target="#b11">[12]</ref>, and SCQI <ref type="bibr" target="#b3">[4]</ref>, and 2) learning-based methods: DOGSSIMc <ref type="bibr" target="#b40">[40]</ref>, Lukin et al. <ref type="bibr" target="#b35">[35]</ref>, Kim et al. <ref type="bibr" target="#b25">[25]</ref>, and Bosse et al. (both no-reference (NR) and full-reference (FR) versions of their method) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. <ref type="bibr" target="#b8">9</ref> In all cases, we use the code/models released by the authors, except for Kim et al. <ref type="bibr" target="#b25">[25]</ref>, whose trained model is not publicly available. Therefore, we used the source code provided by the authors to train their model as described in their paper, and validated it by getting their reported results.</p><formula xml:id="formula_6">KRCC = 1 ‚àí 2BER.</formula><p>As <ref type="table" target="#tab_7">Table 2</ref> shows, our proposed method significantly outperforms existing state-of-the-art IQA methods. Our PLCC and SRCC are 0.842 and 0.831, respectively, outperforming the second-best method, Bosse et al. (FR) <ref type="bibr" target="#b6">[7]</ref>, by 48.24% and 54.75%, respectively. This shows that our predicted perceptual error is considerably more consistent with the ground-truth scores than state-of-the-art methods. Furthermore, the KRCC of our approach over the entire test set  <ref type="table">Table 4</ref>: Our novel pairwise-learning framework (PLF) can also be used to improve the quality of existing learning-based IQA methods.</p><formula xml:id="formula_7">(p AB ‚àà [0, 1]) is</formula><p>Here, we replaced the error-estimating f blocks in our pairwise-learning framework (see <ref type="figure">Fig. 3</ref>) with the learning models of Kim et al. <ref type="bibr" target="#b25">[25]</ref> and Bosse et al. <ref type="bibr" target="#b6">[7]</ref>. We then trained their architectures using our pairwise-learning process on our training set. As a result, the algorithms improved considerably on our test set, as can be seen by comparing these results to those of their original versions in <ref type="table" target="#tab_7">Table 2</ref>. Furthermore, we also evaluated these methods on the standard CSIQ <ref type="bibr" target="#b26">[26]</ref> and TID13 <ref type="bibr" target="#b42">[42]</ref>   <ref type="bibr" target="#b24">24</ref>.85% in the same range. This means that the best IQA method to date gets almost 25% of the pairwise comparisons wrong, but our approach offers a 2.7√ó improvement. The fact that we get these results on our test set (which is disjoint from the training set) indicates that PieAPP is capable of generalizing to new image distortions and content much better than existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Testing our architecture on other IQA datasets</head><p>For completeness, we also compare our architecture against other IQA methods on two of the largest existing IQA datasets, CSIQ <ref type="bibr" target="#b26">[26]</ref> and TID2013 <ref type="bibr" target="#b42">[42]</ref>. <ref type="bibr" target="#b9">10</ref> Since these have MOS labels which are noisy with respect to ground-truth preferences, we trained our error-estimation function of the pairwise-learning framework (i.e., f (A, R; Œ∏) in <ref type="figure">Fig. 3</ref>) on the MOS labels of CSIQ and TID2013, respectively. <ref type="bibr" target="#b10">11</ref> This allows us to directly compare our DCNN architecture to existing methods on the datasets they were designed for.</p><p>For these experiments, we randomly split the datasets into 60% training, 20% validation, and 20% test set, and report the performance averaged over 5 such random splits, as is usual. The standard deviation of the correlation coefficients on the test sets of these five random splits is at most 0.008 on CSIQ and at most 0.005 on TID2013, indicating that our random splits are representative of the data and are not outliers. <ref type="table">Table 3</ref> shows that our proposed DCNN architecture outperforms the state-of-the-art in both CSIQ and TID2013, except for the PLCC on TID2013 where we are only 0.11% worse than Kim et al. The fact that we are better than (or comparable to) existing methods on the standard datasets (which are smaller and have fewer distortions) while significantly outperforming them in our test set (which contains new distortions) validates our method. <ref type="bibr" target="#b9">10</ref> Comparisons on other datasets can be found in the supplementary file. <ref type="bibr" target="#b10">11</ref> Here, we train our error-estimation function directly on MOS labels as existing datasets do not provide probabilistic pairwise labels. However, this does not change our architecture of f or its number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Improving other learning-based IQA methods</head><p>As discussed earlier, our pairwise-learning framework is a better way to learn IQA because it has less noise than either subjective human quality scores or Swiss tournaments. In fact, we can use it to improve the performance of existing learning-based algorithms. We observe that since typical FR-IQA methods use a distorted image A and a reference R to compute a quality score, they are effectively an alternative to our error-estimation block f in <ref type="figure">Fig. 3</ref>. Hence, we can replace our implementation of f (A, R; Œ∏) and f (B, R; Œ∏) with a previous learning-based IQA method, and then use our pairwise-learning framework to train it with our probability labels. To do this, we duplicate the block to predict the scores for inputs A and B, and then subtract the predicted scores and pass them through a sigmoid (i.e., block h(s A , s B ) in <ref type="figure">Fig. 3</ref>). This estimated probability is then compared to the ground-truth preference for backpropagation. <ref type="bibr" target="#b11">12</ref> This enables us to train the same learning architectures previously proposed, but with our probabilistic-preference labels. We show results of experiments to train the methods of Kim et al. <ref type="bibr" target="#b25">[25]</ref> and Bosse et al. <ref type="bibr" target="#b6">[7]</ref> (both FR and NR) in <ref type="table">Table 4</ref>. By comparing with the corresponding entries in <ref type="table" target="#tab_7">Table 2</ref>, we can see that their performance on our test set has improved considerably after our training. This makes sense because probabilistic preference is a better metric and it also allows us to leverage our large, robust IQA dataset. Still, however, our proposed DCNN architecture for errorestimation block f performs better than the existing methods. Finally, the table also shows the performance of these architectures trained on our pairwise-preference dataset, but tested on CSIQ <ref type="bibr" target="#b26">[26]</ref> and TID2013 <ref type="bibr" target="#b42">[42]</ref>, using their MOS labels as ground-truth. While our modifications have improved existing approaches, PieAPP still performs better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a novel, perceptual image-error metric which surpasses existing metrics by leveraging the fact that pairwise preference is a robust way to create large IQA datasets and using a new pairwise-learning framework to train an error-estimation function. Overall, this approach could open the door for new, improved learning-based IQA methods in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Our DCNN implementation of the error-estimation function f . (a) The feature-extraction (FE) subnet of f has 11 convolutional (CONV) layers with skip connections to compute the features for an input patch A m . The number after "CONV" indicates the number of feature maps. Each layer has 3√ó3 filters and a non-linear ReLU, with 2 √ó 2 max-pooling after every even layer. (b) The score-computation (SC) subnet uses two fully-connected (FC) networks (each with 1 hidden layer with 512 neurons) to compute patch-wise weights and errors, followed by weighted averaging over all patches to compute the final image score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Ground-truth (GT) human-preference probabilities vs. estimated probabilities. (a) Estimating probabilities based on all pairwise comparisons with 100 human responses per pair to validate the BT model. (b) Estimating probabilities with only 10 comparisons per image (still 100 human responses each). (c) Estimating probabilities based on only 10 comparisons per image with only 40 responses per pair. The last two plots show the accuracy of using ML estimation to fill in the missing labels (see Sec. 4.3.2). The blue segments near the fitted line indicate the 25 th and 75 th percentile of the estimated probabilities within each 0.1 bin of GT probabilities. The fitted line is close to y = x as shown by the equations on the bottom-right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Figure 1: Which image, A or B, is more similar to the reference R? This is an example of a pairwise image comparison where most people have no difficulty determining which image is closer. In this case, according to our Amazon Mechanical Turk (MTurk) experiments, 88%</figDesc><table>.414 
0.283 
PieAPP (error) 
2.541 
0.520 
PieAPP (prob.) 
0.117 
0.883 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>, that all distorted versions of aFigure 2: Like all IQA methods, we assume distorted images can be placed on a linear scale based on their underlying perceptual- error scores (e.g., s A , s B , s C ) with respect to the reference. In our case, we map the reference to have 0 error. We assume the proba- bility of preferring distorted image A over B can be computed by applying a function h to their errors, e.g., p AB = h(s A , s B ).</figDesc><table>image error scale 

0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1</head><label>1</label><figDesc>compares our proposed dataset with the four largest existing IQA datasets. 3 Our dataset is substantially bigger than all these existing IQA datasets combined in 2 See supplementary file for a detailed analysis. 3 See Chandler et al. [11] for a complete list of existing IQA datasets.</figDesc><table>Dataset 

Ref. images 
Distortions 
Distorted images 
LIVE [47] 
29 
5 
779 
CSIQ [26] 
30 
6 
866 
TID2008 [43] 
25 
17 
1,700 
TID2013 [42] 
25 
24 
3,000 
Our dataset 
200 
75 
20,280 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Comparison of the four largest IQA dataset and our pro- posed dataset, in terms of the number of reference images, the number of distortions, and the number of distorted images.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>4.3). For each reference image R, we design two kinds of pairwise comparisons: inter-type and intra-type. In an inter-type comparison, A and B are gener- ated by applying two different types of distortions to R. For each reference image, there are 4 groups of inter-type com- parisons, each containing 15 distorted images generated us- ing 15 randomly-sampled distortions.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance of our approach compared to existing IQA 

methods on our test set. PieAPP beats all the state-of-the-art meth-
ods because the test set contains many different (and complex) dis-
tortions not found in standard IQA datasets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head></head><label></label><figDesc>Mean Absolute Error (MAE), Root-Mean-Square Er- ror (RMSE), SSIM</figDesc><table>Since this is less meaningful when 
human preference is not strong (i.e.,p AB ‚àà [0.35, 0.65]), we 
show numbers for both the full range andp AB ‚àà [0.35, 0.65]. 
For comparisons, we test against: 1) model-based meth-
ods: </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head></head><label></label><figDesc>0.668, and is 0.815 whenp AB / ‚àà [0.35, 0.65] (i.e., when there is a stronger preference by humans). This</figDesc><table>Our test set 

CSIQ [26] 
TID2013 [42] 

KRCC 

Our PLF Modifications:p 

AB ‚àà [0, 1]p AB / 
‚àà [0.35, 0.65] 
PLCC 
SRCC 
KRCC 
PLCC 
SRCC 
KRCC 
PLCC 
SRCC 

PLF + Kim et al. 
0.491 
0.608 
0.654 
0.632 
0.708 
0.863 
0.873 
0.649 
0.795 
0.837 
PLF + Bosse et al. (NR) 
0.470 
0.593 
0.590 
0.593 
0.663 
0.809 
0.842 
0.654 
0.781 
0.831 
PLF + Bosse et al. (FR) 
0.588 
0.729 
0.734 
0.748 
0.739 
0.844 
0.898 
0.682 
0.828 
0.859 
Our method (PieAPP) 
0.668 
0.815 
0.842 
0.831 
0.754 
0.842 
0.907 
0.710 
0.836 
0.875 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head></head><label></label><figDesc>datasets using the original MOS labels for ground truth. is a significant improvement over Bosse et al. (FR) [7] of 61.35% and 62.03%, respectively. Translating these to bi- nary error rate (BER), we see that our method has a BER of 9.25% whenp AB / ‚àà [0.35, 0.65], while Bosse et al. has a BER of</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In contrast, most previous learning-based IQA algorithms test on the same distortions that they train on. Even in the "cross-dataset" tests presented in previous papers, there is a significant overlap between the training and test distortions. This makes it impossible to tell whether previous learning-based algorithms would work for new, unseen distortions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The choice of 15 is based on a balance between properly sampling the training distortions and the cost of obtaining labels in an inter-type group. 6 Details on MTurk experiments and interface are in the supplementary.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">The is the same estimation as in Sec 4.3.2, but with the ground-truth labels for all the pairs in each set.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">For existing IQA methods, the PLCC on our test set is computed after fitting the predicted scores to the ground-truth scores via a nonlinear regression as is commonly done [47].</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">See supplementary for brief descriptions of these methods, as well as comparisons to other IQA methods.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">All details of the training process can be found in the supplementary.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Basic rules for Swiss systems</title>
		<ptr target="https://www.fide.com/component/handbook/?id=83&amp;view=article" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image quality assessment by comparing CNN features between images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Amirshahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Imaging Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="60410" to="60411" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A novel image quality assessment with globally and locally consilient visual quality perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2392" to="2406" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of perceptual image processing methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beghdadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Larabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouzerdoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Iftekharuddin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="811" to="831" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Full-reference image quality assessment using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maniry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Video Processing and Quality Metrics</title>
		<meeting>the International Workshop on Video Processing and Quality Metrics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep neural networks for no-reference and full-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maniry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="206" to="219" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image quality assessment using a neural network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouzerdoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Havstad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beghdadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on Signal Processing and Information Technology</title>
		<meeting>the IEEE International Symposium on Signal Processing and Information Technology</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rank analysis of incomplete block designs: I. The method of paired comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="324" to="345" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image quality assessment by using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Heynderickz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gastaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zunino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on Circuits and Systems</title>
		<meeting>the IEEE International Symposium on Circuits and Systems</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Seven challenges in image quality assessment: Past, present, and future research. ISRN Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sparse feature fidelity for perceptual image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4007" to="4018" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic triage for a photo series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ashley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">148</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Edge-based structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Po</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradient-based structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Subjective quality assessment for wireless image communication: The wireless imaging quality database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Engelke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zepernick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Kusuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Video Processing and Quality Metrics for Consumer Electronics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deepsim: Deep similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Neurocomputing</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Objective quality assessment of displayed images by using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gastaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zunino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Heynderickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vicario</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="643" to="661" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Supporting visual quality assessment with machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gastaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zunino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CBP neural network for objective assessment of image quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gastaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zunino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vicario</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Heynderickx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="194" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Perceptual image quality assessment using block-based multi-metric fusion (BMMF)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuo</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1145" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On contrast sensitivity in an image difference model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Fairchild</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IS and TS PICS Conference</title>
		<meeting>the IS and TS PICS Conference</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="18" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recent developments in image quality assessment algorithms: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Joy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Sarma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Theoretical and Applied Information Technology</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="192" to="201" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1733" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning of human visual sensitivity in image quality assessment framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Most apparent distortion: full-reference image quality assessment and the role of strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11006" to="011006" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Subjective quality assessment irccyn/ivc database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Autrusseau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image quality assessment using similar scene as reference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Perceptual visual quality metrics: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="297" to="312" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Perceptual image quality assessment: recent progress and trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Narwaria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Communications and Image Processing</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="774403" to="774403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A multi-metric fusion approach to visual quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Quality of Multimedia Experience</title>
		<meeting>the International Workshop on Quality of Multimedia Experience</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="72" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recent developments and future trends in visual quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APSIPA ASC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image quality assessment using multi-method fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1793" to="1807" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">CID: IQ-a new image quality database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Hardeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image and Signal Processing</title>
		<meeting>the International Conference on Image and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Combining full-reference image visual quality metrics by neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">I</forename><surname>Ieremeiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Astola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE/IS&amp;T Electronic Imaging</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="93940" to="93940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Waterloo exploration database: New challenges for image quality assessment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1004" to="1016" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Group MAD competition-A new methodology to compare objective image quality models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1664" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mean deviation similarity index: Efficient and reliable fullreference image quality evaluator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Z</forename><surname>Nafchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahkolaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hedjam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="5579" to="5590" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Exploratory font selection using crowdsourced attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>O&amp;apos;donovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lƒ´beks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">92</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image quality assessment using human visual dog model fused with random forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3282" to="3292" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Modified image visual quality metrics for contrast change and mean shift accounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ieremeiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on the Experience of Designing and Application of CAD Systems in Microelectronics</title>
		<meeting>the International Conference on the Experience of Designing and Application of CAD Systems in Microelectronics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="305" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Image database TID2013: Peculiarities, results and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ieremeiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Astola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vozel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Battisti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="57" to="77" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">TID2008-A database for evaluation of full-reference visual quality assessment metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zelensky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances of Modern Radioelectronics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="30" to="45" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A Haar wavelet-based perceptual similarity index for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Reisenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kutyniok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06140</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Complex wavelet structural similarity: A new image similarity index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Sampat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Markey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2385" to="2401" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Image information and visual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="430" to="444" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A statistical evaluation of recent full reference image quality assessment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Sabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3440" to="3451" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Visibility of DCT basis functions: Effects of contrast masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahumada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Data Compression Conference</title>
		<meeting>the Data Compression Conference</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="361" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Impact of subjective dataset on the performance of image quality metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tourancheau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Autrusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">P</forename><surname>Sazzad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Horita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="365" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">How to analyze paired comparison data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tsukida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>University of Washington, Seattle, Deptartment of Electrical Engineering</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Modern image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Synthesis Lectures on Image, Video, and Multimedia Processing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asilomar Conference on Signals, Systems and Computers</title>
		<meeting>the Asilomar Conference on Signals, Systems and Computers</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Gradient magnitude similarity deviation: A highly efficient perceptual image quality index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="684" to="695" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">VCL@FER image quality assessment database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zariƒá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tataloviƒá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brajkoviƒá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hlevnjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lonƒçariƒá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dumiƒá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grgiƒá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AUTOMATIKA</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="344" to="354" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">VSI: A visual saliency-induced index for perceptual image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4270" to="4281" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">FSIM: A feature similarity index for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2378" to="2386" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A comprehensive evaluation of full reference image quality assessment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Mirror mirror: Crowdsourcing better portraits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">234</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
