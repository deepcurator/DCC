<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Input Switched Affine Networks: An RNN Architecture Designed for Interpretability</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sussillo</surname></persName>
						</author>
						<title level="a" type="main">Input Switched Affine Networks: An RNN Architecture Designed for Interpretability</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>There exist many problem domains where the interpretability of neural network models is essential for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations -in other words an RNN without any explicit nonlinearities, but with inputdependent recurrent weights. This simple form allows the RNN to be analyzed via straightforward linear methods: we can exactly characterize the linear contribution of each input to the model predictions; we can use a change-of-basis to disentangle input, output, and computational hidden unit subspaces; we can fully reverse-engineer the architecture's solution to a simple task. Despite this ease of interpretation, the input switched affine network achieves reasonable performance on a text modeling tasks, and allows greater computational efficiency than networks with standard nonlinearities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1. Introduction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">The importance of interpretable machine learning</head><p>As neural networks move into applications where the outcomes of human lives depend on their decisions, it is increasingly crucial that we are able to interpret the decisions they make. Indeed, the European Union is considering legislation with a clause that asserts that individuals have 'rights to explanation', i.e. individuals should be able to understand how algorithms make decisions about them <ref type="bibr">(Council of European Union, 2016)</ref>. Example problem domains re- quiring interpretable ML include self-driving cars <ref type="bibr" target="#b4">(Bojarski et al., 2016)</ref>, air traffic control <ref type="bibr" target="#b18">(Katz et al., 2017)</ref>, power grid control <ref type="bibr" target="#b28">(Siano et al., 2012)</ref>, hiring and promotion decisions while preventing bias <ref type="bibr" target="#b27">(Scarborough &amp; Somers, 2006)</ref>, automated sentencing decisions in US courts <ref type="bibr" target="#b32">(Tashea, 2017;</ref><ref type="bibr" target="#b3">Berk et al., 2017)</ref>, and medical diagnosis <ref type="bibr" target="#b11">(Gulshan et al., 2016)</ref>. For many of these applications, practitioners will not adopt ML models without fully understanding what drives their predictions, including understanding when and how these models fail <ref type="bibr" target="#b6">(Ching et al., 2017;</ref><ref type="bibr" target="#b9">Deo, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Post hoc analysis</head><p>One approach to interpreting neural networks is to train the network as normal, and then apply analysis techniques after training. Often this approach yields systems that perform extremely well, but where interpretability is challenging. For example, <ref type="bibr" target="#b29">Sussillo &amp; Barak (2013)</ref> used linearization and nonlinear dynamical systems theory to understand RNNs solving a set of simple but varied tasks. <ref type="bibr" target="#b17">Karpathy et al. (2015)</ref> analyzed an LSTM <ref type="bibr" target="#b13">(Hochreiter &amp; Schmidhuber, 1997</ref>) trained on a character-based language modeling task. They were able to break down LSTM language model errors into classes, such as e.g., "rare word" errors. Concurrently with our submission, <ref type="bibr" target="#b25">Murdoch &amp; Szlam (2017)</ref> decomposed LSTM outputs using telescoping sums of statistics computed from memory cells at different RNN steps. The decomposition is exact, but not unique and the authors justify it by demonstrating good performance of decision rules formed using the computed cell statistics.</p><p>The community is also interested in post hoc interpretation of feed-forward networks. Examples include the use of linear probes in <ref type="bibr" target="#b0">Alain &amp; Bengio (2016)</ref>, and a variety of techniques (most driven by back-propagation) to assign credit for activations to specific inputs or input patterns in feed-forward networks <ref type="bibr" target="#b33">(Zeiler et al., 2010;</ref><ref type="bibr" target="#b19">Le et al., 2012;</ref><ref type="bibr" target="#b24">Mordvintsev et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Building interpretability into the architecture</head><p>A second approach is to build a neural network where interpretability is an explicit design constraint. In this approach, a typical outcome is a system that can be better understood, but at the cost of reduced performance. Model classes whose decisions are naturally interpretable include logistic regression <ref type="bibr" target="#b10">(Freedman, 2009)</ref>, decision trees <ref type="bibr" target="#b26">(Quinlan, 1987)</ref>, and support vector machines with simple (e.g. linear) kernels <ref type="bibr" target="#b1">(Andrew, 2013)</ref>.</p><p>In this work we follow this second approach and build interpretability into our network model, while maintaining good, though not always state-of-the-art, performance for the tasks we study. We focus on the commonly studied task of character based language modeling. We develop and analyze a model trained on a one-step-ahead prediction task of the Text8 dataset, which is 10 million characters of Wikipedia text <ref type="bibr" target="#b21">(Mahoney, 2011)</ref>, on the Billion Word Benchmark <ref type="bibr" target="#b5">(Chelba et al., 2013)</ref>, and finally on a toy multiple parentheses counting task which we fully reverse engineer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.">Switched affine systems</head><p>The model we introduce is an Input Switched Affine Network (ISAN), where the input determines the switching behavior by selecting a transition matrix and bias as a function of that input, and there is no nonlinearity. Linear timevarying systems are standard material in undergraduate electrical engineering text books, and are closely related to our technique.</p><p>Although the ISAN is deterministic, probabilistic versions of switching linear models with discrete latent variables have a history in the context of probabilistic graphical models. A recent example is the switched linear dynamical system in <ref type="bibr" target="#b20">(Linderman et al., 2016)</ref>. Focusing on language modeling, <ref type="bibr" target="#b2">(Belanger &amp; Kakade, 2015)</ref> defined a probabilistic linear dynamical system (LDS) as a generative language model for creating context-dependent token embeddings and then used steady-state Kalman filtering for inference over token sequences. They used singular value decomposition and discovered that the right and left singular vectors were semantically and syntactically related. A critical difference between the ISAN and the LDS is that the ISAN weight matrices are input token dependent (while the biases of both models are input dependent).</p><p>Multiplicative neural networks (MRNNs) were proposed precisely for character based language modeling in . The MRNN architecture is similar to our own, in that the dynamics matrix switches as a function of the input character. However, the MRNN relied on a tanh nonlinearity, while the ISAN is explicitly linear. It is this property of our model which makes it both amenable to analysis, and computationally efficient.</p><p>The Observable Operator Model (OOM) <ref type="bibr" target="#b15">(Jaeger, 2000)</ref> is similar to the ISAN in that the OOM updates a latent state using a separate transition matrix for each input symbol and performs probabilistic sequence modeling. Unlike the ISAN, the OOM requires that a linear projection of the hidden state corresponds to a normalized sequence probability. This imposes strong constraints on both the model parameters and the model dynamics, and restricts the choice of training algorithms. In contrast, the ISAN applies an affine readout to the hidden state to obtain logits, which are then pushed through the softmax function to obtain probabilities. Therefore no constraints need to be imposed on the ISAN's parameters and training is easy using backprop. Lastly, the ISAN is formulated as an affine, rather than linear model. While this doesn't change the class of processes that can be modeled, it stabilizes training and greatly enhances interpretability, facilitating the analysis in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5.">Paper structure</head><p>In what follows, we define the ISAN architecture, demonstrate its performance on the one-step-ahead prediction task, and then analyze the model in a multitude of ways, most of which would be currently difficult or impossible to accomplish with modern nonlinear recurrent architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Model definition</head><p>In what follows W x and b x respectively denote a transition matrix and a bias vector for a specific input x, the symbol x t is the input at time t, and h t is the hidden state at time t. Our ISAN model is defined as</p><formula xml:id="formula_0">h t = W xt h t−1 + b xt .<label>(1)</label></formula><p>The network also learns an initial hidden state h 0 . We emphasize the intentional absence of any nonlinear activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Character level language modeling with ISAN</head><p>We trained RNNs on the Text8 Wikipedia dataset and the billion word benchmark (BWB), for one-step-ahead character prediction. The Text8 dataset consists only of the 27 characters 'a'-'z' and '_' (space). The BWB dataset consist of Unicode text and was modelled as a sequence of bytes (256 discrete tokens) that formed the UTF8-encoded data. Given a character sequence of x 1 , ..., x t , the RNNs are trained to minimize the cross-entropy between the true next character, and the output prediction. We map from the hidden state, h t , into a logit space via an affine map. The probabilities are computed as</p><formula xml:id="formula_1">p (x t+1 ) = softmax (l t ) (2) l t = W ro h t + b ro ,<label>(3)</label></formula><p>where W ro and b ro are the readout weights and biases, and l t is the logit vector. For the Text8 dataset, we split the data into 90%, 5%, and 5% for train, validation, and test respectively, in line with <ref type="bibr" target="#b23">(Mikolov et al., 2012)</ref>. The network was trained with the same hyperparameter tuning infrastructure as in <ref type="bibr" target="#b7">(Collins et al., 2016)</ref>. For the BWB dataset, we used data splits and evaluation setup identical to <ref type="bibr" target="#b16">(Józefowicz et al., 2016)</ref>. Due to long experiment running times, we manually tuned the hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results and analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">ISAN performance on Text8 prediction</head><p>The results on Text8 are shown in <ref type="table" target="#tab_0">Table 1</ref>. For the largest parameter count, the ISAN matches almost exactly the performance of all other nonlinear models with the same number of maximum parameters: RNN, IRNN, GRU, LSTM. However, we note that for small numbers of parameters the ISAN performs considerably worse than other architectures. All analyses use ISAN trained with 1.28e6 maximum parameters (1.58 bpc cross entropy). Samples of generated text from this model are relatively coherent. We show two examples, after priming with "annual reve", at inverse temperature of 1.5, and 2.0, respectively:</p><p>• "annual revenue and producer of the telecommunications and former communist action and saving its new state house of replicas and many practical persons" • "annual revenue seven five three million one nine nine eight the rest of the country in the united states and south africa new".</p><p>As a preliminary, comparative analysis, we performed PCA on the state sequence over a large set of sequences for the vanilla RNN, GRU of varying sizes, and ISAN. This is shown in <ref type="figure" target="#fig_1">Figure 1</ref>. The eigenvalue spectra, in log of variance explained, was significantly flatter for the ISAN than the other architectures.</p><p>We compared the ISAN performance to a fully linear RNN without input switched dynamics. This achieves a crossentropy of 3.1 bits / char, independent of network size. This perplexity is only slightly better than that of a Naive Bayes model on the task, at 3.3 bits / char. The output probability of the fully linear network is a product of contributions from each previous character, as in Naive Bayes. Those factorial contributions are learned however, giving the non-switched affine network a slight advantage. We also trained a fully linear network with a nonlinear readout. This achieves 2.15 bits / char, independent of network size. Both of these comparisons illustrate the importance of the input switched dynamics for achieving good results.</p><p>Lastly we also test to what extent the ISAN can deal with large dictionaries by running it on a byte-pair encoding of the text8 task, where the input dictionary consists of the 27 2 different possible character combinations. We find that in this setup the LSTM consistently outperforms the ISAN for the same number of parameters. At 1.3m parameters the LSTM achieves a cross entropy of 3.4 bits / char-pair, while ISAN achieves 3.55. One explanation for this finding is that the matrices in ISAN are 27 times smaller than the matrices of the LSTMs. For very large numbers of parameters the performance of any architecture saturates in the number of parameters, at which point the ISAN can 'catch-up' with more parameter efficient architectures like LSTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ISAN performance on Billion Word Benchmark prediction</head><p>We trained ISAN and LSTM models on the BWB dataset. All networks were trained using asynchronous gradient descent using the Adagrad learning rule. Our best LSTM model reached 1.1 bits per character, which matches published results <ref type="bibr" target="#b14">(Hwang &amp; Sung, 2016</ref>). The LSTM model had one layer of 8192 LSTM units whose outputs were projected onto 1024 dimensions (44e6 parameters). Our best ISAN models reached 1.4 bits per character and used <ref type="figure">Figure 2</ref>. Using the linearity of the hidden state dynamics, predictions at step t can be broken out into contributions, κ t s , from previous steps. Accordingly, each row of the top panel corresponds to the propagated contribution (κ t s ) of the input character at time s, to the prediction at time t (summed to create the logit at time t). The penultimate row contains the output bias vector replicated at every time step. The last row contains the logits of the predicted next character, which is the sum of all rows above. The bottom panel contains the corresponding softmax probabilities at each time t for all characters (time is separated by gray lines). Labeled is the character with the maximum predicted probability. The time step boxed in red is examined in more detail in <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>512 hidden units, a reduced set of most common 70 input tokens and 256 output tokens (18e6 parameters). Increasing ISAN's hidden layer size to 768 units (41e6 parameters) yielded a perplexity improvement to 1.36 bits/char. Investigation of generated samples shows that the ISAN learned the distinction between lower-and upper-cased letters and is able to generate text which is coherent over short segments.</p><p>To demonstrate sample variability we show continuations of the prompt "The [Pp]ol" generated using the ISAN:</p><p>• The Pol|ish pilgrims are as angry over the holiday trip • The Pol|ice Department subsequently slipped toward • The Pol|ice Federation has sought Helix also investors • The Pol|itico is in a tight crowd ever to moderated the • The pol|itical scientist in the Red Shirt Romance cannot • The pol|icy for all Balanchine had formed when it set a • The pol|l conducted when a suspected among Hispanic • The pol|itical frenzy sparked primary care programs 3.3. Decomposition of current predictions based on previous time steps Analysis in this paper is carried out on the best-performing Text8 ISAN model, which has 1, 271, 619 parameters, corresponding to 216 hidden units, and 27 dynamics matrices W x and biases b x .</p><p>With ISAN we can analyze which factors were important in the past for determining the current character prediction. Taking advantage of the linearity of the hidden state dynamics for any sequence of inputs, we decompose the current latent state h t into contributions originating from different time points s in the history of the input:</p><formula xml:id="formula_2">h t = t s=0 t s =s+1 W x s b xs ,<label>(4)</label></formula><p>where the empty product when s + 1 &gt; t is 1 by convention, and b x0 = h 0 is the learned initial hidden state.</p><p>Using this decomposition and the fact that matrix multiplication is a linear transformation we can also write the unnormalized logit-vector, l t , as a sum of terms linear in the biases,</p><formula xml:id="formula_3">l t = b ro + t s=0 κ t s (5) κ t s = W ro t s =s+1 W x s b xs ,<label>(6)</label></formula><p>where κ t s is the contribution from time step s to the logits at time step t, and κ t t = b xt . For notational convenience we will sometimes replace the subscript s with the corresponding input character x s at step s when referring to κ t s . For example, κ t 'q' refers to the contribution from the character 'q' in a string. Similarly, when discussing the summed contributions from a word or substring we will sometimes write κ  terms that are propagated and transformed through time. We emphasize that κ t s includes the multiplicative contributions from the W x s for s &lt; s ≤ t. It is however independent of prior inputs, x s for s &lt; s. This is the main difference between the analysis we can carry out with the ISAN compared to a nonlinear RNN. In a general recurrent network the contribution of a specific character sequence will depend on the hidden state at the start of the sequence. Due to the linearity of the dynamics, this dependency does not exist in the ISAN.</p><p>In <ref type="figure">Figure 2</ref> we show an example of how this decomposition allows us to understand why a particular prediction is made at a given point in time, and how previous characters influence the decoding. For example, the sequence '_an-nual_revenue_' is processed by the ISAN: Starting with an all-zero hidden state, we use equation <ref type="formula" target="#formula_3">(6)</ref> to accumulate a sequence of κ</p><formula xml:id="formula_4">t '_ , κ t 'a , κ t 'n , κ t 'n , .</formula><p>... We then used these values to understand the prediction of the network at some time t, by simple addition across the s index.</p><p>We provide a detailed view of how past characters contribute to the logits predicting the next character in <ref type="figure" target="#fig_3">Figure 3</ref>. There are two competing options for the next letter in the word stem 'reve': either 'revenue' or 'reverse'. We show that without the contributions from '_annual' the most likely decoding of the character after the second 'e' is 'r' (to form 'reverse'), while the contributions from '_annual' tip the balance in favor of 'n', decoding to 'revenue'.</p><p>Using ISAN, we can investigate information timescales in the network. For example, we investigated how quickly the contributions of κ t s decay as a function of t − s on average. <ref type="figure" target="#fig_4">Figure 4a</ref> shows that this contribution decays on two different exponential timescales. We hypothesize that the first time scale corresponds to the decay within a word, while the , plotted as a function of t − s, and averaged across all source characters. The norm appears to decay exponentially at two rates, a faster rate for the first ten or so characters, and then a slower rate for more long term contributions. b) The median cross entropy as a function of the position in the word under three different circumstances: the red line uses all of the κ t s (baseline), the green line sets all κ next corresponds to the decay of information across words and sentences. We also show the relevance of the κ t s contributions to the decoding of characters at different positions in the word <ref type="figure" target="#fig_4">(Figure 4b</ref>). For example, we observe that κ t '_' makes important contributions to the prediction of the next character at time t. We show that using only the κ t '_' , the model can achieve a cross entropy of less than 1 bit / char when the position of the character is more than 3 letters from the beginning of the word. Finally, we link the norm-decay of κ t s to the importance of past characters for the decoding quality <ref type="figure" target="#fig_4">( Figure 4c)</ref>. By artificially limiting the number of past κ available for prediction we show that the prediction quality improves rapidly when extending the history from 0 to 10 characters and then saturates. This rapid improvement aligns with the range of faster decay in <ref type="figure" target="#fig_4">Figure 4a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">From characters to words</head><p>The ISAN provides a natural means of moving from character level representation to word level. Using the linearity of the hidden state dynamics we can aggregate all of the κ t s belonging to a given word and visualize them as a single contribution to the prediction of the letters in the next word. This allows us to understand how each preceding word impacts the decoding for the letters of later words. In <ref type="figure" target="#fig_7">Figure 5</ref> we show that the words 'was' and 'higher' make large contributions to the prediction of the characters in 'than' as measured by the norm of the κ  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Change of basis</head><p>We are free to perform a change of basis on the hidden state, and then to run the affine ISAN dynamics in that new basis. Note that this change of basis is not possible for other RNN architectures, since the action of the nonlinearity depends on the choice of basis.</p><p>In particular we can construct a 'readout basis' that explicitly divides the latent space into a subspace P ro spanned by the rows of the readout matrix W ro , and its orthogonal complement P ro ⊥ . This representation explicitly divides the hidden state dynamics into a 27-dimensional 'readout' subspace that is accessed by the readout matrix to make predictions, and a 'computational' subspace comprising the remaining 216 − 27 dimensions that are orthogonal to the readout matrix.</p><p>We apply this change of basis to analyze an intriguing observation about the hidden offsets b x . As shown in <ref type="figure">Figure 6</ref>, the norm of the b x is strongly correlated to the log-probability of the unigram x in the training data. Reexpressing network parameters using the 'readout basis' shows that this correlation is not related to reading out the next-step prediction. This is because the norm of the projection of b x into P ro ⊥ remains strongly correlated with character frequency, while the projection into P ro shows little correlation. This indicates that the information content or 'surprise' of a letter is encoded through the norm of the <ref type="figure">Figure 6</ref>. By transforming the ISAN dynamics into a new basis, we can better understand the action of the input-dependent biases. a) We observe a strong correlation between the norms of the input dependent biases, bx, and the log-probability of the unigram x in the training data. We can begin to understand this correlation structure using a basis transform into the 'readout basis'. Breaking out the norm into its components in P ro and P ro ⊥ in b) and c) respectively, shows that the correlation is due to the component orthogonal to Wro. This implies a connection between information or 'surprise' and distance in the 'computational' subspace of state space. <ref type="figure">Figure 7</ref>. By transforming ISAN dynamics into a new basis, we can better interpret structure in the input-dependent biases. In a) we show the cosine distance between the input dependent bias vectors, split between vowels and consonants (' ' is first). In b) we show the correlation only considering the components in the subspace P ro spanned by the rows of the readout matrix Wro. c) shows the correlation of the components in the orthogonal complement P ro ⊥ . In all plots white corresponds to 0 (aligned) and black to 2. component of b x in the computational space, rather than in the readout space.</p><p>Similarly, in <ref type="figure">Figure 7</ref> we illustrate that the structure in the correlations between the biases b x (across all x) is due to their components in P ro , while the correlation in P ro ⊥ is relatively uniform. We can clearly see two blocks of high correlations between the vowels and consonants respectively, while b '_' is uncorrelated to either.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Comparison with n-gram model with back-off</head><p>We compared the computation performed by n-gram language models and those performed by the ISAN. An n-gram model with back-off weights expresses the conditional probability p (x t |x 1 ...x t−1 ) as a sum of smoothed count ratios of n-grams of different lengths, with the contribution of shorter n-grams down-weighted by back-off weights. On the other hand, the computations performed by the ISAN start with the contribution of b ro to the logits, which as shown in <ref type="figure">Fig-ure</ref> 8a, corresponds to the unigram log-probabilities. The logits are then additively updated with contributions from longer n-grams, represented by κ t s . This additive contribution to the logits corresponds to a multiplicative modification of the emission probabilities from histories of different length. For long time lags, the additive correction to logprobabilities becomes small <ref type="figure">(Figure 2)</ref>, which corresponds to multiplication by a uniform distribution. Despite these differences in how n-gram history is incorporated, we nevertheless observe an agreement between empirical models estimated on the training set and model predictions for unigrams and bigrams. <ref type="figure">Figure 8</ref> shows that the bias term b ro gives the unigram probabilities of letters, while the addition of the offset terms b x accurately predict the bigram distribution of P (x t+1 |x t ). Shown in panel b is an example, P (x|'_ ), and in panel c, a summary plot for all 27 letters.</p><p>We further explore the n-gram comparison by artificially limiting the length of the character history that is available to the ISAN for making predictions, as shown in <ref type="figure" target="#fig_4">Figure 4c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Analyses of a parentheses counting task</head><p>To show the possibility of complete interpretability of the ISAN we train a model on a parenthesis counting task. Bringing together ideas from section 3.5 we re-express the transition dynamics in a new basis that fully reveals performed computations.</p><p>We analyze the task of counting the nesting levels of multiple parentheses types, a simplified version of a task defined in <ref type="bibr" target="#b7">(Collins et al., 2016)</ref>. Briefly, a 35-unit ISAN is required to keep track of the nesting level of 2 different types of parentheses independently. The inputs are the one-hot encoding of the different opening and closing parentheses (e.g. '(', ')', '[', ']') as well as a noise character ('a'). The output is the one-hot encoding of the nesting level between (0-5), one set of counts for each parenthesis type (so the complete output vector is a 12 dimensional 2-hot vector). Furthermore, the target output is the nesting level at the previous time step. This artificial delay requires the model to develop a memory. One change from <ref type="bibr" target="#b7">(Collins et al., 2016</ref>) is that we exchange the cross-entropy error with an L2 error. This leads to slightly cleaner figures, but does not qualitatively change the results.</p><p>To elucidate the mechanism of ISAN's operation we first reexpress the affine transitions h t+1 = Wh t + b by their linear equivalents h t+1 = W h t , where W = [W b; 0</p><p>T 1] and h t = [h t ; 1]. Next, we used linear regression to find a change of basis for which all augmented character matrices and the hidden states are sparse. To do this we construct the 'readout' (P ro ) and 'computational' ( P ro ⊥ ) subspace decomposition as discussed in Section 3.5. We choose a basis for P ro ⊥ which makes the projections of the hidden <ref type="figure">Figure 8</ref>. The predictions of ISAN for one and two characters well approximate the predictions of unigram and bigram models. In a) we compare softmax(bro) to the empirical unigram distribution P (x). In b) we compare softmax(Wrob'_' + bro) with the empirical distribution P (xt+1|'_'). In c) we show the correlation of softmax(Wrobx + bro) with P (xt+1|xt) for all 27 characters (y-axis), and compare this to the correlation between the empirical unigram probabilities P (x) to P (xt+1|xt) (x-axis). The plot shows that the readout of the bias vector is a better predictor of the conditional distribution than the unigram probability.</p><p>states into this computational subspace 2-hot vectors. With this subspace decomposition, the hidden states and character matrices have the form</p><formula xml:id="formula_5">W x =   W rr x W rc x b r x W cr x W cc x b c x 0 T 0 T 1   h t =   h r t h c t 1  <label>(7)</label></formula><p>and the update equation can be written as</p><formula xml:id="formula_6">h t+1 = W x h t =   W rr x h r t + W rc x h c t + b r x W cr x h r t + W cc x h c t + b c x 1   .<label>(8)</label></formula><p>Here h In <ref type="figure" target="#fig_12">Figure 9d</ref> we show the hidden states in the rotated basis as a sequence of column vectors. The 35 dimensional hidden states are all 4-hot. We can treat them as a concatenation of a readout h r t and a computation h c t part. The 12-dimensional readout h r t corresponds to network's output at time step t and encodes the counts from time step t − 1 as a 2-hot vector (one count per parenthesis type). The computational space h c t is 35 − 12 = 23 dimensional, and encodes the current counts as another 2-hot vector. Note that in this basis the ISAN effectively uses only 24 dimensions and the remaining 11 dimensions have no noticeable effect on the computation. In <ref type="figure" target="#fig_12">Figure 9c</ref> we show W [ in the rotated basis. We see from the leftmost 12 columns that W </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this paper we motivated an input-switched affine recurrent network for the purpose of interpretability. We showed that a switched affine architecture achieves the same performance as LSTMs on the Text8 dataset for the same number of maximum parameters, and reasonable performance on the BWB. We performed a series of analyses, demonstrating the ability to understand how inputs at one point in the input sequence affect the outputs later in the output sequence. We showed further in the multiple parentheses counting task that the ISAN dynamics can be completely reverse engineered. In summary, this work provides evidence that the ISAN is able to express complex dynamical systems, yet its operation can in principle be fully understood, a prospect that remains out of reach for many popular recurrent architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Computational benefits</head><p>Switched affine networks hold the potential to be massively more computationally and memory efficient for text processing than other recurrent architectures. First, input-dependent affine transitions reduce the number of parameters used at every step. For K possible inputs and N parameters, the computational cost per update step is O Furthermore, the ISAN is unique in its ability to precompute affine transformations corresponding to input strings. This is possible because the composition of affine transformations is also an affine transformation. This property is used in Section 3.4 to evaluate the linear contributions of words, rather than characters. This means that the hidden state update corresponding to an entire input sequence can be computed with identical cost to the update for a single character (plus the dictionary look-up cost for the composed transformation). ISAN can therefore achieve very large speedups on input processing, at the cost of increased memory use, by accumulating large look-up tables of the W x and b x corresponding to common input sequences. Of course, practical implementations will have to incorporate Transition matrix for '[' in original basis <ref type="bibr">( ( ( ( ( ( ( ( ( [ [ [ [ [ [ )</ref>  Transition matrix for '[' in rotated basis <ref type="bibr">( ( ( ( ( ( ( ( ( [ [ [ [ [ [ )</ref>  complexities of memory management, batching, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Future work</head><p>There are some obvious future directions to this work. Currently, we define switching behavior using an input set with finite and manageable cardinality. Studying word-level language models with enormous vocabularies may require some additional logic to scale. Another idea is to build a language model that switches on bigrams or trigrams, rather than characters or words, targeting an intermediate number of affine transformations. Adapting this model to continuous-valued inputs is another important direction. One approach is to use a tensor factorization similar to that employed by the MRNN <ref type="bibr" target="#b31">(Sutskever et al., 2014)</ref> or defining weights via additional networks, as in HyperNetworks <ref type="bibr" target="#b12">(Ha et al., 2016)</ref>. Finally, we expect that automated methods for changing bases to enable sparse representations in the hidden state and dynamics matrices will be a particularly fruitful direction to pursue.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The ISAN makes fuller and more uniform use of its latent space than vanilla RNNs or GRUs. Figure shows explained variance ratio of the first 210 most significant PCA dimensions of the hidden states across several architectures for the Text8 dataset. The legend provides the number of latent units for each architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the total contribution from the word 'the' to the logit. While in standard RNNs the nonlinearity causes interde- pendence of the bias terms across time steps, in the ISAN the bias terms contribute to the state as independent linear</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Detailed view of the prediction stack for the final 'n' in '_annual_revenue'. In a) all κ t s are shown, in b) only the contributions to the 'n' logit and 'r' logits are shown, in orange and red respectively, from each earlier character in the string. This corresponds to a zoom in view of the columns highlighted in orange and red in a). In c) we show how the sum of the contributions from the string '_annual', κ t '_annual' , pushes the prediction at '_annual_reve' from 'r' to 'n'. Without this contribution the model decodes based only on κ t '_reve' , leading to a MAP prediction of 'reverse'. With the contribution from κ t '_annual' it instead predicts 'revenue'. The contribution of κ t '_annual' to the 'n' and 'r' logits is linear and exact.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The time decay of the contributions from each character to prediction. a) Average norm of κ t s across training text, E κ t s 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>zero. The results from panel b demonstrate the disproportionately large importance of '_' in decoding, especially at the onset of a word. c) The cross-entropy as a function of history when artificially limiting the number of characters available for prediction. This corresponds to only considering the most recent n of the κ, where n is the length of the history.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The ISAN architecture can be used to precisely characterize the relationship between words and characters. The top panel shows how exploiting the linearity of the network's operation we can combine the κ t s 1 ..κ t sn in a word to a single contribution, κ t word , for each word. Shown is the norm of κ t word , a measure of the magnitude of the effect of the previous word on the selection of the current character (red corresponds to a norm of 10, blue to 0). The bottom panel shows the probabilities assigned by the network to the next sequence character. Lighter lines show predictions conditioned on a decreasing number of preceding words. For example, when predicting the characters of 'than' there is a large contribution from both κ t 'was' and κ t 'higher' , as shown in the top pane. The effect on the log probabilities can be seen in the bottom panel as the model becomes less confident when excluding κ t 'was' and significantly less confident when excluding both κ t 'was'</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>the readout to readout, readout to computation, computation to readout, and computation to computation blocks of the character matrix for character x, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>for storage of the latent state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. A visualization of the dynamics of an ISAN for the two parentheses counting task with 1 time lag (count either '()' or '[]' nesting levels with a one-step readout delay). In a) the weight matrix for '[' is shown in the original basis. In c) it is shown transformed to highlight the delay-line dynamics. The activations of the hidden units are shown b) in the original basis, and d) rotated to the same basis as in c), to highlight the delay-line dynamics in a more intelligible way. The white line delineates the transition matrix elements and hidden state dimensions that directly contribute to the output. All matrices for parentheses types appear similarly, with closing parentheses, e.g. ']', changing the direction of the delay line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>The</figDesc><table>ISAN has similar performance to other RNN archi-
tectures on the Text8 dataset. Performance of RNN architectures 
on Text8 one-step-ahead prediction, measured as cross-entropy 
loss on a held-out test set, in bits per character. The loss is shown 
as a function of the maximum number of parameters a model is 
allowed. The values reported for all other architectures are taken 
from (Collins et al., 2016). 

Parameter count 8e4 3.2e5 1.28e6 

RNN 
1.88 1.69 
1.59 
IRNN 
1.89 1.71 
1.58 
GRU 
1.83 1.66 
1.59 
LSTM 
1.85 1.68 
1.59 
ISAN 
1.92 1.71 
1.58 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>rr [</head><label>rr</label><figDesc>and W cr [ are both nearly 0. This means that h r t has no influence on h t+1 . Furthermore, the computation to readout block, W rc [ , is identity on the first 12 dimensions, effectively implementing the lagging output h r t = h c t−1 . The current counts are implemented as delay lines and identity sub-matrices in W cc [ , which respectively has the effect of incrementing the count of '[' by one, saturating at 5, and leaving the count of '()' parentheses fixed. The matrices W ] , W ( , W ) behave analogously. It is clear that this solution is general, in that retraining for increased numbers of parentheses types or an increased counting maximum, would have the analogous solution.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Jasmine Collins for her help and advice, and Quoc Le, David Ha and Mohammad Norouzi for helpful discussions. We would also like to thank Herbert Jaeger for insightful discussions regarding the ObservableOperator-Model.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Understanding intermediate layers using linear classifier probes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01644</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An introduction to support vector machines and other kernel-based learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Andrew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Kybernetes</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A linear dynamical system model for text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="833" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fairness in criminal justice risk assessments: The state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Berk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heidari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jabbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shahin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09207</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">End to end learning for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariusz</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Del</forename><surname>Testa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dworakowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Firner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prasoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiakai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07316</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Opportunities and obstacles for deep learning in biology and medicine. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travers</forename><surname>Ching</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Himmelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beaulieu-Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandr</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">P</forename><surname>Way</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Enrico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul-Michael</forename><surname>Agapow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gail</forename><forename type="middle">L</forename><surname>Rosen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">142760</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Capacity and trainability in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasmine</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR 2017 submission</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">General Data Protection Regulation</title>
		<ptr target="http://www.privacy-regulation.eu/en/22.htm" />
	</analytic>
	<monogr>
		<title level="j">Article</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>Council of European Union</orgName>
		</respStmt>
	</monogr>
	<note>Regulation (EU) 2016/679)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Machine learning in medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><forename type="middle">C</forename><surname>Deo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Circulation</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="1920" to="1930" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Statistical models: theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Freedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Varun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">C</forename><surname>Stumpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Derek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arunachalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Subhashini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Widner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kasumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Madams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Cuadros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">316</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2402" to="2410" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Character-level language modeling with hierarchical recurrent neural networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyuyeon</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonyong</forename><surname>Sung</surname></persName>
		</author>
		<idno>abs/1609.03777</idno>
		<ptr target="http://arxiv.org/abs/1609.03777" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Observable operator models for discrete stochastic time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Jaeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1371" to="1398" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1602.02410</idno>
		<ptr target="http://arxiv.org/abs/1602.02410" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02078</idno>
		<title level="m">Visualizing and understanding recurrent networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykel</forename><surname>Kochenderfer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01135</idno>
		<title level="m">Reluplex: An efficient smt solver for verifying deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">W</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">C</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Paninski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1610.08466</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Recurrent switching linear dynamical systems</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Large text compression benchmark: About the test data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Mahoney</surname></persName>
		</author>
		<ptr target="http://mattmahoney.net/dc/textdata" />
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning recurrent neural networks with hessian-free optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1033" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Subword language modeling with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anoop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haison</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Inceptionism: Going deeper into neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Tyka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Google Research Blog</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2015-06-20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic rule extraction from long short term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Murdoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Simplifying decision trees. International journal of man-machine studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quinlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="221" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Neural networks in organizational research: Applying pattern recognition to the analysis of organizational behavior. American Psychological Association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Scarborough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Somers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Real time operation of smart grids via fcn networks and optimal power flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierluigi</forename><surname>Siano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cecati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janusz</forename><surname>Kolbusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="944" to="952" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Barak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="626" to="649" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Courts are using ai to sentence criminals. that must stop now. WIRED magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Tashea</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dilip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
