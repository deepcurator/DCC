<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conditional Probability Models for Deep Image Compression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Mentzer</surname></persName>
							<email>mentzerf@vision.ee.ethz.chaeirikur@vision.ee.ethz.chmichaelt@nari.ee.ethz.chtimofter@vision.ee.ethz.chvangool@vision.ee.ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">⇤</forename><surname>Michael</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tschannen</forename><surname>Radu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timofte</forename><surname>Luc</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Gool</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zürich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Switzerland</surname></persName>
						</author>
						<title level="a" type="main">Conditional Probability Models for Deep Image Compression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image compression refers to the task of representing images using as little storage (i.e., bits) as possible. While in lossless image compression the compression rate is limited by the requirement that the original image should be perfectly reconstructible, in lossy image compression, a greater reduction in storage is enabled by allowing for some distortion in the reconstructed image. This results in a so-called rate-distortion trade-off, where a balance is found between the bitrate R and the distortion d by minimizing d + βR, where β &gt; 0 balances the two competing objectives. Recently, deep neural networks (DNNs) trained as image autoencoders for this task led to promising results, achieving better performance than many traditional techniques for image compression <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9]</ref>. Another advantage of * The first two authors contributed equally.  <ref type="figure">Figure 1</ref>: State-of-the-art performance achieved by our simple compression system composed of a standard convolutional auto-encoder and a 3D-CNN-based context model. DNN-based learned compression systems is their adaptability to specific target domains such as areal images or stereo images, enabling even higher compression rates on these domains. A key challenge in training such systems is to optimize the bitrate R of the latent image representation in the auto-encoder. To encode the latent representation using a finite number of bits, it needs to be discretized into symbols (i.e., mapped to a stream of elements from some finite set of values). Since discretization is non-differentiable, this presents challenges for gradient-based optimization methods and many techniques have been proposed to address them. After discretization, information theory tells us that the correct measure for bitrate R is the entropy H of the resulting symbols. Thus the challenge, and the focus of this paper, is how to model H such that we can navigate the trade-off d + βH during optimization of the auto-encoder.</p><p>Our proposed method is based on leveraging context models, which were previously used as techniques to im-prove coding rates for already-trained models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref>, directly as an entropy term in the optimization. We concurrently train the auto-encoder and the context model with respect to each other, where the context model learns a convolutional probabilistic model of the image representation in the auto-encoder, while the auto-encoder uses it for entropy estimation to navigate the rate-distortion trade-off. Furthermore, we generalize our formulation to spatially-aware networks, which use an importance map to spatially attend the bitrate representation to the most important regions in the compressed representation. The proposed techniques lead to a simple image compression system 1 , which achieves state-of-the-art performance when measured with the popular multi-scale structural similarity index (MS-SSIM) distortion metric <ref type="bibr" target="#b22">[23]</ref>, while being straightforward to implement with standard deep-learning toolboxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Full-resolution image compression using DNNs has attracted considerable attention recently. DNN architectures commonly used for image compression are auto-encoders <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9]</ref> and recurrent neural networks (RNNs) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. The networks are typically trained to minimize the meansquared error (MSE) between original and decompressed image <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9]</ref>, or using perceptual metrics such as MS-SSIM <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b13">14]</ref>. Other notable techniques involve progressive encoding/decoding strategies <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, adversarial training <ref type="bibr" target="#b13">[14]</ref>, multi-scale image decompositions <ref type="bibr" target="#b13">[14]</ref>, and generalized divisive normalization (GDN) layers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Context models and entropy estimation-the focus of the present paper-have a long history in the context of engineered compression methods, both lossless and lossy <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b9">10]</ref>. Most of the recent DNN-based lossy image compression approaches have also employed such techniques in some form. <ref type="bibr" target="#b3">[4]</ref> uses a binary context model for adaptive binary arithmetic coding <ref type="bibr" target="#b10">[11]</ref>. The works of <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref> use learned context models for improved coding performance on their trained models when using adaptive arithmetic coding. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b1">2]</ref> use non-adaptive arithmetic coding but estimate the entropy term with an independence assumption on the symbols.</p><p>Also related is the work of van den Oord et al. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref>, who proposed PixelRNN and PixelCNN, powerful RNNand CNN-based context models for modeling the distribution of natural images in a lossless setting, which can be used for (learned) lossless image compression as well as image generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method</head><p>Given a set of training images X , we wish to learn a compression system which consists of an encoder, a quan-1 https://github.com/fab-jul/imgcomp-cvpr tizer, and a decoder. The encoder E : R d ! R m maps an image x to a latent representation z = E(x). The quantizer Q : R ! C discretizes the coordinates of z to L = |C| centers, obtainingẑ withẑ i := Q(z i ) 2 C, which can be losslessly encoded into a bitstream. The decoder D then forms the reconstructed imagex = D(ẑ) from the quantized latent representationẑ, which is in turn (losslessy) decoded from the bitstream. We want the encoded representationẑ to be compact when measured in bits, while at the same time we want the distortion d(x,x) to be small, where d is some measure of reconstruction error, such as MSE or MS-SSIM. This results in the so-called rate-distortion trade-off</p><formula xml:id="formula_0">d(x,x)+βH(ẑ),<label>(1)</label></formula><p>where H denotes the cost of encodingẑ to bits, i.e., the entropy ofẑ. Our system is realized by modeling E and D as convolutional neural networks (CNNs) (more specifically, as the encoder and decoder, respectively, of a convolutional auto-encoder) and minimizing (1) over the training set X , where a large/small β draws the system towards low/high average entropy H. In the next sections, we will discuss how we quantize z and estimate the entropy H(ẑ). We note that as E, D are CNNs,ẑ will be a 3D feature map, but for simplicity of exposition we will denote it as a vector with equally many elements. Thus,ẑ i refers to the i-th element of the feature map, in raster scan order (row by column by channel).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Quantization</head><p>We adopt the scalar variant of the quantization approach proposed in <ref type="bibr" target="#b1">[2]</ref> to quantize z, but simplify it using ideas from <ref type="bibr" target="#b16">[17]</ref>. Specifically, given centers C = {c 1 , ··· ,c L } ⇢ R, we use nearest neighbor assignments to computê</p><formula xml:id="formula_1">z i = Q(z i ):=arg min j kz i − c j k,<label>(2)</label></formula><p>but rely on (differentiable) soft quantizatioñ</p><formula xml:id="formula_2">z i = L X j=1 exp(−σkz i − c j k) P L l=1 exp(−σkz i − c l k) c j<label>(3)</label></formula><p>to compute gradients during the backward pass. This combines the benefit of <ref type="bibr" target="#b1">[2]</ref> where the quantization is restricted to a finite set of learned centers C (instead of the fixed (nonlearned) integer grid as in <ref type="bibr" target="#b16">[17]</ref>) and the simplicity of <ref type="bibr" target="#b16">[17]</ref>, where a differentiable approximation of quantization is only used in the backward pass, avoiding the need to choose an annealing strategy (i.e., a schedule for σ) as in <ref type="bibr" target="#b1">[2]</ref> to drive the soft quantization (3) to hard assignments (2) during training. In TensorFlow, this is implemented as</p><formula xml:id="formula_3">z i = tf.stopgradient(ẑ i −z i )+z i .<label>(4)</label></formula><p>We note that for forward pass computations,z i =ẑ i , and thus we will continue writingẑ i for the latent representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Entropy estimation</head><p>To model the entropy H(ẑ) we build on the approach of PixelRNN <ref type="bibr" target="#b21">[22]</ref> and factorize the distribution p(ẑ) as a product of conditional distributions</p><formula xml:id="formula_4">p(ẑ)= m Y i=1 p(ẑ i |ẑ i−1 ,...,ẑ 1 ),<label>(5)</label></formula><p>where the 3D feature volumeẑ is indexed in raster scan order. We then use a neural network P (ẑ), which we refer to as a context model, to estimate each term p(ẑ i |ẑ i−1 ,...,ẑ 1 ):</p><formula xml:id="formula_5">P i,l (ẑ) ⇡ p(ẑ i = c l |ẑ i−1 ,...,ẑ 1 ),<label>(6)</label></formula><p>where P i,l specifies for every 3D location i inẑ the probabilites of each symbol in C with l =1 ,...,L. We refer to the resulting approximate distribution as q(ẑ):</p><formula xml:id="formula_6">= Q m i=1 P i,I(ẑi) (ẑ), where I(ẑ i ) denotes the index ofẑ i in C.</formula><p>Since the conditional distributions p(ẑ i |ẑ i−1 ,...,ẑ 1 ) only depend on previous valuesẑ i−1 ,...,ẑ 1 , this imposes a causality constraint on the network P : While P may compute P i,l in parallel for i =1, . . . , m, l =1,...,L, it needs to make sure that each such term only depends on previous</p><formula xml:id="formula_7">valuesẑ i−1 ,...,ẑ 1 .</formula><p>The authors of PixelCNN <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref> study the use of 2D-CNNs as causal conditional models over 2D images in a lossless setting, i.e., treating the RGB pixels as symbols. They show that the causality constraint can be efficiently enforced using masked filters in the convolution. Intuitively, the idea is as follows: If for each layer the causality condition is satisfied with respect to the spatial coordinates of the layer before, then by induction the causality condition will hold between the output layer and the input. Satisfying the causality condition for each layer can be achieved with proper masking of its weight tensor, and thus the entire network can be made causal only through the masking of its weights. Thus, the entire set of probabilities P i,l for all (2D) spatial locations i and symbol values l can be computed in parallel with a fully convolutional network, as opposed to modeling each term p(ẑ i |ẑ i−1 , ··· ,ẑ 1 ) separately.</p><p>In our case,ẑ is a 3D symbol volume, with as much as K = 64 channels. We therefore generalize the approach of PixelCNN to 3D convolutions, using the same idea of masking the filters properly in every layer of the network. This enables us to model P efficiently, with a light-weight 2 3D-CNN which slides overẑ, while properly respecting the causality constraint. We refer to the supplementary material <ref type="bibr" target="#b2">3</ref> for more details.</p><p>As in <ref type="bibr" target="#b20">[21]</ref>, we learn P by training it for maximum likelihood, or equivalently (see <ref type="bibr" target="#b15">[16]</ref>) by training P i,: to classify the index I(ẑ i ) ofẑ i in C with a cross entropy loss:</p><formula xml:id="formula_8">CE := Eẑ ⇠p(ẑ) [ m X i=1 − log P i,I(ẑi) ].<label>(7)</label></formula><p>Using the well-known property of cross entropy as the coding cost when using the wrong distribution q(ẑ) instead of the true distribution p(ẑ), we can also view the CE loss as an estimate of H(ẑ) since we learn P such that P = q ⇡ p.</p><p>That is, we can compute</p><formula xml:id="formula_9">H(ẑ)=Eẑ ⇠p(ẑ) [− log(p(ẑ))] (8) = Eẑ ⇠p(ẑ) [ m X i=1 − log p(ẑ i |ẑ i−1 , ··· ,ẑ 1 )] (9) ⇡ Eẑ ⇠p(ẑ) [ m X i=1 − log q(ẑ i |ẑ i−1 , ··· ,ẑ 1 )] (10) = Eẑ ⇠p(ẑ) [ m X i=1 − log P i,I(ẑi) ] (11) = CE<label>(12)</label></formula><p>Therefore, when training the auto-encoder we can indirectly minimize H(ẑ) through the cross entropy CE. We refer to argument in the expectation of <ref type="formula" target="#formula_8">(7)</ref>,</p><formula xml:id="formula_10">C(ẑ):= m X i=1 − log P i,I(ẑi) ,<label>(13)</label></formula><p>as the coding cost of the latent image representation, since this reflects the coding cost incurred when using P as a context model with an adaptive arithmetic encoder <ref type="bibr" target="#b10">[11]</ref>. From the application perspective, minimizing the coding cost is actually more important than the (unknown) true entropy, since it reflects the bitrate obtained in practice.</p><p>To backpropagate through P (ẑ) we use the same approach as for the encoder (see <ref type="bibr" target="#b3">(4)</ref>). Thus, like the decoder D, P only sees the (discrete)ẑ in the forward pass, whereas the gradient of the soft quantizationz is used for the backward pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Concurrent optimization</head><p>Given an auto-encoder (E, D), we can train P to model the dependencies of the entries ofẑ as described in the previous section by minimizing <ref type="bibr" target="#b6">(7)</ref>. On the other hand, using the model P , we can obtain an estimate of H(ẑ) as in <ref type="bibr" target="#b11">(12)</ref> and use this estimate to adjust (E, D) such that d(x,D(Q(E(x)))) + βH(ẑ) is reduced, thereby navigating the rate distortion trade-off. Therefore, it is natural to concurrently learn P (with respect to its own loss), and (E, D) (with respect to the rate distortion trade-off) during training, such that all models which the losses depend on are continuously updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Importance map for spatial bit-allocation</head><p>Recall that since E and D are CNNs,ẑ is a 3D featuremap. For example, if E has three stride-2 convolution layers and the bottleneck has K channels, the dimensions ofẑ will be</p><formula xml:id="formula_11">W 8 ⇥ H 8 ⇥ K.</formula><p>A consequence of this formulation is that we are using equally many symbols inẑ for each spatial location of the input image x. It is known, however, that in practice there is great variability in the information content across spatial locations (e.g., the uniform area of blue sky vs. the fine-grained structure of the leaves of a tree).</p><p>This can in principle be accounted for automatically in the trade-off between the entropy and the distortion, where the network would learn to output more predictable (i.e., low entropy) symbols for the low information regions, while making room for the use of high entropy symbols for the more complex regions. More precisely, the formulation in (7) already allows for variable bit allocation for different spatial regions through the context model P .</p><p>However, this arguably requires a quite sophisticated (and hence computationally expensive) context model, and we find it beneficial to follow Li et al. <ref type="bibr" target="#b8">[9]</ref> instead by using an importance map to help the CNN attend to different regions of the image with different amounts of bits. While <ref type="bibr" target="#b8">[9]</ref> uses a separate network for this purpose, we consider a simplified setting. We take the last layer of the encoder E, and add a second single-channel output y 2 R </p><formula xml:id="formula_12">m i,j,k = 8 &gt; &lt; &gt; : 1 if k&lt;y i,j (y i,j − k) if k  y i,j  k +1 0 if k +1&gt;y i,j ,<label>(14)</label></formula><p>where y i,j denotes the value of y at spatial location (i, j). The transition value for k  y i,j  k +1is such that the mask smoothly transitions from 0 to 1 for non-integer values of y. We then mask z by pointwise multiplication with the binarization of m, i.e., z z dme. Since the ceiling operator d·e is not differentiable, as done by <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b8">9]</ref>, we use identity for the backward pass.</p><p>With this modification, we have simply changed the architecture of E slightly such that it can easily "zero out" portions of columns z i,j,: of z (the rest of the network stays the same, so that (2) still holds for example). As suggested by <ref type="bibr" target="#b8">[9]</ref>, the so-obtained structure in z presents an alternative coding strategy: Instead of losslessly encoding the entire symbol volumeẑ, we could first (separately) encode the mask dme, and then for each columnẑ i,j,: only encode the first dm i,j e +1symbols, since the remaining ones are the constant Q(0), which we refer to as the zero symbol.</p><p>Work <ref type="bibr" target="#b8">[9]</ref> uses binary symbols (i.e., C = {0, 1}) and assumes independence between the symbols and a uniform prior during training, i.e., costing each 1 bit to encode. The importance map is thus their principal tool for controlling the bitrate, since they thereby avoid encoding all the bits in the representation. In contrast, we stick to the formulation in <ref type="bibr" target="#b4">(5)</ref> where the dependencies between the symbols are modeled during training. We then use the importance map as an architectural constraint and use their suggested coding strategy to obtain an alternative estimate for the entropy H(ẑ), as follows.</p><p>We observe that we can recover dme fromẑ by counting the number of consecutive zero symbols at the end of each columnẑ i,j,: . <ref type="bibr" target="#b3">4</ref> dme is therefore a function of the maskedẑ, i.e., dme = g(ẑ) for g recovering dme as described, which means that we have for the conditional entropy H(dme|ẑ)=0. Now, we have</p><formula xml:id="formula_13">H(ẑ)=H(dme|ẑ)+H(ẑ) (15) = H(ẑ, dme) (16) = H(ẑ|dme)+H(dme).<label>(17)</label></formula><p>If we treat the entropy of the mask, H(dme), as constant during optimization of the auto-encoder, we can then indirectly minimize H(ẑ) through H(ẑ|m).</p><p>To estimate H(ẑ|m), we use the same factorization of p as in <ref type="formula" target="#formula_4">(5)</ref>, but since the mask dme is known we have p(ẑ i = c 0 )=1deterministic for the 3D locations i inẑ where the mask is zero. The logs of the corresponding terms in (9) then evaluate to 0. The remaining terms, we can model with the same context model P i,l (ẑ), which results in</p><formula xml:id="formula_14">H(ẑ|dme) ⇡ Eẑ ⇠p(ẑ) [ m X i=1 −dm i e log P i,I(ẑi) ],<label>(18)</label></formula><p>where m i denotes the i-th element of m (in the same raster scan order asẑ). Similar to the coding cost (13), we refer to the argument in the expectation in <ref type="bibr" target="#b17">(18)</ref>,</p><formula xml:id="formula_15">MC(ẑ):= m X i=1 −dm i e log P i,I(ẑi)<label>(19)</label></formula><p>as the masked coding cost ofẑ. While the entropy estimate <ref type="formula" target="#formula_0">(18)</ref> is almost estimating the same quantity as (7) (only differing by H(dme)), it has the benefit of being weighted by m i . Therefore, the encoder E has an obvious path to control the entropy ofẑ, by simply increasing/decreasing the value of y for some spatial location of x and thus obtaining fewer/more zero entries in m.</p><p>When the context model P (ẑ) is trained, however, we still train it with respect to the formulation in <ref type="bibr" target="#b7">(8)</ref>, so it does not have direct access to the mask m and needs to learn the dependencies on the entire masked symbol volumeẑ. This means that when encoding an image, we can stick to standard adaptive arithmetic coding over the entire bottleneck, without needing to resort to a two-step coding process as in <ref type="bibr" target="#b8">[9]</ref>, where the mask is first encoded and then the remaining symbols. We emphasize that this approach hinges critically on the context model P and the encoder E being trained concurrently as this allows the encoder to learn a meaningful (in terms of coding cost) mask with respect to P (see the next section).</p><p>In our experiments we observe that during training, the two entropy losses <ref type="formula" target="#formula_8">(7)</ref> and <ref type="formula" target="#formula_0">(18)</ref> converge to almost the same value, with the latter being around ⇡ 3.5% smaller due to H(dme) being ignored.</p><p>While the importance map is not crucial for optimal ratedistortion performance, if the channel depth K is adjusted carefully, we found that we could more easily control the entropy ofẑ through β when using a fixed K, since the network can easily learn to ignore some of the channels via the importance map. Furthermore, in the supplementary material we show that by using multiple importance maps for a single network, one can obtain a single model that supports multiple compression rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Putting the pieces together</head><p>We made an effort to carefully describe our formulation and its motivation in detail. While the description is lengthy, when putting the resulting pieces together we get a quite straightforward pipeline for learned image compression, as follows.</p><p>Given the set of training images X , we initialize (fully convolutional) CNNs E, D, and P , as well as the centers C of the quantizer Q. Then, we train over minibatches X B = {x <ref type="bibr" target="#b0">(1)</ref> , ··· , x (B) } of crops from X . At each iteration, we take one gradient step for the auto-encoder (E, D) and the quantizer Q, with respect to the rate-distortion trade-off</p><formula xml:id="formula_16">L E,D,Q = 1 B B X j=1 d(x (j) ,x (j) )+βMC(ẑ (j) ),<label>(20)</label></formula><p>which is obtained by combining (1) with the estimate <ref type="formula" target="#formula_0">(18)</ref> &amp; <ref type="bibr" target="#b18">(19)</ref> and taking the batch sample average. Furthermore, we take a gradient step for the context model P with respect to its objective (see <ref type="formula" target="#formula_8">(7)</ref>&amp; <ref type="formula" target="#formula_0">(13)</ref>)</p><formula xml:id="formula_17">L P := 1 B B X j=1 d(x (j) ,x (j) )+βC(ẑ (j) ).<label>(21)</label></formula><p>To compute these two batch losses, we need to perform the following computation for each x 2 X B :</p><p>1. Obtain compressed (latent) representation z and importance map y from the encoder: (z, y)=E(x) 2. Expand importance map y to mask m via <ref type="formula" target="#formula_0">(14)</ref> 3. Mask z, i.e., z z dme</p><formula xml:id="formula_18">4. Quantizeẑ = Q(z)</formula><p>5. Compute the context P (ẑ)</p><formula xml:id="formula_19">6. Decodex = D(ẑ),</formula><p>which can be computed in parallel over the minibatch on a GPU since all the models are fully convolutional.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Relationship to previous methods</head><p>We are not the first to use context models for adaptive arithmetic coding to improve the performance in learned deep image compression. Work <ref type="bibr" target="#b19">[20]</ref> uses a PixelRNN-like architecture <ref type="bibr" target="#b21">[22]</ref> to train a recurrent network as a context model for an RNN-based compression auto-encoder. Li et al. <ref type="bibr" target="#b8">[ 9]</ref> extract cuboid patches around each symbol in a binary feature map, and feed them to a convolutional context model. Both these methods, however, only learn the context model after training their system, as a post-processing step to boost coding performance.</p><p>In contrast, our method directly incorporates the context model as the entropy term for the rate-distortion term (1) of the auto-encoder, and trains the two concurrently. This is done at little overhead during training, since we adopt a 3D-CNN for the context model, using PixelCNN-inspired <ref type="bibr" target="#b20">[21]</ref> masking of the weights of each layer to ensure causality in the context model. Adopting the same approach to the context models deployed by <ref type="bibr" target="#b19">[20]</ref> or <ref type="bibr" target="#b8">[9]</ref> would be non-trivial since they are not designed for fast feed-forward computation. In particular, while the context model of <ref type="bibr" target="#b8">[9]</ref> is also convolutional, its causality is enforced through masking the inputs to the network, as opposed to our masking of the weights of the networks. This means their context model needs to be run separately with a proper input cuboid for each symbol in the volume (i.e., not fully convolutionally).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Architecture Our auto-encoder has a similar architecture as <ref type="bibr" target="#b16">[17]</ref> but with more layers, and is described in <ref type="figure" target="#fig_2">Fig. 2</ref>.W e adapt the number of channels K in the latent representation for different models. For the context model P , we use a simple 4-layer 3D-CNN as described in <ref type="figure" target="#fig_3">Fig. 3</ref>.</p><p>Distortion measure Following <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref>, we use the multiscale structural similarity index (MS-SSIM) <ref type="bibr" target="#b22">[23]</ref> as measure of distortion d(x,x) = 100 · (1 − MS-SSIM(x,x)) for our models. MS-SSIM reportedly correlates better with human perception of distortion than mean squared error (MSE). We train and test all our models using MS-SSIM. For the encoder, "k5 n64-2" represents a convolution layer with kernel size 5, 64 output channels and a stride of 2. For the decoder it represents the equivalent deconvolution layer. All convolution layers are normalized using batch norm <ref type="bibr" target="#b5">[6]</ref>, and use SAME padding. Masked quantization is the quantization described in Section 3.4. Normalize normalizes the input to [0, 1] using a mean and variance obtained from a subset of the training set. Denormalize is the inverse operation. Training We use the Adam optimizer <ref type="bibr" target="#b7">[8]</ref> with a minibatch size of 30 to train seven models. Each model is trained to maximize MS-SSIM directly. As a baseline, we used a learning rate (LR) of 4 · 10 −3 for each model, but found it beneficial to vary it slightly for different models. We set σ =1in the smooth approximation (3) used for gradient backpropagation through Q. To make the model more predictably land at a certain bitrate t when optimizing (1), we found it helpful to clip the rate term (i.e., replace the entropy term βH with max(t, βH)), such that the entropy term is "switched off" when it is below t. We found this did not hurt performance. We decay the learning rate by a factor 10 every two epochs. To obtain models for different bitrates, we adapt the target bitrate t and the number of channels K, while using a moderately large β = 10. We use a small regularization on the weights and note that we achieve very stable training. We trained our models for 6 epochs, which took around 24h per model on a single GPU. For P , we use a LR of 10 −4 and the same decay schedule.</p><p>Datasets We train on the the ImageNet dataset from the Large Scale Visual Recognition Challenge 2012 (ILSVRC2012) <ref type="bibr" target="#b14">[15]</ref>. As a preprocessing step, we take random 160 ⇥ 160 crops, and randomly flip them. We set aside 100 images from ImageNet as a testing set, ImageNetTest. Furthermore, we test our method on the widely used Kodak <ref type="bibr" target="#b0">[1]</ref> dataset. To asses performance on high-quality fullresolution images, we also test on the datasets B100 <ref type="bibr" target="#b17">[18]</ref> and Urban100 <ref type="bibr" target="#b4">[5]</ref>, commonly used in super-resolution.</p><p>Other codecs We compare to JPEG, using libjpeg <ref type="bibr" target="#b4">5</ref> , and JPEG2000, using the Kakadu implementation <ref type="bibr" target="#b5">6</ref> . We also compare to the lesser known BPG <ref type="bibr" target="#b6">7</ref> , which is based on HEVC, the state-of-the-art in video compression, and which outperforms JPEG and JPEG2000. We use BPG in the nondefault 4:4:4 chroma format, following <ref type="bibr" target="#b13">[14]</ref>.</p><p>Comparison Like <ref type="bibr" target="#b13">[14]</ref>, we proceed as follows to compare to other methods. For each dataset, we compress each image using all our models. This yields a set of (bpp, MS-SSIM) points for each image, which we interpolate to get a curve for each image. We fix a grid of bpp values, and average the curves for each image at each bpp grid value (ignoring those images whose bpp range does not include the grid value, i.e., we do not extrapolate). We do this for our method, BPG, JPEG, and JPEG2000. Due to code being unavailable for the related works in general, we digitize the Kodak curve from Rippel &amp; Bourdev <ref type="bibr" target="#b13">[14]</ref>, who have carefully collected the curves from the respective works. With this, we also show the results of Rippel &amp; Bourdev <ref type="bibr" target="#b13">[14]</ref>, Johnston et al. <ref type="bibr" target="#b6">[ 7]</ref>, Ballé et al. <ref type="bibr" target="#b3">[ 4]</ref>, and Theis et al. <ref type="bibr" target="#b16">[ 17]</ref>. To validate that our estimated MS-SSIM is correctly implemented, we independently generated the BPG curves for Kodak and verified that they matched the one from <ref type="bibr" target="#b13">[14]</ref>.</p><p>Results <ref type="figure">Fig. 1</ref> shows a comparison of the aforementioned methods for Kodak. Our method outperforms BPG, JPEG, and JPEG2000, as well as the neural network based approaches of Johnston et al. <ref type="bibr" target="#b6">[ 7]</ref>, Ballé et al. <ref type="bibr" target="#b3">[ 4]</ref>, and Theis et al. <ref type="bibr" target="#b16">[17]</ref>. Furthermore, we achieve performance comparable to that of <ref type="bibr">Rippel &amp; Bourdev [14]</ref>. This holds for all bpps we tested, from 0.3 bpp to 0.9 bpp. We note that while Rippel &amp; Bourdev and Johnston et al. also train to maximize (MS-)SSIM, the other methods minimize MSE. In each of the other testing sets, we also outperform BPG, JPEG, and JPEG2000 over the reported bitrates, as shown in <ref type="figure">Fig. 4</ref>.</p><p>In <ref type="figure">Fig. 5</ref>, we compare our approach to BPG, JPEG, and JPEG2000 visually, using very strong compression on kodim21 from Kodak. It can be seen that the output of our network is pleasant to look at. Soft structures like the clouds are very well preserved. BPG appears to handle high frequencies better (see, e.g., the fence) but loses structure in the clouds and in the sea. Like JPEG2000, it produces block artifacts. JPEG breaks down at this rate. We refer to the supplementary material for further visual examples.</p><p>Ablation study: Context model In order to show the effectiveness of the context model, we performed the following ablation study. We trained the auto-encoder without entropy loss, i.e., β =0in (20), using L =6centers and K = 16 channels. On Kodak, this model yields an average MS-SSIM of 0.982, at an average rate of 0.646 bpp (calculated assuming that we need log 2 (L)=2 .59 bits per symbol). We then trained three different context models for this auto-encoder, while keeping the auto-encoder fixed: A zeroth order context model which uses a histogram to estimate the probability of each of the L symbols; a first order (one-step prediction) context model, which uses a conditional histogram to estimate the probability of each of the L symbols given the previous symbol (scanningẑ in raster order); and P , i.e., our proposed context model. The resulting average rates are shown in <ref type="table" target="#tab_0">Table 1</ref>. Our context model reduces the rate by 10 %, even though the auto-encoder was optimized using a uniform prior (see supplementary material for a detailed comparison of <ref type="table" target="#tab_0">Table 1</ref> and <ref type="figure">Fig. 1</ref> Importance map As described in detail in Section 3.4, we use an importance map to dynamically alter the number of channels used at different spatial locations to encode an image. To visualize how this helps, we trained two autoencoders M and M 0 , where M uses an importance map and at most K = 32 channels to compress an image, and M 0 compresses without importance map and with K = 16 channels (this yields a rate for M 0 similar to that of M ). In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Our experiments showed that combining a convolutional auto-encoder with a lightweight 3D-CNN as context model and training the two networks concurrently leads to a highly effective image compression system. Not only were we able to clearly outperform state-of-the-art engineered compression methods including BPG and JPEG2000 in terms of MS-SSIM, but we also obtained performance competitive with the current state-of-the-art learned compression method from <ref type="bibr" target="#b13">[14]</ref>. In particular, our method outperforms BPG and JPEG2000 in MS-SSIM across four different testing sets (ImageNetTest, Kodak, B100, Urban100), and does so significantly, i.e., the proposed method generalizes well. We emphasize that our method relies on elementary techniques both in terms of the architecture (standard convolutional auto-encoder with importance map, convolutional context model) and training procedure (minimize the rate-distortion trade-off and the negative log-likelihood for the context model), while <ref type="bibr" target="#b13">[14]</ref> uses highly specialized techniques such as a pyramidal decomposition architecture, adaptive codelength regularization, and multiscale adversarial training.</p><p>The ablation study for the context model showed that our 3D-CNN-based context model is significantly more powerful than the first order (histogram) and second order (onestep prediction) baseline context models. Further, our experiments suggest that the importance map learns to condensate the image information in a reduced number of channels of the latent representation without relying on explicit supervision. Notably, the importance map is learned as a part of the image compression auto-encoder concurrently with the auto-encoder and the context model, without introducing any optimization difficulties. In contrast, in <ref type="bibr" target="#b8">[9]</ref> the importance map is computed using a separate network, learned together with the auto-encoder, while the context model is learned separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we proposed the first method for learning a lossy image compression auto-encoder concurrently with a lightweight context model by incorporating it into an entropy loss for the optimization of the auto-encoder, leading to performance competitive with the current state-of-the-art in deep image compression <ref type="bibr" target="#b13">[14]</ref>.</p><p>Future works could explore heavier and more powerful context models, as those employed in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref>. This could further improve compression performance and allow for sampling of natural images in a "lossy" manner, by samplingẑ according to the context model and then decoding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>of the same dimensionality as z as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of our auto-encoder. Dark gray blocks represent residual units. The upper part represents the encoder E, the lower part the decoder D. For the encoder, "k5 n64-2" represents a convolution layer with kernel size 5, 64 output channels and a stride of 2. For the decoder it represents the equivalent deconvolution layer. All convolution layers are normalized using batch norm [6], and use SAME padding. Masked quantization is the quantization described in Section 3.4. Normalize normalizes the input to [0, 1] using a mean and variance obtained from a subset of the training set. Denormalize is the inverse operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The architecture of our context model. "3D k3 n24" refers to a 3D masked convolution with filter size 3 and 24 output channels. The last layer outputs L values for each voxel inẑ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Performance of our approach on ImageNetTest, B100, Urban100, where we outperform BPG, JPEG and JPEG2000 in MS-SSIM. Ours 0.124bpp 0.147 bpp BPG</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 ,Figure 6 :</head><label>66</label><figDesc>Figure 6: Visualization of the latent representation of the auto-encoder for a high-bpp operating point, with (M ) and without (M 0 ) incorporating an importance map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>).Rates for different context models, for the same architecture (E, D).</figDesc><table>Model 
rate 
Baseline (Uniform) 0.646 bpp 
Zeroth order 
0.642 bpp 
First order 
0.627 bpp 
Our context model P 0.579 bpp 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use a 4-layer network, compared to 15 layers in [22]. 3 Available at https://arxiv.org/abs/1801.04260</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">If z contained zeros before it was masked, we might overestimate the number of 0 entries in dme. However, we can redefine those entries of m as 0 and this will give the same result after masking.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://libjpeg.sourceforge.net/ 6 http://kakadusoftware.com/ 7 https://bellard.org/bpg/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by ETH Zürich and by NVIDIA through a GPU grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kodak</forename><surname>Photocd</surname></persName>
		</author>
		<ptr target="http://r0k.us/graphics/kodak/.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Soft-to-hard vector quantization for end-to-end learning compressible representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cavigelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00648</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">End-to-end optimization of nonlinear transform codes for perceptual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ballé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.05006</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Endto-end optimized image compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ballé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01704</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improved lossy image compression with priming and spatially adaptive bit rates for recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Covell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Jin</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10114</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning convolutional networks for content-weighted image compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10553</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Context-based adaptive binary arithmetic coding in the h. 264/avc video compression standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on circuits and systems for video technology</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="620" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Context-based adaptive binary arithmetic coding in the h. 264/avc video compression standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on circuits and systems for video technology</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tmw-a new method for lossless image compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ITG FACHBERICHT</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="533" to="540" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glicbawls-grey level image compression by adaptive weighted least squares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Tischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Compression Conference</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">503</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Real-time adaptive image compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>International Convention Centre. PMLR. 2, 5, 6</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. CoRR, abs/1409.0575</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shalizi</surname></persName>
		</author>
		<ptr target="http://www.stat.cmu.edu/˜cshalizi/754/2006/notes/lecture-28.pdf" />
		<title level="m">Lecture notes on stochastic processes</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Lossy image compression with compressive autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A+: Adjusted Anchored Neighborhood Regression for Fast Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="111" to="126" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>O&amp;apos;malley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Covell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06085</idno>
		<title level="m">Variable rate image compression with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Covell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05148</idno>
		<title level="m">Full resolution image compression with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asilomar Conference on Signals</title>
		<imprint>
			<date type="published" when="2003-11" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Applications of universal context modeling to lossless compression of gray-scale images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Rissanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Arps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="575" to="586" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Piecewise 2d autoregression for predictive image coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barthel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing, 1998. ICIP 98. Proceedings. 1998 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
