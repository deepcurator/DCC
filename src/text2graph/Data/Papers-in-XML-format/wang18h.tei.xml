<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader>
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisy</forename><surname>Stanton</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Skerry-Ryan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Battenberg</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Shor</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Xiao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Ren</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Jia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
						</author>
						<title level="a" type="main">Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract xml:lang="en">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In this work, we propose "global style tokens" (GSTs), a bank of embeddings that are jointly trained within Tacotron, a state-of-the-art end-toend speech synthesis system. The embeddings are trained with no explicit labels, yet learn to model a large range of acoustic expressiveness. GSTs lead to a rich set of significant results. The soft interpretable "labels" they generate can be used to control synthesis in novel ways, such as varying speed and speaking style -independently of the text content. They can also be used for style transfer, replicating the speaking style of a single audio clip across an entire long-form text corpus. When trained on noisy, unlabeled found data, GSTs learn to factorize noise and speaker identity, providing a path towards highly scalable but robust speech synthesis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text>
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The past few years have seen exciting developments in the use of deep neural networks to synthesize natural-sounding human speech <ref type="bibr" target="#b21">van den Oord et al., 2016;</ref><ref type="bibr" target="#b23">Wang et al., 2017a;</ref><ref type="bibr" target="#b0">Arik et al., 2017;</ref><ref type="bibr" target="#b18">Taigman et al., 2017;</ref><ref type="bibr" target="#b15">Shen et al., 2017)</ref>. As text-to-speech (TTS) models have rapidly improved, there is a growing opportunity for a number of applications, such as audiobook narration, news readers, and conversational assistants. Neural models show the potential to robustly synthesize expressive long-form speech, and yet research in this area is still in its infancy.</p><p>To deliver true human-like speech, a TTS system must learn to model prosody. Prosody is the confluence of a number of phenomena in speech, such as paralinguistic information, intonation, stress, and style. In this work we focus on style modeling, the goal of which is to provide models the capability to choose a speaking style appropriate for the given context. While difficult to define precisely, style contains rich information, such as intention and emotion, and influences the speaker's choice of intonation and flow. Proper stylistic rendering affects overall perception (see e.g. "affective prosody" in <ref type="bibr" target="#b19">(Taylor, 2009</ref>)), which is important for applications such as audiobooks and newsreaders.</p><p>Style modeling presents several challenges. First, there is no objective measure of "correct" prosodic style, making both modeling and evaluation difficult. Acquiring annotations for large datasets can be costly and similarly problematic, since human raters often disagree. Second, the high dynamic range in expressive voices is difficult to model. Many TTS models, including recent end-to-end systems, only learn an averaged prosodic distribution over their input data, generating less expressive speech -especially for long-form phrases. Furthermore, they often lack the ability to control the expression with which speech is synthesized.</p><p>This work 1 attempts to address the above issues by introducing "global style tokens" (GSTs) to Tacotron <ref type="bibr" target="#b23">(Wang et al., 2017a;</ref><ref type="bibr" target="#b15">Shen et al., 2017)</ref>, a state-of-the-art end-to-end TTS model. GSTs are trained without any prosodic labels, and yet uncover a large range of expressive styles. The internal architecture itself produces soft interpretable "labels" that can be used to perform various style control and transfer tasks, leading to significant improvements for expressive long-form synthesis. GSTs can be directly applied to noisy, unlabeled found data, providing a path towards highly scalable but robust speech synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model Architecture</head><p>Our model is based on Tacotron <ref type="bibr" target="#b23">(Wang et al., 2017a;</ref><ref type="bibr" target="#b15">Shen et al., 2017)</ref>, a sequence-to-sequence (seq2seq) model that predicts mel spectrograms directly from grapheme or phoneme inputs. These mel spectrograms are converted to waveforms either by a low-resource inversion algorithm During training, the log-mel spectrogram of the training target is fed to the reference encoder followed by a style token layer. The resulting style embedding is used to condition the Tacotron text encoder states. During inference, we can feed an arbitrary reference signal to synthesize text with its speaking style. Alternatively, we can remove the reference encoder and directly control synthesis using the learned interpretable tokens. <ref type="bibr" target="#b4">(Griffin &amp; Lim, 1984)</ref> or a neural vocoder such as WaveNet <ref type="bibr" target="#b21">(van den Oord et al., 2016)</ref>. We point out that, for Tacotron, the choice of vocoder does not affect prosody, which is modeled by the seq2seq model.</p><p>Our proposed GST model, illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>, consists of a reference encoder, style attention, style embedding, and sequence-to-sequence (Tacotron) model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Training</head><p>During training, information flows through the model as follows:</p><p>• The reference encoder, proposed in <ref type="bibr" target="#b17">(Skerry-Ryan et al., 2018)</ref>, compresses the prosody of a variablelength audio signal into a fixed-length vector, which we call the reference embedding. During training, the reference signal is the ground-truth audio.</p><p>• The reference embedding is passed to a style token layer, where it is used as the query vector to a contentbased attention module. Here, attention is not used to learn an alignment. Instead, it learns a similarity measure between the reference embedding and each token in a bank of trainable embeddings. This set of embeddings, which we alternately call global style tokens, GSTs, or token embeddings, are shared across all training examples.</p><p>• The attention module outputs a set of combination weights that represent the contribution of each style token to the encoded reference embedding. The weighted sum of the GSTs, which we call the style embedding, is passed to the text encoder for conditioning at every timestep.</p><p>• The style token layer weights (including token embeddings) are jointly trained with the rest of the model, driven only by the reconstruction loss from the Tacotron decoder. GSTs thus do not require any explicit style or prosody labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Inference</head><p>The GST architecture is designed for powerful and flexible control at inference time. In this mode, information can flow through the model in one of two ways:</p><p>1. We can directly condition the text encoder outputs on certain tokens. This allows for style control and manipulation without a reference signal, or, 2. We can feed a different audio signal (whose transcript does not need to match the text to be synthesized) to achieve style transfer.</p><p>These will be discussed in more detail in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Tacotron Architecture</head><p>For our baseline and GST-augmented Tacotron systems, we use the same architecture and hyperparameters as <ref type="bibr" target="#b23">(Wang et al., 2017a)</ref> except for a few details. We use phoneme inputs to speed up training, and slightly change the decoder, replacing GRU cells with two layers of 256-cell LSTMs; these are regularized using zoneout <ref type="bibr" target="#b9">(Krueger et al., 2017</ref>) with probability 0.1. The decoder outputs 80-channel logmel spectrogram energies, two frames at a time, which are run through a dilated convolution network that outputs linear spectrograms. We run these through Griffin-Lim for fast waveform reconstruction. It is straightforward to replace Griffin-Lim by a WaveNet vocoder to improve the audio fidelity <ref type="bibr" target="#b15">(Shen et al., 2017</ref>).</p><p>The baseline model achieves a 4.0 mean opinion score (MOS), outperforming the 3.82 MOS reported in <ref type="bibr" target="#b23">(Wang et al., 2017a)</ref> on the same evaluation set. It is thus a very strong baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Style Token Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">REFERENCE ENCODER</head><p>The reference encoder is made up of a convolutional stack, followed by an RNN. It takes as input a log-mel spectrogram, which is first passed to a stack of six 2-D convolutional layers with 3×3 kernel, 2×2 stride, batch normalization and ReLU activation function. We use 32, 32, 64, 64, 128 and 128 output channels for the 6 convolutional layers, respectively. The resulting output tensor is then shaped back to 3 dimensions (preserving the output time resolution) and fed to a single-layer 128-unit unidirectional GRU. The last GRU state serves as the reference embedding, which is then fed as input to the style token layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">STYLE TOKEN LAYER</head><p>The style token layer is made up of a bank of trainable embeddings and an attention module. Unless stated otherwise, our experiments use 10 tokens with dimension 256 (to match that of each text encoder state); we found this sufficient to represent a small but rich variety of prosodic dimensions in the training data. Since each Tacotron text encoder state is the output of a (tanh activation) GRU, we also pass raw GST values through a tanh before computing attention scores. We found this led to greater token diversity.</p><p>While we use content-based additive attention as a similarity measure, it is trivial to substitute alternatives. Dot-product attention, location-based attention, or even combinations of attention mechanisms may learn different types of style tokens. In our experiments, we found that using multi-head attention <ref type="bibr" target="#b22">(Vaswani et al., 2017)</ref> significantly improves style transfer performance, and is more effective than simply increasing the number of tokens. When using h attention heads, we set the token embedding size to be 256/h and concatenate the attention outputs, such that the final style embedding size remains the same. Unlike in <ref type="bibr" target="#b22">(Vaswani et al., 2017)</ref>, we use MLP-based attention instead of dot-product attention for each attention head in our experiments.</p><p>We experimented with different combinations of conditioning sites; the best-performing configuration simply adds the replicated style embedding to every text encoder state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Model Interpretation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">End-to-End Clustering/Quantization</head><p>Intuitively, the GST model can be thought of as an end-toend method for decomposing the reference embedding into a set of basis vectors or soft clusters -i.e. the style tokens. As mentioned above, the contribution of each style token is represented by an attention score, but can be replaced with any desired similarity measure. The GST layer is conceptually somewhat similar to the VQ-VAE encoder <ref type="bibr" target="#b21">(van den Oord et al., 2017)</ref>, in that it learns a quantized representation of its input. We also experimented with replacing the GST layer with a discrete, VQ-like lookup table layer, but have not seen comparable results yet.</p><p>This decomposition concept can also be generalized to other models, e.g. the factorized variational latent model in <ref type="bibr" target="#b5">(Hsu et al., 2017)</ref>, which exploits the multi-scale nature of a speech signal by explicitly formulating it within a factorized hierarchical graphical model. GSTs could potentially reduce the dimension of the sequence-dependent prior embeddings; rather than storing one embedding per sequence, the model would simply represent each prior embedding as a weighted combination of GSTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Memory-Augmented Neural Network</head><p>GST embeddings can also be viewed as an external memory that stores style information extracted from training data. The reference signal guides memory writes at training time, and memory reads at inference time. We may leverage recent advances from memory-augmented networks <ref type="bibr" target="#b3">(Graves et al., 2014)</ref> to further improve GST learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Prosody and speaking style models have been studied for decades in the TTS community. However, most existing models require explicit labels, such as emotion or speaker codes <ref type="bibr" target="#b10">(Luong et al., 2017)</ref>. While a small amount of research has explored automatic labeling, learning is still supervised, requiring expensive annotations for model training. AuToBI, for example, <ref type="bibr" target="#b14">(Rosenberg, 2010)</ref> aims to produce ToBI <ref type="bibr" target="#b16">(Silverman et al., 1992)</ref> labels that can be used by other TTS models. However, AuToBI still needs annotations for training, and ToBI, as a hand-designed label system, is known to have limited performance <ref type="bibr" target="#b25">(Wightman, 2002)</ref>.</p><p>Cluster-based modeling <ref type="bibr" target="#b2">(Eyben et al., 2012;</ref><ref type="bibr" target="#b6">Jauk, 2017</ref>) is related to our work. Jauk (2017), for example, uses i-vectors <ref type="bibr" target="#b1">(Dehak et al., 2011)</ref> and other acoustic features to cluster the training set and train models in different partitions. These methods rely on a complex set of hand-designed features, however, and require training a neutral voice model in a separate step.</p><p>As mentioned previously, <ref type="bibr" target="#b17">(Skerry-Ryan et al., 2018)</ref> introduces the reference embedding used in this work, and shows that it can be used to transfer prosody from a reference signal. This embedding does not enable interpretable style control, however, and we show in Section 6 that it generalizes poorly on some style transfer tasks.</p><p>Our work substantially extends the research in <ref type="bibr" target="#b24">(Wang et al., 2017b)</ref>, but there are several fundamental differences. First, <ref type="bibr" target="#b24">(Wang et al., 2017b</ref>) uses a single frame from the Tacotron decoder as the query to learn tokens. It thus only models "local" variations that primarily correspond to F0. GSTs instead use a summary of the entire reference signal as input, and are thus able to uncover both local and global attributes that are essential for expressive synthesis. Second, in contrast to the decoder-side conditioning in <ref type="bibr" target="#b24">(Wang et al., 2017b)</ref>, the design of GSTs allows textual input to be conditioned on a disentangled style embedding. We show crucial implications of this for style control and transfer in Section 6.2. Finally, GSTs can be applied to both clean recordings and noisy found data. We discuss this and its significance in detail in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments: Style Control and Transfer</head><p>In this section, we measure the ability of GSTs to control and transfer speaking style, using the inference methods from Section 2.2.</p><p>We train models using 147 hours of American English audiobook data. These are read by the 2013 Blizzard Challenge speaker, Catherine Byers, in an animated and emotive storytelling style. Some books contain very expressive character voices with high dynamic range, which are challenging to model.</p><p>As is common for generative models, objective metrics often do not correlate well with perception <ref type="bibr" target="#b20">(Theis et al., 2015)</ref>. While we use visualizations for some experiments below, we strongly encourage readers to listen to the samples provided on our demo page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Style Control</head><p>The experiments in this section use single-head GST attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1.">STYLE SELECTION</head><p>The simplest method of control is conditioning the model on an individual token. At inference time, we simply replace the style embedding with a specific, optionally scaled token.</p><p>Conditioning in this manner has several benefits. First, it allows us to examine which style attributes each token encodes. Empirically, we find that each token can represent not just pitch and intensity, but also a variety of other attributes, such as speaking rate and emotion. This can be seen in <ref type="figure" target="#fig_2">Figure 2</ref>, which shows two sentences synthesized with three different style tokens (scale=0.3) from a 10-token GST model. The plots show that F0 and C0 (energy) curves are quite different across style tokens. However, the F0 and C0 contours generated by each token follow a clear relative trend, despite the fact that input sentences A and B are completely different. Indeed, perceptually, the red token corresponds to a lower-pitch voice, the green token to a decreasing pitch, and the blue token to a faster speaking rate (note the total audio duration in both plots).</p><p>Single-token conditioning also reveals that not all tokens capture single attributes: while one token may learn to represent speaking rate, others may learn a mixture of attributes that reflect stylistic co-occurrence in the training data (a low-pitched token, for example, can also encode a slower speaking rate). Encouraging more independent style attribute learning is an important focus of ongoing work.</p><p>In addition to providing interpretability, style token conditioning can also improve synthesis quality. Consider the problem of long-form synthesis on training data with lots of prosodic variation. Many TTS models learn to generate the "average" prosodic style, which can be problematic for expressive datasets, since the very variation that characterizes them is collapsed. This can also lead to undesirable side effects, such as pitch continuously declining towards the end of each sentence. We find that conditioning on "lively"-sounding tokens can address both of these problems, significantly improving the prosodic variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2.">STYLE SCALING</head><p>Another method for controlling style token output is via scaling. We find that multiplying a token embedding by a scalar value intensifies its style effect. (Note that large scaling values may lead to unintelligible speech, which suggests future work on improving stability.) This is illustrated in <ref type="figure">Figure 3</ref>, which shows spectrograms of utterances synthesized by two different tokens. Perceptually, these tokens encode  <ref type="figure">Figure 3</ref>. Effect of token scaling. From left to right, we scale the two tokens by -0.3, 0.1, 0.3, 0.5, respectively. Note that the model seems to exhibit the reverse effect (e.g. fast to slow or animated to calm) with a negative scale, which is never seen during training.  two different speaking styles: a faster speaking rate (3(a)), and more animated speech (3(b)). <ref type="figure">Figure 3(a)</ref> shows that increasing the scaling factor of the faster speaking rate token causes a gradual compression of the spectrogram in the time domain. Similarly, <ref type="figure">Figure 3</ref>(b) shows that increasing the scaling factor of the animated speech token yields commensurate increases in pitch variation. These style scaling effects hold even for negative values (speaking rate becomes slower, and speech becomes calmer), despite the fact that the model only sees positive (softmax) values during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3.">STYLE SAMPLING</head><p>We can also control synthesis during inference by modifying the attention module weights inside the style token layer. Since the GST attention produces a set of combination weights, these may be refined manually to yield a desired interpolation. We can also use randomly generated softmax weights to sample the style space. The sampling diversity can be controlled by tuning the softmax temperature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.4.">TEXT-SIDE STYLE CONTROL/MORPHING</head><p>While the same style embedding is added to all text encoder states during training, this doesn't need to be the case in inference mode. As our audio samples demonstrate, this allows us to do piecewise style control or morphing by conditioning on one or more tokens for different segments of input text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Style Transfer</head><p>Style transfer is an active area of research that aims to synthesize a phrase in the prosodic style of a reference signal <ref type="bibr" target="#b26">(Wu et al., 2013;</ref><ref type="bibr" target="#b12">Nakashika et al., 2016;</ref><ref type="bibr" target="#b8">Kinnunen et al., 2017)</ref>. The property that a GST model can be conditioned on any convex combination of style tokens lends itself well to this task; at inference time (method 2 of Section 2.2), we can simply feed a reference signal to guide the choice of token combination weights. The experiments below use 4-head GST attention. fine prosody transfer. The GST model is somewhere in between: while its output duration and formant transitions don't precisely match those of the reference, the overall spectrotemporal envelopes do. Perceptually, the GST model resembles the prosodic style of the reference signal, regardless of its style, and even when it comes from an unseen speaker. This also indicates that the model isn't merely learning to copy the reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1.">PARALLEL STYLE TRANSFER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2.">NON-PARALLEL STYLE TRANSFER</head><p>We next show results for a non-parallel transfer task, in which a TTS system must synthesize arbitrary text in the prosodic style of a reference signal. We chose three different reference signals for this task, and tested how well a GST model replicated each style when synthesizing the same target phrase. Since long-form synthesis can benefit significantly from proper stylistic rendering, we used a long (258-character) target phrase. We chose source phrases of varying lengths (10, 96, and 321 characters, respectively). <ref type="figure">Figure 5</ref> shows alignment matrices for synthesis conditioned on each source signal.</p><p>The top row shows a 10-token GST model. This model robustly generalizes to all three conditioning inputs, as evidenced by the good alignment plots. The bottom row shows a 256-token GST model exhibiting the same behavior; we include this model to show that GSTs remain robust even when the number of tokens <ref type="formula">(256)</ref> is larger than the reference embedding dimensionality (128).</p><p>The middle row shows a model with direct reference embedding conditioning. The attention matrices show that this model fails when conditioned on the shorter source phrases, since it tries to squeeze its synthesis into the same time interval as that of the reference. While the model successfully aligns when conditioned on the longest input, intelligibility is poor for some words: the per-utterance embedding captures too much information (such as timing and phonetics) from the source, hurting generalization.</p><p>To evaluate the quality of this method at scale, we ran sideby-side subjective tests of non-parallel GST style transfer against a Tacotron baseline. We used an evaluation set of 60 audiobook sentences, including many long phrases. We generated two sets of GST output by conditioning the model on two different narrative-style reference signals, unseen during training. A side-by-side subjective test indicated that raters preferred both sets of GST synthesis against a Tacotron baseline, as shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>The performance of GSTs on non-parallel style transfer is significant, since it allows using a source signal to guide robust stylistic synthesis of arbitrary text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments: Unlabeled Noisy Found Data</head><p>Studio-quality data can be both expensive and time consuming to record. While the internet holds vast amounts of rich real-life expressive speech, it is often noisy and difficult to label.</p><p>In this section, we demonstrate how GSTs can be used to train robust models directly from noisy found data, without modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Artificial Noisy Data</head><p>As a first experiment, we artificially generate training sets by adding noise to clean speech. The motivation here is to simulate real noisy data while performing controlled experiments. To achieve this, we pass a single-speaker US English dataset into a room simulator <ref type="bibr" target="#b7">(Kim et al., 2017)</ref>, which adds varying types of background noise and room reverberations.</p><p>The signal-to-noise ratio (SNR) ranges from 5-25 dB, and the T60s of room reverberation range from 100-900 ms. We create four different training sets where 50%, 75%, 90% and 95% of the input is noisified, respectively.</p><p>After training a GST-augmented Tacotron on these datasets, we run inference in the first mode described in Section 2.2. Instead of providing a reference signal, we condition the model on each individual style token, which gives us an interpretable, audible sense of what each token has learned. Interestingly, we find that different noises are treated as styles and "absorbed" into different tokens. We illustrate the spectrograms from a few tokens in <ref type="figure" target="#fig_7">Figure 6</ref>. We can see (and hear) that these tokens clearly correspond to different interference types, such as music, reverberation and general background noise. Importantly, this method reveals that a subset of the learned tokens also correspond to completely clean speech. This means that we can synthesize clean speech for arbitrary text input by conditioning the model on a single, clean style token.</p><p>To demonstrate this, we run inference using a manuallyidentified clean style token (scaled to 0.3), and then evaluate the output using MOS naturalness tests. We use the same 100-phrase evaluation set as <ref type="bibr" target="#b23">(Wang et al., 2017a)</ref>, collecting 8 ratings each from crowdsourced native speakers. <ref type="table">Table  2</ref> shows MOS results for both a baseline Tacotron and a "clean-token" GST model. While the baseline Tacotron achieves a 4.0 MOS when the dataset is 100% clean, MOS decreases as interference increases, dropping to a low score of 1.353. Because the model has no prior knowledge of speech or noise, it blindly models all statistics in the training set, resulting in substantial amounts of noise during synthesis.    By contrast, the GST model achieves about 4.0 MOS in all noise conditions. Note that the number of tokens needs to increase along with the percentage of noise to achieve this result. For example, a 10-token GST model yields clean tokens when trained on a 50% noise dataset, but the noisier datasets required a 20-token model. Future work may explore how to adapt the number of tokens automatically to a given data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Real Multi-Speaker Found Data</head><p>Our second experiment uses real data. This dataset is made up of audio tracks mined from 439 official TED YouTube channel videos. The tracks contain significant acoustic variations, including channel variation (near-and far-field speech), noise (e.g. laughs), and reverberation. We use an endpointer to segment the audio tracks into short clips, followed by an ASR model to create &lt;text, audio&gt; training pairs. Despite the fact that the ASR model generates a significant number of transcription and misalignment errors, we perform no other preprocessing. The final training set is about 68 hours long and contains about 439 speakers.</p><p>Without using any metadata as labels, we train a baseline Tacotron and a 1024-token GST model for comparison. As expected, the baseline fails to learn, since the multi-speaker data is too varied. The GST model results are presented in <ref type="figure" target="#fig_8">Figure 7</ref>. This shows spectrograms for the same phrase overlaid with F0 tracks, generated by conditioning the model on two randomly chosen tokens. Examining the trained GSTs, we find that different tokens correspond to different speakers. This means that, to synthesize with a specific speaker's voice, we can simply feed audio from that speaker as a reference signal. See Section 7.3 for more quantitative evaluations. Finally, we exploit the fact that most of the talks are in English, but a small fraction are in Spanish. For this experiment, we compare baseline and GST-enabled noisy data models on a cross-lingual style transfer task. For a baseline, we train a multi-speaker Tacotron similar to <ref type="bibr" target="#b13">(Ping et al., 2017)</ref>, using video IDs as a proxy for speaker labels. Conditioned on a Spanish speaker label, we then synthesize 100 English phrases for which we have ground-truth transcriptions. For the GST system, we feed a reference signal from the same Spanish speaker and synthesize the same 100 English phrases. While the Spanish accent from the speaker is not preserved, we find that the GST model produces completely intelligible English speech with a similar pitch range as the speaker. By contrast, the multi-speaker Tacotron output is much less intelligible.</p><p>To evaluate this result objectively, we compute word error rates (WER) of an English ASR model on the synthesized speech. As shown in <ref type="table" target="#tab_1">Table 3</ref>, the WER of the GST utterances is much lower than that of the multi-speaker model.</p><p>The results strongly corroborate that GSTs learn embeddings disentangled from text content. Though this is an exciting early result, an in-depth study of using GST for prosody-preserving language transfer is in order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Quantitative Evaluations</head><p>We use t-SNE <ref type="bibr" target="#b11">(Maaten &amp; Hinton, 2008)</ref> to visualize the style embeddings learned from both the artificial noise and TED datasets. <ref type="figure" target="#fig_9">Figure 8(a)</ref> shows that the embeddings learned from the artificial noisy dataset (50% clean) are clearly separated into two classes. <ref type="figure" target="#fig_9">Figure 8</ref>(b) shows style embeddings for 2,000 randomly drawn samples containing 14 TED talk data speakers. We see that samples are well separated into 14 clusters, each corresponding to an individual speaker. Female and male speakers are linearly separable.</p><p>We also use style embeddings as features to perform noise and speaker classification with Linear Discriminative Analysis. Results are shown in <ref type="table" target="#tab_2">Table 4</ref>. For noise classification, GSTs uncover the true label with 99.2% accuracy. For speaker classification, we use TED video IDs as true labels and compare with the i-vector method <ref type="bibr" target="#b1">(Dehak et al., 2011)</ref>, a standard representation used in modern speaker verification systems. For this task, the test set contains 431 speakers. While both trained and tested on short utterances (mean duration 3.75 secs), we can see that GSTs are comparable with i-vectors. This is an encouraging result, given that i-vectors were specifically designed for speaker classification. We speculate that GST has the potential to be applied to speaker diarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Implications</head><p>The results above have important implications for future TTS research on found data. First, due to the robustness of GSTs to both acoustic and textual noise, the design of automated data mining pipelines may be greatly simplified.</p><p>The robustness as a function of the accuracy of individual pipeline component is worth a systematic study. Second, style attributes, such as emotion, are often very difficult to label for large-scale noisy data. Using GSTs or weights to automatically generate style annotations may substantially reduce human-in-the-loop effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions and Discussions</head><p>This work has introduced Global Style Tokens, a powerful method for modeling style in end-to-end TTS systems. GSTs are intuitive, easy to implement, and learn without explicit labels. We have shown that, in addition to learning interpretable embeddings that can be used to control and transfer style, GSTs are a general technique for uncovering latent variations in data.</p><p>There is still much to be investigated, including improving GST learning, and using GST weights as targets to predict from text. Finally, while this work adds GSTs only to Tacotron, we believe the method can be readily used by other types of end-to-end TTS models. More generally, we envision that GSTs can be applied to models in other domains -such as text-to-image and neural machine translation systems -that would benefit from interpretability, controllability and robustness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Model diagram. During training, the log-mel spectrogram of the training target is fed to the reference encoder followed by a style token layer. The resulting style embedding is used to condition the Tacotron text encoder states. During inference, we can feed an arbitrary reference signal to synthesize text with its speaking style. Alternatively, we can remove the reference encoder and directly control synthesis using the learned interpretable tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. F0 and C0 (log scale) of two different sentences, synthesized using three tokens. Independent of the text content, the same token exhibits the same F0/C0 trend relative to the other tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Log-mel spectrograms for parallel style transfer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 Figure 5 .</head><label>45</label><figDesc>Figure 4 shows spectrograms for a parallel transfer task, where the text to synthesize matches the text of the reference signal. The GST model spectrogram is at the bottom right, compared to three other baselines: (a) the ground-truth input signal (i.e. the reference); (b) inference performed by a baseline Tacotron model (which infers acoustics only from text); and (c) inference as performed by (Skerry-Ryan et al., 2018), a Tacotron system which conditions the text encoder directly on an 128-D reference embedding. We see that, given only text input, the baseline Tacotron model does not closely match the prosodic style of the reference signal. By contrast, the direct conditioning method of (Skerry-Ryan et al., 2018) results in nearly time-aligned</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Noisy and clean tokens uncovered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Log-mel spectrograms (overlaid with F0 tracks) of two randomly chosen tokens from a GST model trained on the TED data. The two tokens uncover two different speakers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Style embedding visualization using t-SNE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>SxS</figDesc><table>subjective preference (%) and p-values of GST au-
diobook synthesis against a Tacotron baseline. Each row shows 
GST inference conditioned a different reference signal (A and B). 
p-values are given for both a 3-point and 7-point rating system. 

PREFERENCE (%) 

P-VALUE 

BASE NEUTRAL GST 3-POINT 7-POINT 

SIGNAL A 32.9 
26.5 
40.6 P=0.0552 P=0.0131 
SIGNAL B 33.1 
21.9 
45.0 P=0.0038 P=0.0003 

Table 2. Robust MOS as a function of the percentage of interfer-
ence in the training set. The total training set size is the same. 

NOISE % BASELINE TACOTRON 
GST 

50% 
2.819 ± 0.269 
4.080 ± 0.075 
75% 
1.819 ± 0.227 
3.993 ± 0.074 
90% 
1.609 ± 0.131 
4.031 ± 0.082 
95% 
1.353 ± 0.090 
3.997 ± 0.066 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>WER for the Spanish to English unsupervised language 
transfer experiment. Note that WER is an underestimate of the true 
intelligibility score; we only care about the relative differences. 

MODEL 
WER (INS/DEL/SUB) 

GST 
18.68 (6.13/2.37/10.18) 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Classification accuracy (noise-vs-clean and TED speaker ID) using GSTs and i-vectors. Despite being trained within a generative model, GSTs encode rich discriminative information.</figDesc><table>EMBEDDING ARTIFICIAL DATA TED (431 SPEAKERS) 

GST 
99.2% 
75.0% 

I-VECTOR 

/ 
73.4% 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Google, Inc.. Correspondence to: Yuxuan Wang &lt;yxwang@google.com&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Sound demos can be found at https://google. github.io/tacotron/publications/global_ style_tokens/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors thank Aren Jansen, Rob Clark, Zhifeng Chen, Ron J. Weiss, Mike Schuster, Yonghui Wu, Patrick Nguyen, and the Machine Hearing, Google Brain and Google TTS teams for their helpful discussions and feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">O</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<title level="m">Deep voice: Real-time neural text-tospeech. ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Front-end factor analysis for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised clustering of emotion and voice styles for expressive tts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Braunschweiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="4009" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Signal estimation from modified short-time fourier transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="243" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled and interpretable representations from sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unsupervised learning for expressive speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jauk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Universitat Politècnica de Catalunya</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generation of large-scale simulated utterances in virtual rooms to train deep-neural networks for far-field speech recognition in google home</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH. ISCA</title>
		<meeting>INTERSPEECH. ISCA</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Nonparallel voice conversion using i-vector plda: Towards unifying speaker verification and transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Juvela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Alku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yamagishi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Zoneout: Regularizing RNNs by randomly preserving hidden activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kramár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pezeshki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adapting and controlling dnn-based speech synthesis using input codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Henter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yamagishi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4905" to="4909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Non-parallel training in voice conversion using an adaptive restricted boltzmann machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakashika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takiguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Minami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakashika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takiguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Minami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech and Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2032" to="2045" />
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep voice 3: 2000-speaker neural text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">O</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.07654</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">AuToBI-a tool for automatic ToBI annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenberg</surname></persName>
		</author>
		<ptr target="http://eniac.cs.qc.cuny.edu/andrew/autobi/" />
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="146" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Skerryryan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05884</idno>
		<title level="m">Natural tts synthesis by conditioning wavenet on mel spectrogram predictions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A standard for labeling english prosody</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beckman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pitrelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pierrehumbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hirschberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tobi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second International Conference on Spoken Language Processing</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Towards end-to-end prosody transfer for expressive speech synthesis with Tacotron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Voice synthesis for in-the-wild speakers via a phonological loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nachmani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06588</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Text-to-speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A note on the evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.01844</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wavenet ; Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>CoRR abs/1609.03499</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6309" to="6318" />
		</imprint>
	</monogr>
	<note>A generative model for raw audio</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Agiomyrgiannakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tacotron</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1703.10135" />
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="page" from="4006" to="4010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Uncovering latent style factors for expressive speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ML4Audio Workshop, NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tobi or not tobi?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Wightman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech Prosody 2002, International Conference</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Conditional restricted boltzmann machine for voice conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<editor>ChinaSIP</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast, compact, and high quality LSTM-RNN based statistical parametric speech synthesizers for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Agiomyrgiannakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Egberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Szczepaniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings Interspeech</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
