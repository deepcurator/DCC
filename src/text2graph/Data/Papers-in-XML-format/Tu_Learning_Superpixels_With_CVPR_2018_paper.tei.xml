<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Superpixels with Segmentation-Aware Affinity Loss</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">NVIDIA 3 UC Merced</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">NVIDIA 3 UC Merced</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">NVIDIA 3 UC Merced</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao-Yi</forename><surname>Chien</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">NVIDIA 3 UC Merced</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">NVIDIA 3 UC Merced</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Superpixels with Segmentation-Aware Affinity Loss</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Superpixel segmentation has been widely used in many computer vision tasks. Existing superpixel algorithms are mainly based on hand-crafted features, which often fail to preserve weak object boundaries. In this work, we leverage deep neural networks to facilitate extracting superpixels from images. We show a simple integration of deep features with existing superpixel algorithms does not result in better performance as these features do not model segmentation. Instead, we propose a segmentation-aware affinity learning approach for superpixel segmentation. Specifically, we propose a new loss function that takes the segmentation error into account for affinity learning. We also develop the Pixel Affinity Net for affinity prediction. Extensive experimental results show that the proposed algorithm based on the learned segmentation-aware loss performs favorably against the state-of-the-art methods. We also demonstrate the use of the learned superpixels in numerous vision applications with consistent improvements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Superpixels are the image regions generated by grouping image pixels. Superpixels provide a more natural representation of image data compared to pixels. In addition, superpixels reduce the number of primitives to operate on, thereby improving the computational efficiency of vision algorithms. The process of extracting superpixels is known as superpixel segmentation or over-segmentation. Superpixels are widely used in both conventional energyminimization <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b29">30]</ref> and recent deep learning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b8">9]</ref> frameworks with applications to a wide range of problems such as salient object detection <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13]</ref>, and semantic segmentation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9]</ref>, to name a few.</p><p>In light of fundamental importance of superpixels in computer vision, numerous superpixel segmentation algorithms <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b1">2]</ref> have been proposed in the literature. Despite their differences in problem formulation, existing algorithms mainly rely on hand-crafted features, and thus often fail to separate objects from the backgrounds if no strong boundaries can be identified.</p><p>In this work, we leverage deep networks to facilitate extracting superpixels from images. There are several challenges in learning superpixels using deep networks. First, there is no groundtruth for superpixels. Second, the indices of different superpixels are interchangeable. Third, existing superpixel algorithms are not differentiable. To overcome these issues, we propose to learn pixel affinities for graphbased superpixel segmentation. Pixel affinities measure the likelihood of two neighboring pixels belonging to the same object. With better pixel affinities that take object boundaries into account, graph-based algorithms can extract semantically more meaningful superpixels. Empirically, we find that deep features learned from other high-level vision tasks do not perform well for superpixel segmentation. Our experiments show that simply replacing the hand-crafted features with pre-trained deep features for computing pixel affinities does not result in good superpixel segmentation. A closely-related problem to affinity computation is edge detection, where a number of CNN-based methods have been developed. One may expect that edges extracted from the state-of-the-art deep networks <ref type="bibr" target="#b27">[28]</ref> provide effective visual cues of pixel affinities for superpixel segmentation. However, our experiments show that this approach results in inferior performance compared to that of using hand-crafted pixel affinities.</p><p>We observe that affinities derived from pre-trained deep features or deep edges are not sensitive to segmentation errors. We therefore propose a method for learning segmentation-aware affinities for graph-based superpixel segmentation. Specifically, we propose a deep network, termed Pixel Affinity Net (PAN), for learning pixel affinities and exploiting existing segmentation datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref> as supervisory signals where pixel affinities should be low at segmentation boundaries and high elsewhere. To ensure the predicted affinities result in good segmentation quality, we propose a new SEgmentation-Aware Loss (SEAL) function. We first use the predicted affinities to compute superpixels with a graph-based algorithm. Then the SEAL function computes a loss at every pixel based on the computed superpixels and groundtruth segmentation. The loss at each pixel is then back-propagated through the proposed PAN model to adjust its parameters, thereby facilitating the use of deep neural networks for learning superpixels while avoid-ing back-propagating through the non-differentiable superpixel segmentation algorithm.</p><p>Extensive experiments on the BSDS500 <ref type="bibr" target="#b2">[3]</ref> and Cityscapes <ref type="bibr" target="#b5">[6]</ref> datasets demonstrate that the proposed learning-based approach performs favorably against the state-of-the-art superpixel segmentation methods. In addition, we show that improvements in superpixel accuracy, obtained with our approach, also translates to performance improvements in vision tasks that rely on superpixels, such as semantic segmentation and salient object detection.</p><p>We make the following contributions in this work:</p><p>• We propose a deep learning based approach to learn pixel affinities for superpixel segmentation. To the best of our knowledge, this is the first deep learning based approach for superpixel segmentation.</p><p>• We develop a novel segmentation-aware loss that makes use of segmentation errors to learn affinities for superpixel segmentation.</p><p>• We demonstrate that with the learned pixel affinities, the computed superpixels preserve object boundaries better than those with hand-crafted features. Experiments show that our algorithm performs favorably against the state-of-the-arts and helps improve superpixel-based vision applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this work, we briefly review the superpixel algorithms closely related to this work in proper context.</p><p>Graph-based algorithms. Graph-based algorithms consider an image as a planar graph, where pixels are vertices and pixel affinities are computed for connected pixels. These algorithms compute superpixels by partitioning the graph. The Normalized Cuts (NC) <ref type="bibr" target="#b21">[22]</ref> method generates superpixels by recursively computing normalized cuts on the graph. The NC method generates compact superpixels but they do not adhere to boundaries well. <ref type="bibr">Felzenszwalb and Huttenlocher (FH)</ref>  <ref type="bibr" target="#b7">[8]</ref> propose a bottom-up method that treats image pixels as disjoint sets and progressively merges the sets based on internal variation <ref type="bibr" target="#b7">[8]</ref> of pixel affinities. Although the FH method preserves boundaries well, it often generates both extremely large and small segments. Moreover, it is difficult to control the number of superpixels in the FH method. In <ref type="bibr" target="#b10">[11]</ref>, Grundmann et al. use the χ 2 distance between color histograms of two adjacent sets as a merging criterion for superpixel segmentation. The ERS <ref type="bibr" target="#b16">[17]</ref> algorithm merges disjoint sets by maximizing the entropy rate of pixel affinities to extract superpixels. While the above methods differ in graph merging or splitting techniques, they all use hand-crafted features to compute superpixels.</p><p>Clustering-based algorithms. Numerous superpixel segmentation methods are developed based on clustering techniques. These algorithms progressively refine an initial clustering of pixels until meeting the specified criteria.</p><p>The SEEDS <ref type="bibr" target="#b26">[27]</ref> algorithm uses uniform blocks as initial approximation of superpixels and iteratively exchanges neighboring blocks in a coarse-to-fine manner based on an objective function. Yao et al. <ref type="bibr" target="#b30">[31]</ref> use the block-based initialization and propose a more complex objective function for superpixel segmentation. Typically, these block-based methods do not take number of superpixels as input. Instead, users need to set several parameters manually (e.g., minimum block size and number of scales) based on image resolution and desired number of superpixels.</p><p>The SLIC <ref type="bibr" target="#b0">[1]</ref> method places initial cluster centers on a uniform grid and performs k-means in the five dimensional CIELab color and position feature space to cluster pixels. The LSC <ref type="bibr" target="#b15">[16]</ref> method projects the five dimensional features to a ten dimensional space and performs clustering in the projected space. The Manifold-SLIC <ref type="bibr" target="#b17">[18]</ref> method clusters pixels in a reduced two dimensional space. Both LSC and Manifold-SLIC schemes suggest that the segmentation quality can be improved by merely changing the feature representation, which also inspires us to leverage learning techniques for superpixel segmentation. More recently, Achanta et al. introduce the SNIC <ref type="bibr" target="#b1">[2]</ref> algorithm based on a non-iterative clustering scheme. It only visits most pixels once during clustering while still achieving state-of-the-art performance among the clustering-based methods.</p><p>Other approaches. Several other algorithms <ref type="bibr" target="#b25">[26]</ref> have been developed based on other techniques such as the Watershed transform <ref type="bibr" target="#b19">[20]</ref> and geometric flows <ref type="bibr" target="#b14">[15]</ref>, also using hand-crafted features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Superpixels Meet Deep Learning</head><p>Learning superpixels with deep neural networks is not straightforward due to the following issues. First, there is no groundtruth for superpixel segmentation. As superpixels are over-segmentation of an image, the superpixel boundaries can be arbitrary as long as a superpixel is within an object. Second, the superpixel indices are interchangeable. We can shuffle the indices while keeping the superpixel representation intact. Third, most superpixel algorithms are not differentiable as they often involve discrete operations of clustering or subset selection.</p><p>A naïve way to address these issues is to use deep features from pre-trained networks and plug them into existing superpixel algorithms. Since pre-trained deep network features were shown to generalize to different vision tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b4">5]</ref>, one may expect that these features would lead to performance gain in superpixel segmentation. We adopt this strategy using the SNIC <ref type="bibr" target="#b1">[2]</ref> and ERS <ref type="bibr" target="#b16">[17]</ref> methods.</p><p>We first replace the hand-crafted features in the SNIC and ERS algorithms with the deep features extracted from the pre-trained VGG16 model <ref type="bibr" target="#b24">[25]</ref>. We also show the result using our learned segmentation-aware affinities in (f).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As illustrated in Fig-</head><p>ure 1 (b) and (c), the extracted superpixels often fail to align with object boundaries. While the deep features provide discriminative information for high-level vision tasks, they are invariant to some spatial information which is of great importance for localizing object boundaries. More details regarding this experiment and quantitative results are presented in Section 5.1 and supplementary materials. As edge detection is closely related to image segmentation, we also consider utilizing edge information to guide the graph-based superpixel algorithms. By letting pixels with high edge probability have low pixel affinities and pixels with low edge probability have high affinities, the graphbased algorithms should be able to keep object boundaries from merging when computing superpixels. We use the ERS algorithm and derive the pixel affinities from the groundtruth segmentation maps <ref type="bibr" target="#b2">[3]</ref>, which we refer to as "oracle affinities". We find the performance of the ERS algorithm can be significantly improved with the oracle affinities. We analyze whether similar improvements can be obtained with pixel affinities derived from the state-of-the-art deep edge detectors such as HED <ref type="bibr" target="#b27">[28]</ref>. However, no significant improvements can be achieved with the HED-based affinities. The reason that pre-trained deep features perform poorly for superpixel segmentation can be attributed to the fact that image segmentation is not explicitly considered in the training objectives. For classification networks, the objective is to correctly classify images in terms of feature representations that are invariant to local edge information, thereby making them unsuitable for superpixel segmentation. Deep edge detectors, on the other hand, are trained with the objective of maximizing boundary accuracy. As a result, a few missing boundary pixels would only contribute to a small increase in edge detection loss but can cause a large segmentation error when used as affinities for superpixel segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning Segmentation-Aware Affinities</head><p>We propose a segmentation-aware pixel affinity learning approach for graph-based superpixel segmentation. It overcomes the challenges discussed in Section 3 via learning pixel affinities by minimizing a novel segmentation-aware loss function. The PAN architecture is designed to predict 4-connected pixel affinities on image graphs. These predicted affinities are then passed to a graph-based superpixel algorithm. Based on the computed superpixels and the groundtruth object segmentation map, the SEAL function is computed and the errors are back-propagated to train the PAN model. An overview of the proposed affinity learning framework is illustrated in <ref type="figure">Figure 2.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Segmentation-aware loss</head><p>We leverage existing segmentation datasets (e.g., BSDS500 <ref type="bibr" target="#b2">[3]</ref>) as supervisory signals to train the PAN model for affinity prediction. We note that the groundtruth segmentation map is for object segmentation rather than for superpixel segmentation. However, as superpixels are over-segmentation of an image, we can still use the object segmentation groundtruth to define a proper loss function for affinity prediction. The SEAL function is for this purpose. To use the segmentation datasets, we first convert the groundtruth segmentation maps into pixel affinities with zeros for the boundary pixels and ones for the remaining pixels. We use horizontal label transitions in the groundtruth segmentation maps to generate horizontal affinities and use vertical transitions to generate vertical affinities. For instance, if there is a groundtruth label transition from pixel (x, y) to pixel (x + 1, y) then we only set the horizontal affinity at (x, y) as zero, but not vertical affinity.</p><p>Given the binary groundtruth affinities and the network predictions in both the horizontal and vertical directions, we formulate the affinity learning task as a supervised learning problem and use the binary cross-entropy (BCE) loss L:</p><formula xml:id="formula_0">L(θ) = − i t i log(a i )+(1−t i ) log(1−a i ) , (1)</formula><p>where a i ∈ (0, 1) denotes predicted affinity at pixel i and t i ∈ {0, 1} denotes the corresponding target affinity computed from groundtruth segmentation maps. In addition, θ denotes the parameters of the PAN model. Note that training affinities is similar to training edge prediction where the simple BCE loss suffers from imbalance of edge and nonedge samples. To overcome data imbalance, extra weights are introduced in the class-balancing BCE loss <ref type="bibr" target="#b27">[28]</ref> to balance two classes when training edge detectors. However, it still does not take segmentation into account. Our experiments (in Section 5.1) also indicate that the affinities trained  <ref type="figure">Figure 2</ref>: Illustration of the proposed affinity learning framework. The PAN model takes the image as input and predicts horizontal and vertical affinities at each pixel. The affinities are then passed into a graph-based superpixel algorithm to compute superpixels. The SEAL function takes the affinities, computed superpixels and the groundtruth segmentation map as input and computes gradients for back-propagation. The dashed lines indicate the flow of gradients. <ref type="figure">Figure 3</ref>: Segmentation errors. Illustration of segmentation error of a superpixel S k (in green) with respect to the groundtruth object boundary Bj (in blue). We weigh the loss on boundary pixels (in red) with the segmentation error S k \ {S k ∩ Gj}.</p><formula xml:id="formula_1">G j S k ∩ G j B j S k ∩ B j S k \{S k ∩ G j }</formula><p>with class-balancing BCE loss do not generate satisfactory superpixels as this loss is agnostic to segmentation errors.</p><p>We propose the SEAL, which improves the BCE loss by tying it to superpixel segmentation errors. Specifically, we first use the predicted affinities to compute superpixels using a graph-based superpixel segmentation algorithm. The resulting superpixels are then compared with the groundtruth segmentation map to measure the segmentation errors at each superpixel.</p><p>As illustrated in <ref type="figure">Figure 3</ref>, we use S k to refer to a superpixel (shaded green) and G j to refer to the groundtruth object segment (shaded blue) that overlaps the most with S k . Because a superpixel must belong to a single object, the over-segmentation error ω S k is given by the number of pixels in S k but not in G j :</p><formula xml:id="formula_2">ω S k = |S k \ {S k ∩ G j }| = |S k | − |S k ∩ G j |. (2)</formula><p>If the predicted affinities perfectly correlate with object boundaries, ω S k should be zero. When ω S k is not zero, it means some predicted affinities are incorrect. The key idea of the SEAL function is to penalize pixel affinity predictions according to the over-segmentation errors.</p><p>Specifically, let B j denote the set of groundtruth boundary pixels (dark blue contour). The pixels that are both in the groundtruth boundaries and in S k are given by S k ∩ B j (red pixels in <ref type="figure">Figure 3</ref>). The SEAL function is defined as a weighted BCE loss:</p><formula xml:id="formula_3">L SEAL (θ) = − i (1+γ i )(t i log a i +(1−t i ) log(1−a i )), (3)</formula><p>where γ i = ω S k if i ∈ S k ∩ B j for some superpixel S k and the corresponding groundtruth segment G j , otherwise γ i = 0. In other words, If the pixels are on the groundtruth boundaries, we weigh the BCE loss with over-segmentation error (1 + γ i ); otherwise, we use plain BCE loss. This way a larger over-segmentation error will lead to a larger loss value, which in term will induce stronger gradients during training with the back-propagation algorithm. Furthermore, since we only weigh the groundtruth boundary pixels, we implicitly handle the data imbalance issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Pixel Affinity Net</head><p>We propose the PAN model to predict affinities from a given image. The input to the network is a color image and the output is a two-channel affinity map with one representing horizontal affinities and the other for vertical affinities. For each pixel, the network predicts affinities towards right and bottom pixels. For instance, the horizontal affinity at pixel (x, y) indicates the affinity value between pixel (x, y) and (x + 1, y) and the vertical affinity at pixel (x, y) is the affinity value between pixel (x, y) and (x, y + 1). The last column of horizontal affinity and the last row of vertical affinity are discarded during testing and gradients with respect to these locations are set to zero during training.</p><p>The local affinity is computed independently of the orientation of an input image. That is, both the horizontal and vertical affinities can be computed using the same network by rotating the input image along the horizontal and vertical axis. We train the PAN model to predict horizontal affinities only and the vertical affinities are computed by passing the rotated input image. These predicted affinities correspond to edge weights on the image graph, which are then fed into a graph-based superpixel segmentation algorithm.</p><p>The PAN model, illustrated in <ref type="figure">Figure 2</ref>, uses 1 × 7 horizontal convolution kernels in the first layer to capture horizontal changes in the image. This is followed by 3 standard residual blocks (ResBlock) <ref type="bibr" target="#b11">[12]</ref> of 128 channels. A single 1 × 1 convolution layer is then used to convert 128 channels into 1-channel intermediate affinity prediction. This intermediate affinity map is concatenated with the Canny edges <ref type="bibr" target="#b3">[4]</ref> and is then passed onto two 1 × 1 convolution layers for final affinity prediction. We use Canny edges here because they help localize the boundaries. Empirically, they lead to improved performance as will be shown in Section 5. We use sigmoid activations at the layers generating intermediate and final affinity maps to constrain the affinity values between 0 and 1. All other convolutional layers are interleaved with the ReLU function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Implementation details. We use the Adam <ref type="bibr" target="#b13">[14]</ref> optimization to train the PAN model for 100k iterations. We set the initial learning rate to 1e-4 and reduce it by a factor of 10 every 30k iterations until the learning rate drops to 1e-6. Other optimization parameters (β 1 , β 2 ) and weight decay are set to (0.9, 0.999) and 1e-4 respectively. We use the ERS <ref type="bibr" target="#b16">[17]</ref> algorithm to compute superpixels in the main paper. In the supplementary materials, we report additional results using the FH <ref type="bibr" target="#b7">[8]</ref> algorithm. During training, the PAN model generates horizontal and vertical affinities that are passed into the 4-connected ERS algorithm to compute superpixels.</p><p>For testing, we develop an approach to derive the diagonal affinities from 4-connected affinities and use the 8-connected ERS to compute superpixels. We observe that using the 8-connected ERS with the learned affinities achieves the best performance, which we refer to as the SEAL-ERS. The details for approximating 8-connected affinities from the 4-connected affinities are described in the supplementary materials.</p><p>Performance metrics. We use the Achievable Segmentation Accuracy (ASA) and Boundary Recall (BR) metrics for performance evaluation. The ASA score evaluates superpixels by measuring the total effective segmentation area of a superpixel representation with respect to the groundtruth segmentation map. For example, if a superpixel straddles an object boundary, only the larger side is considered correct. The BR score, on the other hand, measures the superpixel boundary adherence with respect to the groundtruth boundary. Higher ASA and BR scores indicate better superpixel segmentation results. More details about these two metrics are presented in the supplementary material. Some existing methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27]</ref> use the Corrected Under-Segmentation Error (CUSE) metric, which is equivalent to (1 − ASA). In this paper, we report all experimental results using the ASA and BR metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparisons with baselines</head><p>We compute a number of baseline affinities using several existing techniques and use them with the ERS algorithm <ref type="bibr" target="#b16">[17]</ref> for superpixel segmentation. We then evaluate the resulting superpixels with the ASA and BR scores on the BSDS500 <ref type="bibr" target="#b2">[3]</ref> test set. These scores for different baseline methods are shown in <ref type="figure" target="#fig_2">Figure 4</ref> where the standard ERS algorithm with affinities computed with RGB pixel differences is referred to as "RGB features", and our proposed method is referred to as "Affinities trained with SEAL loss". The plots indicate that the proposed algorithm performs favorably against the standard ERS method in terms of ASA scores with slightly better BR scores.</p><p>Affinities with pre-trained deep features. As stated in Section 3, we evaluate affinities computed using deep features from pre-trained networks. Specifically, we extract features from the VGG16 <ref type="bibr" target="#b24">[25]</ref> model trained for the ImageNet classification and use them to compute pixel affinities. We experiment with features from different layers and find that conv1 1 layer features result in the best performance for the pre-trained deep feature based superpixels. We refer to this experiment as "VGG16 Features" in <ref type="figure" target="#fig_2">Figure 4</ref>. The ASA and BR scores indicate that using VGG16 features for affinity measure performs worse than that using basic RGB pixel features. We also observe similar performance drop with the SNIC [2] method using pre-trained deep features, and present the experimental results in the supplementary material.</p><p>Edge-based affinities. Edge detection is closely related to segmentation. As discussed in Section 3, we experiment with the affinities computed from the state-of-the-art deep edge detectors. Specifically, we use the HED <ref type="bibr" target="#b27">[28]</ref> method and convert the predicted edge map E into affinity map A by A = 1 − E. The results with these edge-based affinities are referred to as "HED-based affinities" in <ref type="figure" target="#fig_2">Figure 4</ref>. Experimental results indicate that the ERS method using HEDbased affinities achieves similar ASA score as the original ERS superpixels but with a lower BR. In <ref type="figure" target="#fig_0">Figure 1</ref>, we show a few missing boundary pixels in the predicted edges can cause large errors in superpixel segmentation.</p><p>Class-balancing BCE loss. In addition to existing deep features, we also experiment with another variant of BCE loss function that is commonly used for training edge detectors <ref type="bibr" target="#b27">[28]</ref>. The class-balancing BCE loss uses different weights for the boundary and non-boundary pixels in the BCE loss function to account for the data imbalance. Both We compute ASA and BR scores on the BSDS500 test set for superpixels computed using the ERS <ref type="bibr" target="#b16">[17]</ref> algorithm with different baseline affinities and our learned affinities.</p><p>this class-balancing loss and the proposed segmentationaware loss are weighted BCE losses, but the class-balancing loss is still agnostic to superpixel segmentation errors. As a baseline, we train the PAN model with this class-balancing BCE loss and report the resulting superpixel scores in <ref type="figure" target="#fig_2">Figure 4</ref> as "Affinities trained with HED loss ". Experimental results indicate that this approach performs worse than that with the affinities trained with the SEAL function.</p><p>The oracle. We also carry out experiments using affinities computed from the groundtruth segmentation maps in the BSDS500 dataset. Since, several groundtruth segmentation annotations are provided for each image, we average the affinities computed from each of the annotations and consider that as groundtruth affinities. We refer to the superpixels computed using those groundtruth affinities as "the oracle" and the results in <ref type="figure" target="#fig_2">Figure 4</ref> show that the oracle affinities can significantly boost the performance of the ERS algorithm. We note that our approach with learned affinities performs almost as well as that based on the oracle in terms of ASA score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparisons with the state-of-the-arts</head><p>We compare our method with the state-of-the-art superpixel algorithms including the ERS <ref type="bibr" target="#b16">[17]</ref>, SLIC <ref type="bibr" target="#b0">[1]</ref>, LSC <ref type="bibr" target="#b15">[16]</ref>, SEEDS <ref type="bibr" target="#b26">[27]</ref> and SNIC <ref type="bibr" target="#b1">[2]</ref>. We evaluate on the BSDS500 <ref type="bibr" target="#b2">[3]</ref> and the Cityscapes <ref type="bibr" target="#b5">[6]</ref> datasets, where accurate segmentation masks are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BSDS500.</head><p>We train the PAN model using 300 images in the BSDS500 train and validation sets and evaluate on 200 test images. Each image in this dataset is provided with multiple groundtruth annotations. For training, we treat each annotation as an independent sample. With this dataset, we have 1633 train pairs and 1063 test pairs of images and annotations. We apply random image rotations and reflections as data augmentation for training the PAN model. Empirically, we observe that the proposed affinity learning framework is not sensitive to the number of superpixels used in training. We use a fixed number of 300 superpixels during training and apply the trained model to compute superpixels of different numbers.</p><p>The experimental results with the ASA and BR scores are shown in <ref type="figure" target="#fig_3">Figure 6</ref>. We set the boundary tolerance to 1 pixel for computing the BR scores. In addition to separate ASA and BR plots, we show a combined plot in <ref type="figure" target="#fig_3">Figure 6</ref> where average ASA and average BR from 100 to 1200 superpixels are used in this plot. Evaluation scores indicate that our proposed SEAL-ERS method performs favorably against the base ERS algorithm and other state-of-the-art superpixel segmentation algorithms. Note that the SEAL-ERS method is able to retain high ASA score even when the number of superpixels is small. <ref type="figure">Figure 5</ref> shows a couple of visual results where foreground objects have similar color to background. The SEAL-ERS method using the learned affinities demonstrate better boundary-preserving ability for superpixel segmentation.</p><p>Cityscapes. We evaluate superpixel segmentation methods on a larger Cityscapes <ref type="bibr" target="#b5">[6]</ref> dataset. Each image in this dataset is provided with a pixel-wise segmentation map. We train our model with 2975 Cityscapes train images and evaluate on 500 validation images. As the image resolution is high (1024×2048), we use random 512×512 crops to train our network. Similar to the experimental setup with the BSDS500 dataset, we fix the number of superpixels to 300 during training. For testing, we measure the ASA and BR scores with the number of superpixels ranging from 1000 to 6000. As the images in this dataset contain long and thin objects, we use a larger number of superpixels for all the evaluated methods. We set the boundary tolerance to 3 pixels for computing BR scores, as this dataset images are of high-resolution. <ref type="figure">Figure 7</ref> shows that significant improvements can be obtained by the SEAL-ERS method for this dataset as well, which demonstrates the generalization capability of the proposed algorithm. Specifically, our approach performs favorably against existing superpixel algorithms in this dataset. Note that the SEEDS <ref type="bibr" target="#b26">[27]</ref> algorithm is not included for comparison on this dataset as we are not able Input SLIC <ref type="bibr" target="#b0">[1]</ref> SNIC <ref type="bibr" target="#b1">[2]</ref> LSC <ref type="bibr" target="#b15">[16]</ref> SEEDS <ref type="bibr" target="#b26">[27]</ref> ERS <ref type="bibr" target="#b16">[17]</ref> SEAL-ERS (Ours) <ref type="figure">Figure 5</ref>: Visual results. We show 200 superpixels generated by the existing state-of-the-arts and our SEAL-ERS superpixel algorithm. to determine reasonable parameters (e.g., minimum block size, number of scales) to generate the desired number of superpixels for fair evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation study of PAN</head><p>We present an ablation study where we evaluate different design choices of the proposed PAN model. There are 3 key features that distinguishes PAN from standard deep architectures: 1). The model predicts only horizontal affinities rather than both horizontal and vertical affinities. We refer to this aspect of our network as "One-way prediction". 2). The model uses horizontal 1D convolution (1 × 7) filters in the first layer. 3). The model fuses Canny edges with the intermediate affinity prediction. For comparison, we include a baseline model that predicts both horizontal and vertical affinities, uses 7 × 7 filters instead of 1 × 7 and without using Canny edge fusion. We evaluate each of these design options of the network. <ref type="figure">Figure 8</ref> shows that each of the three alternatives in the PAN model perform relatively well. In other words, the proposed PAN model performs robustly with respect to different design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Applications</head><p>We evaluate whether the improvements in superpixel quality achieved by our learning technique can translate to the improvements in downstream vision tasks that use superpixels. For this study, we choose existing semantic segmentation and salient object detection techniques, and replace the superpixels used in those techniques with our SEAL-ERS superpixels.</p><p>Semantic segmentation. CNN models achieve the stateof-the-art performance for semantic segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>. However, most CNN architectures for semantic segmentation generates lower resolution outputs which are then upsampled using post-processing techniques such as Dense-CRF. Recently, Gadde et al. <ref type="bibr" target="#b8">[9]</ref> propose the Bilateral Inception (BI) networks, where superpixels are used for long-range and edge-aware propagation across CNN units, thereby alleviating the need for post-processing CRF tech-  niques. In the original BI network <ref type="bibr" target="#b8">[9]</ref>, 1000 SLIC superpixels are used for segmentation on the PascalVOC dataset. Here, we replace those with the ERS and SEAL-ERS superpixels and evaluate the resulting semantic segmentation on the PascalVOC 2012 test set <ref type="bibr" target="#b6">[7]</ref>. We report the standard Intersection over Union (IoU) scores in <ref type="table" target="#tab_1">Table 1</ref>. The results indicate that we can also obtain significant IoU improvements when using the learned SEAL-ERS superpixels (75.4) compared to standard ERS superpixels (74.5). In addition, we observe that SEAL-ERS can obtain better IoU (75.0) even when using less superpixels (600).</p><p>Salient object detection. Numerous salient object detection algorithms are based on superpixels. We demonstrate the effectiveness of SEAL-ERS superpixels by sub- stituting SLIC superpixels used in existing salient object detection algorithms with our superpixels. We experiment with two salient object detection algorithms: Saliency Optimization (SO) <ref type="bibr" target="#b31">[32]</ref> and Graph-based Manifold Ranking (GMR) <ref type="bibr" target="#b28">[29]</ref>. We report standard Mean Absolute Error (MAE) and weighted F β <ref type="bibr" target="#b20">[21]</ref> scores on the ECSSD dataset <ref type="bibr" target="#b23">[24]</ref>. With the same setting as in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b28">29]</ref>, we use 200 superpixels for this study. Experimental results in <ref type="table" target="#tab_2">Table 2</ref> show that the use of SEAL-ERS superpixels consistently improves the performance of both the SO and GMR methods in both metrics. These results on semantic segmentation and salient object detection demonstrate the potential of using learned superpixels for downstream vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we present a segmentation-aware affinity learning framework for superpixel segmentation. We propose the PAN model for affinity prediction and develop the SEAL that makes use of superpixel segmentation errors for affinity learning. The resulting SEAL-ERS method generates better boundary-preserving superpixels compared to those using hand-crafted features or deep features trained without taking segmentation into account. Our SEAL-ERS performs favorably against several existing state-of-the-art superpixels. Furthermore, we demonstrate that the SEAL-ERS can be used in numerous vision applications with significant performance gains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Segmentation leakage. (a) shows the typical challenge in natural images that the object has similar colors with background. (b) and (c) show the results of the SNIC and ERS methods using the deep features from VGG16. (d) is the result of the ERS method using the HED-based affinities. (e) is the corresponding affinities derived from HED edges. We also show the result using our learned segmentation-aware affinities in (f).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 (</head><label>1</label><figDesc>d) shows that a few missing boundary pixels in the near-perfect boundary map (Figure 1(e)) lead to merging of foreground and background regions and intro- duce segmentation errors. More experimental details along with the quantitative results are presented in Section 5.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison to baseline affinities. We compute ASA and BR scores on the BSDS500 test set for superpixels computed using</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Results on BSDS500. We compare our SEAL-ERS method with the state-of-the-art algorithms on the BSDS500 test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Results on Cityscapes. We compare our SEAL-ERS method with the state-of-the-arts on the Cityscapes validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Superpixels for semantic segmentation. We compute semantic segmentation using the BI network [9] with different types of superpixels and compare the IoU scores on the Pascal- VOC 2012 test set.</figDesc><table>Method 
# of Superpixels 
IoU 

DeepLab [5] 
-
68.9 

+ CRF [5] 
-
72.7 

+ BI (SLIC) [9] 
1000 
74.1 

+ BI (ERS) 
1000 
74.5 

+ BI (SEAL-ERS) 
600 
75.0 

+ BI (SEAL-ERS) 
1000 
75.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Superpixels for salient object detection. We run the SO [32] and GMR [29] algorithms with different types of super- pixels and evaluate on the ECSSD dataset.</figDesc><table>SLIC [1] ERS [17] SEAL-ERS 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Yang is supported in part by NSF CAREER (No. 1149783) and gifts from Adobe, Toyota, Panasonic, Samsung, NEC, Verisk, and NVidia.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Superpixels and polygons using simple non-iterative clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient graphbased image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Superpixel convolutional networks using bilateral inceptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kappler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Supercnn: A superpixelwise convolutional neural network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="330" to="344" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>arXiv, abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levinshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Kutulakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
		<title level="m">Turbopixels: Fast superpixels using geometric flows. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2290" to="2297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Superpixel segmentation using linear spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Entropy rate superpixel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Manifold slic: A fast method to compute content-sensitive superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Machairas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cárdenas-Peña</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chabardes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Decencière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Waterpixels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3707" to="3716" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">How to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning a classification model for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recursive context propagation network for semantic scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical image saliency detection on extended cssd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="717" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Very deep convolutional networks for large-scale image recognition. arXiv, abs/1409.1556</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Superpixels: An evaluation of the state-of-the-art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno>arXiv, abs/1612.01601</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Seeds: Superpixels extracted via energy-driven sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bergh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="298" to="314" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust superpixel tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1639" to="1651" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Real-time coarse-to-fine topologically preserving segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Saliency optimization from robust background detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
