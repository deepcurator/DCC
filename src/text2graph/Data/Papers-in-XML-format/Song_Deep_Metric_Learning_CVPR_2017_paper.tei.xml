<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Metric Learning via Facility Location</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Oh</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
							<email>kpmurphy@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mit</surname></persName>
						</author>
						<title level="a" type="main">Deep Metric Learning via Facility Location</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning to measure the similarity among arbitrary groups of data is of great practical importance, and can be used for a variety of tasks such as feature based retrieval <ref type="bibr" target="#b30">[30]</ref>, clustering <ref type="bibr" target="#b10">[10]</ref>, near duplicate detection <ref type="bibr" target="#b41">[41]</ref>, verification <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4]</ref>, feature matching <ref type="bibr" target="#b6">[6]</ref>, domain adaptation <ref type="bibr" target="#b27">[27]</ref>, video based weakly supervised learning <ref type="bibr" target="#b38">[38]</ref>, etc. Furthermore, metric learning can be used for challenging extreme classification settings <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b40">40]</ref>, where the number of classes is very large and the number of examples per class becomes scarce. For example, <ref type="bibr" target="#b2">[2]</ref> uses this approach to perform product search with 10M images, and <ref type="bibr" target="#b25">[25]</ref> shows superhuman performance on face verification with 260M images of 8M distinct identities. In this setting, any direct classification or regression methods become impractical due to the prohibitively large size of the label set.</p><p>Currently, the best approaches to metric learning employ state of art neural networks <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b9">9]</ref>, which are trained to produce an embedding of each input vector so that a certain loss, related to distances of the points, is minimized. However, most current methods, such as <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b29">29]</ref>, are very myopic in the sense that the loss is defined in terms of pairs or triplets inside the training mini-batch. These methods don't take the global structure of the embedding space into consideration, which can result in reduced clustering and retrieval performance.</p><p>Furthermore, most of the current methods <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b29">29]</ref> in deep metric learning require a separate data preparation stage where the training data has to be first prepared in pairs <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b2">2]</ref>, triplets <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b25">25]</ref>, or n-pair tuples <ref type="bibr" target="#b29">[29]</ref> format. This procedure has very expensive time and space cost as it often requires duplicating the training data and needs to repeatedly access the disk.</p><p>In this paper, we propose a novel learning framework which encourages the network to learn an embedding function that directly optimizes a clustering quality metric (We use the normalized mutual information or NMI metric <ref type="bibr" target="#b21">[21]</ref> to measure clustering quality, but other metrics could be used instead.) and doesn't require the training data to be preprocessed in rigid paired format. Our approach uses a structured prediction framework <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b14">14]</ref> to ensure that the score of the ground truth clustering assignment is higher than the score of any other clustering assignment. Following the evaluation protocol in <ref type="bibr" target="#b30">[30]</ref>, we report state of the art results on CUB200-2011 <ref type="bibr" target="#b37">[37]</ref>, Cars196 <ref type="bibr" target="#b18">[18]</ref>, and Stanford online products <ref type="bibr" target="#b30">[30]</ref> datasets for clustering and retrieval tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The seminal work in deep metric learning is to train a siamese network with contrastive loss <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b4">4]</ref> where the task is to minimize the pairwise distance between a pair of examples with the same class labels, and to push the pairwise distance between a pair of examples with different class labels at least greater than some fixed margin.</p><p>One downside of this approach is that it focuses on absolute distances, whereas for most tasks, relative distances matter more. For this reason, more recent methods have proposed different loss functions. We give a brief review of these below, and we compare our method to them experimentally in Section 4. <ref type="figure" target="#fig_2">Figure 1</ref>. Overview of the proposed framework. The network first computes the embedding vectors for each images in the batch and learns to rank the clustering scoreF for the ground truth clustering assignment higher than the clustering score F for any other assignment at least by the structured margin ∆(y, y * ).</p><formula xml:id="formula_0">...F ! F ! ! y * * = FF ! ! * = y 1 FF ! ! * = y n ∆(y 1 , y * ) ∆(y n , y * ) n . . . n ( score n | {z } CNN</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Triplet learning with semi-hard negative mining</head><p>One improvement over contrastive loss is to use triplet loss <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b26">26]</ref>. This first constructs a set of triplets, where each triplet has an anchor, a positive, and a negative example, where the anchor and the positive have the same class labels and the negative has the different class label. It then tries to move the anchor and positive closer than the distance between the anchor and the negative with some fixed margin. More precisely, it minimizes the following loss:</p><formula xml:id="formula_1">ℓ(X, y) = 1 |T | (i,j,k)∈T D 2 i,j + α − D 2 i,k + (1)</formula><p>where T is the set of triples,</p><formula xml:id="formula_2">D i,j = ||f (X i ) − f (X j )|| 2</formula><p>is the Euclidean distance in embedding space, the operator [·] + denotes the hinge function which takes the positive component of the argument, and α denotes a fixed margin constant.</p><p>In practice, the performance of these methods depends highly on the triplet sampling strategy. FaceNet <ref type="bibr" target="#b25">[25]</ref> suggested the following online hard negative mining strategy. The idea is to construct triplets by associating with each positive pair in the minibatch a "semi-hard" negative example. This is an example which is further away from the anchor i than the positive exemplar j is, but still hard because the distance is close to the i − j distance. More precisely, it minimizes</p><formula xml:id="formula_3">ℓ (X, y) = 1 |P| (i,j)∈P D 2 i,j + α − D 2 i,k * (i,j) + where k * (i, j) = arg min k: y[k] =y[i] D 2 i,k s.t. D 2 i,k &gt; D 2 i,j</formula><p>and P is the set of pairs with the same class label. If there is no such negative example satisfying the constraint, we just pick the furthest negative example in the minibatch, as follows:</p><formula xml:id="formula_4">k * (i, j) = arg max k: y[k] =y[i] D 2 i,k</formula><p>In order to get good results, the FaceNet paper had to use very large minibatches (1800 images), to ensure they picked enough hard negatives. This makes it hard to train the model on a GPU due to the GPU memory constraint. Below we describe some other losses which are easier to minimize using small minibatches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Lifted structured embedding</head><p>Song et al. <ref type="bibr" target="#b30">[30]</ref> proposed lifted structured embedding where each positive pair compares the distances against all the negative pairs weighted by the margin constraint violation. The idea is to have a differentiable smooth loss which incorporates the online hard negative mining functionality using the log-sum-exp formulation.</p><formula xml:id="formula_5">ℓ (X, y) = 1 2|P| (i,j)∈P log (i,k)∈N exp {α − D i,k } + (j,l)∈N exp {α − D j,l } + D i,j 2 + ,<label>(2)</label></formula><p>where N denotes the set of pairs of examples with different class labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">N-pairs embedding</head><p>Recently, Sohn et al. <ref type="bibr" target="#b29">[29]</ref> proposed N-pairs loss which enforces softmax cross-entropy loss among the pairwise similarity values in the batch.</p><formula xml:id="formula_6">ℓ (X, y) = −1 |P| (i,j)∈P log exp{S i,j } exp{S i,j } + k: y[k] =y[i] exp{S i,k } + λ m m i ||f (X i )|| 2 ,<label>(3)</label></formula><p>where S i,j means the feature dot product between two data points in the embedding space;</p><formula xml:id="formula_7">S i,j = f (X i ) ⊺ f (X j )</formula><p>, m is the number of the data, and λ is the regularization constant for the ℓ 2 regularizer on the embedding vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Other related work</head><p>In addition to the above work on metric learning, there has been some recent work on learning to cluster with deep networks. Hershey et al. <ref type="bibr" target="#b10">[10]</ref> uses Frobenius norm on the residual between the binary ground truth and the estimated pairwise affinity matrix; they apply this to speech spectrogram signal clustering. However, using the Frobenius norm directly is suboptimal, since it ignores the fact that the affinity matrix is positive definite.</p><p>To overcome this, matrix backpropagation <ref type="bibr" target="#b12">[12]</ref> first projects the true and predicted affinity matrix to a metric space where Euclidean distance is appropriate. Then it applies this to normalized cuts for unsupervised image segmentation. However, this approach requires computing the eigenvalue decomposition of the data matrix, which has cubic time complexity in the number of data and is thus not very practical for large problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>One of the key attributes which the recent state of the art deep learning methods in Section 2 have in common is that they are all local metric learning methods. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates a case where this can fail. In particular, whenever a positive pair (such as the two purple bold dots connected by the blue edge) is separated by examples from other classes, the attractive gradient signal from the positive pair gets outweighed by the repulsive gradient signal from the negative data points (yellow and green data points connected with the red edges). This failure can lead to groups of examples with the same class labels being separated into partitions in the embedding space that are far apart from each other. This can lead to degradation in the clustering and nearest neighbor based retrieval performance. For example, suppose we incorrectly created 4 clusters in <ref type="figure" target="#fig_0">Figure 2</ref>. If we asked for the 12 nearest neighbors of one of purple points, we would retrieve points belonging to other classes.</p><p>To overcome this problem, we propose a method that learns to embed points so as to minimize a clustering loss, as we describe below.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Facility location problem</head><p>Suppose we have a set of inputs X i , and an embedding function f (X i ; Θ) that maps each input to a point in some K dimensional space. Now suppose we compress this set of points by mapping each example i to its nearest point from a chosen set of landmarks S ⊆ V, where V = {1, . . . , |X|} is the ground set. We can define the resulting function as follows:</p><formula xml:id="formula_8">F (X, S; Θ) = − i∈|X| min j∈S ||f (X i ; Θ) − f (X j ; Θ)||, (4)</formula><p>This is called the facility location function, and has been widely used in data summarization and clustering <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b34">34]</ref>.</p><p>The idea is that this function measures the sum of the travel distance for each customer in X to their respective nearest facility location in S. In terms of clustering, data points in S correspond to the cluster medoids, and the cluster assignment is based on the nearest medoid from each data point. Maximizing equation 4 with respect to subset S is NP-hard, but there is a well established worst case optimality bound of O 1 − 1 e for the greedy solution of the problem via submodularity <ref type="bibr" target="#b17">[17]</ref>.</p><p>Below we show how to use the facility location problem as a subroutine for deep metric learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Structured facility location for deep metric learning</head><p>The oracle scoring functionF measures the quality of the clustering given the ground truth clustering assignment y * and the embedding parameters Θ:</p><formula xml:id="formula_9">F (X, y * ; Θ) = |Y| k max j∈{i: y * [i]=k} F X {i: y * [i]=k} , {j}; Θ ,<label>(5)</label></formula><p>where {i : y * [i] = k} denotes the subset of the elements in V with the ground truth label equal to k.</p><p>We would like the clustering score of the oracle clustering assignment to be greater than the score for the maximally violating clustering assignment. Hence we define the following structured loss function:</p><formula xml:id="formula_10">ℓ (X, y * ) = max S⊂V |S|=|Y| F (X, S; Θ) + γ ∆ (g(S), y * ) ( * ) −F (X, y * ; Θ) +<label>(6)</label></formula><p>We will define the structured margin ∆ (y, y * ) below. The function y = g(S) maps the set of indices S to a set of cluster labels by assigning each data point to its nearest facility in S:</p><formula xml:id="formula_11">g(S)[i] = arg min j ||f (X i ; Θ) − f (X {j| j∈S} ; Θ)|| (7)</formula><p>Intuitively, the loss function in <ref type="figure">Equation 6</ref> encourages the network to learn an embedding function f (·; Θ) such that the oracle clustering scoreF is greater than the clustering score F for any other cluster assignments g(S) at least by the structured margin ∆ (y, y * ). <ref type="figure" target="#fig_2">Figure 1</ref> gives the pictorial illustration of the overall framework.</p><p>The structured margin term ∆ (y, y * ) measures the quality of the clustering. The margin term outputs 0 if the clustering quality of y with respect to the ground truth clustering assignment y * is perfect (up to a permutation) and 1 if the quality is the worst. We use the following margin term</p><formula xml:id="formula_12">∆ (y, y * ) = 1 − NMI (y, y * )<label>(8)</label></formula><p>where NMI is the normalized mutual information (NMI) <ref type="bibr" target="#b21">[21]</ref>. This measures the label agreement between the two clustering assignments ignoring the permutations. It is defined by the ratio of mutual information and the square root of the product of entropies for each assignments:</p><formula xml:id="formula_13">N M I(y 1 , y 2 ) = M I(y 1 , y 2 ) H(y 1 )H(y 2 )<label>(9)</label></formula><p>The marginal and joint probability mass used for computing the entropy and the mutual information can be estimated as follows:</p><formula xml:id="formula_14">P (i) = 1 m j I[y[j] == i] P (i, j) = 1 m k,l I[y 1 [k] == i] · I[y 2 [l] == j],<label>(10)</label></formula><p>where m denotes the number of data (also equal to |X|). <ref type="figure" target="#fig_1">Figure 3</ref> illustrates the advantages of the proposed algorithm. Since the algorithm is aware of the global landscape of the embedding space, it can overcome the bad local optima in figure 2. The clustering loss encourages small intra cluster (outlined by the dotted lines in <ref type="figure" target="#fig_1">figure 3</ref>) sum of distances with respect to each cluster medoids (three data points outlined in bold) while discouraging different clusters from getting close to each other via the NMI metric in the structured margin term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Backpropagation subgradients</head><p>We fit our model using stochastic gradient descent. The key step is to compute the derivative of the loss, which is given by the following expression:</p><formula xml:id="formula_15">∂ ℓ (X, y * ) = I [ℓ (X, y * ) &gt; 0] ∇ Θ F (X, S PAM ; Θ) − ∇ ΘF (X, y * ; Θ)<label>(11)</label></formula><p>Here S PAM is the solution to the subproblem marked ( * ) in <ref type="figure">Equation 6</ref>; we discuss how to compute this in Section 3.4. The first gradient term is as follows:</p><formula xml:id="formula_16">∇ Θ F (X, S; Θ) = − i∈|X| f (X i ; Θ) − f (X j * (i) ; Θ) ||f (X i ; Θ) − f (X j * (i) ; Θ)|| • ∇ Θ f (X i ; Θ) − f (X j * (i) ; Θ)<label>(12)</label></formula><p>where j * (i) denotes the index of the closest facility location in the set S PAM . The gradient for the oracle scoring function can be derived by computing</p><formula xml:id="formula_17">∇ ΘF (X, y * i ; Θ) = k ∇ Θ F X {i: y * [i]=k} , {j * (k)}; Θ<label>(13)</label></formula><p>Equation <ref type="formula" target="#formula_15">11</ref> is the formula for the exact subgradient and we find an approximate maximizer S PAM in the equation (section 3.4) so we have an approximate subgradient. However, this approximation works well in practice and have been used for structured prediction setting <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b34">34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss augmented inference</head><p>We solve the optimization problem ( * ) in Equation 6 in two steps. First, we use the greedy Algorithm 1 to select an initial good set of facilities. In each step, it chooses the element i * with the best marginal benefit. The running time of the algorithm is O |Y| 3 · |V| , where |Y| denotes the number of clusters in the batch and V = {1, . . . , |X|}. This time is linear in the size of the minibatch, and hence does not add much overhead on top of the gradient computation. Yet, if need be, we can speed up this part via a stochastic version of the greedy algorithm <ref type="bibr" target="#b22">[22]</ref>. This algorithm is motivated by the fact that the first term, F (X, S; Θ), is a monotone submodular function in S. We observed that throughout the learning process, this term is large compared to the second, margin term. Hence, in this case, our function is still close to submodular. For approximately submodular functions, the greedy algorithm can still be guaranteed to work well <ref type="bibr" target="#b7">[7]</ref>.</p><p>Yet, since A(S) is not entirely submodular, we refine the greedy solution with a local search, Algorithm 2. This algorithm performs pairwise exchanges of current medoids S[k] with alternative points j in the same cluster. The running time of the algorithm is O T |Y| 3 · |V| , where T is the maximum number of iterations. In practice, it converges quickly, so we run the algorithm for T = 5 iterations only.</p><p>Algorithm 2 is similar to the partition around medoids (PAM) <ref type="bibr" target="#b15">[15]</ref> algorithm for k-medoids clustering, which independently reasons about each cluster during the medoid swapping step. Algorithm 2 differs from PAM by the structured margin term, which involves all clusters simultaneously.</p><p>The following lemma states that the algorithm can only improve over the greedy solution: Proof. In any step t and for any k, let c = S[k] be the kth medoid in S. The algorithm finds the point j in the kth cluster such that A((S \{c})∪{j}) is maximized. Let j * be a maximizing argument. Since j = c is a valid choice, we have that A((S\{c})∪{j * }) ≥ A((S\{c})∪{c}) = A(S), and hence the value of A(S) can only increase.</p><p>In fact, with a small modification and T large enough, the algorithm is guaranteed to find a local optimum, i.e., a set S such that A(S) ≥ A(S ′ ) for all S ′ with |S∆S ′ | = 1 (Hamming distance one). Note that the overall problem is NP-hard, so a guarantee of global optimality is impossible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 2. If the exchange point j is chosen from X and</head><p>T is large enough that the algorithm terminates because it makes no more changes, then Algorithm 2 is guaranteed to find a local optimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation details</head><p>We used Tensorflow <ref type="bibr" target="#b0">[1]</ref> package for our implementation. For the embedding vector, we ℓ 2 normalize the embedding vectors before computing the loss for our method. The model slightly underperformed when we omitted the embedding normalization. We also tried solving the loss augmented inference using Algorithm 2 with random initialization, but it didn't work as well as initializing the algorithm with the greedy solution from Algorithm 1.</p><p>For the network architectures, we used the Inception <ref type="bibr" target="#b32">[32]</ref> network with batch normalization <ref type="bibr" target="#b11">[11]</ref> pretrained on ILSVRC 2012-CLS <ref type="bibr" target="#b24">[24]</ref> and finetuned the network on our datasets. All the input images are first resized to square size (256 × 256) and cropped at 227 × 227. For the data augmentation, we used random crop with random horizontal mirroring for training and a single center crop for testing. In Npairs embedding <ref type="bibr" target="#b29">[29]</ref>, they take multiple random crops and average the embedding vectors from the cropped Algorithm 2: Loss augmented refinement for ( * )</p><formula xml:id="formula_18">Input : X ∈ R m×d , y * ∈ |Y| m , S init , γ, T Output : S Initialize: S = S init , t = 0 1 for t &lt; T do // Perform cluster assignment 2 yPAM = g (S)</formula><p>// Update each medoids per cluster 3 for k &lt; |Y| do // Swap the current medoid in cluster k if it increases the score.</p><formula xml:id="formula_19">4 S[k] = arg max j∈{i: y PAM [i]=k} F X {i: y PAM [i]=k} , {j}; Θ 5 + γ∆ (g (S \ {S[k]} ∪ {j}) , y * ) 6 end 7 end 8 return S</formula><p>images during testing. However, in our implementation of <ref type="bibr" target="#b29">[29]</ref>, we take a single center crop during testing for fair comparison with other methods.</p><p>The experimental ablation study reported in <ref type="bibr" target="#b30">[30]</ref> suggested that the embedding size doesn't play a crucial role during training and testing phase so we decided to fix the embedding size at d = 64 throughout the experiment (In <ref type="bibr" target="#b30">[30]</ref>, the authors report the recall@K results with d = 512 and provided the results for d = 64 to us for fair comparison). We used RMSprop <ref type="bibr" target="#b33">[33]</ref> optimizer with the batch size m set to 128. For the margin multiplier constant γ, we gradually decrease it using exponential decay with the decay rate set to 0.94.</p><p>As briefly mentioned in section 1, the proposed method does not require the data to be prepared in any rigid paired format (pairs, triplets, n-pair tuples, etc). Instead we simply sample m (batch size) examples and labels at random. That said, the clustering loss becomes trivial if a batch of data all have the same class labels (perfect clustering merging everything into one cluster) or if the data all have different class labels (perfect clustering where each data point forms their own clusters). In this regard, we guarded against those pathological cases by ensuring the number of unique classes (C) in the batch is within a reasonable range. We tried three different settings </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>Following the experimental protocol in <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b29">29]</ref>, we evaluate the clustering and k nearest neighbor retrieval <ref type="bibr" target="#b13">[13]</ref> results on data from previously unseen classes on the CUB-200-2011 <ref type="bibr" target="#b37">[37]</ref>, Cars196 <ref type="bibr" target="#b18">[18]</ref>, and Stanford Online Products <ref type="bibr" target="#b30">[30]</ref> datasets. We compare our method with three current state of the art methods in deep metric learning: (1) triplet learning with semi-hard negative mining strategy <ref type="bibr" target="#b25">[25]</ref>, (2) lifted structured embedding <ref type="bibr" target="#b30">[30]</ref>, (3) N-pairs metric loss <ref type="bibr" target="#b29">[29]</ref>. To be comparable with prior work, we ℓ 2 normalize the embedding for the triplet (as prescribed by <ref type="bibr" target="#b25">[25]</ref>) and our method, but not for the lifted structured loss and the N-pairs loss (as in the implementation sections in <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b29">29]</ref>).</p><p>We used the same train/test split as in <ref type="bibr" target="#b30">[30]</ref> for all the datasets. The CUB200-2011 dataset <ref type="bibr" target="#b37">[37]</ref> has 11, 788 images of 200 bird species; we used the first 100 birds species for training and the remaining 100 species for testing. The Cars196 dataset <ref type="bibr" target="#b18">[18]</ref> has 16, 185 images of 196 car models. We used the first 98 classes of cars for training and the rest for testing. The Stanford online products dataset <ref type="bibr" target="#b30">[30]</ref> has 120, 053 images of 22, 634 products sold online on eBay.com. We used the first 11, 318 product categories for training and the remaining 11, 316 categories for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative results</head><p>The training procedure for all the methods converged at 10k iterations for the CUB200-2011 <ref type="bibr" target="#b37">[37]</ref> and at 20k iterations for the Cars196 <ref type="bibr" target="#b18">[18]</ref> and the Stanford online products <ref type="bibr" target="#b30">[30]</ref> datasets. <ref type="table">Tables 1, 2</ref>, and 3 shows the results of the quantitative comparison between our method and other deep metric learning methods. We report the NMI score, to measure the quality of the clustering, as well as k nearest neighbor performance with the Recall@K metric. The tables show that our proposed method has the state of the art performance on both the NMI and R@K metrics outperforming all the previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NMI R@1 R@2 R@4 R@8</head><p>Triplet semihard <ref type="bibr" target="#b25">[25]</ref>   <ref type="figure" target="#fig_4">Figure 4</ref>, 5, and 6 visualizes the t-SNE <ref type="bibr" target="#b36">[36]</ref> plots on the embedding vectors from our method on CUB200-NMI R@1 R@2 R@4 R@8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative results</head><p>Triplet semihard <ref type="bibr" target="#b25">[25]</ref>   2011 <ref type="bibr" target="#b37">[37]</ref>, Cars196 <ref type="bibr" target="#b18">[18]</ref>, and Stanford online products <ref type="bibr" target="#b30">[30]</ref> datasets respectively. The plots are best viewed on a monitor when zoomed in. We can see that our embedding does a great job on grouping similar objects/products despite the significant variations in view point, pose, and configuration.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We described a novel learning scheme for optimizing the deep metric embedding with the learnable clustering function and the clustering metric (NMI) in a end-to-end fashion within a principled structured prediction framework.</p><p>Our experiments on CUB200-2011 <ref type="bibr" target="#b37">[37]</ref>, Cars196 <ref type="bibr" target="#b18">[18]</ref>, and Stanford online products <ref type="bibr" target="#b30">[30]</ref> datasets show state of the art performance both on the clustering and retrieval tasks.</p><p>The proposed clustering loss has the added benefit that it doesn't require rigid and time consuming data preparation (i.e. no need for preparing the data in pairs <ref type="bibr" target="#b8">[8]</ref>, triplets <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b25">25]</ref>, or n-pair tuples <ref type="bibr" target="#b29">[29]</ref> format). This characteristic of the proposed method opens up a rich class of possibilities for advanced data sampling schemes.</p><p>In the future, we plan to explore sampling based gradient averaging scheme where we ask the algorithm to cluster several random subsets of the data within the training batch and then average the loss gradient from multiple sampled subsets in similar spirit to Bag of Little Bootstraps (BLB) <ref type="bibr" target="#b16">[16]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Example failure mode for local metric learning methods. Whenever a positive pair (linked with the blue edge) is separated by negative examples, the gradient signal from the positive pair (attraction) gets outweighed by the negative pairs (repulsion). Illustration shows the failure case for 2D embedding where the purple clusters can't be merged into one cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Proposed clustering loss for the same embedding layout in figure 2. Nodes highlighted in bold are the cluster medoids. The proposed method encourages small sum of distances within each cluster, while discouraging different clusters from getting close to each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Lemma 1 .</head><label>1</label><figDesc>Algorithm 2 monotonically increases the objec- tive function A(S) = F (X, S; Θ) + γ∆ (g(S), y * ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>{0.25, 0.50, 0.75} and the choice of the ratio did not lead to significant changes in the experi- mental results. For the CUB-200-2011 [37] and Cars196 [18], we set C m = 0.25. For the Stanford Online Products [30] dataset, C m = 0.75 was the only possible choice be- cause the dataset is extremely fine-grained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Barnes-Hut t-SNE visualization [36] of our embedding on the CUB-200-2011 [37] dataset. Best viewed on a monitor when zoomed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Barnes-Hut t-SNE visualization [36] of our embedding on the Cars196 [18] dataset. Best viewed on a monitor when zoomed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>55.38 42.59 55.03 66.44 77.23</figDesc><table>Lifted struct [30] 
56.50 43.57 56.55 68.59 79.63 
Npairs [29] 
57.24 45.37 58.41 69.51 79.49 
Clustering (Ours) 
59.23 48.18 61.44 71.83 81.92 

Table 1. Clustering and recall performance on CUB-200-2011 [37] 
@10k iterations. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>53.35 51.54 63.78 73.52 82.41Table 2. Clustering and recall performance on Cars196 [18] @20k iterations.</figDesc><table>Lifted struct [30] 
56.88 52.98 65.70 76.01 84.27 
Npairs [29] 
57.79 53.90 66.76 77.75 86.35 
Clustering (Ours) 
59.04 58.11 70.64 80.27 87.81 

NMI R@1 R@10 R@100 

Triplet semihard [25] 89.46 66.67 82.39 
91.85 
Lifted struct [30] 
88.65 62.46 80.81 
91.93 
Npairs [29] 
89.37 66.41 83.24 
93.00 
Clustering (Ours) 
89.48 67.02 83.65 
93.23 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Clustering and recall performance on Products [30] @20k iterations.</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Figure 6. Barnes-Hut t-SNE visualization [36] of our embedding on the Stanford online products dataset [30]. Best viewed on a monitor when zoomed in</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org. 5</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning visual similarity for product design with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extreme multi class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Universal correspondence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kempe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Training deep networks with structured layers by matrix backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vantzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAMI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cutting-plane training of structural svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Clustering by means of medoids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rousseeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Data Analysis Based on the L1-Norm and Related Methods</title>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The big data bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Submodular function maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Golovin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tractability: Practical Approaches to Hard Problems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">19</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">3d object representations for fine-grained categorization. ICCV 3dRR-13</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning mixtures of submodular shells with application to document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sch¡9f¿tze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lazier than lazy greedy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mirzasoleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Badanidiyuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vondrák</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. on Artificial Intelligence (AAAI)</title>
		<meeting>Conf. on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>ImageNet Large Scale Visual Recognition Challenge. IJCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning a distance metric from relative comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning transferrable representations for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Lecture 6.5-RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning mixtures of submodular functions for image collection summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tschiatschek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Support vector machine learning for interdependent and structured output spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Accelerating t-sne using tree-based algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pd-sparse: A primal and dual sparse approach to extreme multiclass and multilabel classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">E</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving the robustness of deep neural networks via stability training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
