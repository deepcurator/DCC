<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-organized Text Detection with Minimal Post-processing via Border Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
							<email>yuewu@isi.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute</orgName>
								<address>
									<addrLine>4676 Admiralty Way, Marina Del Rey</addrLine>
									<postCode>#1001</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
							<email>pnatarajan@isi.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute</orgName>
								<address>
									<addrLine>4676 Admiralty Way, Marina Del Rey</addrLine>
									<postCode>#1001</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-organized Text Detection with Minimal Post-processing via Border Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As one of the most well-known and oldest problems in computer vision <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b42">43]</ref>, text detection has attracted increased attention in recent years <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b14">15]</ref>, bringing benefits to many real-world applications, including but not limited to image search, document analysis, automatic driving, and human-computer interactions. A good review of the text detection problem can be found in <ref type="bibr" target="#b36">[37]</ref>.</p><p>Although the text detection problem seems to be selfexplanatory, namely detecting texts in a given image, it can be interpreted in many different ways according to the used detection unit, e.g., component detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">39]</ref>, character detection <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>, word detection <ref type="bibr" target="#b9">[10]</ref> , line detection <ref type="bibr" target="#b40">[41]</ref>, and region/block detection <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b33">34]</ref>. From the perspective of detection speed, the component detection is the most handy because components can be efficiently extracted from low-level features like edges, and some heuristic rules are also effective to distinguish text from non-text. From the perspective of rigorous problem definition, character detection is highly preferred because a character is naturally a unit in any language, and there are always a small number of characters. From the perspective of human readable results, the most preferred is either the word-or line-level detection. From the perspective of reducing false alarms, the region/block-level detection is preferred because it contains a larger image context region to differentiate text from non-text. However, text detection is not an isolated task, but is closely related to text recognition. From the perspective of a down-streaming recognition system, the most preferred is the line-or word-level detection depending on whether a recognition system is trained by text lines or isolated words. Consequently, there are three common strategies: 1) bottom-up <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b18">19]</ref>: first detect text at the component-or character-level, and convert initial detection results to a desired word-or line-level; 2) top-down <ref type="bibr" target="#b41">[42]</ref>: first detect text on the block level, and break a block to a word-or line-level if necessary; and 3) hybrid <ref type="bibr" target="#b33">[34]</ref>: take advantage of both bottom-up and top-down strategies. Actually, these are the mainstream frameworks used in text detection until now, although people may use different component/character detectors, rules to connect character bounding boxes to a word/line, rules to divide a region into words/lines, etc.</p><p>Early efforts under the mainstream frameworks focus on various heuristics that are helpful for detecting characters or character components. The two most well-known ones are: 1) maximally stable extremal regions (MSER) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22]</ref> and 2) stroke-width transform (SWT) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref>. MSER assumes that text components have similar homogeneous background and foreground and thus are stable with respect to a range of thresholds. SWT assumes that text components have comparable stroke width, and thus finding components with comparable stroke width finds text. Both MSER and SWT are very useful heuristics, but they have to be combined with additional post-processing to produce reasonable text candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">5000</head><p>More recent efforts focus on reducing the amount of handcrafted features or man-made rules in text detection, especially after the big success of deep convolutional neural networks (CNN) in the ImageNet Challenge <ref type="bibr" target="#b13">[14]</ref>. These recent approaches show very exciting performance scores on both document images and scene text images. Tian et al. <ref type="bibr" target="#b23">[24]</ref> proposed the Text Flow method that sequences character CNN candidate detection, false character removal, text line extraction and text line verification using minimum cost flow networks. Cho et al. <ref type="bibr" target="#b4">[5]</ref> suggested the Canny Text Detector that proposes character candidates using maximum stable regions and edge similarities, text line tracking and grouping using heuristic rules. Zhang et al. <ref type="bibr" target="#b41">[42]</ref> suggests a fully convolutional neural network (FCN) to obtain text block candidates, a character-centroid FCN to assist text line generation from a text block, and a set of heuristic rules based on intensity and geometry consistencies to reject false candidates. Yao et al. <ref type="bibr" target="#b33">[34]</ref> constructs a similarity graph based on block-level text detection results, characterlevel text detection results and estimated text orientations, and proposes line-level predictions via region splitting according to graph similarities.</p><p>There is nothing wrong with those mainstream frameworks, but they are not desired for neither training nor decoding. The multi-step nature of the framework implies that a text detection system of this type is not fully optimized even if each individual step is optimized. In other words, it is very difficult to tune a text detection system in an end-toend manner. Furthermore, obtaining line-level text candidates from candidates of other levels is not an easy task due to variations in both image foreground and background. It is not rare to see that authors <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38]</ref> may spend 1/4 to 2/3 of all paper pages discussing how to obtain good linelevel detection results via all kinds of post-processingranging from very simple operations like finding connected components to complicated region operations <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b18">19]</ref> like grouping, division, chaining, linkage, etc., and even more complicated operations like coarse-to-fine analysis <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b8">9]</ref>, data clustering <ref type="bibr" target="#b37">[38]</ref>, additional classifiers <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b28">29]</ref>, conditional random fields <ref type="bibr" target="#b27">[28]</ref>, etc. At the end of the day, one may spend more time on tuning post-processing than on the core "text detection" algorithm.</p><p>Instead of considering only non-text and text classes <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34]</ref>, we consider three classes, namely nontext, border and text classes, and propose a new text detection framework that self-organizes predicted text groups as text line candidates. As far as we know, this is the first attempt to tackle the text detection problem using the border class. The introduction of the new border class makes a huge difference in both training and decoding. In training, all kinds of heuristics about text lines and semantic segmentation are now implicitly encoded into the border and text classes. Thus, we not only train a text detection system in an end-to-end manner, but also avoid learning and tuning explicit post-processing related rules, classifiers, and other parameters. In testing, we can directly obtain meaningful text candidates from a predicted probability map, because borders help isolate each individual text region from the others (see examples in <ref type="figure" target="#fig_1">Fig. 2)</ref>. In this way, we achieve the self-organized text detection. In addition to this main contribution, we also make an effort to 1) collect and release a text detection PPT dataset with single-column and single-line annotations, and 2) develop a lightweight FCN supporting multi-resolution analysis for self-organized text detection. Our experimental results on the public benchmarks of ICDAR 2015 Robust Reading Competition Task 1.1 (born-digital images) and 2.1 (focused scene text), and the MSRA-TD500 dataset support that the proposed new framework outperforms the state-of-the-art approaches with heavy post-processing and parameter tuning.</p><p>The remainder of this paper is organized as follows: Sec. 2 discusses our motivations; Sec 3. proposes our newly created PPT dataset and baseline FCN; Sec. 4 shows our experimental results on public benchmarks; and we conclude our paper in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Motivation</head><p>When we analyze why so much post-processing is involved under the mainstream text detection frameworks, we notice that it is mainly to close the gap between text candidates at a desired level (line-level) and initial candidates at undesired levels (character-level, component-level or blocklevel). However, why do we have to format text candidates at a desired level from initial candidates at other levels? Why can't we directly predict text candidates at a desired line-level? Although we failed to find any answer from existing literature, we found that <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b33">34]</ref> use line-level annotations during training. Intuitively, if they train a system using line-level annotations, then the resulting system should also give line-level text candidates. Unfortunately, this is not the case: as Yao et al. stated in <ref type="bibr" target="#b33">[34]</ref>, "when several text However, two important but unanswered questions remain: why are line-level annotations not sufficient? and what can be done to resolve this problem? After several rounds of failure analysis we determined the problem was an inherent limitation of the two-class (i.e. text and nontext) settings in text detection. As seen in <ref type="figure" target="#fig_0">Fig. 1</ref>-(a), when lines are separated by sufficient space, two "text" lines on an annotated group truth map can be easily identified. When the space between two lines becomes small enough, it becomes very hard to distinguish the two lines, and two "text" lines merge to one on a corresponding annotated group as shown in <ref type="figure" target="#fig_0">Fig. 1-(b)</ref>. Hence, it is not surprising to see that adjacent text lines eventually stick together and are predicted as a single one in testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The Border Class for Text Detection</head><p>Instead of seeking additional post-processing to separate multiple text lines, we propose a new solution by introducing a third class, namely the border class, to the text detection problem. As shown in <ref type="figure" target="#fig_0">Fig. 1-(c)</ref>, two text lines will never be indistinguishable on the ground truth map after the border class is used. Moreover, visually similar blocks, i.e., those yellow blocks in <ref type="figure" target="#fig_0">Fig. 1-(c)</ref>, are now labeled differently according to physical meanings, i.e. #1: space in background (non-text), #2: space between text lines (border), and #3: space between words (text). Now we are ready to reformulate the text detection problem: Given a text image, our aim is to find all text candidates according to detected borders. <ref type="figure" target="#fig_1">Fig. 2</ref> shows sample detection results from our system, which is a fully convolutional network trained with the three-class annotations (see Sec. 3). It is clear that with the help of the border class: 1) we are able to distinguish one text candidate from the other, and 2) a text candidate is no longer restricted to only a rectangular shape, but could be an arbitrary shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Text Detection via Border Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Collection and Border Annotation</head><p>To initiate self-organized text detection, we first need enough training data. Specifically, we expect to have a dataset that is big enough (e.g., &gt; 10000 sample images) and is annotated in the format of non-text, border, and text classes. Although there are many public datasets for text detection, e.g., MSRA-TD500 <ref type="bibr" target="#b32">[33]</ref>, HUST-TR400 <ref type="bibr" target="#b31">[32]</ref>, COCO-Text <ref type="bibr" target="#b25">[26]</ref>, and so on, none of them meet our expectations. We therefore create our own dataset.</p><p>In total, our PPT dataset contains 10,692 images (72 dpi) with 93,228 text regions mixed of English and Arabic. As its name implies, all source images are converted from PowerPoint slides, which are downloaded from the Internet through the Google search API. Text regions are first automatically retrieved by using the Office 365 API for each image, and saved for later human inspection to ensure each text region is single-line and single-column.</p><p>A typical slide of aspect ratio 4:3 is of size 720×540 pixels after converting to an image. <ref type="table" target="#tab_0">Table 1</ref> summarizes the statistics of the text line height at different percentiles, where "Abs. LH" computes the actual line height of each text region in pixels; "Rel. LH" computes the relative line  height of each text region normalized by corresponding image height; and "{max/min}(LH)" computes the ratio of the largest line height to the smallest line height for all text regions within an image. Each element in the table tells the value of the top p percentile of a given variable. For example, the element 85 at the row "Abs. LH" and column Perc = 1 indicates that 1% of the entire data are of "Abs. LH"-above or equal to 85 pixels.</p><p>To effectively learn borders, we also annotate border pixels for each text region by 1) estimating the original text region line heightĥ, and 2) marking the outer ribbon-like region of width c ·ĥ as text borders. For each slide image in our dataset, we then generate a "target" of the three classes, i.e., non-text, border, and text. This completes our PPT dataset. <ref type="figure" target="#fig_2">Fig. 3</ref> shows samples of the PPT dataset and automatically generated target images (border width coefficient c = 15%); this entire dataset can be downloaded from https://gitlab.com/ rex-yue-wu/ISI-PPT-Dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Baseline Training</head><p>We partition the PPT dataset into training (90%), validation (5%), and testing (5%) sets. Instead of choosing a popular architecture inspired by a different problem, e.g. ImageNet object classification, we develop our own lightweight architecture as shown in <ref type="figure" target="#fig_5">Fig. 4</ref>, which is a simple feed-forward network with targets defined as the probability map of the three classes, i.e., non-text, border, and text. The reason why a lightweight architecture is still capable of text detection is that text is, in general a simpler class than various physical object classes like cat and dog, in the sense that it often has a homogeneous foreground and surroundings. Both single and multi-resolution baselines are implemented in Theano and trained w.r.t. the cross-entropy loss.</p><p>With regard to data argumentation, we use 1) random resize for a ratio in o ]. We use the adadelta optimizer and batch size 16 in training, and the single resolution baseline model gets converged after about 150 epochs. Once we obtain the single resolution baseline model, we plug it into our multi-resolution baseline model and continue training until convergence. This multiresolution baseline predicts non-text, border, and text probability maps on four scales and fuses all maps together. In this way, we handle the font size variations in the PPT dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Self-Organized Text Detector</head><p>One salient advantage of the proposed method is that we predict non-text, border and text. The knowledge of borders is very important for two reasons: 1) border separates close-by regions, and 2) the concurrence of both text class and border class could help further improve text region prediction. As one can see from <ref type="figure" target="#fig_1">Fig. 2</ref>, a predicted non-text, border, and text probability map has already clearly identified text regions and separated near-by regions via borders. Therefore, we could obtain reasonably well text candidates with minimal postprocessing, namely simply analyzing connected components. Alg. 1 shows an example of how to obtain horizontal rectangular bounding boxes from a predicted probability map, but one can easily modify it to predict text bounding boxes in other forms, e.g. an oriented bounding box. Our pretrained models are available at https://gitlab.com/rex-yue-wu/ ISI-PPT-Text-Detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">The Effect of Border Class in Text Detection</head><p>To understand the effect of the proposed border class in text detection, we evaluate models trained with and without using the border class. Specifically, besides the two proposed models shown in <ref type="figure" target="#fig_5">Fig. 4</ref>, we train two additional models: 1) a single-resolution FCN variant without border, which adopts the exact same architecture as the single resolution FCN, except for changing the last layer from predicting three channels with softmax activation to predicting only one channel with sigmoid activation to determine text or non-text; and 2) a state-of-the-art text-block FCN <ref type="bibr" target="#b41">[42]</ref> which does not use the border class. All four models share the same training, validation and testing dataset. Model performance is evaluated by the widely used Wolf's object detection evaluation method <ref type="bibr" target="#b29">[30]</ref> with default parameters 1 as shown in <ref type="table" target="#tab_1">Table 2</ref>. Decoding speed is estimated based on the NVidia TitanX GPU and the Intel Xeon CPU E5-2695 v2 @2.4GHz. From Table 2, it is clear that 1) using a lightweight architecture (2% of <ref type="bibr" target="#b41">[42]</ref>'s #param.) greatly improves decoding speed by 2x while not trading off detection quality (see row 1-2); 2) the border class helps better localize a text region and thus largely improve text detection by 0.27 in F-score (see row 2-3); and 3) multi-resolution analysis further boosts detection performance to 0.96 in F-score (see row 3-4). Qualitative results can be found in <ref type="figure" target="#fig_6">Fig. 5</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>To validate the performance of the proposed new framework, we evaluate our method on the latest ICDAR 2015 Robust Reading Competition dataset, 2 namely Task 1.1 (born-digital images), and Task 2.1 (focused scene text), and the oriented scene text dataset MSRA-TD400 <ref type="bibr" target="#b32">[33]</ref>.</p><p>ICDAR 2015 Task 1.1 dataset contains images of born-digital e.g. online advertisement images. It was first introduced in the ICDAR 2011 Robust Reading Competition <ref type="bibr" target="#b20">[21]</ref>. In total, it contains 551 images, where 410 images belong to the training set. IC-DAR 2015 Task 2.1 dataset (also known as ICDAR 2013 dataset <ref type="bibr" target="#b12">[13]</ref>) is composed of "focused scene text" images, whose text regions are placed nearly frontal and horizontal. The total number of images in this dataset is 462, and 229 belong to the training set. MSRA-TD500 was introduced by <ref type="bibr" target="#b32">[33]</ref> and contains 500 images with scene-text (English and Chinese) of different orientations.</p><p>Although the proposed border learning method is also applicable to word-level text detection, which is the ground truth files' format of the used ICDAR datasets, we use synthesized line-level annotation instead for two reasons: 1) the provided horizontal rectangular boxes for rotated texts may mix non-text regions and thus are inaccurate for border learning; 2) our pretrained models use on the line-level annotation, and it is better to finetune it to a dataset with the same annotation type than a different one. Specifically, we synthesize a new version of line-level annotation for each training image by simply merging word-level annotations if they are on the same horizontal line. Human inspection is involved to verify and correct the automatically generated line-level annotations. It is worth noting that an oriented text region in this dataset is originally treated as a horizontal one (see <ref type="figure" target="#fig_7">Fig. 6-left)</ref>, but we manually correct it to a polygon covering the oriented text region. <ref type="figure" target="#fig_7">Fig. 6</ref> shows the difference between the original ICDAR 2015 Task 1.1 (borndigital images ) annotation and the variant we used for training. This line-level annotation can be provided upon request. Finally, we apply the same trick as we did in the baseline training to obtain annotations of the border class. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Fine-Tuning Settings</head><p>Fine-tuning is performed with respect to each individual dataset. We reuse all settings in the multi-resolution baseline training, except that we resize a training image when its size is too large. Specifically, we keep all images whose dimensions are below 540×720 unchanged, and only resize images whose dimensions are larger than 540×720 (720×540), while keeping an image's original aspect ratio. The reasons why we apply resizing to those very large images are 1) in most cases 540×720 is large enough for text detection; and 2) a large size image may crash the entire training process because the required GPU memory is linearly dependent on the size of a training sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Protocol and Decoding Settings</head><p>Both ICDAR 2015 Task 1.1 (born-digital images) and 2.1 (focused scene text) are evaluated through the online system of the ICDAR 2015 Robust Reading Competition, which uses the Wolf's object detection evaluation method <ref type="bibr" target="#b29">[30]</ref>. Precision and recall are computed with respect to the best matches between one-to-one, one-to-many, and many-to-one matches. More details about this online evaluation system can be found in <ref type="bibr" target="#b11">[12]</ref>. For the MSRA-TD500 dataset, we follow the evaluation protocol employed by <ref type="bibr" target="#b32">[33]</ref>, which considers both the area overlapping ratios and the orientation differences between hypothesis and reference.</p><p>It is worth noting that ICDAR and MSRA-TD500 datasets expect a hypothesis text box in the format of horizontal rectangular and oriented rectangular, respectively. We therefore parse a detected text bounding box from a predicted probability map by finding a smallest horizontal/oriented rectangular (see <ref type="figure" target="#fig_8">Fig. 7</ref>). <ref type="figure" target="#fig_8">Fig. 7</ref> shows detection results of the proposed self-organized text detection method after finetuning our pretrained models on all three datasets. As one can see, the proposed self-organized text detector not only works well on those images with a nearhomogeneous text background, but also those images with complicated background and foreground. Even for very difficult cases, e.g., images with a large perspective change (see CHINA POST in <ref type="figure" target="#fig_8">Fig. 7-(c)</ref>), and images with curved texts (see VALUT and Chocome in <ref type="figure" target="#fig_8">Fig. 7-(b)</ref>), and bing and COSTA COFFEE in <ref type="figure" target="#fig_8">Fig. 7-(c)</ref>), the proposed detector works reasonably well. <ref type="table" target="#tab_2">Table 3</ref> compares the performance of the proposed selforganized text detector on the three testing datasets with the stateof-the-art methods, where * denotes methods relying on no extra external training data or pretrained weights. As one can see, the proposed self-organized text detector achieves better or comparable performance on all three testing datasets compared to recent peer algorithms. It is worth noting that all peer methods listed in the table have their own post-processing to group small regions, split a big region, or reject a text line etc., but we do not apply any post-processing except for finding the smallest rectangle for a given self-organized text region predicted on a probability map. We find that the ICDAR online evaluation metric <ref type="bibr" target="#b29">[30]</ref> unfairly penalizes our system, primarily due to the fact that our system outputs line-level bounding boxes, and the ICDAR online evaluation metric uses word-level annotations. This results in poor scores for what are qualitatively very good outputs. <ref type="figure" target="#fig_9">Fig. 8</ref> highlights some failure cases for the ICDAR evaluation metric. The middle row of images in each pane is our system's output probability map, and the upper and lower rows are the recall and precision visualizations taken directly from ICDAR's online evaluation tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results</head><p>Colors mean different things in each image: for our output probability map, blue means a region without text, red means a region with text, and green means a border region. In the ICDAR metric visualizations, red means either false negative or false positive, green means true positive, and yellow and blue indicate true positive under the rule of many-to-one or one-to-many matching. A more detailed explanation of this online system can be found at http://rrc.cvc.uab.es/.</p><p>The main take away from this figure is that: in all cases our system correctly detects line boundaries, but is nonetheless penal- ized by the evaluation metric. For instance, in <ref type="figure" target="#fig_9">Fig. 8-(a)</ref> our precision is hurt due to the large amount of space between some of the words; in <ref type="figure" target="#fig_9">Fig. 8-(b)</ref> we correctly determine the diagonal line, but because the evaluation metric expects three separate non-rotated rectangles for each word, both our precision and recall are penalized. The lower portion of <ref type="figure" target="#fig_9">Fig. 8</ref> shows the original online evaluation scores and corrected scores after manual evaluation of these samples. As one can see, the F-score difference (far-right column) between the online evaluation and the manual evaluation is huge. This indicates that our actual performance is even better than that shown in <ref type="table" target="#tab_2">Table 3</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Failure Cases</head><p>Although the proposed text detector works well on the majority of testing images, we find several failure cases. <ref type="figure" target="#fig_10">Fig. 9</ref> shows the three types of failures that we found. It is worth noting that texts in <ref type="figure" target="#fig_10">Fig. 9</ref>-(c) should be vertically grouped according to word meanings. However, this kind of grouping is rarely seen in training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Beyond the Border</head><p>The border class provides more information than just barriers to separate adjacent text candidates. Specifically, 1) border and text have a strong co-occurrence relationship, and this can be used to further improve the accuracy of text detection; and 2) border shape is closely related to the contour of a text region, and thus it can used to rectify a text region. Though neither is used in this paper, they can be used to further improve text detection accuracy and are very important and useful for text recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we analyze why previous efforts failed to distinguish adjacent text groups in the text detection problem, and we show that this issue can be solved by introducing the new border class. To fully validate this conceptual solution, we first created a new text detection dataset with the three-class annotations, i.e., non-text, border, and text, and proposed a new lightweight multiresolution FCN for text detection. We demonstrated that text detection models with the border class outperform those without the border class by a large margin (see <ref type="table" target="#tab_1">Table 2</ref>). Our extensive experiments on external benchmarks further indicate that we can effectively detect text lines via border learning. This is a consummate improvement compared to all previous methods that require heavy post-processing to decide when to merge, split, or reject a detected text candidate. The proposed solution is applicable to other-levels of text detection, e.g. word-level, and also reveals the possibility of using the "border" class to distinguish different object instances in a more general image segmentation/localization problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Why are non-text and text settings insufficient for text line segmentation? Color encodes different classes: non-text, border, and text. (a) two-class annotation with well separated lines; (b) two-class annotation with very close lines; (c) three-class annotation with very close lines. Yellow shaded blocks indicate space regions of different semantic meanings (see text body).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Sample results of our self-organized text detection. Upper row: original images; middle row: predicted three-class probability map (non-text, border, and text); and lower row: rectangular bounding boxes drawn for each text line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Samples images (upper row) and annotations (lower row) of the PPT dataset, where target colors encode the probability map of classes non-text, border, and text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>[0.25, 1.25]; 2) random color shift in [−64, 64]; 3) random negation; and 4) random rotation for an angle within [−25 o , +25</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1 :</head><label>1</label><figDesc>Rectangular text region decoder. 1 Function RTRD(Y): Input : Y, a predicted probability map of non-text, border and text. Output: L, a list of decoded text bounding boxes 2 for each pixel in Y, decide its membership.; 3 obtain a mask M by assigning 1s to pixels belong to the text class and 0s otherwise; 4 initialize L to a empty list; 5 for r ∈ connected component of M do 6 find out text coordinates {(x, y)|(x, y) ∈ r}; 7 top ← min{y ∈ r}, bot ← max{y ∈ r}; 8 left ← min{x ∈ r}, right ← max{x ∈ r}; 9 height ← bot-top, width ← right-left; 10 relax this rectangular box according to the border coefficient c used in training data; 11 append this relaxed rectangular box to L 12 end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Fully convolutional networks used in the proposed method. Left: single resolution FCN. Right: multi-resolution FCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Effectiveness of the border class for text detection. (a), (b), and (c) are decoding results on the testing PPT dataset using models of single-res. w/o border, single-res. w/ border, and multires. w/ border, respectively. See differences in arrowed regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Original ICDAR 2015 Task 1.1 (born-digital images) word-level annotation (left) and our modified line-level annotation (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Probability maps and decoded results of the proposed self-organized text detection on testing dataset. (a) ICDAR 2015 Robust Reading Competition Task 1.1 (born-digital images); (b) ICDAR 2015 Robust Reading Competition Task 2.1 (focused scene text); and (c) MSRA-TD500 dataset. Colors in a probability map represent non-text, border, and text, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Samples of underestimated performance. This underestimate is due to use of the online evaluation tool which unfairly penalizes word spacing. Details of underestimation for each sample are shown in the above figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Failure cases. (a) over-segmentation; (b) undersegmentation; and (c) incorrect semantic grouping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Line height (LH) statistics of PPT dataset.</figDesc><table>Name \ 

Perc. 

1 10 20 30 40 50 60 70 80 90 99 
Abs. LH (pixel) 
85 57 49 44 40 37 34 32 29 24 
9 
Rel. LH (%) 
11.55 7.78 6.56 6.00 5.48 5.11 4.78 4.11 3.96 3.37 1.56 
{max/min}(LH) 27.69 3.46 2.24 1.85 1.63 1.49 1.37 1.28 1.20 1.13 1.00 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Performance comparisons on PPT testing Dataset Models Prec. Recall F-score #Param.(M) sec/img (GPU/CPU)</figDesc><table>Text-Block FCN[42] 0.63 0.67 
0.65 
14.71 
0.55 / 14.7 
single-res. w/o border 0.61 0.69 
0.65 
0.27 
0.24 / 7.2 
single-res. w/ border 0.91 0.94 
0.92 
0.27 
0.25 / 7.4 
multi-res. w/ border 0.94 0.97 
0.96 
0.28 
0.32 / 9.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Performance comparisons between the proposed approach and state-of-the-art methods.</figDesc><table>Algorithm 
Precision 
Recall 
F-Score 
ICDAR 2015 RRT 1.1 (born-digital images) 
[35] 
0.96 
0.91 
0.93 
*[3] 
0.92 
0.86 
0.89 
[5] 
0.95 
0.91 
0.93 
*[39] 
0.94 
0.87 
0.90 
*[4] 
0.92 
0.89 
0.90 
Ours 
0.91 
0.95 
0.93 
ICDAR 2015 RRT 2.1 (focused scene text) 
[38] 
0.84 
0.65 
0.73 
*[40] 
0.84 
0.65 
0.73 
*[19] 
0.82 
0.71 
0.76 
*[1] 
0.84 
0.69 
0.77 
[41] 
0.88 
0.74 
0.80 
*[29] 
0.84 
0.77 
0.80 
*[24] 
0.85 
0.76 
0.80 
[5] 
0.86 
0.79 
0.82 
[42] 
0.88 
0.78 
0.83 
[34] 
0.89 
0.80 
0.84 
Ours 
0.91 
0.78 
0.84 
MSRA-TD500 
[33] 
0.63 
0.63 
0.63 
[11] 
0.71 
0.62 
0.61 
*[39] 
0.71 
0.61 
0.66 
[38] 
0.81 
0.63 
0.71 
[42] 
0.83 
0.67 
0.74 
[34] 
0.76 
0.75 
0.76 
Ours 
0.77 
0.78 
0.77 

4.5. Discussions 

4.5.1 Imperfect Matching Metrics 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>.Sample Precision Recall F-Score Precision Recall F-Score Difference</figDesc><table>Online Evaluation 
Manual Evaluation 
F-score. 
(a) 
0.33 
0.67 
0.44 
1.00 
1.00 
1.00 
0.56 
(b) 
0.50 
0.67 
0.57 
1.00 
1.00 
1.00 
0.43 
(c) 
0.43 
0.52 
0.47 
1.00 
1.00 
1.00 
0.53 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://liris.cnrs.fr/christian.wolf/software/ deteval/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://rrc.cvc.uab.es/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fastext: Efficient unconstrained scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust text detection in natural images with edge-enhanced maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2609" to="2612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient text localization in born-digital images by local contrast-based segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (IC-DAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="291" to="295" />
		</imprint>
	</monogr>
	<note>13th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Effective candidate component extraction for text localization in born-digital images by combining text contours and stroke interior regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis Systems (DAS), 2016 12th IAPR Workshop on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="352" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Canny text detector: Fast and robust scene text localization algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting text in natural scenes with stroke width transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Epshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2963" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A robust algorithm for text string separation from mixed text/graphics images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kasturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="910" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Accurate text localization in natural image with cascaded convolutional text network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09423</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Orientation robust text line detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4034" to="4041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (IC-DAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>13th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Icdar 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4161" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scale based region growing for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM International Conference on Multimedia, MM &apos;13</title>
		<meeting>the 21st ACM International Conference on Multimedia, MM &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1007" to="1016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image text detection using a bandlet-based edge detector and stroke width transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mosleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bouguila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Hamza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Real-time scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Real-time lexicon-free scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1872" to="1885" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic separation of text, graphic and picture segments in printed material</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Scherl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fuchsberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition in Practice</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="213" to="221" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Icdar 2011 robust reading competition challenge 2: Reading text in scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 international conference on document analysis and recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1491" to="1496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scene text detection using graph model built upon maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust seed-based stroke width transform for text detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR), 2015 13th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="916" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Text flow: A unified text detection system in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lim Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="56" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Coco-text: Dataset and benchmark for text detection and recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1601.07140</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Block segmentation and text extraction in mixed text/image documents. Computer graphics and image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Wahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Casey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="375" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Natural scene text detection with multi-layer segmentation and higher order conditional random field based analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="41" to="47" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Text detection in scene images based on exhaustive segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object count/area graphs for the evaluation of object detection and segmentation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Jolion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Document Analysis and Recognition (IJDAR)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient exploration of text regions in natural scene images using adaptive image sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Z</forename><surname>Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="427" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A unified framework for multioriented text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4737" to="4749" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1083" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Scene text detection via holistic, multi-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<idno>abs/1606.09002</idno>
		<imprint>
			<date type="published" when="2006" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.09207</idno>
		<title level="m">Incidental scene text understanding: Recent progresses on icdar 2015 robust reading competition challenge 4</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rotation-invariant features for multi-oriented text detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">70173</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Text detection and recognition in imagery: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1480" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Multiorientation scene text detection with adaptive clustering. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Hao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1930" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust text detection in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="970" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scene text localization using edge analysis and feature pool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="page" from="652" to="661" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Symmetry-based text line detection in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2558" to="2567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-oriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Extracting text from www images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopresti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on</title>
		<meeting>the Fourth International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="248" to="252" />
		</imprint>
	</monogr>
	<note>Document Analysis and Recognition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
