<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Input Ground Truth Source Images Predicted Splice Mask Ground Truth Mask</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Huh</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">UC</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Input Ground Truth Source Images Predicted Splice Mask Ground Truth Mask</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>2 Code and additional results can be found on our website. 2 Huh et al. EXIF CameraMake: NIKON CORPORATION EXIF CameraModel: NIKON D5300 EXIF ColorSpace: sRGB EXIF DateTimeOriginal: 2016:09:13 16:58:26 EXIF ExifImageLength: 3947 EXIF ExifImageWidth: 5921 EXIF Flash: No EXIF FocalLength: 31.0mm EXIF WhiteBalance: Auto EXIF CompressedBitsPerPixel: 2 … EXIF CameraMake: EASTMAN KODAK COMPANY EXIF CameraModel: KODAK EASYSHARE CX7300… EXIF ColorSpace: sRGB EXIF DateTimeOriginal: 2005:09:29 01:31:02 EXIF ExifImageLength: 1544 EXIF ExifImageWidth: 2080 EXIF Flash: No (Auto) EXIF FocalLength: 5.9mm EXIF WhiteBalance: Auto EXIF CompressedBitsPerPixel: 181/100 …</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual forensics</term>
					<term>image splicing</term>
					<term>self-supervised learning</term>
					<term>EXIF * Indicates equal contribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>: Our algorithm learns to detect and localize image manipulations (splices), despite being trained only on unmanipulated images. The two input images above might look plausible, but our model correctly determined that they have been manipulated because they lack self-consistency: the visual information within the predicted splice region was found to be inconsistent with the rest of the image. IMAGE CREDITS: automatically created splice from Hays and Efros [1] (top), manual splice from Reddit user /u/Name-Albert Einstein (bottom).</p><p>Abstract. Advances in photo editing and manipulation tools have made it significantly easier to create fake imagery. Learning to detect such manipulations, however, remains a challenging problem due to the lack of sufficient amounts of manipulated training data. In this paper, we propose a learning algorithm for detecting visual image manipulations that is trained only using a large dataset of real photographs. The algorithm uses the automatically recorded photo EXIF metadata as supervisory signal for training a model to determine whether an image is self-consistent -that is, whether its content could have been produced by a single imaging pipeline. We apply this self-consistency model to the task of detecting and localizing image splices. The proposed method obtains state-ofthe-art performance on several image forensics benchmarks, despite never seeing any manipulated images at training. That said, it is merely a step in the long quest for a truly general purpose visual forensics tool.</p><p>Keywords: Visual forensics, image splicing, self-supervised learning, EXIF * Indicates equal contribution. Code and additional results can be found on our website.  <ref type="figure">Fig. 2</ref>: Anatomy of a splice: One of the most common ways of creative fake images is splicing together content from two different real source images. The insight explored in this paper is that patches from a spliced image are typically produced by different imaging pipelines, as indicated by the EXIF meta-data of the two source images. The problem is that in practice, we never have access to these source images at test time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Malicious image manipulation, long the domain of dictators <ref type="bibr">[?]</ref> and spy agencies, has now become accessible to legions of common Internet trolls and Facebook conmen <ref type="bibr" target="#b1">[2]</ref>. With only rudimentary editing skills, it is now possible to create realistic image composites <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, fill in large image regions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, generate plausible video from speech <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, etc. One might have hoped that these new methods for creating synthetic visual content would be met with commensurately powerful techniques for detecting fakes, but this has not been the case so far.</p><p>One problem is that standard supervised learning approaches, which have been very successful for many types of detection problems, are not well-suited for image forensics. This is because the space of manipulated images is so vast and diverse, that it is rather unlikely we will ever have enough manipulated training data for a supervised method to fully succeed. Indeed, detecting visual manipulation can be thought of as an anomaly detection problem -we want to flag anything that is "out of the ordinary," even though we might not have a good model of what that might be. In other words, we would like a method that does not require any manipulated training data at all, but can work in an unsupervised/self-supervised regime.</p><p>In this work, we turn to a vast and previously underutilized source of data, image EXIF metadata. EXIF tags are camera specifications that are digitally engraved into an image file at the moment of capture and are ubiquitously available. Consider the photo shown in <ref type="figure">Figure 2</ref>. While at first glance it might seem authentic, we see on closer inspection that a car has been inserted into the scene. The content for this spliced region came from a different photo, shown on the right. Such a manipulation is called an image splice, and it is one of the most common ways of creating visual fakes. If we had access to the two source photographs, we would see from their EXIF metadata that there are a number of differences in the imaging pipelines: one photo was taken with an Nikon camera, the other with a Kodak camera; they were shot using different focal lengths, and saved with different JPEG quality settings, etc. Our insight is that one might be able to detect spliced images because they are composed of regions that were captured with different imaging pipelines. Of course, in forensics applications, we do not have access to the original source images nor, in general, the fraudulent photo's metadata.</p><p>Instead, in this paper, we propose to use the EXIF metadata as a supervisory signal for training a classification model to determine whether an image is self-consistentthat is, whether different parts of the same image could have been produced by a single imaging pipeline. The model is self-supervised in that only real photographs and their EXIF meta-data are used for training. A consistency classifier is learned for each EXIF tag separately using pairs of photographs, and the resulting classifiers are combined together to estimate self-consistency of pairs of patches in a novel input image. We validate our approach using several datasets and show that the model performs better than the state-of-the-art -despite never having seen annotated splices or using handcrafted detection cues.</p><p>The main contributions of this paper are: 1) posing image forensics as a problem of detecting violations in learned self-consistency (a kind of anomaly detection), 2) proposing photographic metadata as a free and plentiful supervisory signal for learning self-consistency, 3) applying our self-consistency model to detecting and localizing splices. We also introduce a new dataset of image splices obtained from the internet, and experimentally evaluate which photographic metadata is predictable from images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Over the years, researchers have proposed a variety of visual forensics methods for identifying various manipulations <ref type="bibr" target="#b1">[2]</ref>. The earliest and most thoroughly studied approach is to use domain knowledge to isolate physical cues within an image. Drawing upon techniques from signal processing, previous methods focused on cues such as misaligned JPEG blocks <ref type="bibr" target="#b9">[10]</ref>, compression quantization artifacts <ref type="bibr" target="#b10">[11]</ref>, resampling artifacts <ref type="bibr" target="#b11">[12]</ref>, color filtering array discrepancies <ref type="bibr" target="#b12">[13]</ref>, and camera-hardware "fingerprints" <ref type="bibr" target="#b13">[14]</ref>. We take particular inspiration from recent work by Agarwal and Farid <ref type="bibr" target="#b14">[15]</ref>, which exploits a seemingly insignificant difference between imaging pipelines to detect spliced image regions -namely, the way that different cameras truncate numbers during JPEG quantization. While these domain-specific approaches have proven to be useful due to their easy interpretability, we believe that the use of machine learning will open the door to discovering many more useful cues while also producing more adaptable algorithms.</p><p>Indeed, recent work has moved away from using a priori knowledge and toward applying end-to-end learning methods for solving specific forensics tasks using labeled training data. For example, Salloum et al. <ref type="bibr" target="#b15">[16]</ref> propose learning to detect splices by training a fully convolutional network on labeled training data. These learning methods have also been applied to the problem of detecting specific tampering cues, such as double-JPEG compression <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and contrast enhancement <ref type="bibr" target="#b18">[19]</ref>. The most closely related of these methods to ours is perhaps Bondi et al. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. This work recognizes camera models from image patches, and proposes to use inconsistencies in camera predictions to detect tampering. Another common forensics strategy is to train models on a small class of automatically simulated manipulations, like face-swapping <ref type="bibr" target="#b21">[22]</ref> or splicing with COCO segmentation masks <ref type="bibr" target="#b22">[23]</ref>. In addition, <ref type="bibr" target="#b21">[22]</ref> propose identifying face swaps by measuring image inconsistencies introduced from splicing and blurring. In concurrent work, Mayer <ref type="bibr" target="#b23">[24]</ref> proposed using a Siamese network to predict whether pairs of image patches have the same camera model -a special case of our metadata consistency model (they also propose using this model for splice detection; while promising, these results are very preliminary). There has also been work that estimates whether a photo's semantic content (e.g., weather) matches its metadata <ref type="bibr" target="#b24">[25]</ref>.</p><p>In our work, we seek to further reduce the amount of information we provide to the algorithm by having it learn to detect manipulations without ground-truth annotations. For this, we take inspiration from recent works in self-supervision <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref> which train models by solving tasks solely defined using unlabeled data. Of these, the most closely related approach is that of Doersch et al. <ref type="bibr" target="#b26">[27]</ref>, in which they trained a model to predict the relative position of pairs of patches within an image. Surprisingly, the authors found that their method learned to utilize very subtle artifacts like chromatic lens aberration as a shortcut for learning the task. While imaging noise was a nuisance in their work, it is a useful signal for us -our self-supervised algorithm is designed to learn about properties of the imaging pipeline while ignoring semantics. Our technical approach is also similar to <ref type="bibr" target="#b31">[32]</ref>, which trains a segmentation model using self-supervision to predict whether pairs of patches co-occur in space or time.</p><p>Individual image metadata tags, such as focal length, GPS, hashtags, etc. have long been employed in computer vision as free supervisory signal. A particularly creative use of EXIF metadata was demonstrated by Kuthirummal et al. <ref type="bibr" target="#b32">[33]</ref>, who used the CameraModel tag of a very large image collection to compute per-camera priors such as their non-linear response functions.</p><p>Our work is also related to the anomaly detection problem. Unlike traditional visual anomaly detection work, which is largely concerned with detecting unusual semantic events like the presence of rare objects and actions <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>, our work needs to find anomalies in photos whose content is designed to be plausible enough to fool humans. Therefore the anomalous cues we search for should be imperceptible to humans and invariant to the semantics of the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Photographic Self-consistency</head><p>Our model works by predicting whether a pair of image patches are consistent with each other. Given two patches, P i and P j , we estimate the probabilities x 1 , x 2 , ..., x n that they share the same value for each of n metadata attributes. We then estimate the patches' overall consistency, c ij , by combining our n observations of metadata consistency. At evaluation time, our model takes a potentially manipulated test image and measures the consistency between many different pairs of patches. A low consistency score indicates that the patches were likely produced by two distinct imaging systems, suggesting that they originate from different images. Although the consistency score for any single pair of patches will be noisy, aggregating many observations provides a reasonably stable estimate of overall image self-consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Predicting EXIF Attribute Consistency</head><p>We use a Siamese network to predict the probability that a pair of 128 × 128 image patches shares the same value for each EXIF metadata attribute. We train this network with image patches randomly sampled from 400, 000 Flickr photos, making predictions on all EXIF attributes that appear in more than 50, 000 photos (n = 80, the full list of attributes can be found in supplementary files). For a given EXIF attribute, we discard EXIF values that occur less than 100 times. The Siamese network uses shared ResNet-50 <ref type="bibr" target="#b35">[36]</ref> sub-networks which each produce 4096-dim. feature vectors. These vectors are concatenated and passed through four-layer MLP with 4096, 2048, 1024 units, followed by the final output layer. The network predicts the probability that the images share the same value for each of the n metadata attributes.</p><p>We found that training with random sampling is challenging because: 1) there are some rare EXIF values that are very difficult to learn, and 2) randomly selected pairs of images are unlikely to have consistent EXIF values by chance. Therefore, we introduce two types of re-balancing: unary and pairwise. For unary re-balancing, we oversample rare EXIF attribute values (e.g. rare camera models). When constructing a mini-batch, we first choose an EXIF attribute and uniformly sample an EXIF value from all possible values of this attribute. For pairwise re-balancing, we make sure that pairs of training images within a mini-batch are selected such that for a given EXIF attribute, half the batch share that value and half do not.</p><p>Analysis. Although we train on all common EXIF attributes, we expect the model to excel at distinguishing ones that directly correlate to properties of the imaging pipeline such as LensMake <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b19">20]</ref>. In contrast, arbitrary attributes such as the exact date an image was taken (DateTimeOriginal) leave no informative cues in an image. In order to identify predictive metadata, we evaluated our EXIF-consistency model on a dataset of 50K held-out photos and report the individual EXIF attribute accuracy in <ref type="figure">Figure 4</ref> (chance is 50% due to rebalancing).</p><p>Our model obtains high accuracy when predicting the consistency of attributes closely associated with the image formation process such as LensMake, which contains values such as Apple and FUJIFILM. But more surprisingly, we found that the most predictable attribute is UserComment. Upon further inspection, we found that UserComment is a generic field that can be populated with arbitrary data, and that its most frequent values were either binary strings embedded by camera manufacturers or </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Post-processing Consistency</head><p>Many image manipulations are performed with the intent of making the resulting image look plausible to the human eye: spliced regions are resized, edge artifacts are smoothed, and the resulting image is re-JPEGed. If our network could predict whether two patches are post-processed differently, then this would be compelling evidence for photographic inconsistency. To model post-processing consistency, we add three augmentation operations during training: re-JPEGing, Gaussian blur, and image resizing. Half of the time, we apply the same operations to both patches; the other half of the time, we apply different operations. The parameters of each operation are randomly chosen from an evenly discretized set of numbers. We introduce three additional classification tasks (one per augmentation type) that are used to train the model to predict whether a pair of patches received the same parameterized augmentation. This increases the number of binary attributes we predict from 80 to 83. Since the order of the post-processing operations matters, we apply them in a random order each time. We note that this form of inconsistency is orthogonal to EXIF consistency. For example, in the (unlikely) event that a spliced region had exactly the same metadata as the image it was inserted into, the splice could still be detected by observing differences in post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combining Consistency Predictions</head><p>Once we have predicted the consistency of a pair of patches for each of our EXIF (plus post-processing) attributes, we would like to estimate the pairs' overall consistency c ij . If we were solving a supervised task, then a natural choice would be to use spliced regions as supervision to predict, from the n EXIF-consistency predictions, the probability that the two patches belong to different regions. Unfortunately, we do not have spliced images to train on. Instead, we use a self-supervised proxy task: we train a simple classifier to predict, from the EXIF consistency predictions, whether the patches come from the same image. More specifically, consider the 83-dimensional vector x of EXIF consistency predictions for a pair of patches i and j. We estimate the overall consistency between the patches as c ij = p θ (y | x) where p θ is a two-layer MLP with 512 hidden units. The network is trained to predict whether i and j come from the same training image (i.e. y = 1 if they're the same; y = 0 if they're different). This has the effect of calibrating the different EXIF predictions while modeling correlations between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Directly Predicting Image Consistency</head><p>An alternative to using EXIF metadata as a proxy for determining consistency between two image patches is to directly predict whether the two patches come from the same image or not. Such a model could be easily trained with pairs of patches randomly sampled from the same or different images. In principle, such a model should work at least as well as the EXIF one, and perhaps better, since it could pick up on differences between images not captured by any of the EXIF tags. In practice, however, such a model would need to be trained on vast amounts of data, because most random patches coming from different images will be easy to detect with trivial cues. For example, the network might simply learn to compare patch color histograms, which is a surprisingly powerful cue for same/different image classification task <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b31">32]</ref>. To evaluate the performance of this model in practice, we trained a Siamese network, similar in structure to the EXIF-consistency model (Section 3.1), to solve the task of same-or-different image consistency (see Image-Consistency in the Results section).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">From Patch Consistency to Image Self-Consistency</head><p>So far we have introduced models that can measure some form of consistency between pairs of patches. In order to transform this into something usable for detecting splices, we need to aggregate these pairwise consistency probabilities into a global selfconsistency score for the entire image.</p><p>Given an image, we sample patches in a grid, using a stride such that the number of patches sampled along the longest image dimension is 25. This results in at most 625 patches (for the common 4:3 aspect ratio, we sample 25 × 18 = 450 patches). For a given patch, we can visualize a response map corresponding to its consistency with every other patch in the image. To increase the spatial resolution of each response map, we average the predictions of overlapping patches. If there is a splice, then the majority of patches from the untampered portion of the image will ideally have low consistency with patches from the tampered region <ref type="figure" target="#fig_0">(Figure 6c</ref>).</p><p>To produce a single response map for an input image, we want to find the most consistent mode among all patch response maps. We do this mode-seeking using Mean Shift <ref type="bibr" target="#b37">[38]</ref>. The resulting response map naturally segments the image into consistent and inconsistent regions <ref type="figure" target="#fig_0">(Figure 6d</ref>). We call the merged response map a consistency map. We can also qualitatively visualize the tampered image region by clustering the affinity matrix, e.g. with Normalized Cuts <ref type="bibr" target="#b38">[39]</ref>.</p><p>To help understand how different EXIF attributes vary in their consistency predictions, we created response maps for each tag for an example image <ref type="figure" target="#fig_1">(Figure 7</ref>). While the individual tags provide a noisy consistency signal, the merged response map accurately localizes the spliced region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We evaluate our models on two closely related tasks: splice detection and splice localization. In the former, our goal is to classify images as being spliced vs. authentic. In the latter, the goal is to localize the spliced regions within an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Benchmarks</head><p>We evaluate our method on five different datasets. This includes three existing datasets: the widely used Columbia dataset <ref type="bibr" target="#b39">[40]</ref>, which consists of 180 relatively sim- ple splices, and two more challenging datasets, Carvalho et al. <ref type="bibr" target="#b40">[41]</ref> (94 images) and Realistic Tampering [42] (220 images), which combine splicing with post-processing operations. The latter also includes other tampering operations, such as copy-move. One potential shortcoming of these existing datasets is that they were created by a small number of artists and may not be representative of the variety of forgeries encountered online. To address this issue, we introduce a new In-the-Wild forensics dataset that consists of 201 images scraped from THE ONION, a parody news website (i.e. fake news), and REDDIT PHOTOSHOP BATTLES, an online community of users who create and share manipulated images (which has been used in other recent forensics work <ref type="bibr" target="#b41">[43]</ref>). Since ground truth labels are not available for internet splices, we annotated the images by hand to obtain approximate ground truth (using the unmodified source images as reference when they were available).</p><p>Finally, we also want to evaluate our method on automatically-generated splices. For this, we used the scene completion data from Hays and Efros <ref type="bibr" target="#b0">[1]</ref>, which comes with inpainting results, masks, and source images for a total of 55 images. We note that the ground-truth masks are only approximate, since the scene completion algorithm may alter a small region of pixels outside the mask in order to produce seamless splices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons</head><p>We compared our model with three methods that use image processing techniques to detect specific imaging artifacts: Color Filter Array (CFA) <ref type="bibr" target="#b42">[44]</ref> detects artifacts in color pattern interpolation; JPEG DCT <ref type="bibr" target="#b43">[45]</ref> detects inconsistencies over JPEG coefficients; and Noise Variance (NOI) <ref type="bibr" target="#b44">[46]</ref> detects anomalous noise patterns using wavelets. We used implementations of these algorithms provided by Zampoglou et al. <ref type="bibr" target="#b45">[47]</ref>.</p><p>Since we also wanted to compare our unsupervised method with approaches that were trained on labeled data, we report results from a learning-based method: E-MFCN <ref type="bibr" target="#b15">[16]</ref>. Given a dataset of spliced images and masks as training data, they use a supervised fully convolutional network (FCN) <ref type="bibr" target="#b46">[48]</ref> to predict splice masks and boundaries in test images. To test on our new datasets, we implemented a simplified version of their model (a standard FCN trained to recognize spliced pixels) that was trained with a training split of the Columbia, Carvalho, and Realistic Tampering datasets. We split every dataset in half to construct train/test sets.</p><p>Finally, we present two variations of self-consistency models. The first, CameraClassification, was trained to directly predict which camera model produced a given image patch. We evaluate the output of the camera classification model by sampling image patches from a test image and assigning the most frequently predicted camera as the natural image and everything else as the spliced region. We consider an image to be untampered when every patch's predicted camera model is consistent.   The second model, Image-Consistency, is a network that directly predicts whether two patches are sampled from the same image (Section 3.4). An image is considered likely to have been tampered if its constituent patches are predicted to have come from different images. The evaluations of these models are performed the same way as our full EXIF-Consistency model.</p><p>We trained our models, including the variations, using a ResNet50 <ref type="bibr" target="#b35">[36]</ref> pretrained on ImageNet <ref type="bibr" target="#b47">[49]</ref>. We used a batch size of 128 and optimized our objective using Adam <ref type="bibr" target="#b48">[50]</ref> with a learning rate of 10 −4 . We report our results after training for 1 million iterations. The 2-layer MLP used to compute patch consistency on top of the EXIF-Consistency model predictions was trained for 10, 000 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Splice Detection</head><p>We evaluate splice detection using the three datasets that contain both untampered and manipulated images: Columbia, Carvalho, and Realistic Tampering. For each algorithm, we extract the localization map and obtain an overall score by spatially averaging the responses. The images are ranked based on their overall scores, and we compute the mean average precision (mAP) for the whole dataset. <ref type="table">Table 1</ref> shows the mAP for detecting manipulated images. Our Consistency models achieves state-of-the-art performance on Columbia and Carvalho and Realistic Tampering, beating supervised methods like FCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Splice Localization</head><p>Having seen that our model can distinguish spliced and authentic images, we next ask whether it can also localize spliced regions within images. For each image, our algorithm produces an unnormalized probability that each pixel is part of a splice.</p><p>Because our consistency predictions are relative, it is ambiguous which of the two segments is spliced. We therefore identify the spliced region using a simple heuristic:  We present typical failure modes of our model. As we can see with outdoor images, overexposure frequently leads to false positives in the sky. In addition some splices are too small that we cannot effectively locate them using consistency. Finally, the flower example produces a partially incorrect result when using the EXIF Consistency model. Since the manipulation was a copy-move, the manipulation is only detectable via post-processing consistency cues (and not EXIF-consistency cues).</p><p>we say that the smaller of the two consistent regions is the splice. We also consider an alternative evaluation metric that flips (i.e. negates) the consistency predictions if this permutation results in higher accuracy. This measures a model's ability to segment the two regions, rather than its ability to say which is which. In both cases, we evaluate the quality of the localization using mean average precision (mAP). We also propose using a per-class intersection over union (cIOU) which averages the IOU of spliced and non-spliced regions after optimal thresholding. In order to compare against previous benchmarks <ref type="bibr" target="#b15">[16]</ref>, we also evaluate our results using MCC and F1 measures <ref type="bibr" target="#b1">2</ref> . These metrics evaluate a binary segmentation and require thresholding our predicted probabilities. We use the same evaluation procedure and pick the best threshold per splice localization prediction. Since <ref type="bibr" target="#b15">[16]</ref> reported their numbers on the full Columbia and Carvalho datasets (rather than our test split), we evaluated our methods on the full dataset and report the comparison in <ref type="table" target="#tab_5">Table 3</ref>.</p><p>The quantitative results on <ref type="table" target="#tab_4">Table 2</ref> show that our EXIF-Consistency model achieves the best performance across all datasets with the exception of the Realistic Tampering (RT) dataset. Notably, the model generally outperformed the supervised baselines, which were trained with actual manipulated images, despite the fact that our model never saw a tampered image during training. The supervised models' poor performance may be due to the small number of artists and manipulations represented in the training data. In <ref type="figure">Figure 5</ref>, we show the model's performance on the Columbia dataset when using individual EXIF attributes (rather than the learned "overall" consistency).</p><p>As expected, EXIF-Consistency outperformed Image-Consistency on most of our evaluations. But, interestingly, we observed that the gap between the models narrowed as training progressed, suggesting that Image-Consistency may eventually become competitive with additional training.</p><p>It is also instructive to look at the qualitative results of our method, which we show in <ref type="figure">Figure 8</ref>. We see that our method can localize manipulations on a wide range of different splices. Furthermore, in <ref type="figure">Figure 9</ref>, we show that our method produces highly consistent predictions when tested on real images. We can also look at the qualitative differences between our method and the baselines in <ref type="figure" target="#fig_3">Figure 11</ref>.</p><p>Finally, we ask which EXIF tags were useful for performing the splice localization task. To study this, we computed a response map for individual tags on the Columbia dataset, which we show in <ref type="figure" target="#fig_1">Figure 7</ref>. We see that the most successful tags correspond to imaging parameters that induce photographic changes to the final image like EXIF DigitalZoomRatio and EXIF GainControl.</p><p>2 F1 score is defined as 2T P 2T P +F N +F P and MCC as (T P ×T N )−(F P ×F N ) √ (T P +F P )(T P +F N )(T N +F P )(T N +F N ) . <ref type="figure" target="#fig_2">Figure 10</ref> we show some common failure cases. Our performance on Realistic Tampering illustrates some shortcomings with EXIF-Consistency. First, our model is not well-suited to finding very small splices, such as the ones that appear in RT. When spliced regions are small, the model's large stride may skip over spliced regions, mistakenly suggesting that no manipulations exist. Second, over-and underexposed regions are sometimes flagged by our model to be inconsistent because they lack any meta-data signal (e.g. because they are nearly uniformly black or white). Finally, RT contains a significant number of additional manipulations, such as copy-move, that cannot be consistently detected via meta-data consistency since the manipulated content comes from exactly the same photo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Failure cases In</head><p>Training and running times Training the EXIF-Consistency and Image-Consistency networks took approximately 4 weeks on 4 GPUs. Running the full self-consistency model took approximately 16 seconds per image (e.g. <ref type="figure" target="#fig_3">Figure 11</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In this paper, we have proposed a self-supervised method for detecting image manipulations. Our experiments show that the proposed method obtains state-of-the-art results on several datasets, even though it does not use labeled data during training. Our work also raises a number of questions. In contrast to physically motivated forensics methods <ref type="bibr" target="#b1">[2]</ref>, our model's results are not easily interpretable, and in particular, it is not clear which visual cues it uses to solve the task. It also remains an open question how best to fuse consistency measurements across an image for localizing manipulations. Finally, while our model is trained without any human annotations, it is still affected in complex ways by design decisions that went into the self-supervision task, such as the ways that EXIF tags were balanced during training.</p><p>Self-supervised approaches to visual forensics hold the promise of generalizing to a wide range of manipulations -potentially beyond those that can feasibly be learned through supervised training. However, for a forensics algorithm to be truly general, it must also model the actions of intelligent forgers that adapt to the detection algorithms. Work in adversarial machine learning <ref type="bibr" target="#b49">[51,</ref><ref type="bibr" target="#b50">52]</ref> suggests that having a self-learning forger in the loop will make the forgery detection problem much more difficult to solve, and will require new technical advances.</p><p>As new advances in computer vision and image-editing emerge, there is an increasingly urgent need for effective visual forensics methods. We see our approach, which successfully detects manipulations without seeing examples of manipulated images, as being an initial step toward building general-purpose forensics tools.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Test Time: Our model samples patches in a grid from an input image (b) and estimates consistency for every pair of patches. (c) For a given patch, we get a consistency map by comparing it to all other patches in the image. (d) We use Mean Shift to aggregate the consistency maps into a final prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Consistency map from different EXIF tags: We compute consistency maps for each metadata attribute independently (response maps sorted by localization accuracy). The merged consistency map accurately localizes the spliced car.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Failure Cases: We present typical failure modes of our model. As we can see with outdoor images, overexposure frequently leads to false positives in the sky. In addition some splices are too small that we cannot effectively locate them using consistency. Finally, the flower example produces a partially incorrect result when using the EXIF Consistency model. Since the manipulation was a copy-move, the manipulation is only detectable via post-processing consistency cues (and not EXIF-consistency cues).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: Comparing Methods: We visualize the qualitative difference between Self-Consistency and baselines. Our model can correctly localizes image splices from In-the-Wild, Columbia and Carvalho that other methods make mistakes on.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Fig. 3: Self-supervised training: Our model takes two random patches from different images and predicts whether they have consistent meta-data. Each attribute is used as a consistency metric during training and testing.</figDesc><table>Image A 

Image B 

Self-supervised Training 

Image A Metadata 

Image B Metadata 

Consistent 
Metadata? 

Image Patches 
(128 x 128) 

EXIF CameraModel: NIKON D3200 
EXIF CameraMake: NIKON CORP 
EXIF ColorSpace: Uncalibrated 
EXIF ISOSpeedRatings: 800 
EXIF DateTimeOriginal: 2016:04:17 
EXIF ImageLength: 2472 
EXIF ImageWidth: 3091 
EXIF Flash: Flash did not fire 
EXIF FocalLength: 90 
EXIF ExposureTime: 1/100 
EXIF WhiteBalance: Auto 
… 

Siamese Networks 

EXIF CameraModel: iPhone 4S 
EXIF CameraMake: Apple 
EXIF ColorSpace: sRGB 
EXIF ISOSpeedRatings: 50 
EXIF DateTimeOriginal: 2015:07:01 
EXIF ImageLength: 2448 
EXIF ImageWidth: 3264 
EXIF Flash: Flash did not fire 
EXIF FocalLength: 107/25 
EXIF ExposureTime: 1/2208 
EXIF WhiteBalance: Auto 
… 

Resnet-50 
Concatenated 
Features 
(8192) 

83 Binary 
Classification 

Diff 
Diff 
Diff 
Diff 
Diff 
Diff 
Diff 
Same 
Diff 
Diff 
Same 
… 
4096 

2048 

1024 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Fig. 4: EXIF Accuracy: How predictable are EXIF attributes? For each attribute, we com- pute pairwise-consistency accuracy on Flickr images using our self-consistency model. Fig. 5: EXIF Splice Localization: How useful are EXIF attributes for localizing splices? We compute individual localization scores on the Columbia dataset. logs left by image processing software. For example, one of its common values, Pro- cessed with VSCOcam, is added by a popular photo-filtering application. Please see the supplementary material for a full list of EXIF attributes and their definitions.</figDesc><table>EXIF UserComment 
EXIF FocalPlaneResolutionUnit 
EXIF FileSource 
EXIF CustomRendered 
EXIF LensMake 
EXIF LightSource 
EXIF SensingMethod 
EXIF LensSpecification 
EXIF SceneType 
Inter InteroperabilityVersion 
EXIF Sharpness 
Image Make 
EXIF Saturation 
EXIF Contrast 
EXIF FlashPixVersion 
Image YResolution 
Image XResolution 
Image YCbCrPositioning 
Inter InteroperabilityIndex 
EXIF ExposureProgram 

EXIF SubSecTime 
EXIF SubSecTimeOriginal 
EXIF SubSecTimeDigitized 
GPS GPSDate 
Chance 

Accuracy 

40 
50 
60 
70 
80 
90 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Splice</figDesc><table>Localization: We evaluate our model on 5 datasets using mean average preci-
sion (mAP, permuted-mAP) over pixels and class-balanced IOU (cIOU) selecting the optimal 
threshold per image. 

Dataset 
Columbia [40] Carvalho [41] 

Metric 
MCC 
F1 
MCC 
F1 

CFA [44] 
0.23 
0.47 
0.16 
0.29 
DCT [45] 
0.33 
0.52 
0.19 
0.31 
NOI [46] 
0.41 
0.57 
0.25 
0.34 

E-MFCN [16] 
0.48 
0.61 
0.41 
0.48 

Camera Classification 0.30 
0.50 
0.13 
0.26 
Image-Consistency 
0.77 
0.85 
0.33 
0.43 
EXIF-Consistency 
0.80 
0.88 
0.42 
0.52 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Comparison</figDesc><table>with Salloum et 
al.: We compare against numbers reported 
by [16] for splice localization. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>Fig. 8: Detecting Fakes: EXIF-Consistency successfully localizes manipulations across many different datasets. We show qualitative results on images from Carvalho, In-the-Wild, Hays and Realistic Tampering.</figDesc><table>Input 
Ground Truth 
Consistency 
Input 
Ground Truth 
Consistency 
Normalized Cut 
Normalized Cut 

Input 
Consistency 
Input 
Consistency 

Fig. 9: Response on Untampered Images: Our algorithm's response map contains fewer incon-
sistencies when given an untampered images. 

Input 
Ground Truth 
Consistency 
Normalized Cut 
Input 
Ground Truth 
Consistency 
Normalized Cut 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Photo credits: NIMBLE dataset [9] and Flickr user James Stave.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported, in part, by DARPA MediFor program and UC Berkeley Center for Long-Term Cybersecurity. We thank Hany Farid and Shruti Agarwal for their advice, assistance, and inspiration in building this project, David Fouhey, Saurabh Gupta, and Allan Jabri for helping with the editing, Peng Zhou for helping with experiments, and Abhinav Gupta for letting us use his GPUs. Finally, we thank the many Reddit and Onion artists who unknowingly contributed to our dataset.</p><p>Fighting Fake News: Image Splice Detection via Learned Self-Consistency</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scene completion using millions of photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="1941" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The commissar vanishes: the falsification of photographs and art in Stalin&apos;s Russia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canongate</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Photo forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a discriminative model for the perception of realism in composite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Deep image harmonization. In: CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Patchmatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="24" to="25" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Synthesizing obama: learning lip sync from audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="95" to="97" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02966</idno>
		<title level="m">You said that? arXiv preprint</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Technology: The 2017 nimble challenge evaluation datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">I</forename><surname>Standards</surname></persName>
		</author>
		<ptr target="https://www.nist.gov/itl/iad/mig/nimble-challenge2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Detection of misaligned cropping and recompression with the same quantization matrix and relevant forgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Jpeg error analysis and its applications to digital image forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="480" to="491" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Detecting double jpeg compression with the same quantization matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="848" to="856" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exposing digital forgeries by detecting traces of resampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on signal processing</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Digital image forensics via intrinsic fingerprints</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="101" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
		<title level="m">Photo forensics from jpeg dimples. Workshop on Image Forensics and Security</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Image splicing localization using A multi-task fully convolutional network (MFCN)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salloum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<idno>CoRR abs/1709.02016</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Aligned and non-aligned double JPEG detection using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bondi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bonettini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bestagini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Costanzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tondi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tubaro</surname></persName>
		</author>
		<idno>CoRR abs/1708.00930</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Localization of jpeg double compression through multi-domain convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Amerini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Uricchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caldelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE CVPR Workshop on Media Forensics</title>
		<meeting>of IEEE CVPR Workshop on Media Forensics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03875</idno>
		<title level="m">Contrast enhancement estimation for digital image forensics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">First steps toward camera model identification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bondi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baroffio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Güera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bestagini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tubaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tampering detection and localization through clustering of camera-based cnn features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bondi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lameri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Güera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bestagini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tubaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1855" to="1864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Two-stream neural networks for tampered face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning rich features for image manipulation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learned forensic source similarity for unknown camra models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Detection of metadata tampering through discrepancy between image content and metadata using multi-task deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning classification with unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De Sa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning image representations tied to ego-motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICCV</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Ambient sound provides supervision for visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Split-brain autoencoders: Unsupervised learning by crosschannel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning visual groups from cooccurrences in space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Priors for large photo collections and what they reveal about cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kuthirummal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="74" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Max-margin early event detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="202" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1975" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Using color compatibility for assessing image realism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mean shift, mode seeking, and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="790" to="799" />
			<date type="published" when="1995-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exposing digital image forgeries by illumination color classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Angelopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pedrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">R</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2004-07" />
		</imprint>
	</monogr>
	<note>A data set of authentic and spliced image blocks</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Evaluation of random field models in multi-modal unsupervised tampering localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Korus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Workshop on Inf. Forensics and Security</title>
		<meeting>of IEEE Int. Workshop on Inf. Forensics and Security</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Parowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06510</idno>
		<title level="m">Image provenance analysis at scale</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image forgery localization via fine-grained analysis of cfa artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ferrara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Detecting digital image forgeries by measuring inconsistencies of blocking artifact</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Using noise inconsistencies for blind image forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mahdian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saic</surname></persName>
		</author>
		<editor>IVC09.</editor>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Web and social media image forensics for news professionals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zampoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kompatsiaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bouwmeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spangenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social Media In the NewsRoom, SMNews16@CWSM, Tenth International AAAI Conference on Web and Social Media workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>CoRR abs/1605.06211</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>CVPR09.</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">B</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<title level="m">Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
