<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Bounding Box Estimation Using Deep Learning and Geometry</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
							<email>amousavi@gmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Inc</orgName>
								<orgName type="department" key="dep2">Inc</orgName>
								<orgName type="institution" key="instit1">George Mason University</orgName>
								<orgName type="institution" key="instit2">George Mason University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Inc</orgName>
								<orgName type="department" key="dep2">Inc</orgName>
								<orgName type="institution" key="instit1">George Mason University</orgName>
								<orgName type="institution" key="instit2">George Mason University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
							<email>john.flynn@zoox.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Inc</orgName>
								<orgName type="department" key="dep2">Inc</orgName>
								<orgName type="institution" key="instit1">George Mason University</orgName>
								<orgName type="institution" key="instit2">George Mason University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Košecká</surname></persName>
							<email>kosecka@gmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Inc</orgName>
								<orgName type="department" key="dep2">Inc</orgName>
								<orgName type="institution" key="instit1">George Mason University</orgName>
								<orgName type="institution" key="instit2">George Mason University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">3D Bounding Box Estimation Using Deep Learning and Geometry</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We present a method for 3D object detection and pose estimation from a single image. </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The problem of 3D object detection is of particular importance in robotic applications that require decision making or interactions with objects in the real world. 3D object detection recovers both the 6 DoF pose and the dimen- sions of an object from an image. While recently developed 2D detection algorithms are capable of handling large variations in viewpoint and clutter, accurate 3D object detection largely remains an open problem despite some promising recent work. The existing efforts to integrate pose estimation with state-of-the-art object detectors focus mostly on viewpoint estimation. They exploit the observation that the appearance of objects changes as a function of viewpoint and that discretization of viewpoints (parametrized by azimuth and elevation) gives rise to sub-categories which can be trained discriminatively <ref type="bibr" target="#b21">[23]</ref>. In more restrictive driving scenarios alternatives to full 3D pose estimation explore exhaustive sampling and scoring of all hypotheses <ref type="bibr" target="#b2">[4]</ref> using a variety of contextual and semantic cues.</p><p>In this work, we propose a method that estimates the pose (R, T ) ∈ SE(3) and the dimensions of an object's 3D bounding box from a 2D bounding box and the surrounding image pixels. Our simple and efficient method is suitable for many real world applications including selfdriving vehicles. The main contribution of our approach is in the choice of the regression parameters and the associated objective functions for the problem. We first regress the orientation and object dimensions before combining these estimates with geometric constraints to produce a final 3D pose. This is in contrast to previous techniques that attempt to directly regress to pose.</p><p>A state of the art 2D object detector <ref type="bibr" target="#b1">[3]</ref> is extended by training a deep convolutional neural network (CNN) to regress the orientation of the object's 3D bounding box and its dimensions. Given estimated orientation and dimensions and the constraint that the projection of the 3D bounding box fits tightly into the 2D detection window, we recover the translation and the object's 3D bounding box. Although conceptually simple, our method is based on several important insights. We show that a novel MultiBin discretecontinuous formulation of the orientation regression significantly outperforms a more traditional L2 loss. Further constraining the 3D box by regressing to vehicle dimensions proves especially effective, since they are relatively lowvariance and result in stable final 3D box estimates.</p><p>We evaluate our method on the KITTI <ref type="bibr" target="#b0">[2]</ref> and Pascal 3D+ <ref type="bibr" target="#b24">[26]</ref> datasets. On the KITTI dataset, we perform an in-depth comparison of our estimated 3D boxes to the results of other state-of-the-art 3D object detection algorithms <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b2">4]</ref>. The official KITTI benchmark for 3D bounding box estimation only evaluates the 3D box orientation estimate. We introduce three additional performance metrics measuring the 3D box accuracy: distance to center of box, distance to the center of the closest bounding box face, and the overall bounding box overlap with the ground truth box, measured using 3D Intersection over Union (3D IoU) score. We demonstrate that given sufficient training data, our method is superior to the state of the art on all the above 3D metrics. Since the Pascal 3D+ dataset does not have the physical dimensions annotated and the intrinsic camera parameters are approximate, we only evaluate viewpoint estimation accuracy showing that our MultiBin module achieves state-ofthe-art results there as well.</p><p>In summary, the main contributions of our paper include: 1) A method to estimate an object's full 3D pose and dimensions from a 2D bounding box using the constraints provided by projective geometry and estimates of the object's orientation and size regressed using a deep CNN. In contrast to other methods, our approach does not require any preprocessing stages or 3D object models. 2) A novel discrete-continuous CNN architecture called MultiBin regression for estimation of the object's orientation. 3) Three new metrics for evaluating 3D boxes beyond their orientation accuracy for the KITTI dataset. 4) An experimental evaluation demonstrating the effectiveness of our approach for KITTI cars, which also illustrates the importance of the specific choice of regression parameters within our 3D pose estimation framework. 5) Viewpoint evaluation on the Pascal 3D+ dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The classical problem of 6 DoF pose estimation of an object instance from a single 2D image has been considered previously as a purely geometric problem known as the perspective n-point problem (PnP). Several closed form and iterative solutions assuming correspondences between 2D keypoints in the image and a 3D model of the object can be found in <ref type="bibr" target="#b8">[10]</ref> and references therein. Other methods focus on constructing 3D models of the object instances and then finding the 3D pose in the image that best matches the model <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b4">6]</ref>.</p><p>With the introduction of new challenging datasets <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b10">12]</ref>, 3D pose estimation has been extended to object categories, which requires handling both the appearance variations due to pose changes and the appearance variations within the category <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b13">15]</ref>. In <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b24">26]</ref> the object detection framework of discriminative part based models (DPMs) is used to tackle the problem of pose estimation formulated jointly as a structured prediction problem, where each mixture component represents a different azimuth section. However, such approaches predict only an Euler angle subset with respect to the canonical object frame, while object dimensions and position are not estimated.</p><p>An alternative direction is to exploit the availability of 3D shape models and use those for 3D hypothesis sampling and refinement. For example, Mottaghi et al. <ref type="bibr" target="#b11">[13]</ref> sample the object viewpoint, position and size and then measure the similarity between rendered 3D CAD models of the object and the detection window using HOG features. A similar method for estimating the pose using the projection of CAD model object instances has been explored by <ref type="bibr" target="#b27">[29]</ref> in a robotics table-top setting where the detection problem is less challenging. Given the coarse pose estimate obtained from a DPM-based detector, the continuous 6 DoF pose is refined by estimating the correspondences between the projected 3D model and the image contours. The evaluation was carried out on PASCAL3D+ or simple table top settings with limited clutter or scale variations. An extension of these methods to more challenging scenarios with significant occlusion has been explored in <ref type="bibr" target="#b20">[22]</ref>, which uses dictionaries of 3D voxel patterns learned from 3D CAD models that characterize both the object's shape and commonly encountered occlusion patterns.</p><p>Recently, deep convolutional neural networks (CNN) have dramatically improved the performance of 2D object detection and several extensions have been proposed to include 3D pose estimation. In <ref type="bibr" target="#b19">[21]</ref> R-CNN <ref type="bibr" target="#b5">[7]</ref> is used to detect objects and the resulting detected regions are passed as input to a pose estimation network. The pose network is initialized with VGG <ref type="bibr" target="#b18">[20]</ref> and fine-tuned for pose estimation using ground truth annotations from Pascal 3D+. This approach is similar to <ref type="bibr" target="#b6">[8]</ref>, with the distinction of using separate pose weights for each category and a large number of synthetic images with pose annotation ground truth for training. In <ref type="bibr" target="#b15">[17]</ref>, Poirson et al. discretize the object viewpoint and train a deep convolutional network to jointly perform viewpoint estimation and 2D detection. The network shares the pose parameter weights across all classes. In <ref type="bibr" target="#b19">[21]</ref>, Tulsiani et al. explore the relationship between coarse viewpoint estimation, followed by keypoint detection, localization and pose estimation. Pavlakos et al <ref type="bibr" target="#b12">[14]</ref>, used CNN to localize the keypoints and they used the keypoints and their 3D coordinates from meshes to recover the pose. However, their approach required training data with annotated keypoints.</p><p>Several recent methods have explored 3D bounding box detection for driving scenarios and are most closely related to our method. Xiang et al. <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b22">24]</ref> cluster the set of possible object poses into viewpoint-dependent subcategories. These subcategories are obtained by clustering 3D voxel patterns introduced previously <ref type="bibr" target="#b20">[22]</ref>; 3D CAD models are required to learn the pattern dictionaries. The subcategories capture both shape, viewpoint and occlusion patterns and are subsequently classified discriminatively <ref type="bibr" target="#b22">[24]</ref> using deep CNNs. Another related approach by Chen et al. <ref type="bibr" target="#b2">[4]</ref> addresses the problem by sampling 3D boxes in the physical world assuming the flat ground plane constraint. The boxes are scored using high level contextual, shape and category specific features. All of the above approaches require complicated preprocessing including high level features such as segmentation or 3D shape repositories and may not be suitable for robots with limited computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">3D Bounding Box Estimation</head><p>In order to leverage the success of existing work on 2D object detection for 3D bounding box estimation, we use the fact that the perspective projection of a 3D bounding box should fit tightly within its 2D detection window. We assume that the 2D object detector has been trained to produce boxes that correspond to the bounding box of the projected 3D box. The 3D bounding box is described by its</p><formula xml:id="formula_0">center T = [t x , t y , t z ] T , dimensions D = [d x , d y , d z ]</formula><p>, and orientation R(θ, φ, α) , here paramaterized by the azimuth, elevation and roll angles. Given the pose of the object in the camera coordinate frame (R, T ) ∈ SE(3) and the camera intrinsics matrix K, the projection of a 3D point</p><formula xml:id="formula_1">X o = [X, Y, Z, 1] T in the object's coordinate frame into the image x = [x, y, 1]</formula><p>T is:</p><formula xml:id="formula_2">x = K R T X o (1)</formula><p>Assuming that the origin of the object coordinate frame is at the center of the 3D bounding box and the object dimensions D are known, the coordinates of the 3D bounding box vertices can be described simply by</p><formula xml:id="formula_3">X 1 = [d x /2, d y /2, d z /2] T , X 2 = [−d x /2, d y /2, d z /2] T , . . . , X 8 = [−d x /2, −d y /2, −d z /2]</formula><p>T . The constraint that the 3D bounding box fits tightly into 2D detection window requires that each side of the 2D bounding box to be touched by the projection of at least one of the 3D box corners. For example, consider the projection of one 3D corner</p><formula xml:id="formula_4">X 0 = [d x /2, −d y /2, d z /2]</formula><p>T that touches the left side of the 2D bounding box with coordinate x min . This point-toside correspondence constraint results in the equation:</p><formula xml:id="formula_5">x min =     K R T     d x /2 −d y /2 d z /2 1         x<label>(2)</label></formula><p>where (.) x refers to the x coordinate from the perspective projection. Similar equations can be derived for the remaining 2D box side parameters x max , y min , y max . In total the sides of the 2D bounding box provide four constraints on the 3D bounding box. This is not enough to constrain the nine degrees of freedom (DoF) (three for translation, three for rotation, and three for box dimensions). There are several different geometric properties we could estimate from the visual appearance of the box to further constrain the 3D box. The main criteria is that they should be tied strongly to the visual appearance and further constrain the final 3D box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Choice of Regression Parameters</head><p>The first set of parameters that have a strong effect on the 3D bounding box is the orientation around each axis (θ, φ, α). Apart from them, we choose to regress the box dimensions D rather than translation T because the variance of the dimension estimate is typically smaller (e.g. cars tend to be roughly the same size) and does not vary as the object orientation changes: a desirable property if we are also regressing orientation parameters. Furthermore, the dimension estimate is strongly tied to the appearance of a particular object subcategory and is likely to be accurately recovered if we can classify that subcategory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Correspondence Constraints</head><p>Using the regressed dimensions and orientations of the 3D box by CNN and 2D detection box we can solve for the translation T that minimizes the reprojection error with respect to the initial 2D detection box constraints in Equation 2. Details of how to solve for translation are included in the supplementary material <ref type="bibr">[1]</ref>. Each side of the 2D detection box can correspond to any of the eight corners of the 3D box which results in 8 4 = 4096 configurations. Each different configuration involves solving an over-constrained system of linear equations which is computationally fast and can be done in parallel. In many scenarios the objects can be assumed to be always upright. In this case, the 2D box top and bottom correspond only to the projection of vertices from the top and bottom of the 3D box, respectively, which reduces the number of correspondences to 1024. Furthermore, when the relative object roll is close to zero, the vertical 2D box side coordinates x min and x max can only correspond to projections of points from vertical 3D box sides. Similarly, y min and y max can only correspond to point projections from the horizontal 3D box sides. Consequently, each vertical side of the 2D detection box can correspond to [±d x /2, ., ±d z /2] and each horizontal side of the 2D bounding corresponds to [., ±d y /2, ±d z /2], yielding 4 4 = 256 possible configurations. In the KITTI dataset, object pitch and roll angles are both zero, which further reduces of the number of configurations to 64. <ref type="figure" target="#fig_1">Fig. 2</ref> visualizes some of the possible correspondences between 2D box sides and 3D box points that can occur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CNN Regression of 3D Box Parameters</head><p>In this section, we describe our approach for regressing the 3D bounding box orientation and dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">MultiBin Orientation Estimation</head><p>Estimating the global object orientation R ∈ SO(3) in the camera reference frame from only the contents of the detection window crop is not possible, as the location of the crop within the image plane is also required. Consider the rotation R(θ) parametrized only by azimuth θ (yaw). <ref type="figure" target="#fig_3">Fig. 4</ref> shows an example of a car moving in a straight line. Although the global orientation R(θ) of the car (its 3D bounding box) does not change, its local orientation θ l with respect to the ray through the crop center does, and generates changes in the appearance of the cropped image.</p><p>We thus regress to this local orientation θ l . <ref type="figure" target="#fig_3">Fig. 4</ref> shows an example, where the local orientation angle θ l and the ray angle change in such a way that their combined effect is a constant global orientation of the car. Given intrinsic  camera parameters, the ray direction at a particular pixel is trivial to compute. At inference time we combine this ray direction at the crop center with the estimated local orientation in order to compute the global orientation of the object.</p><p>It is known that using the L2 loss is not a good fit for many complex multi-modal regression problems. The L2 loss encourages the network to minimize to average loss across all modes, which results in an estimate that may be poor for any single mode. This has been observed in the context of the image colorization problem, where the L2 norm produces unrealistic average colors for items like clothing <ref type="bibr" target="#b25">[27]</ref>. Similarly, object detectors such as Faster R-CNN <ref type="bibr" target="#b16">[18]</ref> and SSD <ref type="bibr" target="#b9">[11]</ref> do not regress the bounding <ref type="figure">Figure 5</ref>. Proposed architecture for MultiBin estimation for orientation and dimension estimation. It consists of three branches. The left branch is for estimation of dimensions of the object of interest. The other branches are for computing the confidence for each bin and also compute the cos(∆θ) and sin(∆θ) of each bin boxes directly: instead they divide the space of the bounding boxes into several discrete modes called anchor boxes and then estimate the continuous offsets that need to be applied to each anchor box.</p><p>We use a similar idea in our proposed MultiBin architecture for orientation estimation. We first discretize the orientation angle and divide it into n overlapping bins. For each bin, the CNN network estimates both a confidence probability c i that the output angle lies inside the i th bin and the residual rotation correction that needs to be applied to the orientation of the center ray of that bin in order to obtain the output angle. The residual rotation is represented by two numbers, for the sine and the cosine of the angle. This results in 3 outputs for each bin i: (c i , cos(∆θ i ), sin(∆θ i )). Valid cosine and sine values are obtained by applying an L2 normalization layer on top of a 2-dimensional input. The total loss for the MultiBin orientation is thus:</p><formula xml:id="formula_6">L θ = L conf + w × L loc (3)</formula><p>The confidence loss L conf is equal to the softmax loss of the confidences of each bin. L loc is the loss that tries to minimize the difference between the estimated angle and the ground truth angle in each of the bins that covers the ground truth angle, with adjacent bins having overlapping coverage. In the localization loss L loc , all the bins that cover the ground truth angle are forced to estimate the correct angle. The localization loss tries to minimize the difference between the ground truth and all the bins that cover that value which is equivalent of maximizing cosine distance as it is shown in supplementary material <ref type="bibr">[1]</ref>. Localization loss L loc is computed as following:</p><formula xml:id="formula_7">L loc = − 1 n θ * cos(θ * − c i − ∆θ i )<label>(4)</label></formula><p>where n θ * is the number of bins that cover ground truth angle θ * , c i is the angle of the center of bin i and ∆θ i is the change that needs to be applied to the center of bin i.</p><p>During inference, the bin with maximum confidence is selected and the final output is computed by applying the estimated ∆θ of that bin to the center of that bin. The MultiBin module has 2 branches. One for computing the confidences c i and the other for computing the cosine and sine of ∆θ. As a result, 3n parameters need to be estimated for n bins.</p><p>In the KITTI dataset cars, vans, trucks, and buses are all different categories and the distribution of the object dimensions for category instances is low-variance and unimodal. For example, the dimension variance for cars and cyclists is on the order of several centimeters. Therefore, rather than using a discrete-continuous loss like the MultiBin loss above, we use directly the L2 loss. As is standard, for each dimension we estimate the residual relative to the mean parameter value computed over the training dataset. The loss for dimension estimation L dims is computed as follows:</p><formula xml:id="formula_8">L dims = 1 n (D * −D − δ) 2 ,<label>(5)</label></formula><p>where D * are the ground truth dimensions of the box,D are the mean dimensions for objects of a certain category and δ is the estimated residual with respect to the mean that the network predicts.</p><p>The CNN architecture of our parameter estimation module is shown in <ref type="figure">Figure 5</ref>. There are three branches: two branches for orientation estimation and one branch for dimension estimation. All of the branches are derived from the same shared convolutional features and the total loss is the weighted combination of L = α × L dims + L θ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>We performed our experiments on the KITTI <ref type="bibr" target="#b0">[2]</ref> and Pascal 3D+ <ref type="bibr" target="#b24">[26]</ref> datasets. KITTI dataset: The KITTI dataset has a total of 7481 training images. We train the MS-CNN <ref type="bibr" target="#b1">[3]</ref> object detector to produce 2D boxes and then estimate 3D boxes from 2D detection boxes whose scores exceed a threshold. For regressing 3D parameters, we use a pretrained VGG network <ref type="bibr" target="#b18">[20]</ref> without its FC layers and add our 3D box module, which is shown in <ref type="figure">Fig. 5</ref>. In the module, the first FC layers in each of the orientation branches have 256 dimensions, while the first FC layer for dimension regression has a dimension of 512. During training, each ground truth crop is resized to 224x224. In order to make the network more robust to viewpoint changes and occlusions, the ground truth boxes are jittered and the ground truth θ l is changed to account for the movement of the center ray of the crop. In addition, we added color distortions and also applied mirroring to images at random. The network is trained with SGD using a fixed learning rate of 0.0001. The training is run for 20K iterations with a batch size of 8 and the best model is chosen by cross validation. <ref type="figure" target="#fig_4">Fig. 6</ref> shows the qualitative visualization of estimated 3D boxes for cars and cyclists on our KITTI validation set. We used two different training/test splits for our experiments. The first split was used to report results on the official KITTI test set and uses the majority of the available training images. The second split is identical to the one used by SubCNN <ref type="bibr" target="#b22">[24]</ref>, in order to enable fair comparisons. They use half of the available data for validation. Pascal3D+ dataset: The dataset consists of images from Pascal VOC and Imagenet for 12 different categories that are annotated with 6 DoF pose. Images from the Pascal training set and Imagenet are used for training and the evaluation is done on the Pascal validation set. Unlike KITTI, the intrinsic parameters are approximate and therefore it is not possible to recover the true physical object dimensions. Therefore we only evaluate on 3 DoF viewpoint estimation to show the effectiveness of our MultiBin loss. We used C × 3 MultiBin modules to predict 3 angles for each of the C classes. For a fair comparison with <ref type="bibr" target="#b19">[21]</ref>, we kept the f c6 and f c7 layers of VGG and eliminated the separate convolution branches of our MultiBin modules. All the necessary inputs are generated using a single fully connected layer that takes f c7 as input. We also reused the hyperparameters chosen in <ref type="bibr" target="#b19">[21]</ref> for training our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">3D Bounding Box Evaluation</head><p>KITTI orientation accuracy. The official 3D metric of the KITTI dataset is Average Orientation Similarity (AOS), which is defined in <ref type="bibr" target="#b0">[2]</ref> and multiplies the average precision (AP) of the 2D detector with the average cosine distance similarity for azimuth orientation. Hence, AP is by definition the upper bound of AOS. At the time of publication, we are first among all methods in terms of AOS for easy car examples and first among all non-anonymous methods for moderate car examples on the KITTI leaderboard. Our results are summarized in <ref type="table">Table 1</ref>, which shows that we outperform all the recently published methods on orientation estimation for cars. For moderate cars we outperform SubCNN <ref type="bibr" target="#b22">[24]</ref> despite having similar AP, while for hard examples we outperform 3DOP <ref type="bibr" target="#b3">[5]</ref> despite much lower AP. The ratio of AOS over AP for each method is representative of how each method performs only on orientation estimation, while factoring out the 2D detector performance. We refer to this score as Orientation Score (OS), which represents the error (1 + cos(∆θ))/2 averaged across all examples. OS can be converted back to angle error by the acos(2 * OS − 1) formula, resulting in 3</p><p>• error for easy, 6</p><p>• for moderate, and 8</p><p>• on hard cars for our MultiBin model on the official KITTI test set. Our method is the only one that does not rely on computing additional features such as stereo, semantic segmentation, instance segmentation and does not need preprocessing as in <ref type="bibr" target="#b22">[24]</ref> and <ref type="bibr" target="#b21">[23]</ref>. Pascal3D+ viewpoint accuracy. Two metrics are used for viewpoint accuracy: Median Error M edErr and the percentage of the estimations that are within π 6 of the groundtruth viewpoint Acc π</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6</head><p>. The distance between rotations is computed as</p><formula xml:id="formula_9">∆(R 1 , R 2 ) = || log(R T 1 R2)|| F √ 2</formula><p>. The evaluation is done using the groundtruth bounding boxes. <ref type="table">Table 2</ref> shows that MultiBin modules are more effective than discretized classification <ref type="bibr" target="#b19">[21]</ref> and also keypoint based method of <ref type="bibr" target="#b12">[14]</ref> which is based on localizing keypoints and solving a sophisticated optimization to recover the pose. MultiBin loss analysis. <ref type="table" target="#tab_2">Table 3</ref> shows the effect of choosing a different number of bins for the Multibox loss on both KITTI and Pascal3D+. In both datasets, using more than one bin consistently outperforms the single-bin variant, which is equivalent to the L2 loss. On KITTI, the best performance is achieved with 2 bins while 8 bins works the best for Pascal3D+. This is due to the fact that the viewpoint distribution in the Pascal3D+ dataset is more diverse. As <ref type="table" target="#tab_2">Table 3</ref> shows, over-binning eventually decreases the effectiveness of the method, as it decreases the training data amount for each bin. We also experimented with different widths of the fully connected layers (see <ref type="table">Table 4</ref>) and found that increasing the width of the FC layers further yielded some limited gains even beyond width 256. 3D bounding box metrics and comparison. The orientation estimation loss evaluates only a subset of 3D bounding box parameters. To evaluate the accuracy of the rest, we introduce 3 metrics, on which we compare our method against SubCNN <ref type="bibr" target="#b22">[24]</ref> for KITTI cars. The first metric is the average error in estimating the 3D coordinate of the center of the objects. The second metric is the average error in estimating the closest point of the 3D box to the camera. This metric is important for driving scenarios where the system needs to avoid hitting obstacles. The last metric is the 3D intersection over union (3D IoU) which is the ultimate metric utilizing all parameters of the estimated 3D bounding boxes. In order to factor away the 2D detector performance for a side-by-side comparison, we kept only the detections from both methods where the detected 2D boxes have IoU ≥ 0.7. <ref type="figure">As Fig. 8</ref> shows, our method outperforms the SubCNN method <ref type="bibr" target="#b22">[24]</ref>, the current state of the art, across the board in all 3 metrics. Despite this, the 3D IoU numbers are significantly smaller than those that 2D detectors typically obtain on the corresponding 2D metric. This is due to the fact that 3D estimation is a more challenging task, especially as the distance to the object increases. For example, if the car is 50m away from the camera, a translation error of 2m corresponds to about half the car length. Our method handles increasing distance well, as its error for the box center and closest point metrics in <ref type="figure">Fig. 8</ref> increases approximately linearly with distance, compared to   SubCNN's super-linear degradation. To evaluate the importance of estimating the car dimensions, we evaluated a variant of our method that uses average sizes instead of estimating them. The evaluation shows that regressing the dimensions makes a difference in all the 3D metrics. To facilitate comparisons with future work on this problem, we have made the estimated 3D boxes on the split of <ref type="bibr" target="#b20">[22]</ref> available at http://bit.ly/2oaiBgi.</p><p>Training data requirements. One downside of our method is that it needs to learn the parameters for the fully connected layers; it requires more training data than methods that use additional information. To verify this hypothesis, we repeated the experiments for cars but limited the number of training instances to 1100. The same method that achieves 0.9808 in <ref type="table" target="#tab_2">Table 3</ref> with 10828 instances can only achieve 0.9026 on the same test set. Moreover, our results on the official KITTI set is significantly better than the split of <ref type="bibr" target="#b20">[22]</ref> (see <ref type="table">Table 1</ref>) because yet more training data is used for training. A similar phenomenon is happening for the KITTI cyclist task. The number of cyclist instances are much less than the number of car instances (1144 labeled cyclists vs 18470 labeled cars). As a result, there is not enough training data for learning the parameters of the fully connected layer well. Although our purely data-driven method achieves competitive results on the cyclists , it cannot outperform other methods that use additional features and assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Implicit Emergent Attention</head><p>In this section, we visualize the parts of cars and bicycles that the network uses in order to estimate the object orientation accurately. Similar to <ref type="bibr" target="#b26">[28]</ref>, a small gray patch is slid around the image and for each location we record the difference between the estimated and the ground truth orientation. If occluding a specific part of the image by the patch causes a significantly different output, it means that the network attends to that part. <ref type="figure" target="#fig_6">Fig. 9</ref> shows such heatmaps of the output differences due to grayed out locations for several car detections. It appears that the network attends to aero bike boat bottle bus car chair  <ref type="table">Table 2</ref>. Viewpoint Estimation with Ground Truth box on Pascal3D+ <ref type="figure">Figure 8</ref>. 3D box metrics for KITTI cars. Left: Mean distance error for box center, in meters. Middle: Error in estimating the closest distance from the 3D box to the camera, which is proportional to time-to-impact for driving scenarios. Right: 3D IoU between the predicted and ground truth 3D bounding boxes.   <ref type="table">Table 4</ref>. effect of FC width in orientation accuracy distinct object parts such as tires, lights and side mirror for cars. Our method seems to learn local features similar to keypoints used by other methods, without ever having seen explicitly labeled keypoint ground truth. Another advantage is that our network learns task-specific local features, while human-labeled keypoints are not necessarily the best ones for the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Directions</head><p>In this work, we show how to recover the 3D bounding boxes for known object categories from a single view. Using a novel MultiBin loss for orientation prediction and an effective choice of box dimensions as regression parameters, our method estimates stable and accurate posed 3D bounding boxes without additional 3D shape models, or sampling strategies with complex pre-processing pipelines. One future direction is to explore the benefits of augmenting the RGB image input in our method with a separate depth channel computed using stereo. Another is to explore 3D box estimation in video, which requires using the temporal information effectively and can enable the prediction of future object position and velocity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Our method takes the 2D detection bounding box and estimates a 3D bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Correspondence between the 3D box and 2D bounding box: Each figure shows a 3D bbox that surrounds an object. The front face is shown in blue and the rear face is in red. The 3D points that are active constraints in each of the images are shown with a circle (best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Left: Car dimensions, the height of the car equals dy. Right: Illustration of local orientation θ l , and global orientation of a car θ. The local orientation is computed with respect to the ray that goes through the center of the crop. The center ray of the crop is indicated by the blue arrow. Note that the center of crop may not go through the actual center of the object. Orientation of the car θ is equal to θray + θ l . The network is trained to estimate the local orientation θ l .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Left: cropped image of a car passing by. Right: Image of whole scene. As it is shown the car in the cropped images rotates while the car direction is constant among all different rows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Qualitative illustration of the estimated 3D projections, in red for cars and green for cyclists.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Visualization of Estimated Poses on Pascal3D+ dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Visualization of the learned attention of the model for orientation estimation. The heatmap shows the image areas that contribute to orientation estimation the most. The network attends to certain meaningful parts of the car such as tires, lights, and side mirrors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Table 1. Comparison of the Average Orientation Estimation (AOS), Average Precision (AP) and Orientation Score (OS) on official KITTI dataset for cars. Orientation score is the ratio between AOS and AP.</figDesc><table>Method 
Easy 
Moderate 
Hard 
AOS 
AP 
OS 
AOS 
AP 
OS 
AOS 
AP 
OS 
3DOP[5] 
91.44% 93.04% 0.9828 86.10% 
88.64% 
0.9713 76.52% 79.10% 0.9673 
Mono3D[4] 
91.01% 92.33% 0.9857 0.8662% 
88.66% 
0.9769 76.84% 78.96% 0.9731 
SubCNN[24] 90.67% 90.81% 0.9984 88.62% 
89.04% 
0.9952 78.68% 79.27% 0.9925 
Our Method 
92.90% 92.98% 0.9991 88.75% 
89.04% 0.9967 76.76% 77.17% 0.9946 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>table mbike sofa train tv mean MedErr ([21]) 13.8 17.7 21.3 12.9</figDesc><table>5.8 
9.1 
14.8 15.2 
14.7 
13.7 8.7 15.4 
13.6 
MedErr ([14]) 
8.0 13.4 40.7 11.7 
2.0 
5.5 
10.4 N/A 
N/A 
9.6 
8.3 32.9 
N/A 
MedErr (Ours) 13.6 12.5 22.8 
8.3 
3.1 
5.8 
11.9 12.5 
12.3 
12.8 6.3 11.9 
11.1 
Acc π 

6 

([21]) 
0.81 0.77 0.59 0.93 0.98 0.89 0.80 0.62 
0.88 
0.82 0.80 0.80 0.8075 
Acc π 

6 

(Ours) 
0.78 0.83 0.57 0.93 0.94 0.90 0.80 0.68 
0.86 
0.82 0.82 0.85 0.8103 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>The effect of the number of bins on viewpoint estimation in KITTI and Pascal3D+ datasets</figDesc><table>FC 
64 
128 
256 
512 
1024 
OS 0.9583 0.9607 0.9808 0.9854 0.9861 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simultaneous object recognition and segmentation from single or multiple model views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="188" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yangyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Leonidas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Categoryspecific object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">EPnP: An Accurate O(n) Solution to the PnP Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nyc3dcars: A dataset of 3d vehicles in geographic context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A coarse-to-fine model for 3d pose estimation and sub-category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Daniilidis. 6-dof object pose from semantic keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d object class detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Teaching 3d geometry to deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fast single shot detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ammirato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d object modeling and recognition using local affine-invariant image descriptors and multi-view spatial constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rothganger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="259" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Viewpoints and keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Data-driven 3d voxed patterns for object categorry recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representation</title>
		<meeting>the International Conference on Learning Representation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Data-driven 3d voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Subcategoryaware convolutional neural networks for object proposals and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04693</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Objectnet3d: A large scale database for 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Beyond pascal: A benchmark for 3d object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representation</title>
		<meeting>the International Conference on Learning Representation</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Single image 3d object detection and pose estimation for grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brahmbhatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lecce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICRA</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
