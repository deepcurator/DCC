<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Finding Influential Training Samples for Gradient Boosted Decision Trees</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Sharchilev</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Ustinovsky</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Serdyukov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
						</author>
						<title level="a" type="main">Finding Influential Training Samples for Gradient Boosted Decision Trees</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We address the problem of finding influential training samples for a particular case of tree ensemble-based models, e.g., Random Forest (RF) or Gradient Boosted Decision Trees (GBDT). A natural way of formalizing this problem is studying how the model's predictions change upon leave-one-out retraining, leaving out each individual training sample. Recent work has shown that, for parametric models, this analysis can be conducted in a computationally efficient way. We propose several ways of extending this framework to non-parametric GBDT ensembles under the assumption that tree structures remain fixed. Furthermore, we introduce a general scheme of obtaining further approximations to our method that balance the trade-off between performance and computational complexity. We evaluate our approaches on various experimental setups and use-case scenarios and demonstrate both the quality of our approach to finding influential training samples in comparison to the baselines and its computational efficiency. </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(2) boosting developer's trust in the model's performance in scenarios when on-line evaluation is not available before deployment; and (3) increasing user satisfaction and/or confidence in provided predictions, etc. Various problem setups <ref type="bibr" target="#b11">(Palczewska et al., 2013;</ref><ref type="bibr" target="#b18">Tolomei et al., 2017;</ref><ref type="bibr" target="#b6">Fong &amp; Vedaldi, 2017)</ref> and interpretation methods, both model-agnostic <ref type="bibr" target="#b14">(Ribeiro et al., 2016;</ref><ref type="bibr" target="#b10">Lundberg &amp; Lee, 2017)</ref> and model-specific <ref type="bibr" target="#b15">(Shrikumar et al., 2017;</ref><ref type="bibr" target="#b18">Tolomei et al., 2017;</ref><ref type="bibr" target="#b17">Sundararajan et al., 2017)</ref>, have recently been proposed in the literature.</p><p>A common trait shared by the majority of these methods is that they do not provide a way of automatically improving the model, since the model is fixed; the main use-case thus becomes manual analytics by the user or the developer, which is both time and resource-consuming. It is thus desirable to derive a framework for obtaining actionable insights into the model's behavior allowing us to automatically improve a model's performance.</p><p>One such framework has recently been introduced by <ref type="bibr" target="#b9">Koh &amp; Liang (2017)</ref>; it deals with finding the most influential training objects. They formalize the notion of "influence" via an infinitesimal approximation to leave-one-out retraining: the core question that this work aims to answer is "how would the model's performance on a test object x test change if the weight of a training object x train is perturbed?" Assuming a smooth parametric model family (e.g., linear models or neural networks), the authors employ the Influence Functions framework from classical statistics <ref type="bibr" target="#b4">(Cook &amp; Weisberg (1980)</ref>; also see <ref type="bibr" target="#b9">Koh &amp; Liang (2017)</ref> for a literature review on the topic) to show that this quantity can be estimated much faster than via straightforward model retraining, which makes their method tractable in a real-world scenario. A natural use-case of such a framework is to consider individual test objects (or groups of them) on which the model performs poorly and either remove the most "harmful" training objects or prioritize a batch of new objects for labeling based on which ones are expected to be the most "helpful," akin to active learning.</p><p>Unfortunately, the method suggested by <ref type="bibr" target="#b9">Koh &amp; Liang (2017)</ref> heavily relies on the smooth parametric nature of the model family. While this is a large class of machine learning models, it is by far not the only one. In particular, decision tree ensembles such as Random Forests <ref type="bibr">(Ho, 1995, RF)</ref> and Gradient Boosted Decision Trees <ref type="bibr">(Friedman, 2001, GBDT)</ref> are probably the most widely used model family in industry, largely due to their state-of-the-art performance on struc-tured and/or multimodal data. Thus, it is important to extend the aforementioned Influence Functions framework to tree ensembles.</p><p>In this paper, we propose a way of doing so, while focusing specifically on GBDT. We consider two proxy metrics for the informal notion of influence. For the first one, leaveone-out retraining, we utilize the inner mechanics of fitting decision trees (in particular, assuming that a small training sample perturbation does not change the trees' structures) to derive LeafRefit and FastLeafRefit, a well-founded family of approximations to leave-one-out retraining that trade off approximation accuracy for computational complexity. For the second, analogously to the Influence Functions framework, we consider infinitesimal training sample weight perturbations and derive LeafInfluence and FastLeafInfluence, methods for estimating gradients of the model's predictions with respect to training objects' weights. From a theoretical perspective, LeafInfluence and FastLeafInfluence allow us to deal with the discontinuous dependency of tree structure on training sample perturbations; from a practical one, they allow us to further reduce computational complexity due to the possibility of precomputing certain derivatives.</p><p>In our experiments we (1) study the conditions under which our methods, FastLeafRefit and FastLeafInfluence, successfully approximate their proxy metrics, (2) demonstrate our methods' ability to target training objects which are influential for specific test objects, and (3) show that our algorithms run much faster than straightforward retraining, which makes them applicable in practical scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem Definition</head><p>First, we formally define the problem setup. We consider standard supervised training of a GBDT ensemble 2 F (x; w) := T t=1 f t P (x)t (A t−1 ) on a training sample X train . Learning consists of two separate stages: model structure selection and picking the optimal leaf values. The way of choosing the model structure is not important for our work; we refer the interested reader to existing implementations, e.g., <ref type="bibr" target="#b3">Chen &amp; Guestrin (2016)</ref>; <ref type="bibr" target="#b5">Dorogush et al. (2017)</ref>. For picking optimal leaf values, we consider two most commonly used formulas:</p><p>Gradient: At leaf l at step t, output negative average gradients (calculated at current predictions) over the leaf objects:</p><formula xml:id="formula_0">f t G;l (A t−1 ) := − G t l (A t−1 ) H t G;l (A t−1 ) .<label>(1)</label></formula><p>This is equivalent to minimizing the empirical loss function w.r.t. the current leaf value by doing a single gradient step in function space <ref type="bibr" target="#b3">(Chen &amp; Guestrin, 2016)</ref>.</p><p>Newton: At leaf l at step t, output the negative total gradient 2 Mathematical notations are defined in <ref type="table" target="#tab_0">Table 1</ref>. Notation Description</p><formula xml:id="formula_1">x = (x, y) Data point X = {(xi, yi)} n i=1</formula><p>Training/test sample L(ytrue, y pred )</p><p>Loss function w = (w1, ..., wn)</p><p>Weights of training samples</p><formula xml:id="formula_2">P (x) = (i1, ..., iT ) Path (leaf indices) of x F (x)</formula><p>GBDT prediction at point x f t l Value in leaf l at step t I t l</p><p>Training points belonging to leaf l at step t</p><formula xml:id="formula_3">A t = {A t i := t τ =1 f τ P (x i )τ } n i=1</formula><p>Intermediate predictions on {xi}</p><formula xml:id="formula_4">n i=1 g t i (A t−1 i ) := ∂L(y i ,z) ∂z | z=A t−1 i i-th first derivative at training step t h t i (A t−1 i ) := ∂ 2 L(y i ,z) ∂z 2 | z=A t−1 i</formula><p>i-th second derivative at training step t</p><formula xml:id="formula_5">k t i (A t−1 i ) := ∂ 3 L(y i ,z) ∂z 3 | z=A t−1 i i-th third derivative at training step t G t l (A t−1 ) := j∈I t l wjg t j (A t−1 j ) Sum of leaf derivatives H t H;l (A t−1 ) := j∈I t l wjh t j (A t−1 j ) Sum of leaf second derivatives H t G;l (A t−1 ) := j∈I t l wj</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sum of leaf weights</head><p>Inf grad (x1, x2) Influence of object x1 on x2 divided by the total second derivative over the leaf objects:</p><formula xml:id="formula_6">f t H;l (A t−1 ) := − G t l (A t−1 ) H t H;l (A t−1 ) .<label>(2)</label></formula><p>This is equivalent to minimizing the empirical loss function w.r.t. the current leaf value by doing a single Newton step in function space <ref type="bibr" target="#b3">(Chen &amp; Guestrin, 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section, we describe our approach to efficiently calculating the influence of training points. Since the notion of "influence" is not rigorously defined and partly intuitive, we need to introduce a well-defined, measurable quantity that aims to capture the desired intuition; we refer to it as a proxy for influence. In this work, we follow the general framework of <ref type="bibr" target="#b9">Koh &amp; Liang (2017)</ref> and quantify influence through train set perturbations. We consider two proxies that reflect two natural variations of this approach. First, we describe an algorithm for faster exact leave-one-out retraining of GBDT under the assumption that the model structure remains fixed, and explain how to use that framework for estimating the influence of training points on specific test samples. Secondly, we derive an iterative algorithm to compute gradients of GBDT predictions w.r. </p><formula xml:id="formula_7">Proxy 1. Inf grad (x train , x test ) := L(y test , F (x test )) − L(y test ,F \xtrain (x test ))</formula><p>, whereF \xtrain is the model retrained without x train .</p><p>Since, in order to rank the training points according to Inf grad (x train , x test ), we would have to compute Proxy 1 for each x train , straightforward leave-one-out retraining would be prohibitively expensive even for moderately-sized datasets. Moreover, as mentioned in Section 1, the parametric model framework of <ref type="bibr" target="#b9">Koh &amp; Liang (2017)</ref> is not directly applicable here. Thus, a solution tailored specifically for tree ensembles is required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">LEAFREFIT</head><p>In the problem definition (Section 2) we noted that training each tree requires picking its structure and leaf values. Moreover, these two operations respond to small training set perturbations differently: the tree structure is piecewise constant (i.e., it either stays the same or changes abruptly), whereas leaf values change more smoothly. Thus, a natural assumption to make is:</p><p>Assumption 1. The effect of removing a single training point can be estimated while treating each tree's structure as fixed.</p><p>Under Assumption 1, it is thus sufficient to estimate how the leaf values of each tree are going to change. Since selecting optimal feature splits, e.g. via CART <ref type="bibr" target="#b12">(Quinlan, 1986)</ref> or C4.5 <ref type="bibr" target="#b13">(Quinlan, 2014)</ref> algorithms, is often the computational bottleneck in fitting decision trees, this observation already yields a significant complexity reduction.</p><p>Thus, our first algorithm for approximate leave-one-out retraining, LeafRefit, is equivalent to fixing the structure of every tree and fitting leaf values without the removed point. A formal listing of the resulting algorithm is given in Algorithm 1.</p><p>Note that the effect of removing a training object x i is twofold: on each step, we have to (1) remove x i from its leaf (Algorithm 1, line 7) and (2) recalculate the leaf values and record the resulting changes of intermediate predictions for each training object (line 14). Thus, despite improving upon straightforward retraining by not having to search for the optimal tree splits, LeafRefit is still an expensive algorithm. Running it for each training sample has an asymptotic complexity of O(T n 2 ); moreover, in practice, for each training step t it involves an expensive routine of recalculating derivatives for each training point. </p><formula xml:id="formula_8">t l } T,L t=1,l=1 3: Initialize ∆ 0 i ← 0, A 0 i ← 0, i = 1 . . . n 4: for t = 1 to T do 5:Â t−1 i ← A t−1 i + ∆ t−1 i , i = 1 . . . n 6: for l = 1 to L do 7:Î t l ← I t l \ {i0} 8: if formula == Gradient then 9:f t l ← f G;t l ({Â t−1 i } i∈Î t l ) 10: else 11:f t l ← f N ;t l ({Â t−1 i } i∈Î t l ) 12: end if 13: ∆f t l ←f t l − f t l 14: ∆ t i ← ∆ t−1 i + ∆f t l , i ∈ I t l 15: end for 16: end for 17: return {f t l } T,L t=1,l=1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">FASTLEAFREFIT</head><p>We seek to limit the number of calculations at each step of LeafRefit. Note that, in LeafRefit, we generally cannot make any use of caching the original first and/or second derivatives, since any ∆ t−1 i (Algorithm 1, line 14) can be nonzero, which forces us to recompute the derivatives for each object. We build on the intuition that, in practice, a lot of ∆ t−1 i may be negligible; an extreme example is when training samples can be separated in disjoint cliques, i.e., I t1 l = I t2 l ∀t 1 , t 2 = 1, . . . , T , l = 1 . . . L. In this case, removing each training point only affects its clique I l0 := I 1 l0 . Thus, at each training step t, we may select a subset of training samples 3 U t whose deltas we take into account, and supposeÂ</p><formula xml:id="formula_9">t−1 i = A t−1 i ∀i / ∈ U t .</formula><p>We refer to U t as the update set. Combining this with caching the original A t−1 i and sums of derivatives in each leaf, we reduce the asymptotic complexity to O(T nC), where C = max t |U t |, which is a significant reduction if C n. A formal listing of the resulting algorithm, FastLeafRefit, is given in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">SELECTING THE UPDATE SET</head><p>In Section 3.1.2, we introduced FastLeafRefit, an approximate algorithm potentially achieving lower complexity than LeafRefit. Its definition, however, allowed for an arbitrary choice of the update set U t telling us which training points' prediction changes to take into account at boosting step t. It is intuitively clear that different strategies of selecting U t allow us to optimize the trade-off between computational complexity and quality of approximating leave-one-out retraining; thus, FastLeafRefit provides a principled way of obtaining approximations of different rigor to LeafRefit. Natural strategies for selecting the update set include:</p><p>SinglePoint: don't update any points' predictions and only</p><formula xml:id="formula_10">Algorithm 2 FastLeafRefit Input: i0, {I t l } T,L t=1,l=1 , {g t i (A t−1 i )} T,n t=1,i=1 , {h t i (A t−1 i )} T,n t=1,i=1 , {G t l (A t−1 )} T,L t=1,l=1 , {H t l (A t−1 )} T,L t=1,l=1 , leaf formula type formula Output: New leaf values {f t l } T,L t=1,l=1 Initialize ∆ 0 i ← 0, i = 1...n for t = 1 to T do U t ← UpdateSet(t) for l = 1 to L do U t l ← U t ∩ I t l f t l ← LeafRecalc(t, l, {I t l } T,L t=1,l=1 , {g t i (A t−1 i )} T,n t=1,i=1 , {h t i (A t−1 i )} T,n t=1,i=1 , G t l (A t−1 ), H t l (A t−1 ), U t l , formula) ∆f t l ←f t l − f t l ∆ t i ← ∆ t−1 i + ∆f t l , i ∈ I t l end for end for return {f t l } T,L t=1,l=1</formula><p>Algorithm 3 LeafRecalc</p><formula xml:id="formula_11">Input: boosting step t, leaf index l, {I t l } T,L t=1,l=1 , {g t i (A t−1 i )} T,n t=1,i=1 , {h t i (A t−1 i )} T,n t=1,i=1 , G t l (A t−1 ), H t l (A t−1 ), U t l , leaf formula type f ormula Output: New leaf valuef t l I ← I[i 0 ∈ I t l ] ∆ g t j ← g t j (A t−1 j + ∆ t−1 j ) − g t j (A t−1 j ), j ∈ U t l G t l ← G t l (A t−1 ) + j∈U t l w j ∆ g t j − Iw i0 g t i0 (A t−1 i0 ) if f ormula == Gradient then H t l ← j∈I t l \{i0} w j else ∆ h t j ← h t j (A t−1 j + ∆ t−1 j ) − h t j (A t−1 j ), j ∈ U t l H t l ← H t l (A t−1 ) + j∈U t l w j ∆ h t j − Iw i0 h t i0 (A t−1 i0 ) end if return −Ĝ t l H t l</formula><p>ignore the derivatives of i (the index of the training point to be removed) in each leaf, i.e., U t = ∅. Also note that this strategy is equivalent to disregarding dependencies between consecutive trees in GBDT and treating the ensemble like a Random Forest. Its complexity is O(T n).</p><p>AllPoints: make no approximations and update each point at each step, i.e., U t = {1, . . . , |X train |}. This reduces FastLeafRefit to LeafRefit.</p><p>TopKLeaves(k): this heuristic builds on the observation that, at each step t, each ∆ t j , j ∈ I t l increases over ∆ t−1 j by the same amount ∆f t l across the leaf l (see Algorithm 2). ∆f t l 's magnitude, in turn, is expected to be larger for leaves where ∆ t−1 j , j ∈ I t l (and, subsequently, ∆g t j ) are already large. Informally, the "snowball" effect holds: the larger the change accumulated in the leaf so far, the greater its value will change. Thus, to exploit this intuition, TopKLeaves(k) only updates ∆ t j of training points in k leaves with the largest accumulated prediction change so far:</p><formula xml:id="formula_12">U t = {i ∈ I t l | l ∈ {L t j } k j=1 }, L t = argsort − i∈I t l |∆ t−1 i |, l = 1 . . . L (3)</formula><p>Note: formally, this strategy is still O(T n 2 ) due to the fact that computing U t according to Eq. 3 takes O(n). In practice, overhead for computing Eq. 3 may be negligible because, firstly, sums of ∆ t−1 i can be quickly computed in a parallel or vectorized fashion and, secondly, because the complexity of addition is negligible compared to, e.g., calculating derivatives. However, if this still poses a problem, a natural way of getting around it is sampling m training points uniformly from X train and using a sample estimator of Eq. 3. The complexity of FastLeafRefit thus becomes O(T n[C + m]), which is useful if m n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Prediction gradients</head><p>In the previous sections we introduced LeafRefit and FastLeafRefit, fast methods of estimating the effect of a training sample on the GBDT ensemble, which can then be used to rank training points, e.g., by their influence on a test point of interest. Under Assumption 1, these methods are valid approximations of leave-one-out retraining, which gives them theoretical grounding. However, perhaps surprisingly, our experiments show that Assumption 1 fails quite often: in particular, for the Adult dataset (see Section 4.2), 58% of training points change the structures of at least one tree in the ensemble. Moreover, as shown in Section 4.3, when Assumption 1 is violated, LeafRefit and FastLeafRefit are no longer valid approximations to Proxy 1.</p><p>The intuition underlying Assumption 1, however, still holds: for a small enough perturbation to the training data, the structure will remain fixed, whereas leaf values will still be changing smoothly. Note that retraining the model without a sample i is equivalent to setting w new i = w old i + ∆w i ; ∆w i = −w old i . This change may be large enough to trigger structural shifts in the ensemble; thus, we need a tool to study a model's response to smaller (arbitrarily small) perturbations.</p><p>An obvious choice for such a tool is the derivative of a model's prediction w.r.t. a sample's weight, which was also a crucial tool in the Influence Functions framework from classical statistics <ref type="bibr" target="#b4">(Cook &amp; Weisberg, 1980)</ref>:</p><formula xml:id="formula_13">Proxy 2. Inf grad (x train , x test ) := ∂L(ytest,F (xtest)) ∂w i(x train )</formula><p>, where i(x train ) is the index of x train in X train .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">LEAFINFLUENCE</head><p>As mentioned above, in the setup of Proxy 2 the statement of Assumption 1 is now guaranteed to hold and is no longer an assumption; we may consider the tree structures to be fixed and only study perturbations of leaf values, which smoothly</p><p>Finding Influential Training Samples for Gradient Boosted Decision Trees depend on the weights. Using the chain rule</p><formula xml:id="formula_14">∂L(y, F (x; w)) ∂w i = ∂L(y, z) ∂z | z=F (x;w) · ∂F (x; w) ∂w i ,<label>(4)</label></formula><p>we can then derive various counterfactuals (e.g., "how would the loss on a test point change if we upweight a training point i?"), similarly to <ref type="bibr" target="#b9">Koh &amp; Liang (2017)</ref>. Since we have</p><formula xml:id="formula_15">∂F (x; w) ∂w i = T t=1 ∂f t P (x)t (A t−1 ) ∂w i ,<label>(5)</label></formula><p>for applying Eq. 4 to arbitrary x (for a fixed i) it is necessary and sufficient to calculate Expressions for leaf value derivatives depend on the type of leaf formula:</p><formula xml:id="formula_16">∂wi = − I t l (i)(f t G;l +g t i )+ j∈I t l w j h t j J(A t−1 ) ij H t G;l and ∂f t H;l ∂wi = − I t l (i)(h t i f t H;l +g t i )+ j∈I t l w j (k t j f t H;l +h t j )J(A t−1 ) ij H t H;l ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_17">I t l (i) := I[i ∈ I t l ] and J(A t ) ij := ∂A t j (w) ∂wi .</formula><p>Proof. First, let us derive the desired expression 5 for f t G;l (A t−1 (w), w):</p><formula xml:id="formula_18">∂f t G;l (A t−1 (w), w) ∂w i = − ∂ ∂w i G t l (A t−1 (w), w) H t G;l (A t−1 (w), w) = = − ∂G t l (A t−1 (w),w) ∂wi H t G;l (A t−1 (w), w) H t G;l (A t−1 (w), w) 2 + + ∂H t G;l (A t−1 (w),w) ∂wi G t l (A t−1 (w), w)</formula><p>H t G;l (A t−1 (w), w) 2 (7) Let us calculate the derivatives of G t l (A t−1 (w), w) and H t G;l (A t−1 (w), w) separately:</p><formula xml:id="formula_19">∂G t l (A t−1 (w), w) ∂w i = = j∈I t l δ ij g t j (A t−1 j (w)) + w j h t j (A t−1 j (w))J(A t−1 ) ij = = I t l (i)g t i (A t−1 i (w)) + j∈I t l w j h t j (A t−1 j (w))J(A t−1 ) ij ; ∂H t G;l (A t−1 (w), w) ∂w i = I t l (i) 4</formula><p>In Proposition 1's statement, arguments such as w or A t i are dropped for brevity.</p><p>5 Throughout this proof, we add w as an extra argument to the functions we study in order to highlight the dependency.</p><p>Plugging this back into Equations 7 and grouping terms with and without I t l (i) separately, we get:</p><formula xml:id="formula_20">∂f t G;l (A t−1 (w), w) ∂w i = −I t l (i) f t G;l + g t i (A t−1 i (w)) H t G;l (A t−1 (w), w) − − j∈I t l w j h t j (A t−1 j (w))J(A t−1 ) ij H t G;l (A t−1 (w), w)</formula><p>, which proves the first part of Proposition 1.</p><p>For the second part, all we have to change is to substitute H t H;l (A t−1 (w), w) for H t G;l (A t−1 (w), w). Its derivative is given by</p><formula xml:id="formula_21">∂H t G;l (A t−1 (w), w) ∂w i = = j∈I t l δ ij h t j (A t−1 j (w)) + w j k t j (A t−1 j (w))J(A t−1 ) ij = = I t l (i)h t i (A t−1 i (w)) + j∈I t l w j k t j (A t−1 j (w))J(A t−1 ) ij</formula><p>Just like before, plugging it back into Equations 7 and grouping terms containing and not containing I t l (i) separately, we get:</p><formula xml:id="formula_22">∂f t H;l (A t−1 (w), w) ∂w i = −I t l (i) h t i (A t−1 i )f t H;l + g t i (A t−1 i ) H t H;l (A t−1 (w), w) − − j∈I t l w j (k t j (A t−1 j )f t H;l + h t j (A t−1 j ))J(A t−1 ) ij H t H;l (A t−1 (w), w) .</formula><p>This concludes the proof of Proposition 1.</p><p>It can be seen from Eq. 6 that leaf value derivatives at step t depend on the Jacobi matrix J(A t−1 ) ij . These values, in turn, are connected by a recursive relationship:</p><formula xml:id="formula_23">J(A t ) ij = J(A t−1 ) ij + ∂f t P (xj )t ∂w i .<label>(8)</label></formula><p>Thus, we can calculate leaf value derivatives in an iterative fashion similar to LeafRefit. A formal listing of the resulting algorithm, LeafInfluence, can be found in Algorithm 4. Besides providing means for analyzing small weight perturbations, two important traits yielding complexity reductions can be seen from Eq. 6:</p><p>A. Using Eq. 6, we can write out ∇ w f  </p><formula xml:id="formula_24">t l } T,L t=1,l=1 , {g t i (A t−1 i )} T,n t=1,i=1 , {h t i (A t−1 i )} T,n t=1,i=1 , {k t i (A t−1 i )} T,n t=1,i=1 , leaf formula type formula Outputs: leaf value derivatives { ∂f t l (A t−1 ) ∂wi } T,L t=1,l=1 J(A 0 ) ij ← 0, i = 1 . . . n, j = 1 . . . n for t = 1 to T do ∂f t l (A t−1 ) ∂wi 0 ←/According to Eq. 6/, l = 1 . . . L J(A t ) ij ←/According to Eq. 8/,i = 1 . . . n, j = 1 . . . n end for return { ∂f t l (A t−1 ) ∂wi } T,L t=1,l=1</formula><p>each training object i whose influence we want to compute. This contrasts LeafInfluence with LeafRefit and FastLeafRefit, where these derivatives had to be recalculated for each i depending on the values of ∆ t−1 j , which change for different i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">FASTLEAFINFLUENCE</head><p>The final step to be made is analogous to the transition from LeafRefit to FastLeafRefit: LeafInfluence is, again, O(T n 2 ) because it has to compute matrix/vector products with the matrix J(A t−1 ) ij for every t. The same approximation that powers FastLeafRefit can be made here as well: at each step, we can select an update set U t and only take into account the influences of a subset of training objects on A t−1 . This is equivalent to assuming J(A t−1 ) ij = 0 ∀ j / ∈ U t , making J(A t−1 ) ij a sparse matrix with the number of nonzero elements in each row bounded by C := max t |U t |. Strategies of selecting U t and the resulting asymptotics become the same as described in Section 3.1.3, with the additional benefit of being able to compute the derivatives "off-line."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Research Questions</head><p>The experiments that we conduct can be broadly categorized as serving two purposes: (1) studying the fundamentals of our framework and (2) evaluating its quality in two applied problem setups. For the first part, the research questions that we seek to answer are as follows:</p><p>RQ1. How well do the different methods introduced in Sections 3.1 and 3.2 approximate their respective influence proxies?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ2.</head><p>Do smaller update sets significantly reduce the runtimes of FastLeafRefit and FastLeafInfluence? Does FastLeafInfluence yield a notable runtime speedup over FastLeafRefit?</p><p>For the second part, we proceed by considering two applied scenarios: (1) classification in the presence of label noise, and (2) classification with train/test domain mismatch. Specifically, the research questions for this part are:</p><p>RQ3. For Scenario 1, do our methods allow to detect noise in general and, more specifically, to identify training objects most harmful for specific test points?</p><p>RQ4. For Scenario 1, how do the proxies and their respective approximations compare in terms of quality?</p><p>RQ5. For Scenario 2, are our methods capable of detecting domain mismatch and, moreover, providing recommendations on how to fix it?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets and Framework</head><p>For our experiments with GBDT, we use CatBoost <ref type="bibr">(cat, 2018)</ref> an open-source implementation of GBDT by Yandex 6 . The datasets used for evaluation are: (1) Adult Data Set <ref type="bibr">(Adult, (dat, 1996)</ref>), (2) Amazon Employee Access Challenge dataset (Amazon, (dat, 2013)), (3) the KDD Cup 2009 Upselling dataset <ref type="bibr">(Upselling, (dat, 2009)</ref>) and, for the domain mismatch experiment, (4) the Hospital Readmission dataset <ref type="bibr" target="#b16">(Strack et al., 2014)</ref>. Dataset statistics and corresponding CatBoost parameters can be found in the supplementary material. Since we approach the problem as a search (for influential examples) problem, the main metrics we will be using are ranking metrics -specifically, DCG and NDCG(dcg, 2018) with linear gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Proxy Approximation Quality</head><p>Here, we evaluate how well do variations of FastLeafRefit and FastLeafInfluence match their respective Proxies 1 and 2. For that, we use the Adult Data Set. For LeafRefit, its validity heavily depends on whether Assumption 1 holds; thus, we split the training points into two disjoint sets based on whether they violate Assumption 1 (Changed in Table 2) or not. We then randomly sample n = 2000 points from both groups to ensure that they are equal in size and, in both of them, for each test object, we rank the train points with respect to their influence on this test object. We then measure NDCG@100 with respect to the relevance labels produced by ground-truth rankings induced by the respective proxies for LeafRefit and FastLeafRefit, Proxy 1 and Proxy 2. Finally, we average the results over the test objects. Results are given in <ref type="table" target="#tab_3">Table 2</ref>. <ref type="table" target="#tab_3">Table 2</ref> answers our RQ1. Firstly, as expected, LeafRefit and its faster variations only approximate Proxy 1 when Assumption 1 holds. When it does, the quality of FastLeafRefit uniformly increases with the update set size, reaching perfect results for Top64Leaves, which is equivalent to LeafRefit. On the other hand, LeafInfluence approximates Proxy 2 regardless of Assumption 1; the dependency of FastLeafInfluence on the update set is analogous to that of FastLeafRefit. This shows that LeafInfluence is more robust in approximating its corresponding proxy than LeafRefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Runtime Comparison</head><p>In this section, we compare different variations (update set choices) of FastLeafRefit and FastLeafInfluence in terms of their runtimes. For each dataset used in the study, we randomly pick k = 100 training objects for influence evaluation, calculate the resulting change in the model (new leaf values for FastLeafRefit and leaf value derivatives for FastLeafInfluence), measure the total elapsed wall time and divide the result by k to obtain the average time elapsed per one training object. The results are given in <ref type="figure">Fig. 1</ref>. Firstly, as expected, we observe that smaller update sets considerably reduce the runtimes of our algorithms, with the most radical speedup yielded by SinglePoint due to not having to recalculate any derivatives at all. Secondly, quite naturally, FastLeafInfluence performs much faster than FastLeafRefit, presumably due to vectorization and gradient precomputation (see end of Section 3.2.1). These observations confirm RQ2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Harmful Object Removal</head><p>In this experiment, we consider a particular use-case scenario, classification in the presence of label noise, and evaluate whether our methods are able to identify training objects that are (1) noisy, (2) harmful for specific test objects. In order to do that, we randomly select k training samples, 7 flip their labels, and obtain GBDT's predictions on test data <ref type="bibr">7</ref> We set k = 4000 for Adult and Amazon, and k = 3500 for Upselling.  before and after noise injection. We then conduct two experiments:</p><p>A. We sort the training points in ascending order of average influence on test objects and measure ROC-AUC of noise detection. In addition to variations of FastLeafRefit and FastLeafInfluence, we also compare against (1) A noise detection method exploiting the problem structure, which scores the training points using GBDT's prediction in favor of the class opposite to its observed label (Detector), (2) actual loss changes after leave-one-out retraining (Leave-One-Out), and (3) ground-truth binary labels of the train object being noisy (Oracle). The results are given in <ref type="figure" target="#fig_3">Fig. 2.</ref> B. We select n = 50 test points that suffered the largest Logloss increase, thus simulating problematic test objects. For each of these objects, we sort the training points in ascending order of influence and incrementally remove them from the training set in batches of m = 50 objects; on each iteration we measure the relative Logloss reduction both on this given test object and on the whole test set X test and, similarly to ranking, calculate DCG using these reductions as gains. Finally, we average these metrics over the n test points. The results are given in <ref type="figure" target="#fig_4">Fig. 3a and 3b.</ref> Firstly, from <ref type="figure" target="#fig_3">Fig. 2</ref>, we note that all variations of FastLeafRefit and FastLeafInfluence perform strongly on the over-all noise detection problem, where they score close to the top-performing Detector. Secondly, our methods greatly outperform their competitors (shown in blue on <ref type="figure" target="#fig_4">Fig. 3a</ref>) in targeting training objects harmful for a particular test object. These two observations confirm the hypothesis of RQ3. Finally, the two parts on <ref type="figure" target="#fig_4">Fig. 3 address RQ4</ref> by clearly showing the way in which larger update sets increase quality: while all approximations score comparably in targeting particular test objects, smaller update sets lead to worsening the overall test quality (except for Upselling); in other words, smaller update sets lead to overfitting the targeted test object. Proper configurations of TopKLeaves, on the other hand, allow to "fix" a specific test object without overfitting it (k=8, 22, 64 for Adult and Amazon).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Debugging Domain Mismatch</head><p>A common issue in the supervised machine learning is domain mismatch. This is a situation, when the joint distribution of points in the test dataset X test differs from the one in the labeled training dataset X train . Often in such scenarios, a model fine-tuned on the training dataset fails to produce accurate predictions on the test data. A standard way to cope with this problem is re-weighting X train .</p><p>In the following experiment we demonstrate how our methods allow to detect domain mismatch and get a hint on how the distribution of points in X train should be modified in order to better match the distribution of points in X test . The design of this experiment is a modification of the corresponding use-case of <ref type="bibr" target="#b9">Koh &amp; Liang (2017)</ref>. We use the same Hospital dataset (see Section 4.2), with each point being a hospital patient represented by 127 features and the goal is to predict the readmission. To introduce domain mismatch we bias the distribution in the training dataset by filtering out a subsample of patients with age ∈ [40; 50) and label y = 1. Originally we had 169/1853 readmitted patients in this group and 2140/20000 overall; after we get 17/1601 in the age ∈ [40; 50) group and 1988/19848 overall. Clearly, the distribution of labels in this specific age group becomes highly biased, while the proportion of positive labels in the whole dataset changes slightly (from 10.7% to 10.0%).</p><p>Training set X train is naturally split into four parts {X i train } 4 i=1 depending on the value of y and whether age ∈ [40; 50). One would expect that in the modified training dataset, samples with age ∈ [40; 50) and y = 1 are the most (positively) influential, so their removal will be the most harmful for the performance on the test dataset, while the removal of the samples with age ∈ [40; 50) and y = 0 might even be beneficial, since it is the most straightforward way to align the distributions in the test and train datasets. Below we confirm this expectation.</p><p>Let us focus on the subset X 0 test := {x ∈ X test | age(x) ∈ [40; 50)}, since its elements are expected to be the most affected by the introduced domain mismatch. We sample 100 points from every part {X i train } 4 i=1 (or take the whole part, if it has &lt; 100 points). For each of the methods FastLeafRefit and FastLeafInfluence with various update sets we compute the influence of the training samples on X 0 test . Specifically, (a) with FastLeafRefit, for an element x ∈ X train we find the average Logloss reduction on X 0 test , introduced by removing x; (b) with FastLeafInfluence, for an element x ∈ X train we find the derivative of the average Logloss on X 0 test with respect to the weight w of x at w = 1. <ref type="table" target="#tab_4">Table 3</ref> provides the average influence among the sampled train points with the fixed label y ∈ {0, 1} and the fixed indicator I(age ∈ [40; 50)) ∈ {0, 1}. As expected, with all methods, the train samples of the same type are consistently the most influential, which corresponds to maximal loss increase upon addition. In all cases removal of elements with y = 0 and age ∈ [40; 50) is estimated to be profitable, also confirming the initial expectations. These results allow us to answer RQ5 in the positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we addressed the problem of finding train objects that exerted the largest influence on the GBDT's prediction on a particular test object. Building on the Influence Function framework for parametric models, we derived LeafRefit and LeafInfluence, methods for estimating influences based on their respective proxy metrics, Proxies 1 and 2. By utilizing the structure of tree ensembles, we also derived computationally efficient approximations to these methods, FastLeafRefit and FastLeafInfluence. In our experiments, through considering several applied scenarios, we showed the practical applicability of these approaches, as well as their ability to produce actionable insights allowing to improve the existing model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>1 . . . T , l = 1 . . . L. Applying Eq. 4 can then be done by running x though a new tree ensemble having {</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>addition and a vector dot product, ∇ w f t l can then be expressed via vector addition and matrix/vector product for easy paral- lelization/vectorization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>j=1 used in Eq. 6 can now be precomputed only once during GBDT training and not for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: ROC-AUC of noise detection qualities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Mean DCG for relative Logloss reductions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Mathematical notations used in the paper.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Algorithm 1 LeafRefit 1: Input: training point index to remove i0, sample-to-leaf as- signments {IOutput: new leaf values {f</figDesc><table>t 

l } 

T,L 

t=1,l=1 , leaf formula type formula 
2: </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 :</head><label>2</label><figDesc>NDCG@100 of proxy ranking approximation.</figDesc><table>Method 
FastLeafRefit 
FastLeafInfluence 
Same Changed Same Changed 

SinglePoint 
0.38 
0.10 
0.39 
0.80 
Top1Leaves 
0.41 
0.10 
0.43 
0.81 
Top2Leaves 
0.53 
0.10 
0.52 
0.83 
Top8Leaves 
0.87 
0.10 
0.87 
0.94 
Top22Leaves 
0.96 
0.10 
0.95 
0.98 
Top64Leaves 
1.00 
0.10 
1.00 
1.00 

Figure 1: Wall times elapsed per training object for different 
variations of FastLeafRefitand FastLeafInfluence. Top k 
denotes the TopKLeaves(k) update set. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Influence of the points in X train on the loss on Xtest averaged in the corresponding sampled group (LR=LeafRefit, LI=LeafInfluence). age ∈ [40; 50) age ∈ [40; 50) Method y = 1 y = 0 y = 1 y = 0</figDesc><table>0 

LR SinglePoint 
-0.525 0.151 
0.084 
0.141 
LR Top1Leaves 
-0.515 0.150 
0.093 
0.140 
LR Top2Leaves 
-0.489 0.150 
0.103 
0.139 
LR Top8Leaves 
-0.397 0.147 
0.120 
0.137 
LR Top22Leaves -0.385 0.146 
0.124 
0.137 
LR Top64Leaves -0.384 0.146 
0.124 
0.137 

LI SinglePoint 
-0.652 0.015 -0.052 0.005 
LI Top1Leaves 
-0.642 0.014 -0.043 0.004 
LI Top2Leaves 
-0.616 0.014 -0.033 0.003 
LI Top8Leaves 
-0.524 0.011 -0.015 0.001 
LI Top22Leaves 
-0.512 0.011 -0.012 0.001 
LI Top64Leaves 
-0.511 0.010 -0.011 0.001 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Methods of selecting U t will be given below.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Proposition 1. Leaf value derivatives are given by: ∂f t G;l</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We use the "Plain" mode which disables CatBoost's conceptual modifications to the standard GBDT scheme.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Anna Veronika Dorogush for valuable commentary and discussions, as well as technical assistance with the CatBoost library.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Amazon employee access challenge dataset</title>
		<ptr target="https://www.kaggle.com/c/amazon-employee-access-challenge" />
	</analytic>
	<monogr>
		<title level="m">Kdd cup 2009 upselling dataset</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Catboost -open-source gradient boosting library</title>
		<ptr target="https://en.wikipedia.org/wiki/Discounted_cumulative_gain" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A unified view of gradient-based attribution methods for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ancona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ceolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Öztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06104</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Characterizations of an empirical influence function for detecting influential cases in regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Weisberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="495" to="508" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Dorogush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gusev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kazeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">O</forename><surname>Prokhorenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vorobev</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09516</idno>
		<title level="m">A. Fighting biases with dynamic boosting</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Interpretable explanations of black boxes by meaningful perturbation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03296</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Greedy function approximation: a gradient boosting machine. Annals of statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1189" to="1232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Random decision forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Document Analysis and Recognition</title>
		<meeting>the 3rd International Conference on Document Analysis and Recognition<address><addrLine>Montreal, QC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="278" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Understanding black-box predictions via influence functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04730</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Consistent feature attribution for tree ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06060</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interpreting random forest models using a feature contribution method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Palczewska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Palczewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neagu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Reuse and Integration (IRI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="112" to="119" />
		</imprint>
	</monogr>
	<note>IEEE 14th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Induction of decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="106" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">5: programs for machine learning. Elsevier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>C4</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Why should I trust you?: Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning important features through propagating activation differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundaje</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02685</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Impact of hba1c measurement on hospital readmission rates: analysis of 70,000 clinical database patient records</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Strack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Deshazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gennings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Olmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Cios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Clore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioMed research international</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01365</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interpretable predictions of tree-based ensembles via actionable feature tweaking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolomei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Silvestri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lalmas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="465" to="474" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
