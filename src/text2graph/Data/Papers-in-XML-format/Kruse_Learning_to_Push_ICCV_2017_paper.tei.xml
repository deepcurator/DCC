<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Push the Limits of Efficient FFT-based Image Deconvolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Kruse</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MPI of Molecular Cell Biology and Genetics</orgName>
								<address>
									<settlement>Dresden</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MPI of Molecular Cell Biology and Genetics</orgName>
								<address>
									<settlement>Dresden</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Schmidt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MPI of Molecular Cell Biology and Genetics</orgName>
								<address>
									<settlement>Dresden</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">U</forename><surname>Dresden</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MPI of Molecular Cell Biology and Genetics</orgName>
								<address>
									<settlement>Dresden</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Push the Limits of Efficient FFT-based Image Deconvolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image deblurring is a classic image restoration problem with a vast body of work in computer vision, signal processing and related fields (see <ref type="bibr" target="#b24">[25]</ref> for a fairly recent survey). In this work, we focus on the case of uniform blur, where the observed blurred image y = k ⊗ x + η is obtained via convolution of the true image x with known blur kernel (point spread function) k and additive Gaussian noise η. The task of recovering x is then called (non-blind) image deconvolution. Note that although the assumption of uniform blur is often not accurate <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>, such image deconvolution techniques can in fact outperform methods which assume a more realistic non-uniform blur model <ref type="bibr">[cf. 12]</ref>. Furthermore, image deconvolution can be used as a building block to address the removal of non-uniform blur <ref type="bibr">[cf. 28]</ref>.</p><p>When it comes to image deconvolution methods, it is useful to broadly separate them into two classes: (1) those where the most costly computational operations are a fixed number of Fourier transforms or convolutions, and (2) those which require more expensive computation, often due to (iterative) solvers for large linear systems of equations. While the first class of methods can scale to large megapixel-sized images, the latter class generally falls short in this regard.</p><p>These computationally demanding methods often exhibit high restoration quality [e.g., <ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31]</ref>, but typically need several minutes, or more, to deconvolve images of 1 megapixel (see current deblurring benchmarks [e.g., <ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24]</ref>). Of course, this runtime issue is even more severe for images that are multiple times larger, which are common nowadays. While the power of computers increases each year, so does the size of images taken by typical cameras. For the reasons outlined above, in this paper we focus on a class of deconvolution methods where the fast Fourier transform (FFT) is the most expensive operation, with computational complexity O(n log n) for input size, i.e. pixel count, n. Note that, while our proposed method is very efficient, we even slightly outperform state-of-the-art techniques which are considerably more computationally expensive.</p><p>Since image deconvolution is mathematically ill-posed in the presence of noise, some form of regularization has to be used to recover a restored image. A classic fast FFTbased deconvolution method is the Wiener filter <ref type="bibr" target="#b26">[27]</ref>, which uses quadratic regularization of the expected image spectrum to obtain the restored image in closed form. However, it is well-known that quadratic regularization is not ideal for natural images, which we assume here. Hence, better methods [e.g., <ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref> employ sparse regularization and iterative optimization, where each iteration is similar to a Wiener filter. More recently, the advent of discriminative deblurring <ref type="bibr" target="#b21">[22]</ref> has generalized these methods to yield even higher quality results without increasing the computational demands <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref>. In Section 2, we study these traditional FFT-based deconvolution methods and their more recent learning-based extensions. Based on this analysis, we propose a new, generalized learning-based approach utilizing the power of convolutional neural networks in Section 3.</p><p>In order to improve the quality of the restored image even further, we address the often neglected topic of image boundary handling. FFT-based deconvolution hinges on a blur model which assumes a convolution with periodic (circular) boundary conditions. Unfortunately, this assumption is almost never satisfied for blurred natural images, such as typical photographs degraded by camera shake or motion blur. To alleviate this problem, an observed blurred image is typically padded and pre-processed by an "edgeta-per" operation <ref type="bibr">1 [cf. 19]</ref>, which applies additional circular convolution to only the boundary region of the padded image. However, we want to go beyond this dated boundary processing approach. Towards this end, we take inspiration from recent work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref> and devise a simple, yet effective, boundary adjustment strategy that can easily be applied to any FFT-based deconvolution method, without introducing additional parameters or computational cost.</p><p>We show the efficacy of our proposed model and boundary adjustment method in various non-blind deconvolution experiments in Section 4, before we conclude in Section 5.</p><p>In this work, we solely focus on non-blind deconvolution, while recent research in the field has arguably shifted its focus towards blind deconvolution, which aims to estimate both the blur kernel k and the restored image x. However, most of these approaches make use of non-blind deconvolution steps [e.g., <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b28">29]</ref>. Recent discriminative methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28]</ref> alternate between updating the blur kernel and employing non-blind deconvolution to update the restored image. Hence, it remains important to develop better nonblind techniques.</p><p>In summary, our main contributions are threefold:</p><p>• We generalize discriminative FFT-based deconvolution approaches by using more powerful regularization based on convolutional neural networks.</p><p>• We propose a simple and effective boundary adjustment method that alleviates the problematic circular convolution assumption, which is necessary for FFTbased deconvolution.</p><p>• We obtain state-of-the-art results on non-blind deconvolution benchmarks, even when including methods that are computationally considerably more expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Review of FFT-based deconvolution</head><p>We consider the common blur model</p><formula xml:id="formula_0">y = k ⊗ x + η,<label>(1)</label></formula><p>where the observed corrupted image y is the result of circular convolution of the image x with blur kernel k plus Gaussian noise with variance σ 2 , i.e. η ∼ N (0, I/λ) with precision λ = 1/σ 2 and I being the identity matrix. For notational convenience, we assume all variables in bold to be vectors (lower case) or matrices (upper case).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Traditional approaches</head><p>A classic solution to obtain an estimate of the restored image is given by the Wiener filter <ref type="bibr" target="#b26">[27]</ref> aŝ</p><formula xml:id="formula_1">x = F −1 F(k ⊛ y) |F(k)| 2 + n/s ,<label>(2)</label></formula><p>1 cf. MATLAB's edgetaper function.</p><p>where F corresponds to the two-dimensional discrete Fourier transform and n = 1/λ and s are the expected power spectra of the noise and image, respectively. Note that k ⊛ y = F −1 (F(k) ⊙ F(y)) denotes correlation of y and k, where ⊙ is the entrywise (Hadamard) product and v is the complex conjugate of v. All other operations in Eq. (2), such as division, are applied entrywise.</p><p>The Wiener filter is very efficient due to FFT-based inference, but not state-of-the-art anymore. Many modern methods are based on minimizing an energy function</p><formula xml:id="formula_2">E(x) = λ 2 k ⊗ x − y 2 + i 1 T ρ i (f i ⊗ x),<label>(3)</label></formula><p>where the data term stems from the blur model of Eq. <ref type="formula" target="#formula_0">(1)</ref>  <ref type="formula" target="#formula_1">(2)</ref>, except that the spectrum s is replaced by β i |F(f i )| 2 :</p><formula xml:id="formula_3">x = F −1 F k ⊛ y |F(k)| 2 + β λ i |F(f i )| 2 .<label>(4)</label></formula><p>It is well-known that quadratic regularization leads to inferior restoration results for natural images. High-quality results can be achieved by using sparse (non-quadratic) regularization terms, e.g. with hyper-Laplacian penalty functions ρ i (u) = |u| α and 0 &lt; α ≤ 1 <ref type="bibr" target="#b12">[13]</ref>. Unfortunately, this makes energy minimization more complicated. To address this issue, it has been shown helpful to use half-quadratic splitting <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref>, in this context also known as quadratic penalty method [cf. 17, § 17.1]. To that end, the energy is augmented with latent variables z = {z 1 , . . . , z N } as</p><formula xml:id="formula_4">E β (x, z) = λ 2 k ⊗ x − y 2 + i 1 T ρ i (z i ) + β 2 f i ⊗ x − z i 2 ,<label>(5)</label></formula><p>such that E(x) = lim β→∞ E β (x, z). Energy minimization is now carried out in an iterative manner, where the latent variables and the restored image are updated at each step t:</p><formula xml:id="formula_5">z t+1 i = arg min zi E β (x t , z) = ψ i (f i ⊗ x t )<label>(6)</label></formula><formula xml:id="formula_6">x t+1 = arg min x E β (x, z t+1 ).<label>(7)</label></formula><p>In Eq. (6), ψ i = prox ρi/β is a 1D shrinkage function obtained as the proximal operator of penalty ρ i with parameter β −1 <ref type="bibr">[cf. 18]</ref>. Note that β needs to be increased during optimization such that the result of the optimization closely resembles a solution to the original energy of Eq. (3). By combining Eqs. <ref type="bibr" target="#b5">(6)</ref> and <ref type="formula" target="#formula_6">(7)</ref>, we obtain the following update equation for the restored image at step t:</p><formula xml:id="formula_7">x t+1 = F −1 F k ⊛ y + β λ φ(x t ) |F(k)| 2 + β λ i |F(f i )| 2 (8) with φ(x t ) = i f i ⊛ ψ i (f i ⊗ x t ).<label>(9)</label></formula><p>Note that Eq. <ref type="formula">(8)</ref> has the same form as Eq. <ref type="formula" target="#formula_3">(4)</ref> when using a quadratic penalty, with the only difference that the term</p><formula xml:id="formula_8">β λ φ(x t )</formula><p>, based on the current image estimate x t , appears in the numerator. While this change may seem insignificant, it does lead to deconvolution results of much higher quality when Eq. <ref type="formula">(8)</ref> is applied iteratively <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Note that there are many different variants of splitting methods [cf. 8] besides the one that we presented above, such as the popular alternating direction method of multipliers (ADMM) [cf. 17, § 17.4]. Applied in our context, ADMM is actually an extension of the splitting approach of Eqs. <ref type="formula" target="#formula_4">(5)</ref> to <ref type="formula" target="#formula_7">(9)</ref> with the benefit of converging more quickly to a minimum of Eq. (3). However, such improved convergence behavior is not relevant for us, since we will use a discriminative generalization of Eqs. <ref type="formula">(8)</ref> and <ref type="formula" target="#formula_7">(9)</ref> that does not aim to minimize Eq. (3) anymore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Discriminative learning-based approaches</head><p>Discriminative non-blind deblurring has been proposed by Schmidt et al. <ref type="bibr" target="#b19">[20]</ref> through "unrolling" iterative halfquadratic optimization for a fixed small number of steps. Good results can be achieved by learning specialized model parameters for each of the few optimization steps. Schmidt and Roth <ref type="bibr" target="#b20">[21]</ref> applied this idea to the iterative FFT-based deconvolution updates of Eq. (8) by learning step-specific weights β t , shrinkage functions ψ it , and linear filters f it to directly optimize the quality of the restored image, instead of minimizing Eq. (3). Crucial to their approach is that they directly modeled ψ it as differentiable functions with closedform expressions, which allowed them to use standard lossbased training with gradient-based methods. They called the resulting model a cascade of shrinkage fields (CSF).</p><p>Zhang et al. <ref type="bibr" target="#b29">[30]</ref> adopted a similar approach to shrinkage fields, but used fixed horizontal and vertical image gradient filters f it and replaced univariate shrinkage functions ψ it with standard convolutional neural networks (CNNs). However, they did not train the CNNs to directly optimize image quality. In the context of combining low-level and high-level image processing, Diamond et al. <ref type="bibr" target="#b8">[9]</ref> extended shrinkage fields specifically for color images by replacing shrinkage functions with CNNs that operate independently in the spatial domain but exploit correlations between color channels. Schuler et al. <ref type="bibr" target="#b22">[23]</ref> addressed discriminative blind deconvolution by making use of CNNs and alternating updates of the restored image and blur kernel. However, their image updates are based on a simple Wiener filter, where they used a flat spectrum n/s = β1 with learned scalar β.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our approach</head><p>Given the insights from the previous section, we now introduce our own approach which further generalizes the formulation of discriminative methods. After that, we describe our second contribution, which is a simple, yet effective boundary adjustment technique. An overview of our full approach is illustrated in <ref type="figure">Fig. 1</ref>.</p><p>Although in a limited manner, previous work <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30]</ref> has already attempted to replace the 1D shrinkage functions ψ i in Eq. <ref type="formula" target="#formula_7">(9)</ref> with CNNs that go beyond pixel-independent processing. However, we want to go further than just replacing ψ i and instead propose to replace φ (Eq. 9) altogether with a CNN, thereby generalizing Eq. <ref type="formula">(8)</ref>, since numerator and denominator are no longer coupled through shared filters f i . As a result, we alter the update step to</p><formula xml:id="formula_9">x t+1 = F −1 F k ⊛ y + 1 ωt(λ) φ CNN t (x t ) |F(k)| 2 + 1 ωt(λ) i |F(f it )| 2 ,<label>(10)</label></formula><p>where we make explicit that we learn a specialized CNNbased term φ CNN t for every step t besides the linear filters f it . Furthermore, we replace λ with a learned scalar function ω t (λ) that acts as a noise-specific regularization weight; this is necessary, because simply using λ = 1/σ 2 based on noise level σ empirically leads to sub-par results. Most previous work [e.g., <ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref> addressed this issue by learning a fixed regularization weight ω t , hence they need to train a separate model for each noise level σ. In contrast, Eq. (10) generalizes well to a range of noise levels if exposed to them during training (cf. Section 4). Note that we also remove the scalar weight β, since it can be absorbed into ω t (λ), which we parameterize as a multilayer perceptron.</p><p>In general, our motivation is to push the limits of a flexible and powerful regularization, without breaking the efficient FFT-based optimization, which is made possible by the assumptions underlying the common blur formation model of Eq. (1). To improve the quality of the restored image even further, we should also make sure that these assumptions are satisfied, which specifically are: (1) convolution is carried out with circular (periodic) boundary conditions and (2) that noise is additive and drawn pixelindependently from a Gaussian distribution.</p><p>While the Gaussian noise assumption can be problematic, especially in low-light conditions, we are not going to address this here. Instead, we focus on the issue that the circular convolution assumption is especially troubling for typical blurred photographs, since it can lead to strong restoration artifacts <ref type="bibr">[cf. 19]</ref>. A more realistic blur model is that the convolution y = k ⊗ V x does not go beyond the image boundary 2 of x. As a result, the observed blurred image y ∈ R m is actually smaller than the true image</p><formula xml:id="formula_10">x t y k λ φ CNN t (x t ) ϕ t (y, k, x t )</formula><p>Eq. (17)</p><formula xml:id="formula_11">F −1 F k ⊛ ϕ t (y, k, x t ) + 1 ωt(λ) φ CNN t (x t ) |F(k)| 2 + 1 ωt(λ) i |F(f it )| 2 Eq. (16) ω t (λ) x t+1 one model stage (1.5) −2</formula><p>Figure 1: Overview for one model stage. We propose an extension of iterative FFT-based deconvolution methods, which update the restored image x t → x t+1 at each step t. Specifically, we generalize shrinkage fields <ref type="bibr" target="#b20">[21]</ref> by removing unnecessary parameter sharing and replacing pixel-wise applied shrinkage functions with CNNs (φ CNN t ) that operate on the whole image, i.e. can take advantage of spatial dependencies between pixels (see <ref type="figure" target="#fig_6">Fig. 6</ref> for an example). Additionally, we take inspiration from <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref> and propose a simple, yet effective, strategy to better adhere to the circular blur assumption that underlies all FFT-based deconvolution methods; to that end, we replace the observed blurred image y with ϕt(y, k, x t ).</p><p>x ∈ R n , i.e. m &lt; n. Hence, we would ideally like to use unknown boundary conditions, i.e. disable the blur model at the boundary and only use the regularization term. Unfortunately, only determinate boundary conditions may lead to structured optimization problems that admit fast inference [cf. <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">16]</ref>. Of those, circular boundary conditions are arguably the most appealing, since they lead to equation systems with matrices that can be diagonalized in Fourier space, hence admit fast and closed-form image updates as presented throughout this section. Given this, we seek to modify the observed blurred image y to better adhere to the circular blur model, which we discuss next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Boundary adjustment</head><p>A common boundary pre-processing step is to first pad the observed blurred image y ∈ R m by replicating its edge pixels <ref type="bibr" target="#b2">3</ref> with linear operator P r ∈ R n×m such that P r y ∈ R n has the same size as the true image x ∈ R n . This is followed by the classic edgetaper operation [cf. 19] to arrive at the modified blurred image</p><formula xml:id="formula_12">y = edgetaper(P r y, k),<label>(11)</label></formula><p>which better adheres to the circular blur model. While this pre-processing approach goes a long way to reduce restoration artifacts, it is several decades old and does not solve the problem completely. In order to come up with a better approach, Matakos et al. <ref type="bibr" target="#b15">[16]</ref> and Almeida and Figueiredo <ref type="bibr" target="#b1">[2]</ref> proposed to change the blur model from Eq. (1) to</p><formula xml:id="formula_13">y = C(k ⊗ x) + η,<label>(12)</label></formula><p>where convolution is still carried out with periodic boundary conditions, but the result is cropped via multiplication <ref type="bibr" target="#b2">3</ref> Often called padding with "replicate" or "edge" mode.</p><p>with matrix C ∈ R m×n , such that only the inner part is retained, which is of the same size as the observed blurred image y. Note that C(k ⊗ x) = k ⊗ V x corresponds to the more realistic blur assumption, as described above. Using common regularizers, both <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref> then develop an efficient deconvolution algorithm based on the ADMM framework.</p><p>Since at the core of their approach is also a quadratic splitting technique, we can adopt it to extend Eq. (5). To that end, we replace k⊗x by the latent vector u and impose a soft constraint based on weight γ that favors both terms to be equal. With such a modification, we arrive at</p><formula xml:id="formula_14">E β,γ (x, z, u) = λ 2 Cu − y 2 + γ 2 k ⊗ x − u 2 + i 1 T ρ i (z i ) + β 2 f i ⊗ x − z i 2 ,<label>(13)</label></formula><p>where again E(x) = lim β→∞,γ→∞ E β,γ (x, z, u), but based on the new blur model of Eq. <ref type="bibr" target="#b11">(12)</ref>. We now employ the same alternating optimization as we have used in Eqs. <ref type="formula" target="#formula_4">(5)</ref> to <ref type="formula" target="#formula_6">(7)</ref>, but based on Eq. (13) and with the additional update step u t+1 = arg min u E β,γ (x t , z t , u). Note that the update steps for x and z actually do not change, the difference is only that u has taken the place of y (and γ that of λ). Again, we combine all equations to obtain the update of the restored image at step t as</p><formula xml:id="formula_15">x t+1 = F −1 F k ⊛ ϕ(y, k, x t ) + β γ φ(x t ) |F(k)| 2 + β γ i |F(f i )| 2 ,<label>(14)</label></formula><p>where φ is defined as in Eq. (9) and</p><formula xml:id="formula_16">ϕ(y, k, x t ) = λ γ C T C+I −1 λ γ C T y+(k ⊗ x t )<label>(15)</label></formula><formula xml:id="formula_17">= M I αP 0 y+(1−α)(k ⊗ x t ) +M E k ⊗ x t</formula><p>with α = λ/(λ + γ), P 0 = C T , and "masking" matrices M I and M E for interior and exterior (i.e. boundary) pixels,</p><formula xml:id="formula_18">x t k y + = M E · k ⊗ x t P 0 · y ϕ t (y, k, x t )</formula><p>Figure 2: Boundary adjustment approach of Eq. (17) for t &gt; 0. The current best estimate x t , the blurred image y and the blur kernel k are used to construct two images (middle), which are combined to give the boundary-adjusted observation (right). The circular blur model behind FFT-based deconvolution methods assumes knowledge of the circularly blurred boundary regions of the true image x. Since these are not available, we instead employ our current best estimate x t as a proxy for x and artificially blur its boundaries to be used instead. This allows us to better adhere to the circular blur model and as a consequence obtain image deconvolution results of higher quality.</p><p>respectively. The diagonal matrix M I = P 0 C ∈ R n×n has entries of 1 for all interior pixels and zeros otherwise; multiplication with M E = I − M I conversely retains all exterior pixels, while setting all interior pixels to zero (cf. <ref type="figure">Fig. 2</ref>). Note that multiplication with P 0 performs 0-padding, i.e. M I v can be understood as first cropping the boundary of v with subsequent padding of zeros.</p><p>While Eq. (15) may look complicated at first glance, its role can be understood quite easily. First, if we compare Eq. <ref type="formula">(8)</ref>, which does not use any boundary adjustment, to Eq. <ref type="formula" target="#formula_0">(14)</ref>, we find that y has been replaced by ϕ(y, k, x t ). Hence, we can interpret Eq. <ref type="formula" target="#formula_0">(15)</ref> as padding y with the boundary of k ⊗ x t (a circularly blurred copy of the current estimate of the deconvolved image); furthermore, the interior of y is replaced by a convex combination of y and k ⊗ x t based on weight α ∈ [0, 1]. Intuitively, it is very sensible to pad y with the boundary of k ⊗ x t , but only if x t is already similar to the true image x. By doing this, we essentially modify y to better adhere to the circular blur assumption. However, replacing the boundary in such a way does not seem to be a good idea for t = 0, since x 0 is usually initialized as the edgetapered blurred image y (Eq. 11) and thus is typically very dissimilar to the true image x. Consequently, we adopt this boundary modification approach only for t &gt; 0.</p><p>For t = 0, we use the standard edgetapered image y. Furthermore, we choose not to use a convex combination of k ⊗ x t and y for the interior pixels; since the blur model of Eq. <ref type="formula" target="#formula_0">(1)</ref> is actually valid for the interior pixels, we simply use y as is. Overall, this leads us to modify our previously chosen update equation (Eq. 10) to arrive at our final model</p><formula xml:id="formula_19">x t+1 = F −1 F k⊛ϕ t (y, k, x t ) + 1 ωt(λ) φ CNN t (x t ) |F(k)| 2 + 1 ωt(λ) i |F(f it )| 2 (16)</formula><p>with the boundary adjustment function</p><formula xml:id="formula_20">ϕ t (y, k, x t ) = y if t = 0 P 0 y + M E k ⊗ x t if t &gt; 0,<label>(17)</label></formula><p>which we visualize in <ref type="figure">Fig. 2</ref>. Our boundary strategy of Eq. <ref type="formula" target="#formula_0">(17)</ref> is very simple, yet effective (cf. Section 4.3), does not add any parameters or increase the computational burden, and can easily be applied to existing FFT-based deconvolution methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In the following we conduct experiments with our proposed model for the task of non-blind image deconvolution. We compare our results to the state-of-the-art on two popular datasets, both in terms of average peak signal-tonoise ratio (PSNR) and test runtime. Additional experiments show the effectiveness of our boundary strategy (Section 4.3) as compared to the common edgetapering (Eq. 11).</p><p>Our implementation <ref type="bibr" target="#b3">4</ref> is based on Keras <ref type="bibr" target="#b5">[6]</ref> and TensorFlow <ref type="bibr" target="#b0">[1]</ref>, allowing us to make use of built-in GPU acceleration and automatic differentiation for all model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model configuration and training</head><p>For all experiments, we parameterize φ CNN t (cf. <ref type="figure">Fig. 1</ref>) with a common CNN architecture of six sequential convolutional layers with 3×3 kernels. While the first five layers each have 32 feature channels followed by ELU <ref type="bibr" target="#b6">[7]</ref> activations, the final layer outputs a single channel and does not use a non-linear activation function. We choose a small multilayer perceptron to specify ω t (λ), using 3 hidden layers of 16 neurons each (with ELU activations); the final output neuron goes through a softplus activation function to ensure positivity. Finally, at each step t we use 24 linear filters f it of size 5×5 pixels for the denominator of Eq. (16).</p><p>Our full model consists of several identically structured stages as defined in Eq. <ref type="formula" target="#formula_0">(16)</ref>, each taking as input the prediction made by its predecessor. Since each stage hinges on the (fast) Fourier transform and all stages together form a single deep network, we call our model Fourier Deconvolution Network (FDN). Following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>, who report their best <ref type="figure">Figure 3</ref>: Top row. Examples of simulated blur kernels from <ref type="bibr" target="#b19">[20]</ref>, which we use for model training. Bottom row. The 8 blur kernels used in the benchmark datasets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>results with a greedy training scheme, we first train each successive stage individually. Since each stage is differentiable, we also investigate to jointly finetune the parameters of all stages in an end-to-end fashion. We apply Adam <ref type="bibr" target="#b10">[11]</ref> to minimize negative PSNR as our objective function.</p><p>We use grayscale images from the Berkeley segmentation dataset <ref type="bibr" target="#b2">[3]</ref> to train our model, specifically by extracting random patches that we then synthetically blur with a randomly selected blur kernel. To that end, we make use of simulated kernels taken from <ref type="bibr" target="#b19">[20]</ref> (see top row of <ref type="figure">Fig. 3</ref> for some examples). We add Gaussian noise to the blurred image and subsequently use 8-bit quantization for all pixels.</p><p>For all experiments, we train on 3000 random image patches x, which are blurred with kernels k of sizes up to 37 × 37 to yield blurred images y of 284 × 284 pixels each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation</head><p>We evaluate our model on two well-known benchmark datasets. The one compiled by Levin et al. <ref type="bibr" target="#b14">[15]</ref> consists of four 255 × 255 grayscale images, each optically blurred with a set of eight real-world blur kernels to yield 32 images in total. The eight kernels are shown in the bottom row of <ref type="figure">Fig. 3</ref>. The standard deviation σ of Gaussian noise on these blurred images is commonly stated as 1% of the dynamic range [e.g., <ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>, i.e. σ = 2.55 for typical images with pixel values 0 . . . 255. However, we found this to be inaccurate and empirically determined σ to be closer to 1.5.</p><p>Sun et al. <ref type="bibr" target="#b23">[24]</ref> use the same eight blur kernels as Levin et al., but apply each of them to synthetically blur 80 higher resolution images (long side scaled to 1024 pixels), yielding a benchmark dataset of 640 images in total. Finally, 1% Gaussian noise (i.e., σ = 2.55) is added to each image before 8-bit quantization.</p><p>Instead of following the common practice [e.g., 20, 21, 23] of training a specialized model for each noise level σ (here, 1.5 and 2.55), we instead learn a more versatile model from training data with various amounts of noise. Specifically, we create our training images by adding noise with σ uniformly chosen at random from the interval [1.0, 3.0], which allows us to train a single model that yields excellent results on both benchmark datasets. Please see the supplemental material for results that were obtained from models trained for either a single σ or wider range of noise levels.  <ref type="table">Table 1</ref>: Results for non-blind deblurring benchmarks. Average PSNR for two well-known deblurring benchmarks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref>, where each method uses the ground truth blur kernels. The second column denotes the noise level that the respective method was trained for, whereas the small numbers in parentheses in columns 3 and 4 denote the noise level assumed or given as input at test time. The upper part of the table shows efficient FFT-based methods, while methods in the lower part have higher computational cost. Scores marked with 1 , 2 and 3 quoted from <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b19">[20]</ref>, respectively; others computed with publicly available code.</p><p>As mentioned above, we consider two training variants: First, we greedily train our FDN model with 10 stages, which we abbreviate as FDN  T . We apply our two models to both benchmark datasets. Note that we strictly adhere to the evaluation protocol of the respective dataset to ensure a fair comparison, which includes discarding regions close to the border of each image. <ref type="table">Table 1</ref> shows the results of our models compared to other state-of-the-art methods on both datasets. We outperform our strongest competitors (EPLL <ref type="bibr" target="#b30">[31]</ref> and RTF <ref type="bibr" target="#b19">[20]</ref>, respectively) by around 0.2 -0.3 dB. Please see <ref type="figure" target="#fig_8">Fig. 7</ref> for a qualitative comparison. While this performance improvement may not seem very large, it is important to note that our approach is orders of magnitude faster than both EPLL and RTF (Section 4.4), neither of which can use efficient FFT-based inference.</p><p>While the FFT-based deconvolution method CSF <ref type="bibr" target="#b20">[21]</ref> has similar computational cost as our approach, we do outperform it on the benchmarks of <ref type="bibr">Sun et al. and Levin et al.</ref> by large margins of around 0.5 and 1 dB, respectively. Note that our improvements are already compared to more powerful CSF models <ref type="bibr" target="#b4">5</ref> that we trained on datasets of the same size as ours. One major reason for the inferior perfor- <ref type="bibr" target="#b4">5</ref> We use 24 filters of 5×5 pixels, since this most closely resembles our FDN model. A simpler pairwise CSF as used in <ref type="bibr" target="#b20">[21]</ref> performed much worse in our tests. CSF results are already saturated after 5 stages. mance of CSF is due to its use of edgetapering after each stage, which is clearly inferior to our boundary adjustment strategy (Section 4.3). In particular, we find that the performance of our FDN </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Boundary adjustment comparison</head><p>We compare our proposed boundary adjustment (BA) strategy (Our BA, cf. Eq. 17 and <ref type="figure">Fig. 2</ref>) to the traditional edgetapering method (ET once, cf. Eq. 11); <ref type="figure" target="#fig_2">Fig. 4</ref> provides an illustration for an example image. Since the CSF model <ref type="bibr" target="#b20">[21]</ref> additionally crops its current estimate of the restored image after each stage and re-applies edgetapering to it (ET each), we also compare against this BA method.</p><p>Furthermore, we not only compare these BA strategies within our FDN model, but also apply them to the CSF model and a standard Wiener filter. To that end, we train separate variants of each model that only differ in their BA strategy, but are otherwise trained in exactly the same way.</p><p>The results of our evaluation on the benchmark of Levin et al. <ref type="bibr" target="#b14">[15]</ref> are shown in <ref type="figure" target="#fig_5">Fig. 5</ref>; more details can be found in the supplemental material. First, we find that our BA strategy is always superior to using edgetapering, which also demonstrates the applicability to other FFT-based deconvolution methods. Especially remarkable is that we can boost the performance of a Wiener filter by over 1 dB when we apply it iteratively with our BA method. Second, the results allow us to better analyze the respective contributions from the CNN-based regularization on one hand, and our BA strategy on the other hand. Using ET each, after 5 stages we only see a modest improvement of 0.16 dB with our FDN model over CSF. However, we see a boost twice as large (0.32 dB) when using Our BA, which suggests that  our BA approach is actually important to exploit our more flexible CNN-based regularization. Third, we find that the performance of FDN does not improve further after stage 3 with ET each; this does not apply to our BA, which enables FDN to increase the PSNR by 0.58 dB within stages 4 -10 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Runtime</head><p>As mentioned before, when it comes to runtime, we find it useful to distinguish between deconvolution methods that admit efficient FFT-based inference on one hand, and much more computationally demanding methods, such as EPLL <ref type="bibr" target="#b30">[31]</ref> and RTF <ref type="bibr" target="#b19">[20]</ref>, on the other hand. Furthermore, FFTbased methods employ closed-form update steps and thus offer predictable runtime. In contrast, slower methods typically need to use iterative equation system solvers, whose runtime may vary significantly based on the given image and blur kernel. t (x t ) with their associated noise-specific weights ωt(λ) and the predictions x t+1 with t ∈ {0 . . . 4} for an example image (bottom far left, associated ground truth shown above; λ = 1/1.5</p><p>2 ). The green lines delineate the padded boundary region.  While the specific runtime of a method depends on software implementation and computing hardware, we find it instructive to give ballpark numbers for some of the methods shown in <ref type="table">Table 1</ref> Whereas CSF should have similar or slightly lower runtime compared to our method, EPLL and RTF are orders of magnitude slower. Based on their public CPU-based implementations, we find that they take around 1 minute for the small images from Levin et al., and in the order of 5 -10 minutes for the bigger Sun et al. images. While it is not entirely fair to compare such numbers to our GPU-based runtimes, it is evident that these slower methods are not practical for large images of potentially many megapixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We generalized efficient FFT-based deconvolution methods, specifically shrinkage fields, by introducing a CNN at each stage to provide more powerful regularization. Our model keeps all the benefits of fast FFT-based inference and discriminative end-to-end training, yet is shown to outperform much less efficient state-of-the-art methods on two non-blind deblurring benchmarks. We also proposed a simple, yet effective, scheme to better cope with the effects of the circular boundary assumption imposed by FFT-based inference. This method is generic, free of parameters, and shown to improve restoration results at essentially no extra cost. There are various avenues for future work, including the extension to blind deconvolution and non-uniform blur.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>initialization to jointly finetune all stages and denote the resulting model as FDN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example result of the proposed boundary adjustment method. Output of our greedy ten-stage model with our proposed boundary adjustment method (c) compared to just using traditional edgetapering (b). The boundary region outside the green inner square is discarded for the final output. While most of the changes are close to the image border, the difference image (d) shows that our boundary approach also has an effect on details within the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>10 G</head><label>10</label><figDesc>model would deteriorate by a substan- tial 0.74 dB on the benchmark of Levin et al. if we used the same boundary approach as CSF does.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of boundary adjustment methods. Average PSNR (in dB) on the benchmark of Levin et al. with different boundary adjustment (BA) strategies. Vertical partitions of bars correspond to respective model stages. Our BA method shows a clear improvement over edgetapering (ET). See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: CNN outputs with associated weights (top) and model predictions (bottom) for first 5 stages of FDN 10 G . From left to right, we show the CNN outputs φ CNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Comparison of deconvolution results. Result of our finetuned ten-stage model (c) compared to two state-of-the-art methods, EPLL (a) and RTF (b), for a challenging image from the Sun et al. benchmark. Our FDN model is able to restore fine details even in highly textured image regions. Images are best viewed magnified on a screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>. Our ten-stage model takes around 0.15 seconds for the small images from the dataset of Levin et al. (255 × 255 pixels), and roughly 0.75 seconds for the somewhat larger images from Sun et al. (less than 1 megapixel). These numbers are based on our TensorFlow implementation with an NVIDIA Titan X GPU.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Often called convolution with "valid" or "inner" boundary conditions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Code is available on our webpages.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Stages 6 -10 not shown in Fig. 5 for fair comparison to 5-stage CSF.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 12th USENIX Symposium on Operating Systems Design and Implementation<address><addrLine>Savannah, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deconvolving images with unknown boundaries using the alternating direction method of multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S C</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A T</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3074" to="3086" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural approach to blind motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 14th European Conference on Computer Vision</title>
		<meeting>14th European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="221" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On learning optimized reaction diffusion processes for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (ELUs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Proximal splitting methods in signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Combettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Pesquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bauschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Burachik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Combettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Elser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Luke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wolkowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fixed-Point Algorithms for Inverse Problems in Science and Engineering</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="185" to="212" />
		</imprint>
	</monogr>
	<note>Springer Optimization and Its Applications</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Dirty pixels: Optimizing image classification architectures for raw sensor data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heide</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06487</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nonlinear image recovery with halfquadratic regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="932" to="946" />
			<date type="published" when="1995-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recording and playback of camera shake: Benchmarking blind deconvolution with a real-world database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th European Conference on Computer Vision</title>
		<editor>A. Fitzgibbon, S. Lazebnik, P. Perona, Y. Sato, and C. Schmid</editor>
		<meeting>the 12th European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">7578</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast image deconvolution using hyper-Laplacian priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image and depth from a conventional camera with a coded aperture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<idno>70:1-70:9</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding and evaluating blind deconvolution algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Miami, Florida</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Accelerated edge-preserving image restoration without boundary artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Fessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2019" to="2029" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Numerical Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Proximal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Optimization</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="123" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast image restoration without boundary artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reeves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1448" to="1453" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cascades of regression tree fields for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="677" to="689" />
			<date type="published" when="2016-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Shrinkage fields for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discriminative non-blind deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to deblur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1439" to="1451" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Edge-based blur kernel estimation using patch priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computational Photography</title>
		<meeting>IEEE International Conference on Computational Photography</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.6838</idno>
		<title level="m">Recent progress in image deblurring</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A new alternating minimization algorithm for Total Variation image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="248" to="272" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The Extrapolation, Interpolation, and Smoothing of Stationary Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wiener</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1949" />
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<pubPlace>New York, N. Y.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning high-order filters for efficient blind deconvolution of document photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 14th European Conference on Computer Vision</title>
		<meeting>14th European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="734" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Two-phase kernel estimation for robust motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Computer Vision</title>
		<editor>K. Daniilidis, P. Maragos, and N. Paragios</editor>
		<meeting>the 11th European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6311</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning fully convolutional networks for iterative non-blind deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06495</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">From learning models of natural image patches to whole image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth IEEE International Conference on Computer Vision</title>
		<meeting>the Thirteenth IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
