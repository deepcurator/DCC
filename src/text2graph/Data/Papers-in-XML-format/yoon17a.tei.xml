<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combined Group and Exclusive Sparsity for Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Yoon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
						</author>
						<title level="a" type="main">Combined Group and Exclusive Sparsity for Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The number of parameters in a deep neural network is usually very large, which helps with its learning capacity but also hinders its scalability and practicality due to memory/time inefficiency and overfitting. To resolve this issue, we propose a sparsity regularization method that exploits both positive and negative correlations among the features to enforce the network to be sparse, and at the same time remove any redundancies among the features to fully utilize the capacity of the network. Specifically, we propose to use an exclusive sparsity regularization based on (1, 2)-norm, which promotes competition for features between different weights, thus enforcing them to fit to disjoint sets of features. We further combine the exclusive sparsity with the group sparsity based on (2, 1)-norm, to promote both sharing and competition for features in training of a deep neural network. We validate our method on multiple public datasets, and the results show that our method can obtain more compact and efficient networks while also improving the performance over the base networks with full weights, as opposed to existing sparsity regularizations that often obtain efficiency at the expense of prediction accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks have shown tremendous success in recent years, achieving near-human performances on tasks such as visual recognition <ref type="bibr" target="#b12">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b17">Szegedy et al., 2015;</ref><ref type="bibr" target="#b7">He et al., 2016)</ref>. One of the key factors in this success of deep network is its expressive power, which is made possible by multiple layers of non-linear transformations. However, this expressive power comes at a cost: increased number of parameters. Due to large 1 UNIST, Ulsan, South Korea 2 AITrics, Seoul, South Korea. Correspondence to: Sung Ju Hwang &lt;sjhwang@unist.ac.kr&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the 34</head><p>th International Conference on Machine Learning, Sydney, <ref type="bibr">Australia, PMLR 70, 2017</ref><ref type="bibr">. Copyright 2017</ref> by the author(s). number of parameters, deep networks require large amount of memory and computation power to train. Further, large number of parameters also mean that the model is highly susceptible to overfitting as well, if trained with insufficient data. To resolve such issues, researchers have sought ways to make the model more compact and lightweight by parameter reduction, via model compression <ref type="bibr" target="#b2">(Ba &amp; Caruana, 2014;</ref><ref type="bibr" target="#b8">Hinton et al., 2014)</ref>, or removing unnecessary weights either by pruning <ref type="bibr" target="#b14">(Reed, 1993;</ref><ref type="bibr" target="#b6">Han et al., 2015)</ref> and 1-regularization <ref type="bibr" target="#b3">(Collins &amp; Kohli, 2014)</ref>. However, one of the main problems of these methods is that they often achieve such efficiency at the expense of accuracy.</p><p>How can we then obtain a compact deep network without sacrificing the prediction accuracy? One way to achieve this goal is better utilizing the capacity of the network, by reducing redundancies in the model parameters. In the optimal case, the weights at each layer will be fully orthogonal to each other, and thus forming an orthogonal basis set. However, since this is a difficult constraint to satisfy, in practice, such constraint is given only at the initialization stage <ref type="bibr" target="#b15">(Saxe et al., 2014)</ref>, or enforced implicitly through regularizations such as dropout <ref type="bibr" target="#b16">(Srivastava et al., 2014</ref>) that prevents feature co-adaption. Contrary to these existing approaches, we propose to impose an explicit regularization to reduce redundancies. Our idea is to enforce network weights at each layer to fit to different sets of input features as much as possible. This exclusive feature learning is implemented by the exclusive sparsity regularization based on (1, 2)-norm <ref type="bibr" target="#b23">(Zhou et al., 2010;</ref><ref type="bibr" target="#b11">Kong et al., 2014)</ref>, which basically promotes network weights at each layer to compete for few meaningful features from the lower layer.</p><p>However, it is not practical nor desirable to restrict each weight to be completely disjoint from others as some features still need to be shared. For example, if the lowerlayer feature is a wheel, and the upper layer weights are features describing car and bicycle respectively, then the two upper layer weights should share the common feature that describes the wheel. Thus, we also allow for sharing of some important features, by introducing an additional group sparsity regularizer based on (2, 1)-norm and combine the two regularization terms, balancing their effect at each layer of the network to adjust the degree of feature sharing and competition.</p><p>Our combined regularizer can be applied to all layers of a generic deep neural network, including plain fullyconnected feedforward networks and convolutional networks. We validate our regularized network on four public datasets with different base networks, on which it achieves a compact, lighter model while achieving superior performance over networks trained with other sparsity-inducing regularizers, sometimes obtaining even better accuracy than the full model. As an example, on CIFAR-10 dataset, our network obtains 2.17% accuracy improvements while using 13.72% less number of parameters and 35.67% less floating point operations. Further empirical analysis shows that exclusive sparsity helps the network to converge faster to a given error rate, and learn less redundant features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Sparsity for deep neural networks Obtaining compact deep networks by removing unnecessary weights, is a longstudied topic in deep learning research. The most simplest yet popular weight removal method is to prune out weak weights by simple thresholding <ref type="bibr" target="#b14">(Reed, 1993;</ref><ref type="bibr" target="#b6">Han et al., 2015)</ref>. Another way to induce sparsity on weights is by 1-regularization <ref type="bibr" target="#b18">(Tibshirani, 1994)</ref>. <ref type="bibr" target="#b3">Collins &amp; Kohli (2014)</ref> applied the 1-regularization to convolutional neural networks, demonstrating that it can obtain a compact, memory-efficient network at the expense of small reduction in the prediction accuracy. Few recent work applied group sparsity <ref type="bibr" target="#b20">(Yuan &amp; Lin, 2006)</ref> regularization to deep networks, as it has a number of nice properties. By removing an entire feature group, group sparsity can automatically decide the number of neurons <ref type="bibr" target="#b1">(Alvarez &amp; Salzmann, 2016)</ref>. Further, if applied between the weights at different layers, it can also be used to decide optimal number of layers to use for the given network <ref type="bibr" target="#b19">(Wen et al., 2016)</ref>. In terms of efficiency, structured sparsity using (2, 1)-norm exhibits better data locality than the regular sparsity, and results in larger speedups <ref type="bibr" target="#b19">(Wen et al., 2016;</ref><ref type="bibr" target="#b1">Alvarez &amp; Salzmann, 2016)</ref>. We also employ the group sparsity in our combined regularizer, but we mainly group the features across multiple filters, to promote feature sharing among the filters. While all the previously introduced models do help reduce number of parameters and result in certain amount of speedups, such memory and time efficiency is mostly obtained at the expense of reduced accuracy. Our combined group and exclusive sparsity regularization, on the other hand, do not degenerate performance, since its aim in learning sparse weights/features is in removing redundancy to better utilize the network capacity.</p><p>Exclusive feature learning There exists quite a number of work on imposing exclusivity among the learned model parameters/features. One popular way is to enforce orthogonality, as this will minimize the dependency and redundancy among the variables that are being regularized. Orthogonality at initialization stage has been much studied in the deep learning context <ref type="bibr" target="#b15">(Saxe et al., 2014)</ref>, as in such a non-convex optimization setting this can lead to convergence to a better local optimum. <ref type="bibr" target="#b22">Zhou et al. (2011)</ref> enforced orthogonality via explicit dot product regularization to make the parameters for parent-level and childlevel classifiers in a hierarchical classifier to be orthogonal. However, the orthogonal regularizer is non-convex and does not scale well, since it scales quadratically to the number of participating vectors. Another way to enforce exclusivity is through (1, 2)-norm, which is basically the 2-norm over 1-norm groups, that results in promoting sparsity across different vectors. The (1, 2)-norm is first proposed in <ref type="bibr" target="#b23">(Zhou et al., 2010)</ref>, where it is used to promote competitions among the models jointly learned in a multitask learning framework. A similar regularizer was used in <ref type="bibr" target="#b9">(Hwang et al., 2011)</ref> in a metric learning setting, with an additional 1-regularization that helps learn discriminative features for each metric. <ref type="bibr" target="#b11">Kong et al. (2014)</ref> generalized the (1, 2)-norm to be used with arbitrary objective and handle overlapping groups. In deep learning context, <ref type="bibr" target="#b5">Goo et al. (2016)</ref> proposed a difference pooling technique that has a similar motivation to exclusive lasso, which subtracts the common superclass level feature map from the classspecific feature maps to learn class-exclusive features for fine-grained classification. In all existing models, exclusivity is applied only at the class-level, and application of the exclusivity regularization to weights at any layers of deep networks through (1, 2)-norm, has not yet been explored. Further, our regularizer is a combined term of both group and exclusive lasso which allows sharing of important features while making each weight to be as different as possible, rather than purely exclusive feature learning that is impractical. The regularizer proposed in <ref type="bibr" target="#b10">(Kim &amp; Xing, 2010</ref>) is similar to ours, which proposes a weighted (2, 1)-norm that has a similar effect of varying the degree of competition and grouping, although our regularizer is more explicit in its effect and optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our main objective is to implement a sparse deep neural network with significantly less number of parameters than what the original non-sparse network has, which at the same time obtains comparable or even better performance to the original model. The training objective for a generic (deep) neural network for classification 1 is given as follows:</p><formula xml:id="formula_0">min {W (l) } L({W (l) }, D) + λ L l=1 Ω(W (l) )<label>(1)</label></formula><formula xml:id="formula_1">Here, D = {x i , y i } N i=1</formula><p>is a training dataset with N instances where x i ∈ R d is a d-dimensional input feature and y i ∈ {1, . . . , K} is its class label which is one of the K classes, {W (l) } is the set of weights across all layers, L(W ) is the loss parameterized by W , L is the total number of layers, W (l) is the weight matrix (or tensor) for layer l, Ω(W (l) ) is some regularization term on the network weights at layer l, and λ is the regularization parameter that balances the loss with the regularization.</p><p>The usual and the most often used regularization term is the 2-norm:</p><formula xml:id="formula_2">Ω(W (l) ) = W (l) 2</formula><p>2 , which is also called as the 2 -regularizer. The regularization has an effect of adding a bias term to reduce variance of the model, which in turn results in a lower generalization error.</p><p>However, since our goal is in obtaining a sparse model where large portion of W (l) is zeroed out, we want Ω(W (l) ) to be a sparsity-inducing regularizer. The most common regularizer for promoting sparsity is the 1-norm:</p><formula xml:id="formula_3">Ω(W (l) ) = W (l) 1<label>(2)</label></formula><p>This 1-norm regularization results in obtaining a sparse weight matrix, since it requires the solution to be found at the corner of the 1-norm ball, thus eliminating unnecessary elements. The element-wise sparsity can be helpful when most of the features are irrelevant to the learning objective, as in the data-driven approaches. However, as aforementioned, when applied to a deep network it usually results in slight accuracy reduction. Further, element-wise sparsity, while achieving a memory-efficient model, usually do not result in meaningful speedups in practical network architectures such as CNNs, since the bottleneck is in the convolutional operations that do not reduce much when the number of filters stays the same <ref type="bibr" target="#b19">(Wen et al., 2016)</ref>.</p><p>Group sparsity, on the other hand, can help reduce the intrinsic complexity of the model by eliminating a neuron or a convolutional filter as a whole, and thus can help obtain practical speedups in deep neural networks <ref type="bibr" target="#b19">(Wen et al., 2016;</ref><ref type="bibr" target="#b1">Alvarez &amp; Salzmann, 2016)</ref>. The group sparsity regularization is defined as follows:</p><formula xml:id="formula_4">Ω(W (l) ) = g W (l) g 2 = g i w (l) g,i 2<label>(3)</label></formula><p>where g ∈ G is a weight group, W</p><p>g is the weight matrix (or a vector) for group g that is defined on W (l) , and w g,i is a weight at index i, for group g. Since 2 -norm has the grouping effect that results in similar weights for correlated features, this will result in complete elimination of some groups, thus removing some input neurons (See <ref type="figure">Figure 3</ref>, (a)). This has an effect of automatically deciding how many neurons to use at each layer. Exclusive sparsity, on the other hand, does not result in removal of any input neurons, but rather it makes each upper layer unit to select from a set of lower-layer units, that is disjoint from the sets used by other units.</p><p>Still, this group sparsity does not maximally utilize the capacity of the network since there still could be redundancy among the features that are selected. Thus, we propose to apply a sparsity-inducing regularization that obtains a sparse network weight matrix, while also minimizing the redundancy among network weights for better utilization of the network capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Exclusive Sparsity Regularization for Deep Neural Networks</head><p>Exclusive sparsity, or exclusive lasso was first introduced in <ref type="bibr" target="#b23">(Zhou et al., 2010)</ref> in multi-task learning context. The main idea in the work is to enforce the model parameters for different tasks to compete for features, instead of sharing features as suggested by previous work on multi-task learning that leverages group lasso. When the task is a classification task, this makes sense since the objective is to differentiate between classes which can be achieved by identifying discriminative feature for each class.</p><p>The generic exclusive sparsity regularization is defined as follows:</p><formula xml:id="formula_6">Ω(W (l) ) = 1 2 g W (l) g 2 1 = 1 2 g i |w (l) g,i | 2<label>(4)</label></formula><p>where w (l) g,i is the i th instance of the submatrix (or the vec-</p><formula xml:id="formula_7">tor) W (l)</formula><p>g . This norm is often called as (1, 2)-norm, and is basically the 2-norm over 1-norm groups. The sparsity is now enforced within each group, as opposed to the group sparsity regularizer which promotes inter-group sparsity. Applying 2-norm over these 1-norm groups will result in even weights among the groups; that is, all groups should have similar number of non-sparse weights, and thus no group can have large number of non-sparse weight. In <ref type="bibr" target="#b23">(Zhou et al., 2010)</ref>, W g is defined to be the model parameter for multiple tasks on the same feature, in which case the (1,2)-norm enforces each task predictor to fit to few features that are most useful for it. Exclusive sparsity can be straightforwardly applied to fully connected layers of a deep network, by grouping network weights from the same neuron at each layer into one group and applying (1, 2)-norm on these groups (See <ref type="figure" target="#fig_0">Figure 1(b)</ref>). This will enforce each output neuron to compete for input neurons, which will result in learning largely disparate network weights at each layer.</p><p>Exclusive sparsity on convolutional filters For convolutional layers of a convolutional neural network, exclusive sparsity can be applied in the same manner as in fully connected layers, where we apply Eq. 4 on the convolutional filters, while defining each group g as the same feature across multiple convolutional filters. <ref type="figure" target="#fig_1">Figure 2</ref>(c) illustrates the feature groups and effect of exclusive sparsity on the convolutional filters. This will enforce the convolutional filters to be as different as possible from each other, removing any redundancies between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Combined Group &amp; Exclusive Sparsity Regularization</head><p>As mentioned earlier, our main intuition is that there are varying degree of sharing and exclusivity among different features. Exclusivity alone cannot result in learning an optimal set of features, since some features need to be shared across multiple higher-level features. Thus we need to allow for some degree of sharing across the features, while still making each weight to be sufficiently different in order for each feature to be meaningful. How can we then come up with a regularizer that can achieve the two seemingly conflicting goals?</p><p>In tree guided group lasso <ref type="bibr" target="#b10">(Kim &amp; Xing, 2010)</ref>, each pair of weights are given different degree of sharing and competition based on the similarity between the tasks given by a taxonomy, which can be either semantically defined or obtained through clustering, through a regularization similar to an elastic-net formulation. While this model can be applied at the final softmax layer, on the softmax weight for each class, such taxonomy does not exist for the intermediate level network weights, and it is also not efficient to obtain them through clustering or other means.</p><p>Thus we propose to simply combine the group sparsity and the exclusive sparsity together, which will result in a similar effect, where network weights exhibit certain degree of sharing if they are correlated, but are learned to be different on other parts that are not shared. Our combined group and exclusive lasso regularizer is given as follows:</p><formula xml:id="formula_8">Ω(W (l) ) = g (1 − µ l ) W (l) g 2 + µ l 2 W (l) g 2 1<label>(5)</label></formula><p>where λ is the parameter that decides the entire regularization effect, W l is the weight matrix for l th layer, and µ l is the parameter for balancing the sharing and competition term at each layer.</p><p>Then how should we set the balancing term µ l at each layer? One simple solution is to set all µ l to be a single constant, but a better way is to set them differently at each layer, based on the degree of sharing and competition required at each layer. At lower layers, features will be quite generic and might need to be shared across all high-level neurons for accurate expression of the input data, wheareas at the top layer softmax weights, it would be better to have the weights to select features as disjoint as possible for better discriminativity. Thus, we set µ l = m + (1 − 2m) l L−1 , to reflect such intuition, where L is a total number of all layers, l ∈ {0, ..., L − 1} is an index of each layer, and 0 ≤ m ≤ 1 is the lowest parameter value for the exclusive sparsity term. If m = 0, the regularizer reduces to (2, 1)-norm regularizer with µ 1 = 0 at the lowest layer, while at the topmost softmax layer, the regularizer is an (1, 2)-norm regularizer µ 1 = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Numerical Optimization</head><p>Our regularized learning objective can be solved using proximal gradient descent, which is often used for optimizing objectives formed as a combination of both smooth and non-smooth terms. The proximal gradient algorithm for regularized objective first obtains the intermediate so-</p><formula xml:id="formula_9">lution W t+ 1 2</formula><p>by taking a gradient step using the gradient computed on the loss only, and then optimize for the regularization term while performing Euclidean projection of it to the solution space, as in the following formulation:</p><formula xml:id="formula_10">min Wt+1 Ω(W t+1 ) + 1 2λs W t+1 − W t+ 1 2 2 2<label>(6)</label></formula><p>where W t+1 is the variable to obtain after the current iteration, λ is the regularization parameter, and s is the step size.</p><p>When Ω(W t+1 ) is a group sparsity regularizer or an exclusive sparsity regularizer, the above problem has a closedform solution.</p><p>The solution, or the proximal operator for the group sparsity regularizer, prox GL (W ) is given as follows:</p><formula xml:id="formula_11">prox GL (W) = 1 − λ w g 2 + w g,i<label>(7)</label></formula><p>for all g and i, where g is each group, and i is an element of in each group. The proximal operator for the exclusive sparsity regularizer, prox EL (W), is obtained as follows:</p><formula xml:id="formula_12">prox EL (W) = 1 − λ w g 1 |w g,i | + w g,i = sign(w g,i )( w g,i − λ w g 1 ) +<label>(8)</label></formula><p>for all g and i. The combined regularizer can be optimized simply by applying the two proximal operators in a row at each gradient step, after updating the variable with the lossbased gradient. Algorithm 1 describes the proximal gradient algorithm for optimizing our regularized objective. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>We perform all experiments with convolutional neural network as the base network model. The regularization is applied at the network weights for all layers, excluding the bias term. All models are implemented and experimented using Tensorflow <ref type="bibr" target="#b0">(Abadi et al., 2016</ref>) framework 2 .</p><p>Baselines and our models We compare our regularized networks against relevant baselines.</p><p>1) 2 . The network trained with 2 -regularization.</p><p>2) 1 . The network trained with 1 -regularization, which has elementwise sparse network weights.</p><p>3) Group Sparsity-Filter. The network regularized with 2,1 -norm on the weights, which groups each convolutional filter as a group at convolutional layers. This network is an implementation of the model in <ref type="bibr" target="#b19">(Wen et al., 2016)</ref>. 4) Group Sparsity-Feature. The network that uses the same 2,1 -regularization as in 3), but with each group defined as the same feature at different filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Exclusive</head><p>Sparsity. This is the network whose weights at each layer are regularized with 1,2 -norm only.</p><p>6) Combined Group and Exclusive Sparsity. The network regularized with our combined structured sparsity on the weights. The combination weight that balances both regularizations are dynamically set at each layer.</p><p>Datasets and base networks We validate our method on four public datasets for classification, with four different convolutional networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) MNIST.</head><p>This dataset contains 70, 000 28 × 28 grayscale images of handwritten digits for training example images, where there is 6, 000 training instances and 1, 000 test instances per class. As for the base network, we use a simple convolutional neural network with two convolutional layers and two fully connected layers.</p><p>2) CIFAR-10. This dataset consists of 60, 000 images sized 32 × 32, from ten animal and vehicle classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck). For each class, there are 5, 000 images for training and 1, 000 images for test. For the base network, we use LeNet <ref type="bibr" target="#b13">(Lecun et al., 1998)</ref>, that has two convolutional layers followed by three fully connected layers.</p><p>3) CIFAR-100. This dataset also consists of 60, 000 images of 32 × 32 pixels as in CIFAR-10, but has 100 generic object classes instead of 10. For each class, 500 images are used for training and 100 images are used for test. For the base network, we use a variant of Wide Residual Network <ref type="bibr" target="#b21">(Zagoruyko &amp; Komodakis, 2016)</ref>, which has 16 layers with the widening factor of k = 10.  <ref type="figure">Figure 3</ref>. Accuracy-efficiency trade-off. We report the accuracy over number of parameters, and accuracy over FLOP to see how each sparsity-inducing regularization at various sparsity range affect the model accuracy. The reported results are average model accuracy over three runs (with random weight initialization), and the errorbars denote standard errors for 95% confidence interval. L2 and L1 are the networks trained with 2-regularization, 1-regularization, GS-filter and GS-feature are filter-wise and feature-wise group sparsity respectively, ES is our exclusive sparsity regularizer, and CGES is our proposed combined group and exclusive sparsity regularizer.</p><p>Large Scale Visual Recognition Challange <ref type="bibr" target="#b4">(Deng et al., 2009</ref>) that consists of 1, 281, 167 images from 1, 000 generic object categories. For evaluation, we used the validation set that consists of 50, 000 images, following the standard procedure. For the base network, we used an implementation of AlexNet <ref type="bibr" target="#b12">(Krizhevsky et al., 2012)</ref>.</p><p>For MNIST and CIFAR-10 experiment, we train all networks from the scratch; for CIFAR-100, and ImageNet-1K experiment where we use larger networks (WRN and AlexNet) we fine-tune the network from the 2 -regularized networks, since training them from scratch takes prohibitively long time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative analysis</head><p>We first validate whether our sparsity-inducing regularizations result in better accuracy-efficiency trade-off compared to baseline methods, by measuring the prediction accuracy over number of parameters, and number of floating point operations (FLOP) for each method. <ref type="figure">Figure 3</ref> shows the prediction accuracy of the different models over number of parameters/FLOP, obtained by differentiating the sparsity-inducing regularization parameter for each method. As expected, 1 -regularization greatly reduces the number of parameters, while maintaining a similar performance to the original model. The group sparsity regularization in general performs worse than 1 , but achieves better accuracy in certain sparsity ranges. The exclusive sparsity improves the performance over the base 2 -regularization model in low-sparsity range which is especially well shown in CIFAR-10 result, but degenerates performance as the sparsity increases. We attribute this to the fact that exclusive sparsity aims to make each weight/filter to fit to completely disjoint sets of low-level features, which is unrealistic as features may need to fit to the same set of low-level features for accurate representation.</p><p>Finally, our combined group and exclusive sparsity, which allows for certain degree of sharing between the weights/features while enforcing exclusivity, achieves the best accuracy/parameter trade-off, achieving similar or better performance gain to the exclusive sparsity while also greatly reducing the number of parameters <ref type="figure">. Fig 3(a)</ref> shows the results on the MNIST dataset, on which our CGES obtains no accuracy reduction, using 36.48% less number of parameters and 14.46% less computation. On CIFAR-10 dataset, CGES improves the classification accuracy over the 2 baselines by 2.17%, using 13.72% less number of parameters using 35.67% less FLOP. CGES obtains slight accuracy reduction of 1.15% on CIFAR-100 dataset, using only 51.22% of parameters and 42.77% less FLOP.</p><p>On ImageNet <ref type="table" target="#tab_1">(Table 1)</ref>, CGES obtains similar or slightly worse performance to the full network while using 60% − 68% of its parameters, while 1 shows noticeable performance degeneration at the same sparsity level. The network regularized with exclusive sparsity at all layers performed better with higher sparsity, compared to models that used ES only at convolutional, or fully connected layers. (c) Effect of µ l : ES-increasing is our combined regularizer, where exclusivity increases with network layer l. For ES-constant, we set µ l = 0.5 at all layers.   <ref type="bibr" target="#b6">Han et al. (2015)</ref> 98.71 ± 0.03% 76.37 ± 0.42% CGES 99.16 ± 0.03% 78.97 ± 0.41%</p><p>Iterative pruning Iterative pruning <ref type="bibr" target="#b6">(Han et al., 2015)</ref> is another effective method for obtaining a sparse network while maintaining high accuracy. As iterative pruning is orthogonal to our method, we can couple the two methods to obtain even better performance per number of parameters used; specifically, we replace the usual weight decay regularizer used in <ref type="bibr" target="#b6">(Han et al., 2015)</ref> with our CGES regularizer. We report the accuracy of this combined model on MNIST and CIFAR-10 dataset, when using 10% of the parameters of the full network ( <ref type="table" target="#tab_2">Table 2</ref>). The results show that CGES coupled with iterative pruning obtains similar or even better results to the original model using only a fraction of the parameters, significantly outperforming the base pruning model which suffers substantial accuracy loss.</p><p>Convergence speed We further analyze the empirical convergence rate of our regularized network, since it will be impractical if the regularized network requires much longer iterations to reach the same accuracy. Interestingly, we empirically found that our exclusive sparsity regularizer also helps network achieve the same error using much fewer iterations <ref type="figure" target="#fig_2">(Figure 4(a)</ref>), compared to base 2 -regularization. This faster convergence agrees with the observations in <ref type="bibr" target="#b15">(Saxe et al., 2014)</ref>, where networks whose weights are initialized as random orthgonal matrices converged faster than networks with random Gaussian initialization.</p><p>Convolutional vs. fully connected layers To see how much effect our combined regularizer has on different types of layers, we experiment applying the model only to the fully connected layer, or convolutional layers, while applying usual 2 -regularizer to other layers. <ref type="figure" target="#fig_2">Figure 4</ref>(b) shows the result of this experiment, where we plot the accuracy over percentage of parameters used, for models that applies ES only to fully connected layers, only to convolutional layers, and both. We observe improvements on all models, which shows the effectiveness of the exclusive sparsity regularizer to all types of network weights. Further, ES results in larger improvements on convolutional layers, which makes sense since lower-layer features are more important as they are more generic across different classes, than the features learned at fully connected layers. However, conv layers obtained the best accuracy at low-sparsity range, since strict enforcement of exclusivity hurts the representational power of the features, whereas FC layers obtained improvements even on high-sparsity range; this may be because loss of expressiveness could be compensated by better discriminativity of the features at high level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sharing vs. Competing for Features</head><p>We further explore how varying the degree of sharing and competition affect the accuracy and efficiency of the model, by experimenting with different configurations of µ l in Eq. 5 at each layer. We report the results in <ref type="figure" target="#fig_2">Figure 4</ref>(c). Specifically, we test two different approaches to balance the degree of sharing and competition at each layer. The first model, ES-Increasing, is the actual combination we have used in our method which increases the effect of exclusive sparsity with increasing l. This model reflects our intuition that competition will help at high layers, while sharing will help more at lower layers. The second model, ES-Constant combines the two terms with µ l = 0.5 throughout all layers. We observe that ES-Increasing works better than ES- Constant across all sparsity ranges, which shows that our scheme of increasing exclusivity at higher layers indeed helps improve the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative analysis</head><p>For further qualitative analysis, we visualize the weights and convolutional filters obtained using the baselines and our methods. <ref type="figure">Figure 5</ref> visualizes the weights of the softmax layer for different regularization methods, from the network trained on the CIFAR-10 dataset. Each row is the softmax parameter for each class. 2 and 1 work as expected, resulting in non-sparse and elementwise sparse weights. The group sparsity regularizer results in the total elimination of certain features that are not shared across multiple classes. The exclusive sparsity regularizer, when used on its own, results in disjoint feature selection for each class. However, when combined with the group lasso, it allows certain degree of feature reuse, while still obtaining parameters that are largely disparate across classes.</p><p>To show that such effect is not confined to the fully connected layer, we also visualize the convolutional filters in the first convolutional layer of the network trained on the CIFAR-10 dataset, in <ref type="figure">Figure 6</ref>. We observe that the combined group and exclusive sparsity regularizer results in filters that are much sharper than the ones that are obtained by 1 or group sparsity regularization, with some spatial features dropped altogether from the competition with other filters. Further, there is less redundancy among the filters, unlike the filters learned by other regularization methods. Note that we set the exclusivity factor µ 1 = 0.8 just for visualization purpose, since our weighting scheme will set µ 1 as a low value in the first convolutional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we proposed a novel regularizer for generic deep neural networks that effectively utilizes the capacity of the network, by exploiting the sharing and competing relationships among different network weights. Specifically, we propose to use an exclusive sparsity regularization based on (1, 2)-norm on the network weights, along with group sparsity regularization using (2, 1)-norm, such that exclusive sparsity enforces the network weights to use input neurons that are as different as possible from the other weights, while the group sparsity allows for some degree of sharing among them, as it is impossible to make the network weights to fit to completely disjoint set of features. We validate our method on some public datasets for both the accuracy and efficiency against other sparsity-inducing regularizers, and the results show that our combined regularizer helps obtain even better performance than the original full network, while significantly reducing the memory and computation requirements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of the regularizers: (a) When grouping weights from the the same input neuron into each group, the group sparsity has an effect of completely removing some neurons that are not shared across different weights (highlighted in red). (b) Exclusive sparsity, on the other hand, does not result in removal of any input neurons, but rather it makes each upper layer unit to select from a set of lower-layer units, that is disjoint from the sets used by other units.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Illustration of the effect of each regularizer on convoultional filters. (a) Group sparsity, when each group is defined as a filter, can result in complete elimination of some filters that are not shared among multiple high-level filters (Wen et al., 2016; Alvarez &amp; Salzmann, 2016). (b) Group sparsity, when applied across filters for the same feature, will remove certain spatial features as a whole. (c) Exclusive sparsity enforces each convolutional filter to learn features that are as different as possible, by promoting competition among the filters for the same spatial feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4 )</head><label>4</label><figDesc>ImageNet-1K. This is the dataset for 2012 ImageNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Further Analysis of the exclusive sparsity on CIFAR-10 dataset. (a) Convergence speed: Networks regularized with ES (Light Blue) or CGES (Dark Blue) converge fastest to a given error rate, compared to 2. (b) Effect of exclusive sparsity at different types of layers: The network regularized with exclusive sparsity at all layers performed better with higher sparsity, compared to models that used ES only at convolutional, or fully connected layers. (c) Effect of µ l : ES-increasing is our combined regularizer, where exclusivity increases with network layer l. For ES-constant, we set µ l = 0.5 at all layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Figure 5. Visualizations of the last fully connected layer weights on the CIFAR-10 dataset. These figures show the weight of first 50 weights out of 192 weights. The rows are output units for each class and the columns are features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Accuracy</figDesc><table>-efficiency trade-off on the Imagenet dataset. 
Model Accuracy % Params. Accuracy % Params. 
L2 
59.89% 
100.0% 
-
-
L1 
57.55% 
60.39% 
58.45% 
66.75% 
ES 
57.99% 
60.41% 
58.89% 
67.37% 
CGES 
58.56% 
60.66% 
59.25% 
67.24% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Performance of CGES coupled with iterative pruning.</figDesc><table>The reported results are averages over 3 runs and standard errors 
for 95% confidence interval. 
Model 
MNIST 
CIFAR-10 
L2 (Full Network) 
99.20% 
78.15% 
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">While our method is generic and can be also applied to regression, we only consider the classification task for simplicity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Codes available at https://github.com/jaehong-yoon93/CGES</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eugene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhifeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Craig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthieu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Tensorflow: Large-scale Machine Learning on Heterogeneous Distributed Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning the number of neurons in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1442</idno>
		<title level="m">Memory bounded deep convolutional networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fei-F˙Imagenet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Taxonomy-Regularized Semantic Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjoon</forename><surname>Goo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Juyong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning a tree of metrics with disjoint visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tree-guided group lasso for multitask regression with structured sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="543" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exclusive feature learning on arbitrary structures via 1 , 2-norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deguang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fujimaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryohei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiping</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Gradientbased Learning Applied to Document Recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pruning algorithms-a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Reed</surname></persName>
		</author>
		<idno type="doi">10.1109/72.248452</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="740" to="747" />
			<date type="published" when="1993-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Surya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yangqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumitru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning structured sparsity in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chunpeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2074" to="2082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical Classification via Orthogonal Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exclusive lasso for multi-task feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="988" to="995" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
