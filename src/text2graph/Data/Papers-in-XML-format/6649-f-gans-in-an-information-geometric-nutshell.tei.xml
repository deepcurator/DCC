<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">f -GANs in an Information Geometric Nutshell</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Australian National University</orgName>
								<orgName type="institution" key="instit2">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zac</forename><surname>Cranko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Australian National University</orgName>
								<orgName type="institution" key="instit2">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
							<email>aditya.menon@data61.csiro.au</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Australian National University</orgName>
								<orgName type="institution" key="instit2">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Australian National University</orgName>
								<orgName type="institution" key="instit2">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
							<email>bob.williamson@data61.csiro.au</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Australian National University</orgName>
								<orgName type="institution" key="instit2">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Data61</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Australian National University</orgName>
								<orgName type="institution" key="instit2">University of Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">f -GANs in an Information Geometric Nutshell</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Nowozin et al showed last year how to extend the GAN principle to all fdivergences. The approach is elegant but falls short of a full description of the supervised game, and says little about the key player, the generator: for example, what does the generator actually converge to if solving the GAN game means convergence in some space of parameters? How does that provide hints on the generator's design and compare to the flourishing but almost exclusively experimental literature on the subject? In this paper, we unveil a broad class of distributions for which such convergence happens -namely, deformed exponential families, a wide superset of exponential families -. We show that current deep architectures are able to factorize a very large number of such densities using an especially compact design, hence displaying the power of deep architectures and their concinnity in the f -GAN game. This result holds given a sufficient condition on activation functions -which turns out to be satisfied by popular choices. The key to our results is a variational generalization of an old theorem that relates the KL divergence between regular exponential families and divergences between their natural parameters. We complete this picture with additional results and experimental insights on how these results may be used to ground further improvements of GAN architectures, via (i) a principled design of the activation functions in the generator and (ii) an explicit integration of proper composite losses' link function in the discriminator.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In a recent paper, Nowozin et al. <ref type="bibr" target="#b29">[30]</ref> showed that the GAN principle <ref type="bibr" target="#b14">[15]</ref> can be extended to the variational formulation of all f -divergences. In the GAN game, there is an unknown distribution P which we want to approximate using a parameterised distribution Q. Q is learned by a generator by finding a saddle point of a function which we summarize for now as f -GAN(P, Q), where f is a convex function (see eq. <ref type="formula" target="#formula_11">(7)</ref> below for its formal expression). A part of the generator's training involves as a subroutine a supervised adversary -hence, the saddle point formulation -called discriminator, which tries to guess whether randomly generated observations come from P or Q. Ideally, at the end of this supervised game, we want Q to be close to P, and a good measure of this is the f -divergence I f (P Q), also known as Ali-Silvey distance <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref>. Initially, one choice of f was considered <ref type="bibr" target="#b14">[15]</ref>. Nowozin et al. significantly grounded the game and expanded its scope by showing that for any f convex and suitably defined, then <ref type="bibr" target="#b29">[30,</ref><ref type="bibr">Eq. 4</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>]:</head><p>f -GAN(P, Q) ≤ I f (P Q) .</p><p>(</p><p>The inequality is an equality if the discriminator is powerful enough. So, solving the f -GAN game can give guarantees on how P and Q are distant to each other in terms of f -divergence. This elegant characterization of the supervised game unfortunately falls short of justifying or elucidating all parameters of the supervised game <ref type="bibr" target="#b29">[30,</ref><ref type="bibr">Section 2.4]</ref>, and the paper is also silent regarding a key part of the game: the link between distributions in the variational formulation and the generator, the main player which learns a parametric model of a density. In doing so, the f -GAN approach and its members remain within an information theoretic framework that relies on divergences between distributions only <ref type="bibr" target="#b29">[30]</ref>. In the GAN world at large, this position contrasts with other prominent approaches that explicitly optimize geometric distortions between the parameters or support of distributions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>, and raises the problem of connecting the f -GAN approach to any sort of information geometric optimization. One such information-theoretic/information-geometric identity is well known: The Kullback-Leibler (KL) divergence between two distributions of the same (regular) exponential family equals a Bregman divergence D between their natural parameters <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b34">35]</ref>, which we can summarize as:</p><formula xml:id="formula_1">I fKL (P Q) = D(θ ϑ) .<label>(2)</label></formula><p>Here, θ and ϑ are respectively the natural parameters of P and Q. Hence, distributions are points on a manifold on the right-hand side, a powerful geometric statement <ref type="bibr" target="#b3">[4]</ref>; however, being restricted to KL divergence or "just" exponential families, it certainly falls short of the power to explain the GAN game. To our knowledge, the only generalizations known fall short of the f -divergence formulation and are not amenable to the variational GAN formulation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">Theorem 9]</ref>, <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">Theorem 3]</ref>.</p><p>Our first contribution is such an identity that connects the general I f -divergence formulation in eq.</p><p>(1) to the general D (Bregman) divergence formulation in eq. <ref type="bibr" target="#b1">(2)</ref>. We now briefly state it, postponing the details to Section 3:</p><formula xml:id="formula_2">f -GAN(P, escort(Q)) = D(θ ϑ) + Penalty(Q) ,<label>(3)</label></formula><p>for P and Q (with respective parameters θ and ϑ) which happen to lie in a superset of exponential families called deformed exponential families, that have received extensive treatment in statistical physics and differential information geometry over the last decade <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25]</ref>. The right-hand side of eq. <ref type="formula" target="#formula_2">(3)</ref> is the information geometric part <ref type="bibr" target="#b3">[4]</ref>, in which D is a Bregman divergence. Therefore, the f -GAN problem can be equivalent to a geometric optimization problem <ref type="bibr" target="#b3">[4]</ref>, like for the Wasserstein GAN and its variants <ref type="bibr" target="#b5">[6]</ref>. Notice also that Q appears in the game in the form of an escort <ref type="bibr" target="#b4">[5]</ref>. The difference vanish only for exponential families (escort(Q) = Q, Penalty(Q) = 0 and f = KL).</p><p>Our second contribution drills down into the information-theoretic and information-geometric parts of (3). In particular, from the former standpoint, we completely specify the parameters of the supervised game, unveiling a key parameter left arbitrary in <ref type="bibr" target="#b29">[30]</ref> (explicitly incorporating the link function of proper composite losses <ref type="bibr" target="#b31">[32]</ref>). From the latter standpoint, we show that the standard deep generator architecture is powerful at modelling complex escorts of any deformed exponential family, factorising a number of escorts in order of the total inner layers' dimensions, and this factorization happens for an especially compact design. This hints on a simple sufficient condition on the activation function to guarantee the escort modelling, and it turns out that this condition is satisfied, exactly or in a limit sense, by most popular activation functions (ELU, ReLU, Softplus, ...). We also provide experiments 1 that display the uplift that can be obtained through a principled design of the activation function (generator), or tuning of the link function (discriminator).</p><p>Due to the lack of space, a supplement (SM) provides the proof of the results in the main file and additional experiments. A longer version with a more exhaustive treatment of related results is available <ref type="bibr" target="#b26">[27]</ref>. The rest of this paper is as follows. Section § 2 presents definition, § 3 formally presents eq. (3), § 4 derives consequences for deep learning, § 5 completes the supervised game picture of <ref type="bibr" target="#b29">[30]</ref>, Section § 6 presents experiments and a last Section concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Definitions</head><p>Throughout this paper, the domain X of observations is a measurable set. We begin with two important classes of distortion measures, f -divergences and Bregman divergences.</p><p>Definition 1 For any two distributions P and Q having respective densities P and Q absolutely continuous with respect to a base measure µ, the f -divergence between P and Q, where f : R + → R is convex with f (1) = 0, is</p><formula xml:id="formula_3">I f (P Q) . = E X∼Q f P (X) Q(X) = X Q(x) · f P (x) Q(x) dµ(x) .<label>(4)</label></formula><p>For any convex differentiable ϕ : R d → R, the (ϕ-)Bregman divergence between θ and is:</p><formula xml:id="formula_4">D ϕ (θ ) . = ϕ(θ) − ϕ( ) − (θ − ) ∇ϕ( ) ,<label>(5)</label></formula><p>where ϕ is called the generator of the Bregman divergence.</p><p>f -divergences are the key distortion measure of information theory, while Bregman divergences are the key distortion measure of information geometry. A distribution P from a (regular) exponential family with cumulant C : Θ → R and sufficient statistics φ :</p><formula xml:id="formula_5">X → R d has density P C (x|θ, φ) . = exp(φ(x) θ − C(θ))</formula><p>, where Θ is a convex open set, C is convex and ensures normalization on the simplex (we leave implicit the associated dominating measure <ref type="bibr" target="#b2">[3]</ref>). A fundamental Theorem ties Bregman divergences and f -divergences: when P and Q belong to the same exponential family, and denoting their respective densities P C (x|θ, φ) and Q C (x|ϑ, φ), it holds that</p><formula xml:id="formula_6">I KL (P Q) = D C (ϑ θ). Here, I KL is Kullback-Leibler (KL) f -divergence (f . = x → x log x).</formula><p>Remark that the arguments in the Bregman divergence are permuted with respect to those in eq. <ref type="formula" target="#formula_1">(2)</ref> in the introduction. This also holds if we consider f KL in eq. (2) to be the Csiszár dual of f <ref type="bibr" target="#b7">[8]</ref>, namely f KL : x → − log x, since in this case I fKL (P Q) = I KL (Q P) = D C (θ ϑ). We made this choice in the introduction for the sake of readability in presenting eqs. <ref type="figure" target="#fig_0">(1 -3)</ref>. We now define generalizations of exponential families, following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref>. Let χ : R + → R + be non-decreasing <ref type="bibr" target="#b24">[25,</ref><ref type="bibr">Chapter 10]</ref>. We define the χ-logarithm, log χ , as log χ (z)</p><formula xml:id="formula_7">. = z 1 1 χ(t) dt. The χ-exponential is exp χ (z) . = 1 + z 0 λ(t)dt, where λ is defined by λ(log χ (z)) . = χ(z).</formula><p>In the case where the integrals are improper, we consider the corresponding limit in the argument / integrand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2 [5]</head><p>A distribution P from a χ-exponential family (or deformed exponential family, χ being implicit) with convex cumulant C : Θ → R and sufficient statistics φ :</p><formula xml:id="formula_8">X → R d has density given by P χ,C (x|θ, φ) . = exp χ (φ(x) θ − C(θ))</formula><p>, with respect to a dominating measure µ. Here, Θ is a convex open set and θ is called the coordinate of P. The escort density (or χ-escort) of P χ,C is</p><formula xml:id="formula_9">P χ,C . = 1 Z · χ(P χ,C ) , Z . = X χ(P χ,C (x|θ, φ))dµ(x) .<label>(6)</label></formula><p>Z is the escort's normalization constant.</p><p>We leaving implicit the dominating measure and denoteP the escort distribution of P whose density is given by eq. (6). We shall name χ the signature of the deformed (or χ-)exponential family, and sometimes drop indexes to save readability without ambiguity, noting e.g.P forP χ,C . Notice that normalization in the escort is ensured by a simple integration <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">Eq. 7]</ref>. For the escort to exist, we require that Z &lt; ∞ and therefore χ(P ) is finite almost everywhere. Such a requirement would be satisfied in the GAN game. There is another generalization of regular exponential families, known as generalized exponential families <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref>. The starting point of our result is the following Theorem, in which the information-theoretic part is not amenable to the variational GAN formulation.</p><p>Theorem 3 <ref type="bibr" target="#b4">[5]</ref>[36] for any two χ-exponential distributions P and Q with respective densities P χ,C , Q χ,C and coordinates θ, ϑ,</p><formula xml:id="formula_10">D C (θ ϑ) = E X∼Q [log χ (Q χ,C (X)) − log χ (P χ,C (X))].</formula><p>We now briefly frame the now popular (f -)GAN adversarial learning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30]</ref>. We have a true unknown distribution P over a set of objects, e.g. 3D pictures, which we want to learn. In the GAN setting, this is the objective of a generator, who learns a distribution Q θ parameterized by vector θ. Q θ works by passing (the support of) a simple, uninformed distribution, e.g. standard Gaussian, through a possibly complex function, e.g. a deep net whose parameters are θ and maps to the support of the objects of interest. Fitting Q . involves an adversary (the discriminator) as subroutine, which fits classifiers, e.g. deep nets, parameterized by ω. The generator's objective is to come up with</p><formula xml:id="formula_11">arg min θ L f (θ) with L f (θ) the discriminator's objective: L f (θ) . = sup ω {E X∼P [T ω (X)] − E X∼Q θ [f (T ω (X))]} ,<label>(7)</label></formula><p>where is Legendre conjugate <ref type="bibr" target="#b9">[10]</ref> and T ω : X → R integrates the classifier of the discriminator and is therefore parameterized by ω. L f is a variational approximation to a f -divergence <ref type="bibr" target="#b29">[30]</ref>; the discriminator's objective is to segregate true (P) from fake (Q . ) data. The original GAN choice, <ref type="bibr" target="#b14">[15]</ref> f GAN (z)</p><note type="other">.</note><p>= z log z − (z + 1) log(z + 1) + 2 log 2 (8) (the constant ensures f (1) = 0) can be replaced by any convex f meeting mild assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A variational information geometric identity for the f -GAN game</head><p>We deliver a series of results that will bring us to formalize eq. (3). First, we define a new set of distortion measures, that we call KL χ divergences.</p><p>Definition 4 For any χ-logarithm and distributions P, Q having respective densities P and Q absolutely continuous with respect to base measure µ, the KL χ divergence between P and Q is defined as KL χ (P Q)</p><formula xml:id="formula_12">. = E X∼P − log χ (Q(X)/P (X)) .</formula><p>Since χ is non-decreasing, − log χ is convex and so any KL χ divergence is an f -divergence. When χ(z) . = z, KL χ is the KL divergence. In what follows, base measure µ and absolute continuity are implicit, as well as that P (resp. Q) is the density of P (resp. Q). We now relate KL χ divergences vs f -divergences. Let ∂f be the subdifferential of convex f and</p><formula xml:id="formula_13">I P,Q . = [inf x P (x)/Q(x)</formula><p>, sup x P (x)/Q(x)) ⊆ R + denote the range of density ratios of P over Q. Our first result states that if there is a selection of the subdifferential which is upperbounded on I P,Q , the f -divergence I f (P Q) is equal to a KL χ divergence.</p><p>Theorem 5 Suppose that P, Q are such that there exists a selection ξ ∈ ∂f with sup ξ(I P,Q ) &lt; ∞.</p><formula xml:id="formula_14">Then ∃χ : R + → R + non decreasing such that I f (P Q) = KL χ (Q P).</formula><p>Theorem 5 essentially covers most if not all relevant GAN cases, as the assumption has to be satisfied in the GAN game for its solution not to be vacuous up to a large extent (eq. <ref type="formula" target="#formula_11">(7)</ref>). We provide a more complete treatment in the extended version <ref type="bibr" target="#b26">[27]</ref>. The proof of Theorem 5 (in SM, Section I) is constructive: it shows how to pick χ which satisfies all requirements. It brings the following interesting corollary: under mild assumptions on f , there exists a χ that fits for all densities P and Q. A prominent example of f that fits is the original GAN choice for which we can pick</p><formula xml:id="formula_15">χ GAN (z) . = 1 log 1 + 1 z .<label>(9)</label></formula><p>We now define a slight generalization of KL χ -divergences and allow for χ to depend on the choice of the expectation's X, granted that for any of these choices, it will meet the constraints to be R + → R + and also increasing, and therefore define a valid signature. For any f : X → R + , we denote KL χ f (P Q)</p><formula xml:id="formula_16">. = E X∼P − log χ f (X) (Q(X)/P (X))</formula><p>, where for any p ∈ R + , χ p (t)</p><formula xml:id="formula_17">. = 1 p · χ(tp).</formula><p>Whenever f = 1, we just write KL χ as we already did in Definition 4. We note that for any x ∈ X, χ f (x) is increasing and non negative because of the properties of χ and f , so χ f (x) (t) defines a χ-logarithm. We are ready to state a Theorem that connects KL χ -divergences and Theorem 3.</p><formula xml:id="formula_18">Theorem 6 Letting P . = P χ,C and Q . = Q χ,C for short in Theorem 3, we have E X∼Q [log χ (Q(X)) − log χ (P (X))] = KL χQ (Q P) − J(Q), with J(Q) . = KL χQ (Q Q).</formula><p>(Proof in SM, Section II) To summarize, we know that under mild assumptions relatively to the GAN game, f -divergences coincide with KL χ divergences (Theorems 5). We also know from Theorem 6 that KL χ. divergences quantify the geometric proximity between the coordinates of generalized exponential families (Theorem 3). Hence, finding a geometric (parameter-based) interpretation of the variational f -GAN game as described in eq. <ref type="formula" target="#formula_11">(7)</ref> can be done via a variational formulation of the KL χ divergences appearing in Theorem 6. Since penalty J(Q) does not belong to the GAN game (it does not depend on P), it reduces our focus on KL χQ (Q P).</p><p>Theorem 7 KL χQ (Q P) admits the variational formulation</p><formula xml:id="formula_19">KL χQ (Q P) = sup T ∈R++ X E X∼P [T (X)] − E X∼Q [(− log χQ ) (T (X))] ,<label>(10)</label></formula><p>with R ++ . = R\R ++ . Furthermore, letting Z denoting the normalization constant of the χ-escort of Q, the optimum T * : X → R ++ to eq. <ref type="formula" target="#formula_19">(10)</ref> is T * (x) = −(1/Z) · (χ(Q(x))/χ(P (x))).</p><p>(Proof in SM, Section III) Hence, the variational f -GAN formulation can be captured in an information-geometric framework by the following identity using Theorems 3, 5, 7.</p><p>Corollary 8 (the variational information-geometric f -GAN identity) Using notations from Theorems 6, 7 and letting θ (resp. ϑ) denote the coordinate of P (resp. Q), we have:</p><formula xml:id="formula_20">sup T ∈R++ X E X∼P [T (X)] − E X∼Q [(− log χQ ) (T (X))] = D C (θ ϑ) + J(Q) .<label>(11)</label></formula><p>We shall also name for short vig-f -GAN the identity in eq. <ref type="bibr" target="#b10">(11)</ref>. We note that we can drill down further the identity, expressing in particular the Legendre conjugate (− log χQ ) with an equivalent "dual" (negative) χ-logarithm in the variational problem <ref type="bibr" target="#b26">[27]</ref>. The left hand-side of Eq. <ref type="formula" target="#formula_0">(11)</ref>   <ref type="bibr" target="#b29">[30]</ref> can guarantees convergence in the parameter space (ϑ vs θ). In the realm of GAN applications, it makes sense to consider that P (the true distribution) can be extremely complex. Therefore, even when deformed exponential families are significantly more expressive than regular exponential families <ref type="bibr" target="#b24">[25]</ref>, extra care should be put before arguing that complex applications comply with such a geometric convergence in the parameter space. One way to circumvent this problem is to build distributions in Q that factorize many deformed exponential families. This is one strong point of deep architectures that we shall prove next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Deep architectures in the vig-f -GAN game</head><p>In the GAN game, distribution Q in eq. <ref type="formula" target="#formula_0">(11)</ref> is built by the generator (call it Q g ), by passing the support of a simple distribution (e.g. uniform, standard Gaussian), Q in , through a series of non-linear transformations. Letting Q in denote the corresponding density, we now compute Q g . Our generator g : X → R d consists of two parts: a deep part and a last layer. The deep part is, given some L ∈ N, the computation of a non-linear transformation</p><formula xml:id="formula_21">φ L : X → R d L as R d l φ l (x) . = v(W l φ l−1 (x) + b l ) , ∀l ∈ {1, 2, ..., L} , (12) φ 0 (x) . = x ∈ X .<label>(13)</label></formula><p>v is a function computed coordinate-wise, such as (leaky) ReLUs, ELUs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>,</p><formula xml:id="formula_22">W l ∈ R d l ×d l−1 , b l ∈ R d l . The last layer computes the generator's output from φ L : g(x) . = v OUT (Γφ L (x)+ β), with Γ ∈ R d×d L , β ∈ R d</formula><p>; in general, v OUT = v and v OUT fits the output to the domain at hand, ranging from linear <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref> to non-linear functions like tanh <ref type="bibr" target="#b29">[30]</ref>. Our generator captures the high-level features of some state of the art generative approaches <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>To carry our analysis, we make the assumption that the network is reversible, which is going to reguire that v OUT , Γ, W l (l ∈ {1, 2, ..., L}) are invertible. We note that popular examples can be invertible (e.g. DCGAN, if we use µ-ReLU, dimensions match and fractional-strided convolutions are invertible). At this reasonable price, we get in closed form the generator's density and it shows the following: for any continuous signature χ net , there exists an activation function v such that the deep part in the network factors as escorts for the χ net -exponential family. Let 1 i denote the i th canonical basis vector.</p><p>Theorem 9 ∀v OUT , Γ, W l invertible (l ∈ {1, 2, ..., L}), for any continuous signature χ net , there exists activation v and b l ∈ R d (∀l ∈ {1, 2, ..., L}) such that for any output z, letting x</p><formula xml:id="formula_23">. = g −1 (z), Q g (z) factorizes as Q g (z) = (Q in (x)/Q deep (x)) · 1/(H out (x) · Z net ), with Z net &gt; 0 a constant, H out (x) . = d i=1 |v OUT (γ i φ L (x) + β i )|, γ i .</formula><p>= Γ 1 i , and (letting w l,i . = W l 1 i ):</p><formula xml:id="formula_24">Q deep (x) . = L l=1 d i=1P χnet,b l,i (x|w l,i , φ l−1 ) .<label>(14)</label></formula><p>Name</p><formula xml:id="formula_25">v(z) χ(z) ReLU ( §) max{0, z} 1z&gt;0</formula><p>Leaky-ReLU  (Proof in SM, Section IV) The relationship between the inner layers of a deep net and deformed exponential families (Definition 2) follows from the theorem: rows in W l s define coordinates, φ l define "deep" sufficient statistics, b l are cumulants and the crucial part, the χ-family, is given by the activation function v. Notice also that the b l s are learned, and so the deformed exponential families' normalization is in fact learned and not specified. We see thatQ deep factors escorts, and in number. What is notable is the compactness achieved by the deep representation: the total dimension of all deep sufficient statistics inQ deep (eq. <ref type="formula" target="#formula_3">(14)</ref>) is L · d. To handle this, a shallow net with a single inner layer would require a matrix W of space Ω(</p><formula xml:id="formula_26">( †) z if z &gt; 0 z if z ≤ 0 1 if z &gt; −δ 1 if z ≤ −δ (α, β)-ELU (♥) βz if z &gt; 0 α(exp(z) − 1) if z ≤ 0 β if z &gt; α z if z ≤ α prop-τ (♣) k + τ (z) τ (0) τ −1 •(τ ) −1 (τ (0)z) τ (0) Softplus (♦) k + log 2 (1 + exp(z)) 1 log 2 · 1 − 2 −z µ-ReLU (♠) k + z+ √ (1−µ) 2 +z 2 2 4z 2 (1−µ) 2 +4z 2 LSU k +    0 if z &lt; −1 (1 + z) 2 if z ∈ [−1, 1] 4z if z &gt; 1 2 √ z if z &lt; 4 4 if z &gt; 4</formula><formula xml:id="formula_27">L 2 · d 2 ). The deep net g requires only O(L · d 2 )</formula><p>space to store all W l s. The proof of Theorem 9 is constructive: it builds v as a function of χ. In fact, the proof also shows how to build χ from the activation function v in such a way thatQ deep factors χ-escorts. The following Lemma essentially says that this is possible for all strongly admissible activations v.</p><p>Definition 10 Activation function v is strongly admissible iff dom(v) ∩ R + = ∅ and v is C 1 , lowerbounded, strictly increasing and convex. v is weakly admissible iff for any &gt; 0, there exists v strongly admissible such that ||v − v || L1 &lt; , where ||f || L1 . = |f (t)|dt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 11</head><p>The following holds: (i) for any strongly admissible v, there exists signature χ such that Theorem 9 holds; (ii) (γ,γ)-ELU (for any γ &gt; 0), Softplus are strongly admissible. ReLU is weakly admissible.</p><p>(proof in SM, Section V) The proof uses a trick for ReLU which can easily be repeated for (α, β)-ELU, and for leaky-ReLU, with the constraint that the domain has to be lowerbounded. <ref type="table">Table 1</ref> provides some examples of strongly / weakly admissible activations. It includes a wide class of so-called "prop-τ activations", where τ is negative a concave entropy, defined on [0, 1] and symmetric around 1/2 <ref type="bibr" target="#b28">[29]</ref>. This concludes our treatment of the information geometric part of the vig-f -GAN identity. We now complete it with a treatment of its information-theoretic part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">A complete proper loss picture of the supervised GAN game</head><p>In their generalization of the GAN objective, Nowozin et al. <ref type="bibr" target="#b29">[30]</ref> leave untold a key part of the supervised game: they split in eq. (7) the discriminator's contribution in two, T ω = g f • V ω , where V ω : X → R is the actual discriminator, and g f is essentially a technical constraint to ensure that V ω (.) is in the domain of f . They leave the choice of g f "somewhat arbitrary" [30, Section 2.4]. We now show that if one wants the supervised loss to have the desirable property to be proper composite <ref type="bibr" target="#b31">[32]</ref> 2 , then g f is not arbitrary. We proceed in three steps, first unveiling a broad class of proper f -GANs that deal with this property. The initial motivation of eq. <ref type="formula" target="#formula_11">(7)</ref> was that the inner maximisation may be seen as the f -divergence between P and Q θ <ref type="bibr" target="#b25">[26]</ref>, L f (θ) = I f (P Q θ ). In fact, this variational 2 informally, Bayes rule realizes the optimum and the loss accommodates for any real valued predictor.</p><p>representation of an f -divergence holds more generally: by <ref type="bibr" target="#b32">[33,</ref><ref type="bibr">Theorem 9]</ref>, we know that for any convex f , and invertible link function Ψ : (0, 1) → R, we have:</p><formula xml:id="formula_28">inf T : X→R E (X,Y)∼D [ Ψ (Y, T (X))] = − 1 2 · I f (P Q)<label>(15)</label></formula><p>where D is the distribution over (observations × {fake, real}) and the loss function Ψ is defined by:</p><formula xml:id="formula_29">Ψ (+1, z) . = −f Ψ −1 (z) 1 − Ψ −1 (z) ; Ψ (−1, z) . = f f Ψ −1 (z) 1 − Ψ −1 (z) ,<label>(16)</label></formula><p>assuming f differentiable. Note now that picking Ψ(z) = f (z/(1 − z)) with z . = T (x) and simplifying eq. <ref type="formula" target="#formula_4">(15)</ref> with P[Y = fake] = P[Y = real] = 1/2 in the GAN game yields eq. <ref type="formula" target="#formula_11">(7)</ref>. For other link functions, however, we get an equally valid class of losses whose optimisation will yield a meaningful estimate of the f -divergence. The losses of eq. <ref type="formula" target="#formula_9">(16)</ref> belong to the class of proper composite losses with link function Ψ <ref type="bibr" target="#b31">[32]</ref>. Thus (omitting parameters θ, ω), we rephrase eq. <ref type="formula" target="#formula_11">(7)</ref> and refer to the proper f -GAN formulation as inf Q L Ψ (Q) with ( is as per eq. <ref type="formula" target="#formula_9">(16)</ref>):</p><formula xml:id="formula_30">L Ψ (Q) . = sup T : X→R E X∼P [− Ψ (+1, T (X))] + E X∼Q [− Ψ (−1, T (X))] .<label>(17)</label></formula><p>Note also that it is trivial to start from a suitable proper composite loss, and derive the corresponding generator f for the f -divergence as per eq. <ref type="bibr" target="#b14">(15)</ref>. Finally, our proper composite loss view of the f -GAN game allows us to elicitate g f in <ref type="bibr" target="#b29">[30]</ref>: it is the composition of f and Ψ in eq. <ref type="bibr" target="#b15">(16)</ref>. The use of proper composite losses as part of the supervised GAN formulation sheds further light on another aspect the game: the connection between the value of the optimal discriminator, and the density ratio between the generator and discriminator distributions. Instead of the optimal T * (x) = f (P (x)/Q(x)) for eq. <ref type="formula" target="#formula_11">(7)</ref> [30, Eq. 5], we now have with the more general eq. (17) the result T * (x) = Ψ((1 + Q(x)/P (x)) −1 ). We now show that proper f -GANs can easily be adapted to eq. (11). We let χ</p><p>• (t)</p><formula xml:id="formula_31">. = 1/χ −1 (1/t).</formula><p>Theorem 12 For any χ, define x (−1, z)</p><formula xml:id="formula_32">. = − log (χ • ) 1 Q(x)</formula><p>(−z), and let (+1, z) (Proof in SM, Section VI) Hence, in the proper composite view of the vig-f -GAN identity, the generator rules over the supervised game: it tempers with both the link function and the loss -but only for fake examples. Notice also that when z = −1, the fake examples loss satisfies x (−1, −1) = 0 regardless of x by definition of the χ-logarithm.</p><formula xml:id="formula_33">. = −z. Then L Ψ (Q) in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Two of our theoretical contributions are (A) the fact that on the generator's side, there exists numerous activation functions v that comply with the design of its density as factoring escorts (Lemma 11), and (B) the fact that on the discriminator's side, the so-called output activation function g f of <ref type="bibr" target="#b29">[30]</ref> aggregates in fact two components of proper composite losses, one of which, the link function Ψ, should be a fine knob to operate (Theorem 12). We have tested these two possibilities with the idea that an experimental validation should provide substantial ground to be competitive with mainstream approaches, leaving space for a finer tuning in specific applications. Also, in order not to mix their effects, we have treated (A) and (B) separately.</p><p>Architectures and datasets -We provide in SM (Section VI) the detail of all experiments. To summarize, we consider two architectures in our experiments: DCGAN <ref type="bibr" target="#b30">[31]</ref> and the multilayer feedforward network (MLP) used in <ref type="bibr" target="#b29">[30]</ref>. Our datasets are MNIST <ref type="bibr" target="#b18">[19]</ref> and LSUN tower category <ref type="bibr" target="#b37">[38]</ref>.</p><p>Comparison of varying activations in the generator (A) -We have compared µ-ReLUs with varying µ in [0, 0.1, ..., 1] (hence, we include ReLU as a baseline for µ = 1), the Softplus and the Least Square Unit (LSU, <ref type="table">Table 1</ref>) activation <ref type="figure" target="#fig_0">(Figure 1</ref>). For each choice of the activation function, all inner layers of the generator use the same activation function. We evaluate the activation functions by using both DCGAN and the MLP used in <ref type="bibr" target="#b29">[30]</ref> as the architectures. As training divergence, we adopt both GAN <ref type="bibr" target="#b14">[15]</ref> and Wasserstein GAN (WGAN, <ref type="bibr" target="#b5">[6]</ref>). Results are shown in <ref type="figure" target="#fig_0">Figure 1</ref> (left). Three behaviours emerge when varying µ: either it is globally equivalent to ReLU (GAN DCGAN) but with local variations that can be better (µ = 0.7) or worse (µ = 0), or it is almost consistently better than ReLU (WGAN MLP) or worse (GAN MLP). The best results were obtained for GAN DCGAN, and we note that the ReLU baseline was essentially beaten for values of µ yielding smaller variance, and hence yielding smaller uncertainty in the results. The comparison between different activation functions <ref type="figure" target="#fig_0">(Figure 1</ref>, center) reveals that (µ-)ReLU performs overall the best, yet with some variations among architectures. We note in particular that, in the same way as for the comparisons intra µ-ReLU <ref type="figure" target="#fig_0">(Figure 1, left)</ref>, ReLU performs relatively worse than the other criteria for WGAN MLP, indicating that there may be different best fit activations for different architectures, which is good news. Visual results on LSUN (SM <ref type="table">, Table A5</ref>) also display the quality of results when changing the µ-ReLU activation.</p><p>Comparison of varying link functions in the discriminator (B) -We have compared the replacement of the sigmoid function by a link which corresponds to the entropy which is theoretically optimal in boosting algorithms, Matsushita entropy <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref>, for which Ψ MAT (z)</p><note type="other">.</note><p>= (1/2) · (1 + z/ √ 1 + z 2 ). <ref type="figure" target="#fig_0">Figure 1</ref> (right) displays the comparison Matsushita vs "standard" (more specifically, we use sigmoid in the case of GAN <ref type="bibr" target="#b29">[30]</ref>, and none in the case of WGAN to follow current implementations <ref type="bibr" target="#b5">[6]</ref>). We evaluate with both DCGAN and MLP on MNIST (same hyperparameters as for generators, ReLU activation for all hidden layer activation of generators). Experiments tend to display that tuning the link may indeed bring additional uplift: for GANs, Matsushita is indeed better than the sigmoid link for both DCGAN and MLP, while it remains very competitive with the no-link (or equivalently an identity link) of WGAN, at least for DCGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>It is hard to exaggerate the success of GAN approaches in modelling complex domains, and with their success comes an increasing need for a rigorous theoretical understanding <ref type="bibr" target="#b33">[34]</ref>. In this paper, we complete the supervised understanding of the generalization of GANs introduced in <ref type="bibr" target="#b29">[30]</ref>, and provide a theoretical background to understand its unsupervised part, showing in particular how deep architectures can be powerful at tackling the generative part of the game. Experiments display that the tools we develop may help to improve further the state of the art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 :</head><label>1</label><figDesc>Some strongly/weakly admissible couples (v, χ). ( §) : 1. is the indicator function; ( †) : δ ≤ 0, 0 &lt; ≤ 1 and dom(v) = [δ/ , +∞). (♥) : β ≥ α &gt; 0; (♣) : is Legendre conjugate; (♠) : µ ∈ [0, 1). Shaded: prop-τ activations; k is a constant (e.g. such that v(0) = 0) (see text).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>eq. (17) equals eq. (11). Its link in eq. (17) is Ψ x (z) = −1/χQ (x) (z/(1 − z)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Summary of our results on MNIST, on experiment A (left+center) and B (right). Left: comparison of different values of µ for the µ-ReLU activation in the generator (ReLU = 1-ReLU, see text). Thicker horizontal dashed lines present the ReLU average baseline: for each color, points above the baselines represent values of µ for which ReLU is beaten on average. Center: comparison of different activations in the generator, for the same architectures as in the left plot. Right: comparison of different link function in the discriminator (see text, best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The code used for our experiments is available through https://github.com/qulizhen/fgan_info_geometric</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgments</head><p>The authors thank the reviewers, Shun-ichi Amari, Giorgio Patrini and Frank Nielsen for numerous comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A general class of coefficients of divergence of one distribution from another</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-D.-S</forename><surname>Silvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society B</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="131" to="142" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Differential-Geometrical Methods in Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Amari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Information Geometry and Its Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Amari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Methods of Information Geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Amari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nagaoka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geometry of deformed exponential families: Invariant, dually-flat and conformal geometries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Amari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Matsuzoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">391</biblScope>
			<biblScope unit="page" from="4308" to="4319" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Relative loss bounds for on-line density estimation with the exponential family of distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Azoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MLJ</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="246" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Certainty equivalents and information measures: Duality and extremal principles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ben-Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ben-Israel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Math. Anal. Appl</title>
		<imprint>
			<biblScope unit="page" from="211" to="236" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bregman voronoi diagrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Boissonnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCG</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="281" to="307" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex optimization</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (ELUs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In 4 th ICLR</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Information-type measures of difference of probability distributions and indirect observation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Csiszár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studia Scientiarum Mathematicarum Hungarica</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="299" to="318" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convex foundations for generalized maxent models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-M</forename><surname>Frongillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33 rd MaxEnt</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="11" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Sinkhorn-autodiff: Tractable Wasserstein learning of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Genevay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<idno>abs/1706.00292</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*27</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-C</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Improved training of wasserstein GANs. CoRR, abs/1704.00028</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-H.-R</forename><surname>Hahnloser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sarpeshkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-A</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-J</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">405</biblScope>
			<biblScope unit="page" from="947" to="951" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the boosting ability of top-down decision tree learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comp. Syst. Sc</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="109" to="128" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On the ability of neural nets to express distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Risteski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<idno>abs/1702.07028</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative moment matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32 nd ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1718" to="1727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Approximation and convergence properties of generative adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<idno>abs/1705.08991</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30 th ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27 th ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Generalized thermostatistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Naudts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Estimating divergence functionals and the likelihood ratio by convex risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5847" to="5861" />
			<date type="published" when="2010-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cranko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-C</forename><surname>Williamson</surname></persName>
		</author>
		<idno>abs/1707.04385</idno>
		<title level="m">GANs in an information geometric nutshell. CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the efficient minimization of classification-calibrated surrogates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*21</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1201" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bregman divergences and surrogates for learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans.PAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2048" to="2059" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">f -GAN: training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*29</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In 4 th ICLR</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-C</forename><surname>Williamson</surname></persName>
		</author>
		<title level="m">Composite binary losses. JMLR, 11</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Information, divergence and risk for binary experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="731" to="817" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*29</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Agglomerative Bregman clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Telgarsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29 th ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On ϕ-families of probability distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-F</forename><surname>Vigelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Cavalcante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Theor. Probab</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Unsupervised creation of parameterized avatars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<idno>abs/1704.05693</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
