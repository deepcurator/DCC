<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning under Privileged Information Using Heteroscedastic Dropout</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lambert</surname></persName>
							<email>johnwl@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
							<email>osener@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning under Privileged Information Using Heteroscedastic Dropout</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction "Better than a thousand days of diligent study is one day with a great teacher." -Japanese Proverb</head><p>It is a common belief that human students require far fewer training examples than any learning machine <ref type="bibr" target="#b37">[38]</ref>. No doubt this has to do with the fact that effective teachers provide much more than the correct answer to their pupils; they provide an explanation in addition to the result.</p><p>In a typical machine learning setup, we present tuples {(x i , y i )} n i=1 to a machine learning model. One way to introduce an "explanation" to a supervised learning system would be to provide some sort of privileged information, * Equal contribution paradigm, a teacher provides additional information during training. In this work, we propose to utilize this information in order to control the variance of the Dropout. Since the Dropout's variance is not constant, we call this a Heteroscedastic Dropout. Our empirical and theoretical analysis suggests that Heteroscedastic Dropout singificantly increses sample efficiency of both CNNs and RNNs resulting in higher accuracy with much less data.</p><p>proposed in <ref type="bibr" target="#b37">[38]</ref> is only valid for SVM based methods. Indeed, many have shown that the privileged information can be introduced into the loss function under a multi-task or a distillation loss in an algorithm-agnostic way. However, we raise the question, could it and should it be fed in as an input instead of an additional task? If so, how would we go about doing so in an algorithm-agnostic way?</p><p>We define a new class of LUPI algorithms by making a structural specification. We consider a hypothesis class such that each hypothesis is a combination of two functions -namely, a deterministic function taking x as an input and a stochastic function taking x ⋆ as an input. When x ⋆ is not available in the test stage, the "Student" simply makes a Bayes optimal decision and marginalizes the model over x ⋆ . Our structural specification makes this marginalization straightforward while not compromising the expressiveness of the model. This structure is natural in the context of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) thanks to the dropout. Dropout is a widely adopted tool to regularize neural networks by multiplying the activations of a neural network at some layer with a random vector. We simply extend the dropout to heteroscedastic dropout by making its variance a function of the privileged information. In other words, dropout becomes the stochastic function taking x ⋆ as an input and marginalizing the function corresponds to not utilizing dropout in the test phase. In order to be able to train the heteroscedastic dropout, we use Gaussian dropout instead of Bernoulli because the key technical tool we use is the reparameterization trick <ref type="bibr" target="#b20">[21]</ref> which is only available for some specific distributions, including the Gaussian.</p><p>The rationale behind heteroscedastic dropout follows the close relationship between Bayesian learning and dropout presented by Gal and Gharamani <ref type="bibr" target="#b12">[13]</ref>. Dropout can be considered a tool to approximate the uncertainty of the output of a neural network. In our proposed heteroscedastic dropout, the privileged information is used to estimate this uncertainty so that hard examples and easy examples are treated accordingly during training. Our theoretical study suggests that the accurate computation of a model's uncertainty can accelerate the rate at which a CNN's upper bound on error drops, from the typical rate of</p><formula xml:id="formula_0">O 1 N to a faster O( 1 n ),</formula><p>where n is the number of training examples. In an oracle case for a dataset with 600K training examples, this theoretical upper bound would allows us to learn a model with the same generalization error with √ 6 × 10 8 ≈ 775 samples instead of 600K and is thus hugely significant. Although the practical gain we observe is nowhere close, it is still very significant.</p><p>We evaluate our method in experiments with both CNNs and RNNs, and show a significant accuracy improvement over two canonical problems, image classification and multi-modal machine translation. As privileged information, we offer a bounding box for image classification and an image of the scene described in a sentence for machine translation. Our method is problem-and modality-agnostic and can be incorporated as long as dropout can be utilized in the original problem and the privileged information can be encoded with an appropriate neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The key aspects that differentiate our work from the literature are: i) our method is applicable to any deep learning architecture which can utilize dropout, ii) we do not use a multi-task or distillation loss, iii) we provide theoretical justification suggesting higher sample efficiency, iv) we perform experiments for both CNNs and RNNs. A thorough review of the related literature is provided below.</p><p>Learning Under Privileged Information: Learning under Privileged Information (LUPI) is initially proposed by Vapnik and Vashist <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b36">37]</ref>. It extends the Support Vector Machine (SVM) by empirically estimating the slack values via privileged information. This method is further applied to various computer vision problems <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b10">11]</ref> as well as ranking <ref type="bibr" target="#b29">[30]</ref>, clustering <ref type="bibr" target="#b9">[10]</ref> and metric learning <ref type="bibr" target="#b11">[12]</ref> problems. These method are based on max-margin learning and are not applicable to CNNs or RNNs.</p><p>One closely related work is <ref type="bibr" target="#b15">[16]</ref>, extending Gaussian processes to the LUPI paradigm. Hernández-Lobato et al. <ref type="bibr" target="#b15">[16]</ref> use privileged information to estimate the variance of the noise in their model. Similarly, we use the privileged information to control the variance of the Dropout in CNN and RNN models. However, their method only applies to Gaussian processes, whereas we target neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning CNNs Under Privileged Information:</head><p>The LUPI paradigm has also been studied recently in the context of CNNs. In contrast to max-margin methods, the literature on learning CNNs under privileged information heavily uses the distillation framework, following the close relationship between distillation and LUPI studied in <ref type="bibr" target="#b24">[25]</ref>.</p><p>Hoffman et al. demonstrated a multi-modal distillation approach to incorporating an additional modality as side information <ref type="bibr" target="#b17">[18]</ref>. They start with a pre-trained network and distill the information from the privileged network to a main neural network in an end-to-end fashion.</p><p>Multi-task learning is a straightforward approach to incorporate privileged information. However, it does not necessarily satisfy a no-harm guarantee (i.e. privileged information can harm the learning). More importantly, the noharm guarantee will very likely be violated since estimating the privileged information (i.e. solving the additional task) might be even more challenging than the original problem.</p><p>When the privileged information is binary and shares the same spatial structure as the original data, such as is the case with segmentation occupancy or bounding box information, it can also directly be incorporated into the training of CNNs by masking the activations. Group Orthogonal neural networks <ref type="bibr" target="#b40">[41]</ref> follow this approach. However, this approach is limited to very specific class of problems.</p><p>The loss value of a CNN can be viewed as analogous to the slack variables. Following this analogy, Yang et al. <ref type="bibr" target="#b39">[40]</ref> use two networks: one for the original task, and one for estimating the loss using the privileged information. Learning occurs through parameter sharing between them.</p><p>Our method is different from aforementioned works since we do not use either a distillation or a multi-task loss.</p><p>Learning Language under Privileged Visual Information: Using images as privileged information to learn language is not new. Chrupala et al. <ref type="bibr" target="#b4">[5]</ref> used a multi-task loss while learning word embeddings under privileged visual information. The embeddings are trained for the task of predicting the next word, as well the representation of the image. Analysis of this model <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref> suggests that the embeddings learned by using vision as a privileged information are significantly different than language only ones and correlate better with human judgments. Recently, Elliott et al. <ref type="bibr" target="#b7">[8]</ref> collected a dataset of images with English captions as well as German translations of captions. Using this dataset, a neural machine translation under privileged information model is developed following the multi-task setup <ref type="bibr" target="#b8">[9]</ref>.</p><p>Dropout and its Variants: Dropout is a well studied regularization technique for training deep networks. To-thebest of our knowledge, we are the first to specifically utilize privileged information to control the variance of a dropout function. Here, we summarize the existing methods which control the variance of the dropout using variational inference or information theoretical tools. Although these tools have never been applied to the LUPI paradigm, we utilize some of the technical tools developed in these works.</p><p>We use multiplicative Gaussian dropout instead of Bernoulli dropout. Gaussian dropout is first introduced in <ref type="bibr" target="#b32">[33]</ref>. Its variational extension <ref type="bibr" target="#b21">[22]</ref> uses local reparameterization to perform Bayesian learning.</p><p>The Information Bottleneck (IB) <ref type="bibr" target="#b33">[34]</ref> is a powerful framework which can enforce various structural assumptions. The IB framework has been applied to CNNs and RNNs using stochastic gradient variational Bayes and the re-parametrization trick <ref type="bibr" target="#b20">[21]</ref>. Perhaps closest to our method, Achille and Soatto <ref type="bibr" target="#b0">[1]</ref> use the information bottleneck principle to learn disentangled representations when a CNN with Gaussian Dropout is used. The authors introduce many ideas upon which we build; specifically, our hypothesis class (Eqn. 4) is very similar to the architecture they proposed. The main architectural difference is their choice to define the variance as a function of x, whereas we make it function of x ⋆ . We also use similar distributional priors and a similar training procedure. On the other hand, we apply these ideas to a completely different problem with a different theoretical analysis. The information bottleneck has been applied to LUPI for SVMs <ref type="bibr" target="#b25">[26]</ref>. However, this method does not apply to neural networks. Although we use IB <ref type="bibr" target="#b34">[35]</ref>, Gaussian dropout <ref type="bibr" target="#b32">[33]</ref> and the re-parametrization trick <ref type="bibr" target="#b20">[21]</ref>, we are the first to our knowledge to apply any of these methods to the LUPI problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Consider a machine learning problem defined over a compact space X and a label space Y. We also consider a loss function l(·, ·) which compares a prediction with a ground truth label. In learning under privileged information, we also have additional information for each data point defined over a space X ⋆ , which is only available during the training. In other words, we have access to i.i.d. samples from the data distribution as</p><formula xml:id="formula_1">x i , x ⋆ i , y i ∼ p(x, x</formula><p>⋆ , y) during training. However, in test we will only be given x ∼ p(x). Formally, given a function class h(·; w) parameterized by w and data {x i , x</p><formula xml:id="formula_2">⋆ i , y i } i∈[n]</formula><p>, a typical aim is to solve the following optimization problem;</p><formula xml:id="formula_3">min w E x,y∼p(x,y) [l(y, h(x; w))]<label>(1)</label></formula><p>We propose to do so by learning a multi-view model using both x and x ⋆ and to use the marginalized model in test when x ⋆ is not available. Consider a parametric function class for the multi-view data h + : X × X ⋆ → Y. The training problem becomes:</p><formula xml:id="formula_4">min w E x,x ⋆ ,y∼p(x,x ⋆ ,y) [l(y, h + (x, x ⋆ ; w))]<label>(2)</label></formula><p>This is equivalent to a classical supervised learning problem defined over a space X × X ⋆ and any existing method like CNNs can be used. In order to solve the inference problem, we consider the following marginalization</p><formula xml:id="formula_5">h(x; w) ≡ E x ⋆ ∼p(x ⋆ |x) [h + (x, x ⋆ ; w)]<label>(3)</label></formula><p>The major problem in this formulation is the intractability of this expectation, as p(x ⋆ |x) is unknown. We propose to restrict the class of functions in a way that the expectation is straightforward to compute. The form we propose is a parametric family such that the privileged information controls the variance, whereas the main information (i.e. information available in both training and test) controls the mean. The specific form we use is: . Moreover, in this formulation, the expectation defined in (3) becomes straightforward and can be shown to be h(x; w) = h o (x; w o ). We visualize this structural specification in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>We use neural networks to represent h o and h ⋆ and learn their parameters using the information bottleneck. Since the output space is discrete (we address classification), we denote the representation of the data as h(x; w) and compute the output as softmax(h(x; w)). We explain the details of training in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Information Bottleneck for Learning</head><p>We need to control the role of x ⋆ in LUPI. And, the information bottleneck has already been used for this <ref type="bibr" target="#b25">[26]</ref>; however, we do not need this explicit specification because our structural specification directly controls the role of x ⋆ . We use information bottleneck for a rather different reason, its original reason, learning a minimal and sufficient joint representation of x, x ⋆ which capture all the information about y. This is similar to <ref type="bibr" target="#b0">[1]</ref>, and we use the same log-Normal assumption. The Lagrangian of the information bottleneck can be written as (see <ref type="bibr" target="#b34">[35]</ref> for details);</p><formula xml:id="formula_6">L = H(y|z) + βI(x, x ⋆ ; z)<label>(5)</label></formula><p>where z is the joint representation of x, x ⋆ computed as z = h + (x, x ⋆ ; w). These terms can be computed as;</p><formula xml:id="formula_7">I(x ⋆ , x; z) = E x,x ⋆ ∼p(x,x ⋆ ) [KL(pw(z|x, x ⋆ )||pw(z))] H(y|z) ≃ Ex,x⋆,y∼p[E z∼pw (z|x,x ⋆ ) [−log pw(y|z)]]<label>(6)</label></formula><p>where p w (·) represents the distributions computed over our model with parameters w. In order to compute the KL divergence, we need an assumption about the prior over representations p(z). As suggested by <ref type="bibr" target="#b0">[1]</ref>, the log-Normal distribution follows the empirical distribution p(z) when ReLu is used. Hence, we use the log-Normal distribution and compute the KL divergence as (see supplementary materials for full derivation);</p><formula xml:id="formula_8">KL(p w (z|x, x ⋆ )||p w (z)) ∼ log h ⋆ (x ⋆ ; w ⋆ ) .<label>(7)</label></formula><p>Combining them, the final optimization problem is;</p><formula xml:id="formula_9">min w 1 n n i=1 E z∼pw(z|x,x ⋆ ) [log p(y i |z)]+β log h ⋆ (x ⋆ i ; w ⋆ )<label>(8)</label></formula><p>This minimization is simply the cross-entropy loss with regularization over the logarithm of the computed variances of the heteroscedastic dropout, and can be performed via the re-parametrization trick in practice when h o and h ⋆ are defined as neural networks. We further justify the choice of IB regularization via experimental observation: without it, optimization leads to NaN loss values. We discuss the details of the re-parametrization trick in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation</head><p>In this section, we discuss the practical implementation details of our framework. We discuss it for image classification with CNNs and machine translation with RNNs. For the classification setup, we use the image as x, object localization information as x ⋆ , and image label as y. For the translation setup, we use the sentence in the source language as x, an image which is the realization of the sentence as x ⋆ , and the sentence in the target language as y.</p><p>We make a sequence of architectural decisions in order to design h</p><p>• and h ⋆ . For the classification problem, we design both of them as CNNs and share the convolutional layers. The inputs are x, an image, and x ⋆ , an image with a blacked-out background. We use the VGG-Network <ref type="bibr" target="#b31">[32]</ref> as an architecture and simply replace each dropout with our form of heteroscedastic dropout. We show the details of the architecture with the re-parameterization trick in <ref type="figure" target="#fig_4">Figure 4</ref>. We also normalize images with the ImageNet pixel mean and variance. As data augmentation, we horizontally flip images from left to right and make random crops.</p><p>We use a two-layered LSTM architecture with 500 units as our RNN cell and use the heteroscedastic dropout between layers of LSTMs. The main reason behind this choice is the fact that dropout in general is only shown to be useful for connections between LSTM layers. We use attention <ref type="bibr" target="#b1">[2]</ref> and feed the image as a feature vector computed using the VGG <ref type="bibr" target="#b31">[32]</ref> architecture pre-trained on ImageNet. We give the details of the LSTM with re-parametrization trick in <ref type="figure">Figure 3</ref>. For the inference, we use beam search over 12 hypotheses. Our LSTM implementation directly follows the baseline implementation provided by OpenNMT <ref type="bibr" target="#b22">[23]</ref>.</p><p>Hyperparameter Settings We used a standard learning rate across all image classification experiments, setting our initial learning rate to 1.0 × 10 −3 , and tolerating 5 epochs of non-increasing validation set accuracy before decaying the learning rate by 10x. For multi-modal machine translation, we use an initial learning rate of 1.0 × 10 −3 and half the learning rate every epoch after the 8 th epoch. We used the ADAM <ref type="bibr" target="#b19">[20]</ref> optimizer in PyTorch for both image classification and multi-modal machine translation. All CNN weights are initialized according to the method of He et al. <ref type="bibr" target="#b14">[15]</ref> and a decay of 1 × 10 −4 was used for image classification. For multi-modal machine translation, we do not use any weight decay and initialize weights according to <ref type="bibr" target="#b22">[23]</ref>. . . <ref type="figure">Figure 3</ref>. Multi-Modal Machine Translation We show the LSTM architecture we use, which incorporates the re-parameterization trick and heteroscedastic dropout connections. We use dropout only between layers and share among cells following <ref type="bibr" target="#b13">[14]</ref>. We do not use any dropout in inference since the image is not available during test.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In order to evaluate our method, we perform various experiments using both CNNs and LSTMs. We test our method with CNNs for the task of image classification and with LSTMs for the task of machine translation. We discuss the baselines against which we compare our algorithm, the datasets we use in the rest of this section.</p><p>Datasets: We perform our experiments using the following datasets; ImageNet <ref type="bibr" target="#b5">[6]</ref>: A dataset of 1.3 million images labelled with a class over 1000 categories. We only use the subset of 600 thousand images which include localization information. Multi-30K <ref type="bibr" target="#b7">[8]</ref>: A dataset of 30 thousand Flickr images which are captioned in both English and German. We use this dataset for multi-modal machine translation experiments. In Multi-30K, the English captions are generated for images; whereas, German captions are directly translated from the English captions. Hence, during the ground truth translation, the images were privileged information translators. This property makes this dataset a perfect benchmark for LUPI. Baselines: We compare our method against the following baselines. No-x ⋆ : a baseline model not using any privileged information Gaussian Dropout <ref type="bibr" target="#b32">[33]</ref>: A multiplicative Gaussian dropout with a fixed variance. MultiTask: We perform multi-task learning as a tool to utilize privileged information. We use regression to bounding box coordinates and denote it as Multi-Task w/ B.Box as well as direct estimation of the RGB mask and denote it as Multi-Task w/ Mask. We use this self-baseline only for CNNs since there are many published multi-task methods for machine translation with multi-modal information and we compare with them all. In addition to these selfbaselines, we also compare with the following published work. GoCNN <ref type="bibr" target="#b40">[41]</ref>: a method for CNNs with segmentation as a privileged information which proposes to mask convolutional weights with segmentation masks. Information Dropout <ref type="bibr" target="#b0">[1]</ref>: a regularization method that utilizes injection of multiplicative noise in the activations of a deep neural network (but as a function of the input x, not x ⋆ ). MIML-FCN <ref type="bibr" target="#b39">[40]</ref>: a CNN-based LUPI framework designed for multi-instance problems. Our problem is not multiple instance; but, we still compare for the sake of completeness. Modality Hallucination <ref type="bibr" target="#b17">[18]</ref>: Distillation-based LUPI method designed for multi-modal CNNs. Imagination <ref type="bibr" target="#b8">[9]</ref>: Distillation-based LUPI method designed for multi-modal machine translation. (see supplementary materials for implementation details.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Effectiveness of Our Method</head><p>We compare our method with the No-x ⋆ baseline for image classification using the ImageNet dataset. We perform experiments by varying the number of training examples logarithmically. This is key since the main motivation behind our LUPI method is learning with less data rather than having higher accuracy. We report several results in <ref type="table" target="#tab_0">Table 1</ref> and visualize additional data points in <ref type="figure" target="#fig_5">Figure 5</ref>.</p><p>Our method is quite effective for a training dataset size of 75K images; however, it has no positive impact for the 200K and 600K cases. Even more importantly, the smaller the training set, the larger the improvement. For 75K images, it results in 5% single crop top-1 accuracy improvement; whereas, for 200K, it matches the performance. This simply suggests that our algorithm is particularly effective for low-and mid-scale datasets. This result is quite intuitive since with increasing dataset size, all algorithms can effectively learn and reach an optimal accuracy which is possible under the model class. Hence, the role of an "intelligent teacher" is providing the privileged information to  The accuracies of models trained with x ⋆ are depicted in green; those trained without are depicted in red (via an adaptive learning rate decay schedule) and blue (via a fixed learning rate decay schedule). Furthermore, adaptively modifying the learning rate according to performance on a hold-out set yields massive gains in low-and mid-scale data regimes, when compared with decaying the learning rate at fixed intervals, e.g. every 30 training epochs.</p><p>learn with less data. In other words, LUPI is not a way to gain extra accuracy regardless of the dataset; rather, it is a way to significantly increase the data efficiency. We do not perform a similar experiment for machine translation since the available dataset is a mid-scale and our LUPI method demonstrates asymptotic accuracy increases at full dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Data Efficiency of Our Method and Baselines</head><p>In order to compare the data efficiency gain of our method against baselines, we perform image classification and multi-modal machine translation experiments. We use 75K ImageNet images since our main goal is identify insights regarding data sample efficiency gains and using a smaller training set makes this analysis possible. We summarize the image classification experiments in <ref type="table">Table 2</ref> and multi-modal machine translation experiments in <ref type="table">Table  3</ref>.Our method outperforms all baselines for both tasks, for image classification with a significant margin. <ref type="table">Table 2</ref>. We compare our method's performance with several baselines. We train with 75 Images per each of the 1000 ImageNet classes, leaving us with 75 ×10 3 images in total. We outperform each model and are competitive with GoCNN, a model specifically designed for the problem of learning with segmentation data using various architectural decisions. Evaluation is carried out on the held-out set of images from our holdout test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Crop</head><p>Multi-Crop  <ref type="table">Table 3</ref>. We compare our method for multi-modal machine translation with several baselines. We report BLEU <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b23">24]</ref> and METEOR <ref type="bibr" target="#b6">[7]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Classification with Privileged Localization</head><p>One interesting result is that our network h ⋆ is clearly learning much more than to predict random constant for its output Σ, the covariance matrix used for reparameterization; in fact, our network outperforms the network that produces a Σ whose entries are drawn from pure Gaussian noise by &gt; 3.8%. We analyze our method for CNNs both theoretically and qualitatively in Section 5 and conclude that our method learns to control the uncertainty of the model and results in an order of magnitude higher data efficiency, explaining this large margin.</p><p>Furthermore, GoCNN <ref type="bibr" target="#b40">[41]</ref>, an architecture specifically designed for the problem of learning with segmentation data using various architectural decisions, results in a significant accuracy improvement competitive with our method in a small dataset regime. However, GoCNN's performance relative to other baselines begins to degrade at a dataset size of 200K images, leading to a top-1 accuracy decrease of −5.26% in comparison with Bernoulli dropout and −4.47% with our heteroscedastic dropout method. This is an intuitive result because GoCNN's rigid architectural decisions inject significant bias into the model.</p><p>Because Information Dropout relies upon sampling from a log-normal distribution with varying variance, it is heteroscedastic. However, compounded Information Dropout layers which exponentiate samples from a normal distribution lead to unbounded activations; thus, a suitable squashing function like the sigmoid must be employed to bound the activations. We find its performance can actually decrease accuracy when compared with a ReLU nonlinearity. Multi-modal Machine Translation Our method results in a significant accuracy improvement measure by both BLEU and METEOR scores. One interesting observation is that our method outperforms various multi-modal methods which use image information in both training and test. This counterintuitive result is due to the way in which the dataset is collected. Multi-30k <ref type="bibr" target="#b7">[8]</ref> dataset is collected by simply translating the English captions of 30K images into German. A LUPI model <ref type="bibr" target="#b8">[9]</ref> was already shown to perform better than multi-modal translation models which can use both images and sentences in test time <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref>. This surprising result is largely due to the fact that the translators did not see the images while providing ground truth translations. More importantly, the effectiveness of visual information in machine translation in a privileged setting is also intuitive following the results of <ref type="bibr" target="#b4">[5]</ref>. Chrupala et al. <ref type="bibr" target="#b4">[5]</ref> show that when image information is used as privileged information in the learning of word representations, the quality of such representations increases. Hence, a multi-modal paradigm for learning language (e.g. with privileged visual information) and vice versa is a fruitful direction for both natural language processing and computer vision communities and our method performs quite effectively on this task.</p><p>In summary, our results overperform all baselines for both multi-modal machine translation and image classification experiments using both CNNs and RNNs. These results  <ref type="figure">Figure 6</ref>. Accuracy vs. x ⋆ % for multi-modal machine translation. An identical experiment on image classification is shown in supplementary material due to limited space.</p><p>suggest that our method is effective and generic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Learning under Partial Privileged Information</head><p>Although privileged information naturally exists for many problems, it is typically not available for all points. Thus, it is common to encounter a scenario in which the entire training data is labelled; however, only a small portion includes privileged information. In other words, we typically have dataset which is the union of {x i , y i } i∈ <ref type="bibr">[n]</ref> and {x j , x ⋆ j , y j } j∈ <ref type="bibr">[m]</ref> where m ≪ n. In order to experiment with this setting, we vary the amount of x ⋆ available. We present the result in <ref type="figure">Figure 6</ref> for machine translation and in supplementary material for image classification.</p><p>The results in <ref type="figure">Figure 6</ref> suggest that even when only a small portion (2% for machine translation, 4% for image classification) of the data has privileged information, our method is effective resulting in a significant accuracy increase very similar to the one we obtained with 100% of privileged information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis of the Algorithm</head><p>Our empirical analysis suggests a strong data-efficiency increase when privileged information is incorporated using our method. It is interesting to quantify this increase in terms of the theoretical learning rate. For the case of SVMs, Vapnik et al. <ref type="bibr" target="#b37">[38]</ref> showed that utilizing the privileged information can result in a generalization error bound with rate O(</p><formula xml:id="formula_10">1 n ) instead of O( 1 n )</formula><p>where n is the dataset size. Our experimental results suggests a similar story for CNNs, but the theoretical justification can not be extended from <ref type="bibr" target="#b37">[38]</ref> since their analysis is specific to SVMs. In this section, we try to answer this question for our algorithm. We show that our method is capable of converting an O( 1 n ) error rate (derived in Proposition 1) into O( 1 n ) in an oracle setting for CNNs. We rigorously prove that it is possible to reach O( 1 n ) rate using our structural assumptions; however, we do not provide any argument for the optimization landscape. In other words, our results are only valid with an oracle optimizer which can find the solution satisfying our assumptions. The study of the loss function and the optimization remains an open problem; however, we give a strong empirical evidence that using SGD with information bottleneck regularization is enabling faster learning.</p><p>We start by presenting a bound over the generalization error of CNNs with no privileged information. This result directly follows from <ref type="bibr" target="#b38">[39]</ref>, and we include it here for the sake of completeness. Loss functions of CNNs based on l 2 distance are Lipschitz continuous when the non-linearity is the rectified linear unit, the pooling operation is maxpooling and softmax function is used to convert activations into logits. Moreover, any learning algorithm with a Lipschitz loss function admits the following result <ref type="bibr" target="#b38">[39]</ref>; Proposition 1 ([39, Example 4]). Given n i.i.d. samples drawn from p(x, y) as {x i , y i } i∈ <ref type="bibr">[n]</ref> , if a loss function l(y, h(x; w)) is λ l -Lipschitz continuous function of x for all y, w, bounded by L and X × Y has a covering number N ǫ (X , | · | 2 ) = K, then with probability at least 1 − δ,</p><formula xml:id="formula_11">E x,y∼p(x,y) [l(y, h(x; w))] − 1 n i∈[n] l(y i , h(x i ; w)) ≤ λ l ǫ + L 2K log 2 + 2 log(1/δ) n .</formula><p>This proposition simply details the baseline O( 1 n ) error rate under no privileged information. In order to intuitively explain how our algorithm can accelerate this learning to O( 1 n ), consider the following oracle algorithm. Using the privileged information, one can estimate the uncertainty (variance) of the neural network and can use the inverse of this estimate as a the variance of the heteroscedastic dropout. Since the heteroscedastic dropout is multiplicative, this results in unit variance regardless of the input. In a similar fashion, this oracle algorithm can bound the variance with an arbitrary constant. Following this oracle algorithm, we show that when the variance is properly controlled, our method can reach an O( The value ǫ y is purely a property of the way in which a was dataset collected and must be treated independently of the learning. Hence, we do not study the rate at it vanishes. We present the following proposition and defer its proof to the supplementary material: Proposition 2. Given n i.i.d. samples drawn from p(x, y) as {x i , y i } i∈ <ref type="bibr">[n]</ref> and a loss function defined as h(x; w) − y where ξ ≤ δ. This proposition means that learning with sample efficiency O( 1 n ) is indeed possible as long as one can bound the variance of the output(ξ) with an arbitrary number δ. Hence, the full control of the output variance, makes learning with higher sample efficiency possible. One remaining question is whether using SGD with information bottleneck regularization can learn this oracle solution or not? Unfortunately, we have no theoretical answer for this question and leave it as an open problem. However, we study this problem empirically and show that there is a strong empirical evidence suggesting that the answer is affirmative. <ref type="figure">Figure 7</ref>. For 8000 random samples from the validation set that our heteroscedastic dropout algorithm mis-classifies, as well as 8000 random samples it correctly classifies, we plot the average of activations per dimension (we sort the 4096 dimensions in terms of average energy over full dataset for clarity).</p><p>A realistic estimate of variance is typically not possible without a strong parametric assumption; however, we can use the simple heuristic that the samples from validation set that our algorithm mis-classifies should have higher variance than the samples which are correctly classified. We plot the average energy of computed dropout variances per fully connected neuron for mis-classified and correctly classified examples in <ref type="figure">Figure 7</ref>. Interestingly, our method consistently assigns larger multiplier (dropout) values for correctly classified samples and significantly smaller values for mis-classified samples. This strongly supports our hypothesis since when the low heteroscedastic dropout is multiplied with the high-variance mis-classified examples, their final variance will be low, possibly bounded by the σ 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We described a learning under privileged information framework for CNNs and RNNs. We proposed a heteroscedastic dropout formulation by making the variance of the dropout a function of privileged information.</p><p>Our experiments on image classification and machine translation suggest that our method significantly increases the sample efficiency of both CNNs and LSTMs. We further provide an upper bound over the generalization error of CNNs suggesting a sample efficient learning (with rate O( 1 n )) in the oracle case when privileged information is available. We make our learned models as well as the source code available <ref type="bibr" target="#b11">12</ref> .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. In Learning Under Privileged Information (LUPI) paradigm, a teacher provides additional information during training. In this work, we propose to utilize this information in order to control the variance of the Dropout. Since the Dropout's variance is not constant, we call this a Heteroscedastic Dropout. Our empirical and theoretical analysis suggests that Heteroscedastic Dropout singificantly increses sample efficiency of both CNNs and RNNs resulting in higher accuracy with much less data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The structure we propose. Privileged information is only used for estimation of the variance of the heteroscedastic dropout. disjoint vectors as w = [w o , w ⋆ ]. Moreover, in this formulation, the expectation defined in (3) becomes straightforward and can be shown to be h(x; w) = h o (x; w o ). We visualize this structural specification in Figure 2. We use neural networks to represent h o and h ⋆ and learn their parameters using the information bottleneck. Since the output space is discrete (we address classification), we denote the representation of the data as h(x; w) and compute the output as softmax(h(x; w)). We explain the details of training in the following sections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Image Classification We show the CNN architecture we used in our experiments, along with the re-parameterization trick and heteroscedastic dropout connections. We do not use any dropout in inference since localization bounding boxes are not available during test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Accuracy vs. training set size for ImageNet classification. Each data point denotes a VGG-16 network trained with batch normalization. The accuracies of models trained with x ⋆ are depicted in green; those trained without are depicted in red (via an adaptive learning rate decay schedule) and blue (via a fixed learning rate decay schedule). Furthermore, adaptively modifying the learning rate according to performance on a hold-out set yields massive gains in low-and mid-scale data regimes, when compared with decaying the learning rate at fixed intervals, e.g. every 30 training epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1 n</head><label>1</label><figDesc>) rate. Consider the population distribution of number images per class versus the empiri- cal distribution as ǫ y = E[y ⊺ y]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 2</head><label>2</label><figDesc>where h(·; w) is a CNN, assume that any path between input and output has maximum weight M w , total number of paths between input and output is P , and for all training points x i , h(x i ; w) ≤ M z and V ar(h(x i ; w)) ≤ ξ 2 . With probability at least 1 − δ, E x,y∼p(x,y) [l(y, h(x; w))]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Classification Test Accuracy on 1000 ILSVRC Classes. Because the ILSVRC server prohibits large numbers of test sub- missions, which we required to evaluate at different sizes of sam- ple data, we use a hold-out set of 50K images from ImageNet CLS-LOC as our test set. The authors of [32] report a 7.4% Multi- Crop, top-5 error rate when training on ∼ 1.3M images. Where we report "No-x ⋆ ," we describe the results of a classical CNN learn- ing method. All 1-crop evaluations below were carried out with a center crop. All 25K models diverged.</figDesc><table>Number of Training Images 
Model 
25K 
75K 
200K 
600K 

Single Crop top-1 
No-x 

⋆ 

-
37.85 55.99 66.66 
Our LUPI 
-
42.30 55.51 66.77 

Single Crop top-5 
No-x 

⋆ 

-
62.76 79.21 86.90 
Our LUPI 
-
67.13 78.89 
86.88 

Multi-Crop top-1 
No-x 

⋆ 

-
39.99 
58.7 69.20 
Our LUPI 
-
44.95 58.41 
69.10 

Multi-Crop top-5 
No-x 

⋆ 

-
64.49 
81.0 
88.60 
Our LUPI 
-
69.19 81.15 88.64 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>metrics. Some baselines only report English(en)→German(de) results, and exclude de→en.</figDesc><table>en→de 
de→en 
Model 
BLEU Meteor BLEU Meteor 

No x 
⋆ (following [23]) 
35.5 
54.0 
40.19 
55.8 

Multi-Modal 
Toyama et al. [36] 
36.5 
56.0 
− 
− 
Hitschler et al. [17] 
34.3 
56.1 
− 
− 
Calixto et al. [3] 
36.5 
55.0 
− 
− 
Calixto et al. [4] 
37.3 
55.1 
− 
− 

LUPI 
Imagination [9] 
36.8 
55.8 
40.5 
56.0 
Ours 
38.4 
56.9 
42.4 
57.1 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">tic function N (1, h ⋆ (x ⋆ ; w ⋆ )) is a normal random variable with a constant mean function and a covariance function parametrized by x ⋆ and w ⋆ . We also decompose w as two</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://svl.stanford.edu/projects/heteroscedastic-dropout 2 https://github.com/johnwlambert/dlupi-heteroscedastic-dropout</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>We thank Alessandro Achille for his help on comparison with information dropout. We acknowledge the support of Toyota (1191689-1-UDAWF), MURI (1186514-1-TBCJE); Panasonic (1192707-1-GWMSX).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01353</idno>
		<title level="m">Information dropout: Learning optimal representations through noisy computation</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Doubly-attentive decoder for multi-modal neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Campbell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01287</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Incorporating global visual features into attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Campbell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06521</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning language through pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chrupała</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kádár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alishahi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03694</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2014 Workshop on Statistical Machine Translation</title>
		<meeting>the EACL 2014 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multi30k: Multilingual english-german image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Imagination improves multimodal translation. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kádár</surname></persName>
		</author>
		<idno>abs/1705.04350</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Privileged information for data clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feyereisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Aickelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="23" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object localization based on structural svm using privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feyereisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS))</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="208" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incorporating privileged information through metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fouad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raychaudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1086" to="1098" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mind the nuisance: Gaussian process classification using privileged noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharmanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Quadrianto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="837" to="845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multimodal pivots for image caption translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schamoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riezler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.03916</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning with side information through modality hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Representation of linguistic form and function in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kádár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chrupała</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alishahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Variational dropout and the local reparameterization trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2575" to="2583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Opennmt: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions</title>
		<meeting>the 45th annual meeting of the ACL on interactive poster and demonstration sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unifying distillation and privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Information bottleneck learning using privileged information for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">In the era of deep convolutional features: Are attributes still useful privileged data?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharmanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Quadrianto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Attributes</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="31" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to rank using privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharmanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Quadrianto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="825" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharmanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Quadrianto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0389</idno>
		<title level="m">Learning to transfer privileged information</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
		<title level="m">The information bottleneck method. arXiv preprint physics/0004057</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Information Theory Workshop (ITW)</title>
		<imprint>
			<date type="published" when="2015-04" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Neural machine translation with latent semantic of image and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Misono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08459</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning using privileged information: Similarity control and knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Izmailov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2023" to="2049" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A new learning paradigm: Learning using privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vashist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="544" to="557" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Robustness and generalization. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="391" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mimlfcn+: Multi-instance multi-label learning via fully convolutional networks with privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Soon</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Training group orthogonal neural networks with privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F S Y</forename><surname>Yunpeng Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
