<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DSOD: Learning Deeply Supervised Object Detectors from Scratch</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
							<email>liuzhuangthu@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
							<email>jianguo.li@intel.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Intel Labs China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
							<email>yurong.chen@intel.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Intel Labs China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
							<email>xyxue@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DSOD: Learning Deeply Supervised Object Detectors from Scratch</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional Neural Networks (CNNs) have produced impressive performance improvements in many computer vision tasks, such as image classification <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, object detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref>, image segmenta- * indicates equal contribution. This work was done when Zhiqiang Shen and Zhuang Liu were interns at Intel Labs China. Jianguo Li is the corresponding author.</p><p>tion <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b35">36]</ref>, etc. In the past several years, many innovative CNN network structures have been proposed. Szegedy et al. <ref type="bibr" target="#b31">[32]</ref> propose an "Inception" module which concatenates features maps produced by various sized filters. He et al. <ref type="bibr" target="#b8">[9]</ref> propose residual learning blocks with skip connections, which enable training very deep networks with more than 100 layers. Huang et al. <ref type="bibr" target="#b9">[10]</ref> propose DenseNets with dense layer-wise connections. Thanks to these excellent network structures, the accuracy of many vision tasks has been greatly improved. Among them, object detection is one of the fastest moving areas due to its wide applications in surveillance, autonomous driving, etc.</p><p>In order to achieve good performance, most of the advanced object detection systems fine-tune networks pretrained on ImageNet <ref type="bibr" target="#b2">[3]</ref>. This fine-tuning process is also viewed as transfer learning <ref type="bibr" target="#b23">[24]</ref>. Fine-tuning from pretrained models has at least two advantages. First, there are many state-of-the-art deep models publicly available. It is very convenient to reuse them for object detection. Second, fine-tuning can quickly generate the final model and requires much less instance-level annotated training data than the classification task.</p><p>However, there are also critical limitations when adopting the pre-trained networks in object detection: (1) Limited structure design space. The pre-trained network models are mostly from ImageNet-based classification task, which are usually very heavy -containing a huge number of parameters. Existing object detectors directly adopt the pre-trained networks, and as a result there is little flexibility to control/adjust the network structures (even for small changes of network structure). The requirement of computing resources is also bounded by the heavy network structures. (2) Learning bias. As both the loss functions and the category distributions between classification and detection tasks are different, we argue that this will lead to different searching/optimization spaces. Therefore, learning may be biased towards a local minimum which is not the best for detection task. (3) Domain mismatch. As is known, fine-tuning can mitigate the gap due to different target category distribution. However, it is still a severe problem when the source  <ref type="figure">Figure 1</ref>: DSOD prediction layers with plain and dense structures (300Ã—300 input). Plain structure is introduced by SSD <ref type="bibr" target="#b20">[21]</ref>. See Sec. 3 for more details.</p><p>domain (ImageNet) has a huge mismatch to the target domain such as depth images, medical images, etc <ref type="bibr" target="#b6">[7]</ref>. Our work is motivated by the following two questions. First, is it possible to train object detection networks from scratch? Second, if the first answer is positive, are there any principles to design a resource efficient network structure for object detection while keeping high detection accuracy? To meet this goal, we propose deeply supervised objection detectors (DSOD), a simple yet efficient framework which could learn object detectors from scratch. DSOD is fairly flexible, so that we can tailor various network structures for different computing platforms such as server, desktop, mobile and even embedded devices.</p><p>We contribute a set of principles for designing DSOD. One key point is that deep supervision plays a critical role, which is motivated by the work of <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35]</ref>. In <ref type="bibr" target="#b34">[35]</ref>, Xie et al. proposed a holistically-nested structure for edge detection, which included the side-output layers to each convstage of base network for explicit deep supervision. Instead of using the multiple cut-in loss signals with sideoutput layers, this paper adopts deep supervision implicitly through the dense layer-wise connections proposed in DenseNet <ref type="bibr" target="#b9">[10]</ref>. Dense structures are not only adopted in the backbone sub-network, but also in the front-end multi-scale prediction layers. <ref type="figure">Figure 1</ref> illustrates the structure comparison in front-end prediction layers. The fusion and reuse of multi-resolution prediction-maps help keep or even improve the final accuracy while reducing model parameters to some extent.</p><p>Our main contributions are summarized as follows: (1) We present DSOD, to the best of our knowledge, the first framework that can train object detection networks from scratch with state-of-the-art performance, even with limited training data. (2) We introduce and validate a set of principles to design efficient object detection networks from scratch through step-by-step ablation studies. (3) We show that our DSOD can achieve state-of-the-art performance on three standard benchmarks (PASCAL VOC 2007, 2012 and MS COCO datasets) with realtime processing speed and more compact models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Object Detection. State-of-the-art CNN based object detection methods can be divided into two groups: (i) region proposal based methods and (ii) proposal-free methods. Proposal based methods include R-CNN <ref type="bibr" target="#b4">[5]</ref>, Fast R-CNN <ref type="bibr" target="#b3">[4]</ref>, Faster R-CNN <ref type="bibr" target="#b26">[27]</ref> and R-FCN <ref type="bibr" target="#b18">[19]</ref>. R-CNN uses selective search <ref type="bibr" target="#b33">[34]</ref> to first generate potential object regions in an image and then perform classification on the proposed regions. R-CNN requires high computational costs since each region is processed by the CNN network separately. Fast R-CNN and Faster R-CNN improve the efficiency by sharing computation and using neural networks to generate the region proposals. R-FCN further improves speed and accuracy by removing fully-connected layers and adopting position-sensitive score maps for final detection.</p><p>Proposal-free methods like YOLO <ref type="bibr" target="#b24">[25]</ref> and SSD <ref type="bibr" target="#b20">[21]</ref> have recently been proposed for real-time detection. YOLO uses a single feed-forward convolutional network to directly predict object classes and locations. Comparing with the region-based methods, YOLO no longer requires a second per-region classification operation so that it is extremely fast. SSD improves YOLO in several aspects, including (1) using small convolutional filters to predict categories and anchor offsets for bounding box locations; (2) using pyramid features for prediction at different scales; and (3) using default boxes and aspect ratios for adjusting varying object shapes. Our proposed DSOD is built upon the SSD framework and thus it inherits the speed and accuracy advantages of SSD, while produces smaller and more flexible models. Network Architectures for Detection. Significant efforts have been devoted to the design of network architectures for image classification. Many different networks are emerged, such as AlexNet <ref type="bibr" target="#b16">[17]</ref>, VGGNet <ref type="bibr" target="#b27">[28]</ref>, GoogLeNet <ref type="bibr" target="#b31">[32]</ref>, ResNet <ref type="bibr" target="#b8">[9]</ref> and DenseNet <ref type="bibr" target="#b9">[10]</ref>. Meanwhile, several regularization techniques <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b11">12]</ref> have also been proposed to further enhance the model capabilities. Most detection methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b20">21]</ref> directly utilize pre-trained ImageNet models as the backbone network. Some other works design specific backbone network structures for object detection, but still require pre-training the network on ImageNet classification dataset first. For instance, YOLO <ref type="bibr" target="#b24">[25]</ref> defines a network with 24 convolutional layers followed by 2 fully connected layers. YOLO9000 <ref type="bibr" target="#b25">[26]</ref> improves YOLO by proposing a new network named Darknet-19, which is a simplified version of VGGNet <ref type="bibr" target="#b27">[28]</ref>. Kim et al. <ref type="bibr" target="#b14">[15]</ref> proposes PVANet for object detection, which consists of the simplified "Inception" block from GoogleNet. Huang et al. <ref type="bibr" target="#b10">[11]</ref> investigated various combination of network structures and detection frameworks, and found that Faster R-CNN <ref type="bibr" target="#b26">[27]</ref> with InceptionResNet-v2 <ref type="bibr" target="#b30">[31]</ref> achieved the highest performance. In this paper, we also consider network structures for generic object detection. However, the pre-training on ImageNet is no longer required by the proposed DSOD. Learning Deep Models from Scratch. To the best of our knowledge, there are no works which train object detection networks from scratch. The proposed approach has very appealing advantages over existing solutions. We will elaborate and validate the method in the following sections. In semantic segmentation, JÃ©gou et al. <ref type="bibr" target="#b12">[13]</ref> demonstrated that a well-designed network structure can outperform state-ofthe-art solutions without using the pre-trained models. It extends DenseNets to fully convolutional networks by adding an upsampling path to recover the full input resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DSOD</head><p>In this section, we first introduce our DSOD architecture and its components, and elaborate several important design principles. Then we describe the training settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">DSOD Architecture</head><p>Overall Framework. The proposed DSOD method is a multi-scale proposal-free detection framework similar to SSD <ref type="bibr" target="#b20">[21]</ref>. The network structure of DSOD can be divided into two parts: the backbone sub-network for feature extraction and the front-end sub-network for prediction over multi-scale response maps. The backbone sub-network is a variant of the deeply supervised DenseNets <ref type="bibr" target="#b9">[10]</ref> structure, which is composed of a stem block, four dense blocks, two transition layers and two transition w/o pooling layers. The front-end subnetwork (or named DSOD prediction layers) fuses multi-scale prediction responses with an elaborated dense structure. <ref type="figure">Figure 1</ref> illustrates the proposed DSOD prediction layers along with the plain structure of multiscale predicting maps as used in SSD <ref type="bibr" target="#b20">[21]</ref>. The full DSOD network architecture 1 is detailed in <ref type="table">Table 1</ref>. We elaborate each component and the corresponding design principle in the following. Principle 1: Proposal-free. We investigated all the stateof-the-art CNN based object detectors, and found that they could be divided into three categories. First, R-CNN and Fast R-CNN require external object proposal generators like selective search. Second, Faster R-CNN and R-FCN require integrated region-proposal-network (RPN) to generate relatively fewer region proposals. Third, YOLO and SSD are single-shot and proposal-free methods, which handle object location and bounding box coordinates as a regression problem. We observe that only the proposal-free method (the 3rd category) can converge successfully without the pre-trained models. We conjecture this is due to the RoI (Regions of Interest) pooling in the other two categories of methods -RoI pooling generates features for each region proposals, which hinders the gradients being smoothly back-propagated from region-level to convolutional feature maps. The proposal-based methods work well with pretrained network models because the parameter initialization is good for those layers before RoI pooling, while this is not true for training from scratch.</p><p>Hence, we arrive at the first principle: training detection network from scratch requires a proposal-free framework. In practice, we derive a multi-scale proposal-free framework from the SSD framework <ref type="bibr" target="#b20">[21]</ref>, as it could reach stateof-the-art accuracy while offering fast processing speed. Principle 2: Deep Supervision.</p><p>The effectiveness of deeply supervised learning has been demonstrated in GoogLeNet <ref type="bibr" target="#b31">[32]</ref>, DSN <ref type="bibr" target="#b17">[18]</ref>, DeepID3 <ref type="bibr" target="#b29">[30]</ref>, etc. The central idea is to provide integrated objective function as direct supervision to the earlier hidden layers, rather than only at the output layer. These "companion" or "auxiliary" objective functions at multiple hidden layers can mitigate the "vanishing" gradients problem. The proposal-free detection framework contains both classification loss and localization loss. The explicit solution requires adding complex sideoutput layers to introduce "companion" objective at each hidden layer for the detection task, similar to <ref type="bibr" target="#b34">[35]</ref>. Here we empower deep supervision with an elegant &amp; implicit solution called dense layer-wise connection, as introduced in DenseNets <ref type="bibr" target="#b9">[10]</ref>. A block is called dense block when all preceding layers in the block are connected to the current layer. Hence, earlier layers in DenseNet can receive additional supervision from the objective function through the skip connections. Although only a single loss function is required on top of the network, all layers including the earlier layers still can share the supervised signals unencumbered. We will verify the benefit of deep supervision in Section 4.1.2. Transition w/o Pooling Layer. We introduce this layer in order to increase the number of dense blocks without reducing the final feature map resolution. In the original design of DenseNet, each transition layer contains a pooling operation to down-sample the feature maps. The number of dense blocks is fixed (4 dense blocks in all DenseNet architectures) if one wants to maintain the same scale of outputs. The only way to increase network depth is adding layers inside each block for the original DenseNet. The transition w/o pooling layer eliminates this restriction of the number of dense blocks in our DSOD architecture, and can also be used in the standard DenseNet. Principle 3: Stem Block. Motivated by Inception-v3 <ref type="bibr" target="#b32">[33]</ref> and v4 <ref type="bibr" target="#b30">[31]</ref>, we define stem block as a stack of three 3Ã—3 convolution layers followed by a 2Ã—2 max pooling layer. The first conv-layer works with stride = 2 and the other two are with stride = 1. We find that adding this simple stem structure can evidently improve the detection performance in our experiments. We conjecture that, compared with the original design in DenseNet (7Ã—7 conv-layer, stride = 2 followed by a 3Ã—3 max pooling, stride = 2), the stem block can reduce the information loss from raw input images. We will show that the reward of this stem block is significant for detection performance in Section 4.1.2. Principle 4: Dense Prediction Structure. <ref type="figure">Figure 1</ref> illustrates the comparison of the plain structure (as in SSD) and our proposed dense structure in the front-end sub-network. SSD designs prediction-layers as an asymmetric hourglass structure. For 300Ã—300 input images, six scales of feature maps are applied for predicting objects. The Scale-1 feature maps are from the middle layer of the backbone subnetwork, which has the largest resolution (38Ã—38) in order to handle the small objects in an image. The remaining five scales are on top of the backbone sub-network. Then, a plain transition layer with the bottleneck structure (a 1Ã—1 conv-layer for reducing the number of feature maps plus a 3Ã—3 conv-layer) <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b8">9]</ref> is adopted between two contiguous scales of feature maps. Learning Half and Reusing Half. In the plain structure as in SSD (see <ref type="figure">Figure 1)</ref>, each later scale is directly transited from the adjacent previous scale. We propose the dense structure for prediction, which fuses multi-scale information for each scale. For simplicity, we restrict that each scale outputs the same number of channels for the prediction feature maps. In DSOD, in each scale (except scale-1), half of the feature maps are learned from the previous scale with a series of conv-layers, while the remaining half feature maps are directly down-sampled from the contiguous high-resolution feature maps. The down-sampling block consists of a 2Ã—2, stride = 2 max pooling layer followed by a 1Ã—1, stride = 1 conv-layer. The pooling layer aims to match resolution to current size during concatenation. The 1Ã—1 conv-layer is used to reduce the number of channels to 50%. The pooling layer is placed before the 1Ã—1 conv-layer for the consideration of reducing computing cost. This down-sampling block actually brings each scale with the multi-resolution feature maps from all of its preceding scales, which is essentially identical to the dense  layer-wise connection introduced in DenseNets. For each scale, we only learn half of new feature maps and reuse the remaining half of the previous ones. This dense prediction structure can yield more accurate results with fewer parameters than the plain structure, as will be studied in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training Settings</head><p>We implement our detector based on the Caffe framework <ref type="bibr" target="#b13">[14]</ref>. All our models are trained from scratch with SGD solver on NVidia TitanX GPU. Since each scale of DSOD feature maps is concatenated from multiple resolutions, we adopt the L2 normalization technique <ref type="bibr" target="#b21">[22]</ref> to scale the feature norm to 20 on all outputs. Note that SSD only applies this normalization to scale-1. Most of our training strategies follow SSD, including data augmentation, scale and aspect ratios for default boxes and loss function (e.g., smooth L1 loss for localization purpose and softmax loss for classification purpose), while we have our own learning rate scheduling and mini-batch size settings. Details will be given in the experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct experiments on the widely used PASCAL VOC 2007, 2012 and MS COCO datasets that have 20, 20, 80 object categories respectively. Object detection performance is measured by mean Average Precision (mAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study on PASCAL VOC2007</head><p>We first investigate each component and design principle of our DSOD framework. The results are mainly summarized in <ref type="table" target="#tab_3">Table 2 and Table 3</ref>. We design several controlled experiments on PASCAL VOC 2007 with our DSOD300 (with 300Ã—300 inputs) for this ablation study. A consistent setting is imposed on all the experiments, unless when some components or structures are examined. In this study, we train the models with the combined training set from VOC 2007 trainval and 2012 trainval ("07+12"), and test on the VOC 2007 testset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Configurations in Dense Blocks</head><p>We first investigate the impact of different configurations in dense blocks of the backbone sub-network.</p><p>Compression Factor in Transition Layers. We compare two compression factor values (Î¸ = 0.5, 1) in the transition layers of DenseNets. Results are shown in <ref type="table" target="#tab_3">Table 3 (rows  2 and 3)</ref>. Compression factor Î¸ = 1 means that there is no feature map reduction in the transition layer, while Î¸ = 0.5 means half of the feature maps are reduced. Results show that Î¸ = 1 yields 2.9% higher mAP than Î¸ = 0.5. # Channels in bottleneck layers. As shown in <ref type="table" target="#tab_5">Table 3</ref> (rows 3 and 4), we observe that wider bottleneck layers (with more channels of response maps) improve the performance greatly (4.1% mAP). # Channels in the 1st conv-layer We observe that a large number of channels in the first conv-layers is beneficial, which brings 1.1% mAP improvement (in <ref type="table" target="#tab_5">Table 3</ref> rows 4 and 5). Growth rate. A large growth rate k is found to be much better. We observe 4.8% mAP improvement in <ref type="table" target="#tab_5">Table 3</ref> (rows 5 and 6) when increase k from 16 to 48 with 4k bottleneck channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Effectiveness of Design Principles</head><p>We now justify the effectiveness of the key design principles elaborated earlier.</p><p>Proposal-free Framework. We tried to train object detectors from scratch using the proposal-based framework such as Faster R-CNN and R-FCN. However, the training process failed to converge for all the network structures we attempted (VGGNet, ResNet, DenseNet). We further tried to train object detectors using the proposal-free framework SSD. The training converged successfully but gives much worse results (69.6% for VGG), compared with the case fine-tuning from pre-trained model (75.8%), as shown in <ref type="table">Table 4</ref>. This experiment validates our design principle to choose a proposal-free framework. Deep Supervision. We then tried to train object detectors from scratch with deep supervision. Our DSOD300 achieves 77.7% mAP, which is much better than the SSD300S that is trained from scratch using VGG16 (69.6%) without deep supervision. It is also much better than the fine-tuned results by SSD300 (75.8%). This validates the principle of deep supervision. Transition w/o Pooling Layer. We compare the case without this designed layer (only 3 dense blocks) and the case with the designed layer (4 dense blocks in our design). The backbone network is DS/32-12-16-0.5. Results are shown in <ref type="table" target="#tab_5">Table 3</ref>. The network structure with the Transition w/o pooling layer brings 1.7% performance gain, which validates the effectiveness of this layer. Stem Block. As can be seen in <ref type="table" target="#tab_5">Table 3</ref> (rows 6 and 9), the stem block improves the performance from 74.5% to 77.3%. This validates our conjecture that using stem block can protect information loss from the raw input images.   <ref type="table">Table 4</ref>: PASCAL VOC 2007 test detection results. SSD300* is updated version by the authors after the paper publication. SSD300S â€  indicates training SSD300* from scratch with ResNet-101 or VGGNet, which serves as our baseline. Note that the speed of Faster R-CNN with ResNet-101 (2.4 fps) is tested on K40, while others are tested on Titan X. The result of SSD300S with ResNet-101 (63.8% mAP, without the pre-trained model) is produced with the default setting of SSD, which may not be optimal.</p><p>Dense Prediction Structure. We analyze the dense prediction structure from three aspects: speed, accuracy and parameters. As shown in <ref type="table">Table 4</ref>, DSOD with dense front-end structure runs slightly lower than the plain structure (17.4 fps vs. 20.6 fps) on a Titan X GPU, due to the overhead from additional down-sampling blocks. However, the dense structure improves mAP from 77.3% to 77.7%, while reduces the parameters from 18.2M to 14.8M. <ref type="table" target="#tab_5">Table 3</ref> gives more details (rows 9 and 10). We also tried to replace the prediction layers in SSD with the proposed dense prediction layers. The accuracy on VOC 2007 test set can be improved from 75.8% (original SSD) to 76.1% (with pre-trained models), and 69.6% to 70.4% (w/o pre-trained models), when using the VGG-16 models as backbone. This verifies the effectiveness of the dense prediction layer.</p><p>What if pre-training on ImageNet? It is interesting to see the performance of DSOD with backbone network pretrained on ImageNet. We trained one lite backbone network DS/64-12-16-1 on ImageNet, which obtains 66.8% top-1 accuracy and 87.8% top-5 accuracy on the validationset (slightly worse than VGG-16). After fine-tuning the whole detection framework on "07+12" trainval set, we achieve 70.3% mAP on the VOC 2007 test-set. The corresponding training-from-scratch solution achieves 70.7% accuracy, which is even slightly better. Future work will investigate this point more thoroughly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Runtime Analysis</head><p>The inference speed is shown in the 6th column of <ref type="table">Table 4</ref>. With 300Ã—300 input, our full DSOD can process an image in 48.6ms (20.6 fps) on a single Titan X GPU with the plain prediction structure, and 57.5ms (17.4 fps) with the dense prediction structure. As a comparison, R-FCN runs at 90ms (11 fps) for ResNet-50 and 110ms (9 fps) for ResNet-101. The SSD300 * runs at 82.6ms (12.1 fps) for ResNet-101 and 21.7ms (46 fps) for VGGNet. In addition, our model uses about only 1/2 parameters to SSD300 with VGGNet, 1/4 to SSD300 with ResNet-101, 1/4 to R-FCN with ResNet-101 and 1/10 to Faster R-CNN with VGGNet. A lite-version of DSOD (10.4M parameters, w/o any speed optimization) can run 25.8 fps with only 1% mAP drops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on PASCAL VOC2007</head><p>Models are trained based on the union of VOC 2007 trainval and VOC 2012 trainval ("07+12") following <ref type="bibr" target="#b20">[21]</ref>. We use a batch size of 128. Note that this batchsize is beyond the capacity of GPU memories (even for an 8 GPU server, each with 12GB memory). We use a trick   to overcome the GPU memory constraints by accumulating gradients over two training iterations, which has been implemented on Caffe platform <ref type="bibr" target="#b13">[14]</ref>. The initial learning rate is set to 0.1, and then divided by 10 after every 20k iterations. The training finished when reaching 100k iterations. Following <ref type="bibr" target="#b20">[21]</ref>, we use a weight decay of 0.0005 and a momentum of 0.9. All conv-layers are initialized with the "xavier" method <ref type="bibr" target="#b5">[6]</ref>. <ref type="table">Table 4</ref> shows our results on VOC2007 test set. SSD300</p><p>* is the updated SSD results which use the new data augmentation technique. Our DSOD300 with plain connection achieves 77.3%, which is slightly better than SSD300 * (77.2%). DSOD300 with dense prediction structure improves the result to 77.7%. After adding COCO as training data, the performance is further improved to 81.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on PASCAL VOC2012</head><p>For the VOC 2012 dataset, we use VOC 2012 trainval and VOC 2007 trainval + test for training, and test on VOC 2012 test set. The initial learning rate is set to 0.1 for the first 30k iterations, then divided by 10 after every 20k iterations. The total training iterations are 110k. Other settings are the same as those used in our VOC 2007 experiments. Our results of DSOD300 are shown in <ref type="table" target="#tab_7">Table 5</ref>. DSOD300 achieves 76.3% mAP, which is consistently better than SSD300 * (75.8%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on MS COCO</head><p>Finally we evaluate our DSOD on the MS COCO dataset <ref type="bibr" target="#b19">[20]</ref>. MS COCO contains 80k images for training, 40k for validation and 20k for testing (test-dev set). Following <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b18">19]</ref>, we use the trainval set (train set + validation set) for training. The batch size is also set as 128. The initial learning rate is set to 0.1 for the first 80k iterations, then divided by 10 after every 60k iterations. The total number of training iterations is 320k.</p><p>Results are summarized in <ref type="table" target="#tab_8">Table 6</ref>. Our DSOD300 achieves 29.3%/47.3% on the test-dev set, which outperforms the baseline SSD300</p><p>* with a large margin. Our result is comparable to the single-scale R-FCN, and is close to the R-FCNmulti-sc which uses ResNet-101 as the pre-trained model. Interestingly, we observe that our result with 0.5 IoU is lower than R-FCN, but our [0.5:0.95] result is better or comparable. This indicates that our predicted locations are more accurate than R-FCN under the larger overlap settings. It is reasonable that our small object detection precision is slightly lower than R-FCN since our input image size (300Ã—300) is much smaller than R-FCN's (âˆ¼ 600Ã—1000). Even with this disadvantage, our large object detection precision is still much better than R-FCN. This further demonstrates the effectiveness of our approach. <ref type="figure" target="#fig_0">Figure 2</ref> shows some qualitative detection examples on COCO with our DSOD300 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Better Model Structure vs. More Training Data. An emerging idea in the computer vision community is that object detection or other vision tasks might be solved with deeper and larger neural networks backed with massive training data like ImageNet <ref type="bibr" target="#b2">[3]</ref>. Thus more and more largescale datasets have been collected and released recently, such as the Open Images dataset <ref type="bibr" target="#b15">[16]</ref>, which is 7.5x larger in the number of images and 6x larger of categories than that of ImageNet. We definitely agree that, under modest assumptions that given boundless training data and unlimited computational power, deep neural networks should perform extremely well. However, our proposed approach and experimental results imply an alternative view to handle this problem: a better model structure might enable similar or better performance compared with complex models trained from large data. Particularly, our DSOD is only trained with 16,551 images on VOC 2007, but achieves competitive or even better performance than those models trained with 1.2 million + 16,551 images.</p><p>In this premise, it is worthwhile rehashing the intuition that as datasets grow larger, training deep neural networks becomes more and more expensive. Thus a simple yet efficient approach becomes increasingly important. Despite its conceptual simplicity, our approach shows great potential under this setting.</p><p>Why Training from Scratch? There have been many successful cases where model fine-tuning works greatly. One may ask why should we train object detectors from scratch. We argue that, as aforementioned briefly, training from scratch is of critical importance at least for two cases. First, there may be big domain differences from pretrained model domain to the target one. For instance, most pre-trained models are trained on large scale RGB image dataset, ImageNet. It is very difficult to transfer ImageNet model to the domains of depth images, multi-spectrum images, medical images, etc. Some advanced domain adaptation techniques have been proposed. But what an amazing thing if we have a technique which can train object detector from scratch. Second, model fine-tuning restricts the structure design space for object detection networks. This is very critical for the deployment of deep neural networks models to resource-limited Internet-of-Things (IoT) scenario.</p><p>Model Compactness vs. Performance. It has often been reported that there is a trade-off between model compactness (in terms of the number of parameters) and performance. Most CNN-based detection solutions require a huge memory space to store the massive parameters. Therefore the models are usually unsuitable for low-end devices like mobile-phones and embedded electronics. Thanks to the parameter-efficient dense block, our model is much smaller than most competitive methods. For instance, our smallest dense model (DS/64-64-16-1, with dense prediction layers) achieves 73.6% mAP with only 5.9M parameters, which shows great potential for applications on low-end devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented Deeply Supervised Object Detector (DSOD), a simple yet efficient framework for training object detector from scratch. Without using pre-trained models on ImageNet, DSOD demonstrates competitive accuracy to state-of-the-art detectors such as SSD, Faster R-CNN and R-FCN on the popular PASCAL VOC 2007, 2012 and MS COCO datasets, with only 1/2, 1/4 and 1/10 parameters compared to SSD, R-FCN and Faster R-CNN, respectively. DSOD has great potential on domain different scenario like depth, medical, multi-spectral images, etc. Our future work will consider these domains, as well as learning ultra efficient DSOD models to support resource-bounded devices. The code and models of this paper are available at: https://github.com/szq0214/DSOD.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of object detection results on the MS COCO test-dev set using DSOD300. The training data is COCO trainval without the ImageNet pre-trained models (29.3% mAP@[0.5:0.95] on the test-dev set). Each output box is associated with a category label and a softmax score in [0, 1]. A score threshold of 0.6 is used for displaying. For each image, one color corresponds to one object category in that image. The running time per image is 57.5ms on one Titan X GPU or 590ms on Intel (R) Core (TM) i7-5960X CPU @ 3.00GHz.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Effectiveness of various designs on VOC 2007 test set. Please refer to Table 3 and Section 4.1 for more details.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on PASCAL VOC 2007 test set. DS/A-B-k-Î¸ describes our backbone network structure. A denotes the number of channels in the 1st conv-layer. B denotes the number of channels in each bottleneck layer (1Ã—1 convolution). k is the growth rate in dense blocks. Î¸ denotes the compression factor in transition layers. See Section 4.1 for more explanations.</figDesc><table>Method 
data 
pre-train 
backbone network 
prediction layer 
speed (fps) 
# parameters 
input size 
mAP 
Faster RCNN [27] 
07+12 
VGGNet 
-
7 
134.7M 
âˆ¼ 600 Ã— 1000 
73.2 
Faster RCNN [27] 
07+12 
ResNet-101 
-
2.4 

 *  

-
âˆ¼ 600 Ã— 1000 
76.4 
R-FCN [19] 
07+12 
ResNet-50 
-
11 
31.9M 
âˆ¼ 600 Ã— 1000 
77.4 
R-FCN [19] 
07+12 
ResNet-101 
-
9 
50.9M 
âˆ¼ 600 Ã— 1000 
79.5 
R-FCNmulti-sc [19] 
07+12 
ResNet-101 
-
9 
50.9M 
âˆ¼ 600 Ã— 1000 
80.5 
YOLOv2 [26] 
07+12 
Darknet-19 
-
81 
-
352 Ã— 352 
73.7 
SSD300 [21] 
07+12 
VGGNet 
Plain 
46 
26.3M 
300 Ã— 300 
75.8 
SSD300* [21] 
07+12 
VGGNet 
Plain 
46 
26.3M 
300 Ã— 300 
77.2 
Faster RCNN 
07+12 
7 
VGGNet/ResNet-101/DenseNet 
Failed 
R-FCN 
07+12 
7 
VGGNet/ResNet-101/DenseNet 
Failed 
SSD300S 

 â€  

07+12 
7 
ResNet-101 
Plain 
12.1 
52.8M 
300 Ã— 300 
63.8 

 *  

SSD300S 

 â€  

07+12 
7 
VGGNet 
Plain 
46 
26.3M 
300 Ã— 300 
69.6 
SSD300S 

 â€  

07+12 
7 
VGGNet 
Dense 
37 
26.0M 
300 Ã— 300 
70.4 
DSOD300 
07+12 
7 
DS/64-192-48-1 
Plain 
20.6 
18.2M 
300 Ã— 300 
77.3 
DSOD300 
07+12 
7 
DS/64-192-48-1 
Dense 
17.4 
14.8M 
300 Ã— 300 
77.7 
DSOD300 
07+12+COCO 
7 
DS/64-192-48-1 
Dense 
17.4 
14.8M 
300 Ã— 300 
81.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc>PASCAL</figDesc><table>VOC 2012 test detection results. 07+12: 07 trainval + 12 trainval, 07+12+S: 07+12 plus segmentation labels, 07++12: 07 
trainval + 07 test + 12 trainval. Result links are DSOD300 (07+12) : http://host.robots.ox.ac.uk:8080/anonymous/PIOBKI. 
html; DSOD300 (07+12+COCO): http://host.robots.ox.ac.uk:8080/anonymous/I0UUHO.html. 

Method 
data 
network 
pre-train 
Avg. Precision, IoU: 
Avg. Precision, Area: 
Avg. Recall, #Dets: 
Avg. Recall, Area: 
0.5:0.95 
0.5 
0.75 
SML 
1 
10 
100 
SML 
Faster RCNN [27] 
trainval 
VGGNet 
21.9 
42.7 
-
---------
ION [1] 
train 
VGGNet 
23.6 
43.2 
23.6 
6.4 
24.1 
38.3 
23.2 
32.7 
33.5 
10.1 
37.7 
53.6 
R-FCN [19] 
trainval 
ResNet-101 
29.2 
51.5 
-
10.3 
32.4 
43.3 
------
R-FCNmulti-sc [19] 
trainval 
ResNet-101 
29.9 
51.9 
-
10.8 
32.8 
45.0 
------
SSD300 (Huang et al.) [11] 
&lt; trainval35k 
MobileNet 
18.8 
-
-
---------
SSD300 (Huang et al.) [11] 
&lt; trainval35k 
Inception-v2 
21.6 
-
-
---------
YOLOv2 [26] 
trainval35k 
Darknet-19 
21.6 
44.0 
19.2 
5.0 
22.4 
35.5 
20.7 
31.6 
33.3 
9.8 
36.5 
54.4 
SSD300* [21] 
trainval35k 
VGGNet 
25.1 
43.1 
25.8 
6.6 
25.9 
41.4 
23.7 
35.1 
37.2 
11.2 
40.4 
58.4 
DSOD300 
trainval 
DS/64-192-48-1 
7 
29.3 
47.3 
30.6 
9.4 
31.5 
47.0 
27.3 
40.7 
43.0 
16.7 
47.1 
65.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc>MS COCO test-dev 2015 detection results.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The visualization of the complete network structure is available at: http://ethereon.github.io/netscope/#/gist/ b17d01f3131e2a60f9057b5d3eb9e04d.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Yu-Gang Jiang and Xiangyang Xue are supported in part by a NSFC project (#61622204), a project from STCSM (#16JC1420401), and an European FP7 project (PIRSESGA-2013-612652).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>ArbelÃ¡ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>JÃ©gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09326</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08021</idno>
		<title level="m">Deep but lightweight neural networks for real-time object detection</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Openimages: A public dataset for large-scale multi-label and multi-class image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<ptr target="https://github.com/openimages" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via regionbased fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.00873</idno>
		<title level="m">Deepid3: Face recognition with very deep neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Inceptionv4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
