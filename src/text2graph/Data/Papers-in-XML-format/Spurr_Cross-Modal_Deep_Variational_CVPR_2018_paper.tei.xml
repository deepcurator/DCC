<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-modal Deep Variational Hand Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Spurr</surname></persName>
							<email>spurra@inf.ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
							<email>jsong@inf.ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonwook</forename><surname>Park</surname></persName>
							<email>spark@inf.ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
							<email>otmarh@inf.ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zurich</surname></persName>
						</author>
						<title level="a" type="main">Cross-modal Deep Variational Hand Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Hands are of central importance to humans in manipulating the physical world and in communicating with each other. Recovering the spatial configuration of hands from natural images therefore has many important applications in AR/VR, robotics, rehabilitation and HCI. Much work exists that tracks articulated hands in streams of depth images, or that estimates hand pose <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35]</ref> from individual depth frames. However, estimating the full 3D hand pose from monocular RGB images only is a more challenging task due to the manual dexterity, symmetries and selfsimilarities of human hands as well as difficulties stemming from occlusions, varying lighting conditions and lack of accurate scale estimates. Compared to depth images the RGB case is less well studied. Recent work relying solely on RGB images <ref type="bibr" target="#b37">[38]</ref> proposes a deep learning architecture that decomposes the task into several substeps, demonstrating initial feasibility and providing a public dataset for comparison. The proposed architecture is specifically designed for the monocular case and splits the task into hand and 2D keypoint detection followed by a 2D-3D lifting step but incorporates no explicit hand model. Our work is also concerned with the estimation of 3D joint-angle configurations of human hands from RGB images but learns a cross-modal, statistical hand model. This is attained via learning of a latent representation that embeds sample points from multiple data sources such as 2D keypoints, images and 3D hand poses. Samples from this latent space can then be reconstructed by independent decoders to produce consistent and physically plausible 2D or 3D joint predictions and even RGB images.</p><p>Findings from bio-mechanics suggest that while articulated hands have many degrees-of-freedom, only few are fully independently articulated <ref type="bibr" target="#b19">[20]</ref>. Therefore a sub-space of valid hand poses is supposed to exist and prior work on depth based hand tracking <ref type="bibr" target="#b25">[26]</ref> has successfully employed dimensionality reduction techniques to improve accuracy.</p><p>This idea has been recently revisited in the context of deep-learning, where Wan et al. <ref type="bibr" target="#b33">[34]</ref> attempt to learn a manifold of hand poses via a combination of variational autoencoders (VAEs) and generative adversarial networks (GANs) for hand pose estimation from depth images. However, their approach is based on two separate manifolds, one for 3D hand joints (VAE) and one for depth-maps (GAN) and requires a mapping function between the two.</p><p>In this work we propose to learn a single, unified latent space via an extension of the VAE framework. We provide a derivation of the variational lower bound that permits training of a single latent space using multiple modalities, where similar input poses are embedded close to each other independent of the input modality. <ref type="figure" target="#fig_0">Fig. 1</ref> visualizes this learned unified latent space for two modalities (RGB &amp; 3D). We focus on RGB images and hence test the architecture on different combinations of modalities where the goal is to produce 3D hand poses as output. At the same time, the VAE framework naturally allows to generate samples consistently in any modality.</p><p>We experimentally show that the proposed approach outperforms the state-of-the art method <ref type="bibr" target="#b37">[38]</ref> in direct RGB to 3D hand pose estimation, as well as in lifting from 2D detections to 3D on a challenging public dataset. Meantime, we note that given any input modality a mapping into the embedding space can be found and likewise hand configurations can be reconstructed in various modalities, thus the approach learns a many-to-many mapping. We demonstrate this capability via generation of novel hand pose configurations via sampling from the latent space and consistent reconstruction in different modalities (i.e., 3D joint positions and synthesized RGB images). These could be potentially used in hybrid approaches for temporal tracking or to generate additional training data. Furthermore, we explore the utility of the same architecture in the case of depth images and show that we are comparable to state-of-art depth based methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">34]</ref> that employ specialized architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Capturing the 3D motion of human hands from images is a long standing problem in computer vision and related areas (cf. <ref type="bibr" target="#b4">[5]</ref>). With the recent emergence of consumer grade RGB-D sensors and increased importance of AR and VR this problem has seen increased attention <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37]</ref>. Generally speaking approaches can be categorized into tracking of articulated hand motion over time (e.g., <ref type="bibr" target="#b17">[18]</ref>) and per-frame classification <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref>. Furthermore, a number of hybrid methods exist that first leverage a discriminative model to initialize a hand pose estimate which is then refined and tracked via carefully designed energy functions to fit a hand model into the observed depth data <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref>. Estimating hand pose from RGB images is more challenging.</p><p>Also using depth-images, a number of approaches have been proposed that extract manually designed features and discriminative machine learning models to predict joint locations in depth images or 3D joint-angles directly <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>. More recently a number of deep-learning models have been proposed that take depth images as input and regress 2D joint locations in multiple images <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32]</ref> which are then used for optimization-based hand pose estimation. Others deploy convolutional neural networks (CNNs) in end-to-end learning frameworks to regress 3D hand poses from depth images, either directly estimating 3D joint configurations <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref>, or estimating joint-angles instead of Cartesian coordinates <ref type="bibr" target="#b15">[16]</ref>. Exploiting the depth information more directly, it has also been proposed to convert depth images into 3D multi-views <ref type="bibr" target="#b5">[6]</ref> or volumetric representations <ref type="bibr" target="#b6">[7]</ref> before feeding them to a 3D CNN. Aiming at more mobile usage scenarios, recent work has proposed hybrid methods for hand-pose estimation from body-worn cameras under heavy occlusion <ref type="bibr" target="#b12">[13]</ref>. While the main focus lies on RGB imagery, our work is also capable of predicting hand pose configurations from depth images due to the multi-modal latent space.</p><p>Wan et al. <ref type="bibr" target="#b33">[34]</ref> is the most related work in spirit to ours. Like our work, they employ deep generative models (a combination of VAEs and GANs) to learn a latent space representation that regularizes the posterior prediction. Our method differs significantly in that we propose a theoretically grounded derivation of a cross-modal training scheme based on the variational autoencoder <ref type="bibr" target="#b10">[11]</ref> framework that allows for joint training of a single cross-modal latent space, whereas <ref type="bibr" target="#b33">[34]</ref> requires training of two separate latent spaces, learning of a mapping function linking them and final endto-end refinement. Furthermore, we experimentally show that our approach reaches parity with the state-of-the-art in depth based hand pose estimation and outperforms existing methods in the RGB case, whereas <ref type="bibr" target="#b33">[34]</ref> report only depth based experiments. In <ref type="bibr" target="#b1">[2]</ref>, VAE is also deployed for depth based hand pose estimation. However, their focus is minimising the dissimilarity coefficient between the true distribution and the estimated distribution.</p><p>To the best of our knowledge there is currently only one approach for learning-based hand pose estimation from RGB images alone <ref type="bibr" target="#b37">[38]</ref>. Demonstrating the feasibility of the task, this work splits 3D hand pose estimation into an image segmentation, 2D joint detection and 2D-3D lifting task. Our approach allows for training of the latent space using either input modality (in this case 2D key points or RGB images) and direct 3D hand pose estimation via decoding the corresponding sample from the latent space. We exper-+ + + <ref type="figure">Figure 2</ref>: Schematic overview of our architecture. Left: a cross-modal latent space z is learned by training pairs of encoder and decoder q, p networks across multiple modalities (e.g., RGB images to 3D hand poses). Auxilliary encoder-decoder pairs help in regularizing the latent space. Right: The approach allows to embed input samples of one set of modalities (here: RGB, 3D) and to produce consistent and plausible posterior estimates in several different modalities (RGB, 2D and 3D).</p><p>imentally show that our methods outperforms <ref type="bibr" target="#b37">[38]</ref> both in the 2D-3D lifting setting and the end-to-end hand pose estimation setting, even when using fewer invariances than the original method. Finally, we demonstrate that the same approach can be directly employed to depth images without any modifications to the architecture.</p><p>Our work builds on literature in deep generative modeling. Generative Adversarial Nets (GAN) <ref type="bibr" target="#b7">[8]</ref> learn an underlying distribution of the data via an adversarial learning process. The Variational Autoencoder (VAE) <ref type="bibr" target="#b10">[11]</ref> learns it via optimizing the log-likelihood of the data under a latent space manifold. However unlike GANs, they provide a framework to embed data into this manifold which has been shown to be useful for diverse applications such as multimodal hashing <ref type="bibr" target="#b3">[4]</ref>. Aytar et al. <ref type="bibr" target="#b0">[1]</ref> use several CNNs to co-embed data from different data modalities for scene classification and Ngiam et al. <ref type="bibr" target="#b13">[14]</ref> reconstruct audio and video across modalities via a shared latent space. Our work also aims to create a cross-modal latent space and we provide a derivation of the cross-modal training objective function that naturally admits learning with different data sources all representing physically plausible hand pose configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The complex and dexterous articulation of the human hand is difficult to model directly with geometric or physical constraints <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref>. However, there is broad agreement in the literature that a large amount of the degrees-offreedom are not independently controllable and that hand motion, in natural movement, lives in a low-dimensional subspace <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31]</ref>. Furthermore, it has been shown that dimensionality reduction techniques can provide data-driven priors in RGB-D based hand pose estimation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26]</ref>. However, in order to utilize such a low-dimensional sub-space directly for posterior estimation in 3D hand-pose estimation it needs to be i) smooth, ii) continuous and iii) consistent. Due to the inherent difficulties of capturing hand poses, most data sets do not cover the full motion space and hence the desired manifold is not directly attainable via simple dimensionality reduction techniques such as PCA.</p><p>We deploy the VAE framework that admits cross-modal training of such a hand pose latent space by using various sources of data representation, even if stemming from different data sets both in terms of input and output. Our crossmodal training scheme, illustrated in <ref type="figure">Fig. 2</ref>, learns to embed hand pose data from different modalities and to reconstruct them either in the same or in a different modality.</p><p>More precisely, a set of encoders q take data samples x in the form of either 2D keypoints, RGB or depth images and project them into a low-dimensional latent space z, representing physically plausible poses. A set of decoders p reconstruct the hand configuration in either modality. The focus of our work is on 3D hand pose estimation and therefore on estimating the 3D joint posterior. The proposed approach is fully generative and experimentally we show that it is capable of generating consistent hand configurations across modalities. During training, each input modality alternatively contributes to the construction of the shared la-tent space. The manifold is continuous and smooth which we show by generating cross-modal samples such as novel pairs of 3D poses and images of natural hands 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Variational Autoencoder</head><p>Our cross-modal training objective can be derived from the VAE framework <ref type="bibr" target="#b10">[11]</ref>, a popular class of generative models, typically used to synthesize data. A latent representation is attained via optimizing the so-called variational lower bound on the log-likelihood of the data:</p><formula xml:id="formula_0">log p(x) ≥ E z∼q(z|x) [log p(x|z)] − D KL (q(z|x)||p(z))</formula><p>(1) Here D KL (·) is the Kullback-Leibler divergence, and the conditional probability distributions q(z|x), p(x|z) are the encoder and decoders, parametrized by neural networks. The distribution p(z) is the prior on the latent space, modeled as N (z|0, I). The encoder returns the mean µ and variance σ 2 of a normal distribution, such that z ∼ N (µ, σ 2 ). In this original form VAEs only take a single data distribution into account. To admit cross-modal training, at least two data modalities need to be considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cross-modal Hand Pose Latent Space</head><p>Our goal is to guide the cross-modal VAE into learning a lower-dimensional latent space of hand poses with the above mentioned desired properties and the ability to project any modality into z and to generate posterior estimates in any modality. For this purpose we re-derive a new objective function for training which leverages multiple modalities. We then detail our training algorithm based on this objective function.</p><p>For brevity we use a concrete example in which a data sample x i (e.g., an RGB image) is embedded into the latent space to obtain the embedding vector z, from which a corresponding data sample x t is reconstructed (e.g., a 3D joint configuration). To achieve this, we maximize the log-probability of our desired output modality x t under our model log p θ (x t ), where θ are the model parameters. We will omit the model parameters to reduce clutter.</p><p>Similar to the original derivation <ref type="bibr" target="#b10">[11]</ref>, we start with the quantity log p(x t ) that we want to maximize:</p><formula xml:id="formula_1">log p(x t ) = z q(z|x i ) log p(x t )dz,<label>(2)</label></formula><p>exploiting the fact that z q(z|x i )dz = 1 and expanding p(x t ) gives:</p><formula xml:id="formula_2">z q(z|x i ) log p(x t )p(z|x t )q(z|x i ) p(z|x t )q(z|x i ) dz.<label>(3)</label></formula><p>Remembering that</p><formula xml:id="formula_3">D KL (p(x)||q(x)) = x p(x) log p(x) q(x)</formula><p>and splitting the integral of Eq (3) we arrive at:</p><formula xml:id="formula_4">z q(z|x i ) log q(z|x i ) p(z|x t ) dz + z q(z|x i ) log p(x t )p(z|x t ) q(z|x i ) dz = D KL (q(z|x i )||p(z|x t )) + z q(z|x i ) log( p(x t |z)p(z) q(z|x i ) )dz.<label>(4)</label></formula><p>Here p(z|x t ) corresponds to the desired but inaccessible posterior, which we approximate with q(z|x i ).</p><p>Since p(x t )p(z|x t ) = p(x t |z)p(z) and because D KL (p(x)||q(x)) ≥ 0 for any distribution p, q, we attain the final lower bound:</p><formula xml:id="formula_5">D KL (q(z|x i )||p(z|x t )) + z q(z|x i ) log( p(x t |z)p(z) q(z|x i ) )dz ≥ z q(z|x i ) log p(x t |z)dz − z q(z|x i ) log q(z|x i ) p(z) dz = E z∼q(z|xi) [log p(x t |z)] − D KL (q(z|x i )||p(z)).<label>(5)</label></formula><p>Note that we changed signs via the identity − log(x) = log( 1 x ). Here q(z|x i ) is our encoder, embedding x i into the latent space and p(x t |z) is the decoder, which transforms the latent sample z into the desired representation x t .</p><p>The derivation shows that input samples x i and target samples x t can be decoupled via a joint embedding space z where i and t can represent any modality. For example, to maximize log p(x 3D ) when given x RGB , we can train with q(z|x RGB ) as our encoder and p(x 3D |z) as the decoder.</p><p>Importantly the above derivation also allows to train additional encoder-decoder pairs such as (q(z|x RGB ), p(x RGB |z)), at the same time, for the same z. This cross-modal training regime results in a single latent space that allows us to embed and reconstruct multiple data modalities, or even train in a unsupervised fashion.</p><p>In the context of hand pose estimation, p(z) represents a hand pose manifold which can be better defined with additional input modalities such as x RGB , x 2D , x 3D , and even x Depth used in combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Architecture</head><p>In practice, the encoder q k for data modality k returns the mean µ and variance σ 2 of a normal distribution for a given sample, from which the embedding z is sampled, i.e z ∼ N (µ, σ 2 ). However, the decoder p l directly reconstructs the latent sample z to the desired data modality l. <ref type="figure">Fig. 2</ref>, illustrates our proposed architecture for the case of RGB based handpose estimation. In this setting we use two encoders for RGB images and 3D keypoints respectively. Furthermore, the architecture contains two decoders for RGB images and 3D joint configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Procedure</head><p>Our cross-modal objective function (Eq 3) follows the training procedure given as pseudo-code in Alg.1. The procedure takes a set of modalities P V AE with corresponding encoders and decoders q i , p j , where i, j signify the respective modality, and trains all such pairs iteratively for E epochs. Note that the embedding space z is always the same and hence we attain a joint cross-modal latent space from this procedure (cf. <ref type="figure" target="#fig_0">Fig. 1</ref>). <ref type="figure">p l2 )</ref>, ...} Encoder/Decoder pairs, where q k1 encodes data from modality k 1 and p l1 reconstructs latent samples to data of modality l 1 . E Number of epochs e ← 0 for e &lt; E do for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Cross-modal Variational Autoencoders</head><formula xml:id="formula_6">P V AE ← {(q k1 , p l1 ), (q k2 ,</formula><formula xml:id="formula_7">(q k , p l ) ∈ P V AE do x k , x l ← X k , X l Sample data pair of modality k, l µ, σ ← q k (x k ) z ∼ N (µ, σ) x l ← p l (z) L M SE ← ||x l −x l || 2 L KL ← −0.5 * (1 + log(σ 2 ) − µ 2 − σ 2 ) θ q k ← θ q k − ∇ θq k (L M SE + L KL ) θ p l ← θ p l − ∇ θp l (L M SE + L KL )</formula><p>end for e ← e + 1 end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To evaluate the performance of the cross-modal VAE we systematically evaluate the utility of the proposed training algorithm and the resulting cross-modal latent space. This is done via estimation of 3D hand joint positions from three entirely different input modalities: 1) 2D joint locations; 2) RGB image; 3) depth images. In our experiments we explored combinations of different modalities during training. We always predict at least the 3D hand configuration but add further modalities. More specifically we run experiments with the following four variants: a) Var. 1:</p><formula xml:id="formula_8">(x i → x t ) b) Var. 2: (x i → x t , x t → x t ) c) Var. 3: (x i → x t , x i → x i ) d) Var. 4: (x i → x t , x i → x i , x t → x t )</formula><p>, where x i always signifies the input modality and i takes one of the following values: [RGB, 2D, Depth] and t equals the output modality. In our experiments this is always t = 3D but can in general be any target modality. Including the x t → x i direction neither directly affects the RGB encoder, nor the 3D joint decoder and hence was dropped from our analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>We employ Resnet-18 <ref type="bibr" target="#b8">[9]</ref> for the encoding of RGB and depth images. Note that the model size of this encoder is much smaller compared to prior work that directly regresses 3D joint coordinates <ref type="bibr" target="#b14">[15]</ref>. The decoders for RGB and depth consist of a series of (TransposedConv, BatchNorm2D and ReLU)-layers. For the case of 2D keypoint and 3D joint encoders and decoders, we use several (Linear, ReLU)-layers. In our experiments we did not observe much increase in accuracy from more complex decoder architectures. We train our architecture with the ADAM optimizer using a learning rate of 10 −4 . Exact architecture details and hyperparameters can be found in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>We evaluate our method in the above settings based on several publicly available datasets. For the input modality of 2D keypoints and RGB images only few annotated datasets are available. We test on the datasets of the Stereo Hand Pose Tracking Benchmark <ref type="bibr" target="#b36">[37]</ref> (STB) and the Rendered Hand Pose Dataset (RHD) <ref type="bibr" target="#b37">[38]</ref>. STB contains 18k images with resolution of 640 × 480, which are split into a training set with 15k samples and test set with 3k samples. These images are annotated with 3D keypoint locations and the 2D keypoints are recovered via projecting them with the camera intrinsic matrix. The depicted hand poses contain little self-occlusion and variation in global orientation, lighting etc. and are relatively easy to recover.</p><p>RHD is a synthetic dataset with rendered hand images, which is composed of 42k training images and 2.7k evaluation images of size 320 × 320. Similar to STB, both 2D and 3D keypoint locations are annotated. The dataset contains a much richer variety of viewpoints and poses. The 3D human model is set in front of randomly sampled images from Flickr to generate arbitrary backgrounds. This dataset is considerably more challenging due to variable viewpoints and difficult hand poses at different scales. Furthermore, despite being a synthetic dataset the images contain significant amount of noise and blur and are relatively low-res.</p><p>For the depth data, we evaluate on the ICVL <ref type="bibr" target="#b26">[27]</ref>, NYU <ref type="bibr" target="#b31">[32]</ref>, and MSRA <ref type="bibr" target="#b24">[25]</ref> datasets. For NYU, we train and test on viewpoint 1 and all 36 available joints, and evaluate on 14 joints as done in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34]</ref> while for MSRA, we perform a leave-one-out cross-validation and evaluate the errors for the 9 models trained as done in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation metrics</head><p>We provide three different metrics to evaluate the performance of our proposed model under various settings: i) The most common metric used in the 3D hand pose estimation literature is the mean 3D joint error which measures the average euclidean distance between predicted joints and ground truth joints. ii) We also report Percentage of Correct Keypoints (PCK) which returns the mean percentage of predicted joints below an euclidean distance of d from the correct joint location. iii) The hardest metric, which reports the Percentage of Correct Frames (PCF) where all the predicted joints are within an euclidean distance of d to its respective GT location. We report this only for depth since it is commonly reported in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison of variants</head><p>We begin with comparing our variants with each other to determine which performs best and experiment on RHD and STB. On both datasets, we test the performance of our model on the task of regressing the 3D joints from RGB directly. Additionally, we predict the 3D joint locations from given 2D joint locations (dimensionality lifting) on RHD. <ref type="table">Table 1</ref> shows our results on the corresponding task and dataset. The errors are given in mean end-point-error (EPE) (median EPE is in the supplementary). Var. 3 outperforms the other variants on two tasks; lifting 2D joint locations to 3D on RHD and regressing 3D joint location directly from RGB on STB. On the other hand, Var. 1 is superior in the task of RGB→3D on RHD. However we note that in general, the individual performance differences are minor. This is to be expected, as we conduct all our experiments within individual datasets. Hence even if multiple modalities are present, they capture the same poses and the same inherent information. This indicates that having a shared latent space for generative purposes does not harm the performance and in certain cases can even enhance it. This may be due to the regularizing effect of introducing multiple modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison to related work</head><p>In this section we perform a qualitative analysis of our performance in relation to prior work for both RGB and depth cases. For this, we pick the best variant of the respective task, as determined in the previous section. For the RGB datasets (RHD and STB), we compare against <ref type="bibr" target="#b37">[38]</ref>. To the best of our knowledge, it is the only prior work that addresses the same task as we do. In order to compare fairly, we conduct the same data preprocessing. Importantly, in <ref type="bibr" target="#b37">[38]</ref> additional information such as handedness (H) and scale of the hand (S) are provided at test time. Furthermore, the cropped hands are normalized to a roughly uniform size. Finally, they change the task from predicting the global 3D joint coordinates to estimating a palm-relative, translation invariant (T) set of joint coordinates by providing ground truth information of the palm center. In our case, the handedness is provided via a boolean flag directly into the model.</p><p>However, in order to assess the influence of our learned hand model we incrementally reduce the reliance on invariances which require access to ground-truth information. These results are shown alongside our main algorithm.</p><p>2D to 3D. As a baseline experiment we compare our method to that of <ref type="bibr" target="#b37">[38]</ref> in the task of lifting 2D keypoints into a 3D hand pose configuration on the RHD dataset. Recently <ref type="bibr" target="#b11">[12]</ref> report that given a good 2D keypoint detector, lifting to 3D can yield surprisingly good results, even with simple methods in the case of 3D human pose estimation. Hand pose estimation is considerably more challenging task due to the more complex motion and flexibility of the human hand. Furthermore, <ref type="bibr" target="#b37">[38]</ref> provide a separate evaluation of their lifting component which serves as our baseline.</p><p>The first column of <ref type="table">Table 2</ref> summarizes the mean squared end-point errors (EPE) for the RHD dataset. In general, our proposed model outperforms <ref type="bibr" target="#b37">[38]</ref> by a relatively large margin. The bottom rows of <ref type="table">Table 2</ref> show results of ours without the handedness invariance (H) and the scale invariance (S), we still surpass the accuracy of <ref type="bibr" target="#b37">[38]</ref>. This suggests that our model indeed encodes physically plausible hand poses and that reconstructing the posterior from the embedding aids the hand pose estimation task.</p><p>RGB to 3D. Here, we evaluate our method on the task of directly predicting 3D hand pose from RGB images, without intermediate 2D keypoint extraction. We run our model and <ref type="bibr" target="#b37">[38]</ref> on cropped RGB images for fair comparison.</p><p>Zimmermann et al. <ref type="bibr" target="#b37">[38]</ref>, in which 2D keypoints are first predicted and then lifted into 3D serves as our baseline. We evaluate the proposed model on the STB <ref type="bibr" target="#b36">[37]</ref> and RHD <ref type="bibr" target="#b37">[38]</ref> datasets. <ref type="figure">Fig. 7a and 7b</ref> show several samples of our prediction on STB and RHD respectively. Even though some images in RHD contain heavily occluded fingers, our method retrieves biomechanically plausible predictions.</p><p>The middle column of <ref type="table">Table 2</ref> summarizes the results for the harder RHD dataset. Our approachs accuracy exceeds that of <ref type="bibr" target="#b37">[38]</ref> by a large margin. Removing available invariances again slightly decreases performance but our models still remains superior to <ref type="bibr" target="#b37">[38]</ref>. Looking at the PCK curve comparison in <ref type="figure">Fig. 4a</ref>, we see that our model outperforms <ref type="bibr" target="#b37">[38]</ref> for all thresholds.</p><p>The rightmost column of <ref type="table">Table 2</ref> shows the performance on the STB dataset. The margin of improvement of our approach is considerably smaller. We argue that the perfor- mance on the dataset is saturated as it is much easier (see discussion in Sec. 4.2). <ref type="figure">Fig. 4b</ref> shows the PCK curves on STB, with the other baselines that operate on noisy stereo depth maps and not RGB (directly taken from <ref type="bibr" target="#b37">[38]</ref>).</p><p>Depth to 3D. Given the ready availability of RGB-D cameras, the task of 3D joint position estimation from depth has been explored in great detail and specialized architectures have been proposed. We evaluate our architecture, designed originally for the RGB case, on the ICVL <ref type="bibr" target="#b26">[27]</ref>, NYU <ref type="bibr" target="#b31">[32]</ref> and MSRA <ref type="bibr" target="#b24">[25]</ref> datasets. Despite the lower model capacity, our method performs comparably (see <ref type="figure">Fig. 5</ref>) to recent works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> with just a modification to take 1-channel images as input compared to our RGB case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Semi-supervised learning</head><p>Due to the nature of cross-training, we can exploit complementary information from additional data. For example, if additional unlabeled images are available, our model can make use of these via cross-training. This is a common scenario, as unlabeled data is plentiful. If not available, acquiring this is by far simpler than recording training data.</p><p>To explore this semi-supervised setting, we perform an additional experiment on STB. We simulate a situation where we have labeled and unlabeled data by discarding different percentages of 3D joint data from our dataset. <ref type="figure" target="#fig_1">Fig.  3</ref>, compares the median EPE of Var. 1 (which can only be trained supervised) with Var. 3 (trained semi-supervised). We see that as more unlabeled data becomes available, Var. 3 can make use of this additional information and improve prediction accuracy up to 22%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Generative capabilities</head><p>Our model is guided to learn a manifold of hand poses. In this section, we demonstrate the smoothness and consistency of it. To this end, we perform a walk on one dimension of the latent space by embedding two RGB images of separate hand poses into the latent space and obtain two corresponding samples z 1 and z 2 . We then decode the latent space samples that reside on the interpolation line between them using our models for RGB and 3D joint decoding. <ref type="figure" target="#fig_3">Fig. 6</ref> shows the resulting reconstructions, demonstrating consistency between both decoders. The fingers move  in synchrony and the generated synthetic samples are both physically plausible and consistent across modalities. This demonstrates that the learned latent space is indeed smooth and represents a valid statistical model of hand poses.</p><p>The smoothness property of the unified latent space is attractive in several regards. Foremost because this potentially enables generation of labeled data which in turn may be used to improve current models. Fully exploring this aspect is subject to further research.  <ref type="figure">Figure 7</ref>: 3D joint predictions. For each triplet, the left most column corresponds to the input image, the middle column is the ground truth 3D joint skeleton and the right column is our corresponding prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a new approach to estimate 3D hand pose configurations from RGB and depth images. Our approach is based on a re-derivation of the variational lower bound that admits training of several independent pairs of encoders and decoders, shaping a joint cross-modal latent space representation. We have experimentally shown that the proposed approach outperforms the state-of-the art on publicly available RGB datasets and is at least comparable to highly specialized state-of-the-art methods on depth data. Finally, we have shown the generative nature of the approach which suggests that we indeed learn a usable and physically plausible statistical hand model, enabling direct estimation of the 3D joint posterior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>This work was supported in parts by the ERC grant OPTINT <ref type="bibr">(StG-2016-717054)</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Cross-modal latent space. t-SNE visualization of 500 input samples of different modalities in the latent space. Embeddings of RGB images are shown in blue, embeddings of 3D joint configurations in green. Hand poses are decoded samples drawn from the latent space. Embedding does not cluster by modality, showing that there is a unified latent space. The posterior across different modalities can be estimated by sampling from this manifold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Median EPE of our model trained supervised and semi-supervised as a function of percentage of labeled data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: PCK curve of our best model on RHD and STB for RGB to 3D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Latent space walk. Example of reconstructing samples of the latent space into multiple modalities. The left-most and right-most figures are reconstruction from latent space samples of two real RGB images. The figures in-between are multi-modal reconstruction from interpolated latent space samples, hence are completely synthetic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>: Related work comparison. Mean EPE given in mm. For explanation of legends, see Sec. 4.5</figDesc><table>2D→3D 
RHD 

RGB→3D 
RHD 

RGB→3D 
STB 
[38] (T+S+H) 
22.43 
30.42 
8.68 
Ours (T+S+H) 
17.14 
19.73 
8.56 
Ours (T+S) 
18.90 
20.20 
10.16 
Ours (T+H) 
19.69 
22.34 
9.59 
Ours (T) 
21.15 
22.53 
9.49 
Table 2</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Generated images are legible but blurry. Creating high quality natural images is a research topic in itself.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross-modal scene networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Disco nets: Dissimilarity coefficients networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouchacourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Mudigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="352" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A collaborative filtering approach to real-time hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Hee</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2336" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cross-modal deep variational hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vision-based hand pose estimation: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nicolescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Twombly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="73" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust 3d hand pose estimation in single depth images: from single-view cnn to multi-view cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3593" to="3601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for efficient and robust hand pose estimation from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real time hand pose estimation using depth sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kıraç</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Kara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akarun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Consumer depth cameras for computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="119" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Real-time hand tracking under occlusion from an egocentric rgb-d sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02201</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepprior++: Improving fast and accurate 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.06807</idno>
		<title level="m">Hands deep in deep learning for hand pose estimation</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.06807</idno>
		<title level="m">Hands deep in deep learning for hand pose estimation</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient model-based 3d tracking of hand articulations using kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oikonomidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kyriazis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Argyros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BmVC</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Realtime and robust hand tracking from depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1106" to="1113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Postural hand synergies for tool use</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Santello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flanders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Soechting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Realtime hand tracking using synergistic inverse kinematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schröder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maycock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="5447" to="5454" />
		</imprint>
	</monogr>
	<note>Robotics and Automation (ICRA)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accurate, robust, and flexible real-time hand tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Leichter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</title>
		<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3633" to="3642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deephand: Robust hand pose estimation by completing a matrix imputed with deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4150" to="4158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Interactive markerless articulated hand motion tracking using rgb and depth data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oulasvirta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2456" to="2463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cascaded hand pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust articulated-icp for realtime hand tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum (Proc. Symposium on Geometry Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Latent regression forest: Structured estimation of 3d articulated hand posture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Opening the black box: Hierarchical sampling optimization for estimating human hand pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3325" to="3333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Real-time articulated hand pose estimation using semi-supervised transductive regression forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3224" to="3231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient and precise interactive hand tracking through joint, continuous optimization of pose and correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bordeaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Corish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Soto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Analysis of the synergies underlying complex hand manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEMBS&apos;04. 26th Annual International Conference of the IEEE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="4637" to="4640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Real-time continuous pose recovery of human hands using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Capturing hands in action using discriminative salient points and physics simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srikantha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Aponte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Crossing nets: Combining gans and vaes with a shared latent space for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hand pose estimation from local surface normals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="554" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spatial attention deep net with partial pso for hierarchical hybrid hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.07214</idno>
		<title level="m">3d hand pose tracking and estimation using stereo matching</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d hand pose from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
