<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recurrent Squeeze-and-Excitation Context Aggregation Net for Single Image Deraining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Recurrent Squeeze-and-Excitation Context Aggregation Net for Single Image Deraining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recurrent neural network</term>
					<term>squeeze and excitation block</term>
					<term>im- age deraining</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Rain streaks can severely degrade the visibility, which causes many current computer vision algorithms fail to work. So it is necessary to remove the rain from images. We propose a novel deep network architecture based on deep convolutional and recurrent neural networks for single image deraining. As contextual information is very important for rain removal, we first adopt the dilated convolutional neural network to acquire large receptive field. To better fit the rain removal task, we also modify the network. In heavy rain, rain streaks have various directions and shapes, which can be regarded as the accumulation of multiple rain streak layers. We assign different alpha-values to various rain streak layers according to the intensity and transparency by incorporating the squeeze-and-excitation block. Since rain streak layers overlap with each other, it is not easy to remove the rain in one stage. So we further decompose the rain removal into multiple stages. Recurrent neural network is incorporated to preserve the useful information in previous stages and benefit the rain removal in later stages. We conduct extensive experiments on both synthetic and real-world datasets. Our proposed method outperforms the state-of-the-art approaches under all evaluation metrics. Codes and supplementary material are available at our project webpage: https://xialipku.github.io/RESCAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>and tracking in autonomous driving, etc. Therefore, it is an important task to remove the rain and recover the background from rain images.</p><p>Image deraining has attracted much attention in the past decade. Many methods have been proposed to solve this problem. Existing methods can be divided into two categories, including video based approaches and single image based approaches. As video based methods can utilize the relationship between frames, it is relatively easy to remove rain from videos <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. However, single image deraining is more challenging, and we focus on this task in this paper. For single image deraining, traditional methods, such as discriminative sparse coding <ref type="bibr" target="#b4">[5]</ref>, low rank representation <ref type="bibr" target="#b5">[6]</ref>, and the Gaussian mixture model <ref type="bibr" target="#b6">[7]</ref>, have been applied to this task and they work quite well . Recently, deep learning based deraining methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> receive extensive attention due to its powerful ability of feature representation. All these related approaches achieve good performance, but there is still much space to improve.</p><p>There are mainly two limitations of existing approaches. On the one hand, according to <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>, spatial contextual information is very useful for deraining. However, many current methods remove rain streaks based on image patches, which neglect the contextual information in large regions. On the other hand, as rain steaks in heavy rain have various directions and shapes, they blur the scene in different ways. It is a common way <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref> to decompose the overall rain removal problem into multiple stages, so that we can remove rain streaks iteratively. Since these different stages work together to remove rain streaks, the information of deraining in previous stages is useful to guide and benefit the rain removal in later stages. However, existing methods treat these rain streak removal stages independently and do not consider their correlations.</p><p>Motivated by addressing the above two issues, we propose a novel deep network for single image deraining. The pipeline of our proposed network is shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. We remove rain streaks stage by stage. At each stage, we use the context aggregation network with multiple full-convolutional layers to remove rain streaks. As rain streaks have various directions and shapes, each channel in our network corresponds to one kind of rain streak. Squeeze-and-Excitation (SE) blocks are used to assign different alpha-values to various channels according to their interdependencies in each convolution layer. Benefited from the exponentially increasing convolution dilations, our network has a large reception field with low depth, which can help us to acquire more contextual information. To better utilize the useful information for rain removal in previous stages, we further incorporate the Recurrent Neural Network (RNN) architecture with three kinds of recurrent units to guide the deraining in later stages. We name the proposed deep network as REcurrent SE Context Aggregation Net (RESCAN).</p><p>Main contributions of this paper are listed as follows:</p><p>1. We propose a novel unified deep network for single image deraining, by which we remove the rain stage by stage. Specifically, at each stage, we use the contextual dilated network to remove the rain. SE blocks are used to assign different alpha-values to various rain streak layers according to their properties.</p><p>2. To the best of our knowledge, this is the first paper to consider the correlations between different stages of rain removal. By incorporating RNN architecture with three kinds of recurrent units, the useful information for rain removal in previous stages can be incorporated to guide the deraining in later stages. Our network is suitable for recovering rain images with complex rain streaks, especially in heavy rain. 3. Our deep network achieves superior performance compared with the stateof-the-art methods on various datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>During the past decade, many methods have been proposed to separate rain streaks and background scene from rain images. We briefly review these related methods as follows.</p><p>Video Based Methods As video based methods can leverage the temporal information by analyzing the difference between adjacent frames, it is relatively easy to remove the rain from videos <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3]</ref>. Garg and Nayar <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b1">2]</ref> propose an appearance model to describe rain streaks based on photometric properties and temporal dynamics. Meanwhile, Zhang et al. <ref type="bibr" target="#b0">[1]</ref> exploit temporal and chromatic properties of rain in videos. Bossu et al. <ref type="bibr" target="#b16">[17]</ref> detect the rain based on the histogram of orientation of rain streaks. In <ref type="bibr" target="#b3">[4]</ref>, Tripathi et al. provide a review of video-based deraining methods proposed in recent years.</p><p>Single Image Based Methods Compared with video deraining, single image deraining is much more challenging, since there is no temporal information in images. For this task, traditional methods, including dictionary learning <ref type="bibr" target="#b17">[18]</ref>, Gaussian mixture models (GMMs) <ref type="bibr" target="#b18">[19]</ref>, and low-rank representation <ref type="bibr" target="#b19">[20]</ref>, have been widely applied. Based on dictionary learning, Kang et al. <ref type="bibr" target="#b20">[21]</ref> decompose high frequency parts of rain images into rain and nonrain components. Wang et al. <ref type="bibr" target="#b21">[22]</ref> define a 3-layer hierarchical scheme. Luo et al. <ref type="bibr" target="#b4">[5]</ref> propose a discriminative sparse coding framework based on image patches. Gu et al. <ref type="bibr" target="#b22">[23]</ref> integrate analysis sparse representation (ASR) and synthesis sparse representation (SSR) to solve a variety of image decomposition problems. In <ref type="bibr" target="#b6">[7]</ref>, GMM works as a prior to decompose a rain image into background and rain streaks layer. Chang et al. <ref type="bibr" target="#b5">[6]</ref> leverage the low-rank property of rain streaks to separate two layers. Zhu et al. <ref type="bibr" target="#b23">[24]</ref> combine three different kinds of image priors. Recently, several deep learning based deraining methods achieve promising performance. Fu et al. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b7">8]</ref> first introduce deep learning methods to the deraining problem. Similar to <ref type="bibr" target="#b20">[21]</ref>, they also decompose rain images into low-and high-frequency parts, and then map high-frequency part to the rain streaks layer using a deep residual network. Yang et al. <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b8">9]</ref> design a deep recurrent dilated network to jointly detect and remove rain streaks. Zhang et al. <ref type="bibr" target="#b26">[27]</ref> use the generative adversarial network (GAN) to prevent the degeneration of background image when it is extracted from rain image, and utilize the perceptual loss to further ensure better visual quality. Li et al. <ref type="bibr" target="#b12">[13]</ref> design a novel multi-stage convolutional neural network that consists of several parallel sub-networks, each of which is made aware of different scales of rain streaks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Rain Models</head><p>It is a commonly used rain model to decompose the observed rain image O into the linear combination of the rain-free background scene B and the rain streak layer R:</p><formula xml:id="formula_0">O = B + R.<label>(1)</label></formula><p>By removing the rain streaks layer R from the observed image O, we can obtain the rain-free scene B.</p><p>Based on the rain model in Eq. (1), many rain removal algorithms assume that rain steaks should be sparse and have similar characters in falling directions and shapes. However, in reality, raindrops in the air have various appearances, and occur in different distances from the camera, which leads to an irregular distribution of rain streaks. In this case, a single rain streak layer R is not enough to well simulate this complex situation.</p><p>To reduce the complexity, we regard rain streaks with similar shape or depth as one layer. Then we can divide the captured rainy scene into the combination of several rain streak layers and an unpolluted background. Based on this, the rain model can be reformulated as follows:</p><formula xml:id="formula_1">O = B + n i=1 R i ,<label>(2)</label></formula><p>where R i represents the i-th rain streak layer that consists of one kind of rain streaks and n is the total number of different rain streak layers.</p><p>According to <ref type="bibr" target="#b27">[28]</ref>, things in reality might be even worse, especially in the heavy rain situation. Accumulation of multiple rain streaks in the air may cause attenuation and scattering, which further increases the diversity of rain streaks' brightness. For camera or eye visualization, the scattering causes haze or frog effects. This further pollutes the observed image O. For camera imaging, due to the limitation of pixel number, far away rain streaks cannot occupy full pixels. When mixed with other things, the image will be blurry. To handle the issues above, we further take the global atmospheric light into consideration and assign different alpha-values to various rain streak layers according to their intensities transparencies. We further generalize the rain model to:</p><formula xml:id="formula_2">O = 1 − n i=0 α i B + α 0 A + n i=1 α i R i , s.t. α i ≥ 0, n i=0 α i ≤ 1,<label>(3)</label></formula><p>where A is the global atmospheric light, α 0 is the scene transmission, α i (i = 1, · · · , n) indicates the brightness of a rain streak layer or a haze layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Deraining Method</head><p>Instead of using decomposition methods with artificial priors to solve the problem in Eq. <ref type="formula" target="#formula_2">(3)</ref>, we intend to learn a function f that directly maps observed rain image O to rain streak layer R, since R is sparser than B and has simpler texture. Then we can subtract R from O to get the rain-free scene B. The function f above can be represented as a deep neural network and be learned by optimizing the loss function</p><formula xml:id="formula_3">f (O) − R 2</formula><p>F . Based on the motivation above, we propose the REcurrent SE Context Aggregation Net (RESCAN) for image deraining. The framework of our network is presented in <ref type="figure" target="#fig_2">Fig. 2</ref>. We remove rain streaks stage by stage. At each stage, we use the context aggregation network with SE blocks to remove rain streaks. Our network can deal with rain streaks of various directions and shapes, and each feature map in the network corresponds to one kind of rain streak. Dilated convolution used in our network can help us have a large reception field and acquire more contextual information. By using SE blocks, we can assign different alpha-values to various feature maps according to their interdependencies in each convolution layer. As we remove the rain in multiple stages, useful information for rain removal in previous stages can guide the learning in later stages. So we incorporate the RNN architecture with memory unit to make full use of the useful information in previous stages.</p><p>In the following, we first describe the baseline model, and then define the recurrent structure, which lifts the model's capacity by iteratively decomposing rain streaks with different characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SE Context Aggregation Net</head><p>The base model of RESCAN is a forward network without recurrence. We implement it by extending Context Aggregation Net (CAN) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> with Squeeze-andExcitation (SE) blocks <ref type="bibr" target="#b28">[29]</ref>, and name it as SE Context Aggregation Net (SCAN). Here we provide an illustration and a further specialization of SCAN. We schematically illustrate SCAN in <ref type="figure" target="#fig_0">Fig. 1</ref>, which is a full-convolution network. In <ref type="figure" target="#fig_0">Fig. 1</ref>, we set the depth d = 6. Since a large receptive field is very helpful to acquire much contextual information, dilation is adopted in our network. For layers L 1 to L 3 , the dilation increases from 1 to 4 exponentially, which leads to the exponential growth in receptive field of every elements. As we treat the first layer as an encoder to transform an image to feature maps, and the last two layers as a decoder to map reversely, we do not apply dilation for layers L 0 , L 4 and L 5 . Moreover, we use 3 × 3 convolution for all layers before L 5 . To recover RGB channels of a color image, or gray channel for a gray scale image, we adopt the 1×1 convolution for the last layer L 5 . Every convolution operation except the last one is followed by a nonlinear operation. The detailed architecture of SCAN is summarized in <ref type="table" target="#tab_0">Table 1</ref>. Generally, for a SCAN with depth d, the receptive field of elements in the output image equals to 2 d−2 + 3 2 .</p><p>For feature maps, we regard each channel of them as the embedding of a rain streak layer R i . Recall in Eq. (3), we assign different alpha-values α i to different rain steak layers R i . Instead of setting fixed alpha-values α i for each rain layer, we update the alpha-values for embeddings of rain streak layers in each network layer. Although the convolution operation implicitly introduces weights for every channel, these implicit weights are not specialized for every image. To explicitly import weight on each network layer for each image, we extend each basic convolution layer with Squeeze-and-Excitation (SE) block <ref type="bibr" target="#b28">[29]</ref>, which computes normalized alpha-value for every channel of each item. By multiplying alpha-values learned by SE block, feature maps computed by convolution are re-weighted explicitly.</p><p>An obvious difference between SCAN and former models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref> is that SCAN has no batch normalization (BN) <ref type="bibr" target="#b29">[30]</ref> layers. BN is widely used in training deep neural network, as it can reduce internal covariate shift of feature maps. By applying BN, each scalar feature is normalized and has zero mean and unit variance. These features are independent with each other and have the same distribution. However, in Eq. (2) rain streaks in different layers have different distributions in directions, colors and shapes, which is also true for each scalar feature of different rain streak layers. Therefore, BN contradicts with the characteristics of our proposed rain model. Thus, we remove BN from our model. Experimental results in Section 5 show that this simple modification can substantially improve the performance. Furthermore, since BN layers keep a normalized copy of feature maps in GPU, removing it can greatly reduce the demand on GPU memory. For SCAN, we can save approximately 40% of memory usage during training without BN. Consequently, we can build a larger model with larger capacity, or increase the mini-batch size to stabilize the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Recurrent SE Context Aggregation Net</head><p>As there are many different rain streak layers and they overlap with each other, it is not easy to remove all rain streaks in one stage. So we incorporate the recurrent structure to decompose the rain removal into multiple stages. This process can be formulated as:</p><formula xml:id="formula_4">O 1 = O,<label>(4)</label></formula><formula xml:id="formula_5">R s = f CN N (O s ) , 1 ≤ s ≤ S,<label>(5)</label></formula><formula xml:id="formula_6">O s+1 = O s − R s , 1 ≤ s ≤ S,<label>(6)</label></formula><formula xml:id="formula_7">R = S s=1 R s ,<label>(7)</label></formula><p>where S is the number of stages, R s is the output of the s-th stage, and O s+1 is the intermediate rain-free image after the s-th stage. The above model for deraining has been used in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>. However, recurrent structure used in their methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref> can only be regarded as the simple cascade of the same network. They just use the output images of last stage as the input of current stage and do not consider feature connection among these stages. As these different stages work together to remove the rain, input images of different stages {O 1 , O 2 , · · · , O S } can be regarded as a temporal sequence of rain images with decreasing levels of rain streaks interference. It is more meaningful to investigate the recurrent connections between features of different stages rather than only using the recurrent structure. So we incorporate recurrent neural network (RNN) <ref type="bibr" target="#b30">[31]</ref> with memory unit to better make use of information in previous stages and guide feature learning in later stages.</p><p>In our framework, Eq. (5) can be further reformulated as: For the deraining task, we further explore three different recurrent unit variants, including ConvRNN, ConvGRU <ref type="bibr" target="#b31">[32]</ref>, and ConvLSTM <ref type="bibr" target="#b32">[33]</ref>. Due to the space limitation, we only present the details of ConvGRU <ref type="bibr" target="#b31">[32]</ref> in the following. For other two kinds of recurrent units, please refer to the supplement materials on our webpage. </p><formula xml:id="formula_8">R s = n i=1 α iR i s = f CN N +RN N (O s , x s−1 ) , 1 ≤ s ≤ S,<label>(8)</label></formula><formula xml:id="formula_9">z j s = σ W j z ⊛ x j−1 s + U j z ⊛ x j s−1 + b j z ,<label>(9)</label></formula><formula xml:id="formula_10">r j s = σ W j r ⊛ x j−1 s + U j r ⊛ x j s−1 + b j r ,<label>(10)</label></formula><formula xml:id="formula_11">n j s = tanh W j n ⊛ x j−1 s + U j n ⊛ r j s ⊙ x j s−1 + b j n ,<label>(11)</label></formula><formula xml:id="formula_12">x j s = 1 − z j s ⊙ x j s−1 + z j s ⊙ n j s ,<label>(12)</label></formula><p>where σ is the sigmoid function σ (x) = 1/ (1 + exp (−x)) and ⊙ denotes elementwise multiplication. W is the dilated convolutional kernel, and U is an ordinary kernel with a size 3 × 3 or 1 × 1. Compared with a convolutional unit, ConvRNN, ConvGRU and ConvLSTM units will have twice, three times and four times parameters, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Recurrent Frameworks</head><p>We further examine two frameworks to infer the final output. Both of them use O s and previous states as input for the s-th stage, but they output different images. In the following, we describe the additive prediction and the full prediction in detail, together with their corresponding loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additive Prediction</head><p>The additive prediction is widely used in image processing. In each stage, the network only predicts the residual between previous predictions and the ground truth. It incorporates previous feature maps and predictions as inputs, which can be formulated as:</p><formula xml:id="formula_13">R s = f (O s , x s−1 ) ,<label>(13)</label></formula><formula xml:id="formula_14">O s+1 = O − s j=1 R j = O s − R s ,<label>(14)</label></formula><p>where x s−1 represents previous states as in Eq. <ref type="bibr" target="#b11">(12)</ref>. For this framework, the loss functions are chosen as follows:</p><formula xml:id="formula_15">L (Θ) = S s=1 s j=1 R j − R 2 F ,<label>(15)</label></formula><p>where Θ represents the network's parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full Prediction</head><p>The full prediction means that in each stage, we predict the whole rain streaks R. This approach can be formulated as:</p><formula xml:id="formula_16">R s = f Ô s , x s−1 ,<label>(16)</label></formula><formula xml:id="formula_17">O s+1 = O −R s ,<label>(17)</label></formula><p>whereR s represents the predicted full rain streaks in the s-th stage, andÔ s+1 equals to B plus remaining rain streaks. The corresponding loss function is:</p><formula xml:id="formula_18">L (Θ) = S s=1 R s − R 2 F ,<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we present details of experimental settings and quality measures used to evaluate the proposed SCAN and RESCAN models. We compare the performance of our proposed methods with the state-of-the-art methods on both synthetic and real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Settings</head><p>Synthetic Dataset Since it is hard to obtain a large dataset of clean/rainy image pairs from real-world data, we first use synthesized rainy datasets to train the network. Zhang et al. <ref type="bibr" target="#b26">[27]</ref> synthesize 800 rain images (Rain800) from randomly selected outdoor images, and split them into testing set of 100 images and training set of 700 images. Yang et al. <ref type="bibr" target="#b8">[9]</ref> collect and synthesize 3 datasets, Rain12, Rain100L and Rain100H. We select the most difficult one, Rain100H, to test our model. It is synthesized with the combination of five streak directions, which makes it hard to effectively remove all rain streaks. There are 1, 800 synthetic image pairs in Rain100H, and 100 pairs are selected as the testing set.</p><p>Real-world Dataset Zhang et al. <ref type="bibr" target="#b26">[27]</ref> and Yang et al. <ref type="bibr" target="#b8">[9]</ref> also provide many real-world rain images. These images are diverse in terms of content as well as intensity and orientation of rain streaks. We use these datasets for objective evaluation.</p><p>Training Settings In the training process, we randomly generate 100 patch pairs with a size of 64 × 64 from every training image pairs. The entire network is trained on an Nvidia 1080Ti GPU based on Pytorch. We use a batch size of 64 and set the depth of SCAN as d = 7 with the receptive field size 35 × 35. For the nonlinear operation, we use leaky ReLU <ref type="bibr" target="#b33">[34]</ref> with α = 0.2. For optimization, the ADAM algorithm <ref type="bibr" target="#b34">[35]</ref> is adopted with a start learning rate 5 × 10 −3 . During training, the learning rate is divided by 10 at 15, 000 and 17, 500 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quality Measures</head><p>To evaluate the performance on synthetic image pairs, we adopt two commonly used metrics, including peak signal to noise ratio (PSNR) <ref type="bibr" target="#b35">[36]</ref> and structure similarity index (SSIM) <ref type="bibr" target="#b36">[37]</ref>. Since there are no ground truth rainfree images for real-world images, the performance on the real-world dataset can only be evaluated visually. We compare our proposed approach with five state-ofthe-art methods, including image decomposition (ID) <ref type="bibr" target="#b20">[21]</ref>, discriminative sparse coding (DSC) <ref type="bibr" target="#b4">[5]</ref>, layer priors (LP) <ref type="bibr" target="#b6">[7]</ref>, DetailsNet <ref type="bibr" target="#b7">[8]</ref>, and joint rain detection and removal (JORDER) <ref type="bibr" target="#b8">[9]</ref>. <ref type="table" target="#tab_2">Table 2</ref> shows results of different methods on the Rain800 and the Rain100H datasets. We can see that our RESCAN considerably outperforms other methods in terms of both PSNR and SSIM on these two datasets. It is also worth noting that our non-recurrent network SCAN can even outperform JORDER and DetailsNet, and is slightly superior to JORDER-R, the recurrent version of JORDER. This shows the high capacity behind SCAN's shallow structure. Moreover, by using RNN to gradually recover the full rain streak layer R, RESCAN further improves the performance.   To visually demonstrate the improvements obtained by the proposed methods, we present results on several difficult sample images in <ref type="figure" target="#fig_4">Fig. 3</ref>. Please note that we select difficult sample images to show that our method can outperform others especially in difficult conditions, as we design it to deal with complex conditions. According to <ref type="figure" target="#fig_4">Fig. 3</ref>, these state-of-the-art methods cannot remove all rain steaks and may blur the image, while our method can remove the majority of rain steaks as well as maintain details of background scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results on Synthetic Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on Real-world Dataset</head><p>To test the practicability of deraining methods, we also evaluate the performance on real-world rainy test images. The predictions for all related methods on four real-world sample rain images are shown in <ref type="figure" target="#fig_5">Fig. 4</ref>. As observed, LP <ref type="bibr" target="#b6">[7]</ref> cannot remove rain steaks efficiently, and DetailsNet <ref type="bibr" target="#b7">[8]</ref> tends to add artifacts on derained outputs. We can see that the proposed method can remove most of rain streaks and maintain much texture of background images. To further validate  the performance on real-world dataset, we also make a user study. For details, please refer to the supplement material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis on SCAN</head><p>To show that SCAN is the best choice as a base model for deraining task, we conduct experiments on two datasets to compare the performance of SCAN and its related network architectures, including Plain (dilation=1 for all convolutions), ResNet used in DetailsNet <ref type="bibr" target="#b7">[8]</ref> and Encoder-Decoder used in ID-CGAN <ref type="bibr" target="#b26">[27]</ref>. For all networks, we set the same depth d = 7 and width (24 channels), so we can keep their numbers of parameters and computations at the same order of magnitude. More specifically, we keep layers L 0 , L d−5 and L d−6 of them with the same structure, so they only differ in layers L 1 to L 4 . The results are shown in <ref type="table" target="#tab_3">Table 3</ref>. SCAN and CAN achieve the best performance in both datasets, which can be attributed to the fact that receptive fields of these two methods exponentially increase with the growing of depth, while receptive fields of other methods only have linear relationship with depth.</p><p>Moreover, the comparison between results of SCAN and CAN indicates that the SE block contributes a lot to the base model, as it can explicitly learn alphavalue for every independent rain streak layer. We also examine the SCAN model  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Analysis on RESCAN</head><p>As we list three recurrent units and two recurrent frameworks in Section 4.2, we conduct experiments to compare these different settings, consisting of {ConvRNN, ConvLSTM, ConvGRU} × {Additive Prediction (Add), Full Prediction (Full)}.</p><p>To compare with the recurrent framework in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>, we also implement the settings in Eq. (5) (Iter), in which previous states are not reserved. In <ref type="table" target="#tab_5">Table 4</ref>, we report the results. It is obvious that Iter cannot compete with all RNN structures, and is not better than SCAN, as it leaves out information of previous stages. Moreover, ConvGRU and ConvLSTM outperform ConvRNN, as they maintain more parameters and require more computation. However, it is difficult to pick the best unit between ConvLSTM and ConvGRU as they perform similarly in experiments. For recurrent frameworks, results indicate that Full Prediction is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We propose the RESCAN for single image deraining in this paper. We divide the rain removal into multiple stages. In each stage, the context aggregation network is adopted to remove rain streaks. We also modify CAN to better match the rain removal task, including the exponentially increasing dilation and removal of the BN layer. To better characterize intensities of different rain streak layers, we adopt the squeeze-and-excitation block to assign different alpha-values according to their properties. Moreover, RNN is incorporated to better utilize the useful information for rain removal in previous stages and guide the learning in later stages. We also test the performance of different network architectures and recurrent units. Experiments on both synthetic and real-world datasets show that RESCAN outperforms the state-of-the-art methods under all evaluation metrics. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: The architecture of SE Context Aggregation Network (SCAN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>the decomposed i-th rain streak layer of the s-th stage and x s−1 is the hidden state of the (s − 1)-th stage. Consistent with the rain model in Eq. (3), R s is computed by summingR i s with different alpha-values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The unfolded architecture of RESCAN. K is the convolution kernel size, DF indicates the dilation factor, and S represents the number of stages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>O</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Results of various methods on synthetic images. Best viewed at screen!</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Results of various methods on real-world images.Best viewed at screen!</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Acknowledgment. Zhouchen Lin is supported by National Basic Research Program of China (973 Program) (Grant no. 2015CB352502), National Natural Science Foundation (NSF) of China (Grant nos. 61625301 and 61731018), Qualcomm, and Microsoft Re- search Asia. Hong Liu is supported by National Natural Science Foundation of China (Grant nos. U1613209 and 61673030). Hongbin Zha is supported by Beijing Municipal Natural Science Foundation (Grant no. 4152006).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>The detailed architecture of SCAN. d is the depth of network.</figDesc><table>Layer 
0 
1 
2 
... 
d − 3 
d − 2 
d − 1 

Convolution 3 × 3 3 × 3 3 × 3 ... 
3 × 3 
3 × 3 
1 × 1 
Dilation 
1 
1 
2 
... 
2 

d−4 

1 
1 
NonLinear 
Yes Yes Yes Yes 
Yes 
Yes 
No 
SE block 
Yes Yes Yes Yes 
Yes 
Yes 
No 
Receptive field 3 × 3 5 × 5 9 × 9 ... 2 
d−2 + 1 
2 2 
d−2 + 3 
2 2 
d−2 + 3 

2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Quantitative experiments evaluated on two synthetic datasets. Best results are marked in bold and the second best results are underlined.</figDesc><table>Dataset 
Rain800 
Rain100H 

Meassure 
PSNR SSIM PSNR SSIM 

ID [21] 
18.88 0.5832 14.02 0.5239 
DSC [5] 
18.56 0.5996 15.66 0.4225 
LP [7] 
20.46 0.7297 14.26 0.5444 
DetailsNet [8] 21.16 0.7320 22.26 0.6928 
JORDER [9] 22.24 0.7763 22.15 0.6736 
JORDER-R [9] 22.29 0.7922 23.45 0.7490 

SCAN 
23.45 0.8112 23.56 0.7456 
RESCAN 
24.09 0.8410 26.45 0.8458 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparison between SCAN and base model candidates.SSIM 0.7816 0.7856 0.7809 0.7871 0.7960 0.7657 0.8112SSIM 0.6921 0.6940 0.6886 0.7256 0.7333 0.7389 0.7456</figDesc><table>Dataset Meassure Plain ResNet EncDec CAN+BN CAN SCAN+BN SCAN 

Rain800 
PSNR 22.10 22.10 22.14 
22.27 
22.45 
23.11 
23.45 
Rain100H 
PSNR 21.46 21.51 21.28 
22.63 
22.93 
23.09 
23.56 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Quantitative comparison between different settings of RESCAN. Iter represents the framework that leaves out states of previous stages as [9,13]. Add and Full represent the addtive prediction framework and the full prediction framework, respectively.ConvRNN+Add 24.09 0.8410 23.34 0.7765 ConvRNN+Full 23.52 0.8269 23.44 0.7643 ConvGRU+Add 23.31 0.8444 24.00 0.7993 ConvGRU+Full 24.18 0.8394 26.45 0.8458 ConvLSTM+Add 22.93 0.8385 25.13 0.8211 ConvLSTM+Full 24.37 0.8384 25.64 0.8334</figDesc><table>Dataset 
Rain800 
Rain100H 

Meassure 
PSNR SSIM PSNR SSIM 

SCAN 
23.45 0.8112 23.56 0.7456 
Iter+Add 
23.36 0.8169 22.81 0.7630 
</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rain removal in video by combining temporal and chromatic properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Leow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICME</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="461" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vision and rain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="27" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Utilizing local phase information to remove rain from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Santhaseelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Asari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="89" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<title level="m">Removal of rain from videos: a review. Signal, Image and Video Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1421" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Removing rain from a single image via discriminative sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3397" to="3405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transformed low-rank model for line pattern noise removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1726" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rain streak removal using layer priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2736" to="2744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Removing rain from single images via a deep detail network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1715" to="1723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep joint rain detection and removal from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1357" to="1366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Context-aware single image rain removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICME</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="164" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast image processing with fully-convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2516" to="2525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Single image deraining using scale-aware multistage recurrent network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.06830</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Analysis of rain and snow in frequency space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Barnum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page">256</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detection and removal of rain from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="528" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">When does a camera see rain?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1067" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rain or snow detection in image sequences through use of a histogram of orientation of streaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bossu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hautière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Tarel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="348" to="367" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Online dictionary learning for sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Speaker verification using adapted gaussian mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="19" to="41" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust recovery of subspace structures by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic single-image-based rain streaks removal via image decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1742" to="1755" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A hierarchical approach for rain or snow removing in a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3936" to="3950" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint convolutional analysis and synthesis sparse representation for single image layer separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1717" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint bi-layer optimization for single-image rain streak removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2526" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Clearing the skies: A deep network architecture for single-image rain removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2944" to="2956" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Joint rain detection and removal via iterative region dependent multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1609.07769</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05957</idno>
		<title level="m">Image de-raining using a conditional generative adversarial network</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Free-space optical channel models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaushal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="41" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<title level="m">Squeeze-and-excitation networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<editor>ICML.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Recurrent neural networks for prediction: learning algorithms, architectures and stability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Mandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Chambers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<title level="m">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scope of validity of psnr in image/video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huynh-Thu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghanbari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Electronics letters</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="800" to="801" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
