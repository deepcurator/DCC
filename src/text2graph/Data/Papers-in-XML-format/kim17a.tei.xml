<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Discover Cross-Domain Relations with Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeksoo</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moonsu</forename><surname>Cha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsoo</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
						</author>
						<title level="a" type="main">Learning to Discover Cross-Domain Relations with Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations when given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Relations between two different domains, the way in which concepts, objects, or people are connected, arise ubiquitously. Cross-domain relations are often natural to humans. For example, we recognize the relationship between an English sentence and its translated sentence in French. We also choose a suit jacket with pants or shoes in the same style to wear.</p><p>Can machines also achieve a similar ability to relate two different image domains? This question can be reformulated as a conditional image generation problem. In other words, finding a mapping function from one domain to the other can be thought as generating an image in one domain given another image in the other domain. While this problem tackled by generative adversarial networks (GAN) <ref type="bibr" target="#b9">(Isola et al., 2016)</ref> has gained a huge attention recently, most of today's training approaches use explicitly paired data, provided by human or another algorithm.</p><p>This problem also brings an interesting challenge from a 1 SK T-Brain, Seoul, South Korea. Correspondence to: Taeksoo Kim &lt;jazzsaxmafia@sktbrain.com&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the 34</head><p>th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s). Our GAN-based model trains with two independently collected sets of images and learns how to map two domains without any extra label. In this paper, we reduce this problem into generating a new image of one domain given an image from the other domain. (a) shows a high-level overview of the training procedure of our model with two independent sets (e.g. handbag images and shoe images). (b) and (c) show results of our method. Our method takes a handbag (or shoe) image as an input, and generates its corresponding shoe (or handbag) image. Again, it is worth noting that our method does not take any extra annotated supervision and can self-discover relations between domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disco GAN</head><p>learning point of view. Explicitly supervised data is seldom available and labeling can be labor intensive. Moreover, pairing images can become tricky if corresponding images are missing in one domain or there are multiple best candidates. Hence, we push one step further by discovering relations between two visual domains without any explic-  <ref type="bibr" target="#b6">(Goodfellow et al., 2014)</ref>, (b) GAN with a reconstruction loss, (c) our proposed model (DiscoGAN) designed to discover relations between two unpaired, unlabeled datasets. Details are described in Section 3.</p><p>itly paired data.</p><p>In order to tackle this challenge, we introduce a model that discovers cross-domain relations with GANs (DiscoGAN). Unlike previous methods, our model can be trained with two sets of images without any explicit pair labels ( <ref type="figure" target="#fig_0">Figure  1a</ref>) and does not require any pre-training. Our proposed model can then take an image in one domain and generate its corresponding image in another domain (see <ref type="figure" target="#fig_0">Figure 1b)</ref>. The core of our model is based on two different GANs coupled together -each of them ensures our generative functions can map each domain to its counterpart domain. A key intuition we rely on is to constrain all images in one domain to be representable by images in the other domain. For example, when learning to generate shoe image from handbag image, we force this generated image to be an image-based representation of the handbag image (and hence reconstruct the handbag image) through a reconstruction loss, and to be as close to images in the shoe domain as possible through a GAN loss. We use these two properties to encourage the mapping between two domains to be well covered on both directions (i.e. encouraging oneto-one rather than many-to-one or one-to-many). In the experiments section, we show that this simple intuition discovers common properties and styles of two domains very well.</p><p>Both experiments on toy domain and real world image datasets support the claim that our proposed model is wellsuited for discovering cross-domain relations. When translating data points between simple 2-dimensional domains and between face image domains, our DiscoGAN model was more robust to the mode collapse problem compared to two other baseline models of Figures 2a and 2b. It also learns the bidirectional mapping between two image domains, such as faces, cars, chairs, edges and photos, and successfully apply the mapping in image translation. Translated images consistently change specific attributes such as hair color, gender and orientation while maintaining all other components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model</head><p>We now formally define cross-domain relations and present the problem of learning to discover such relations in two different domains. Standard GAN model and a similar variant model with additional components are investigated for their applicability for this task. Limitations of these models are then explained, and we propose a new architecture based on GANs that can be used to discover cross-domain relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Formulation</head><p>Relation is mathematically defined as a function G AB that maps elements from domain A to elements in its codomain B and vice versa for G BA . In fully unsupervised setting, G AB and G BA can be arbitrarily defined. To find a meaningful relation, we need to impose a condition on the relation of interest. Here, we constrain relation to be a one-toone correspondence (bijective mapping). That means G AB is the inverse mapping of G BA .</p><p>The range of function G AB -the complete set of all possible resulting values G AB (x A ) for all x A 's in domain A -should be contained in domain B and similarly for G BA (x B ).</p><p>We now relate these constraints to objective functions. Ideally, the equality G BA • G AB (x A ) = x A should be satisfied, but this hard constraint is difficult to optimize and a relaxed soft constraint is more desirable. For this reason, we minimize the distance</p><formula xml:id="formula_0">d(G BA • G AB (x A ), x A ),</formula><p>where any form of metric function such as L 1 , L 2 or Huber loss can be used. Similarly, we also need to minimize</p><formula xml:id="formula_1">d(G AB • G BA (x B ), x B ).</formula><p>Guaranteeing that G AB maps to domain B is also very difficult to optimize. We relax this constraint as follows: we instead minimize generative adversarial loss</p><formula xml:id="formula_2">−E x A ∼P A [log D B (G AB (x A ))]. Similarly, we minimize −E x B ∼P B [log D A (G BA (x B ))].</formula><p>Now, we explore several GAN architectures to learn with these loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Notation and Architecture</head><p>We use the following notations in sections below. A generator network is denoted as G AB : R Each generator takes an image of size h×w ×c and feeds it through an encoder-decoder pair. The encoder part of each generator is composed of convolution layers, each followed by leaky ReLU <ref type="bibr" target="#b13">(Maas et al., 2013;</ref><ref type="bibr" target="#b24">Xu et al., 2015)</ref>. The decoder part is composed of deconvolution layers, followed by a ReLU, and outputs a target domain image of size h × w ×c. The number of convolution and deconvolution layers ranges from four to five depending on the domain.</p><p>The discriminator is similar to the encoder part of the generator. In addition to the convolution layers and leaky ReLUs, the discriminator has an additional convolution layer, and a final sigmoid to output a scalar output between [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">GAN with a Reconstruction Loss</head><p>We first consider a standard GAN model <ref type="bibr" target="#b6">(Goodfellow et al., 2014</ref>) for the relation discovery task (see <ref type="figure" target="#fig_1">Figure 2a</ref>). Originally, a standard GAN takes random Gaussian noise z, encodes it into hidden features h and generates images in domains such as MNIST. We make a slight modification to this model to fit our task: the model we use takes in image as input instead of noise.</p><p>In addition, since this architecture only learns one mapping from domain A to domain B, we add a second generator that maps domain B back into domain A (see <ref type="figure" target="#fig_1">Figure 2b)</ref>. We also add a reconstruction loss term that compares the input image with the reconstructed image. With these additional changes, each generator in the model can learn mapping from its input domain to output domain and discover relations between them.</p><p>A generator G AB translates input image x A from domain A into x AB in domain B. The generated image is then translated into a domain A image x ABA to match the original input image (Equation 1, 2). Various forms of distance functions, such as MSE, cosine distance, and hinge-loss, can be used as the reconstruction loss d <ref type="figure" target="#fig_2">(Equation 3</ref>). The translated output x AB is then scored by the discriminator which compares it to a real domain B sample x B .</p><formula xml:id="formula_3">x AB = G AB (x A ) (1) x ABA = G BA (x AB ) = G BA • G AB (x A ) (2) L CON ST A = d(G BA • G AB (x A ), x A )<label>(3)</label></formula><formula xml:id="formula_4">L GAN B = −E x A ∼P A [log D B (G AB (x A ))]<label>(4)</label></formula><p>The generator G AB receives two types of losses -a reconstruction loss L CON ST A (Equation 3) that measures how well the original input is reconstructed after a sequence of two generations, and a standard GAN generator loss L GAN B (Equation 4) that measures how realistic the generated image is in domain B. The discriminator receives the standard GAN discriminator loss of Equation <ref type="formula" target="#formula_5">6</ref>.</p><formula xml:id="formula_5">L G AB = L GAN B + L CON ST A (5) L D B = − E x B ∼P B [log D B (x B )] − E x A ∼P A [log(1 − D B (G AB (x A )))]<label>(6)</label></formula><p>During training, the generator G AB learns the mapping from domain A to domain B under two relaxed constraints: that domain A maps to domain B, and that the mapping on domain B is reconstructed to domain A. However, this model lacks a constraint on mapping from B to A, and these two conditions alone does not guarantee a cross-domain relation (as defined in section 2.1) because the mapping satisfying these constraints is one-directional. In other words, the mapping is an injection, not bijection, and one-to-one correspondence is not guaranteed.</p><p>Consider two possibly multi-modal image domains A and B. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates the two multi-modal data domains on a simplified one-dimensional representation. <ref type="figure" target="#fig_2">Figure 3a</ref> shows the ideal mapping from input domain A to domain B, where each mode of data is mapped to a separate mode in the target domain. <ref type="figure" target="#fig_2">Figure 3b</ref>, in contrast, shows the mode collapse problem, a prevalent phenomenon in GANs, where data from multiple modes of a domain map to a single mode of a different domain. For instance, this case is where the mapping G AB maps images of cars in two different orientations into the same mode of face images.</p><p>In some sense, the addition of a reconstruction loss to a standard GAN is an attempt to remedy the mode collapse problem. In <ref type="figure" target="#fig_2">Figure 3c</ref>, two domain A modes are matched with the same domain B mode, but the domain B mode can only direct to one of the two domain A modes. Although the additional reconstruction loss L CON ST A forces the reconstructed sample to match the original <ref type="figure" target="#fig_2">(Figure 3c</ref>), this change only leads to a similar symmetric problem. The reconstruction loss leads to an oscillation between the two states and does not resolve mode-collapsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Our Proposed Model: Discovery GAN</head><p>Our proposed GAN model for relation discovery -Disco-GAN -couples the previously proposed model <ref type="figure" target="#fig_1">(Figure 2c)</ref>. Each of the two coupled models learns the mapping from one domain to another, and also the reverse mapping for reconstruction. The two models are trained together simultaneously. The two generators G AB 's and the two generators G BA 's share parameters, and the generated images x BA and x AB are each fed into separate discriminators L D A and L D B , respectively.</p><p>One key difference from the previous model is that input images from both domains are reconstructed and that there are two reconstruction losses:</p><formula xml:id="formula_6">L CON ST A and L CON ST B . L G = L G AB + L G BA (7) = L GAN B + L CON ST A + L GAN A + L CON ST B L D = L D A + L D B<label>(8)</label></formula><p>As a result of coupling two models, the total generator loss is the sum of GAN loss and reconstruction loss for each partial model <ref type="formula">(Equation 7</ref>). Similarly, the total discriminator loss L D is a sum of discriminator loss for the two discriminators D A and D B , which discriminate real and fake images of domain A and domain B <ref type="figure">(Equation 8</ref>). Now, this model is constrained by two L GAN losses and two L CON ST losses. Therefore a bijective mapping is achieved, and a one-to-one correspondence, which we defined as cross-domain relation, can be discovered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Toy Experiment</head><p>To empirically demonstrate our explanations on the differences between a standard GAN, a GAN with reconstruction loss and our proposed model (DiscoGAN), we designed an illustrative experiment based on synthetic data in 2-dimensional domains A and B. Both source (domain A) and target (domain B) data samples are drawn from Gaussian mixture models.</p><p>In <ref type="figure" target="#fig_4">Figure 4</ref>, the left-most figure shows the initial state of the toy experiment where all the domain A modes map to almost a single point because of initialization of the generator. For all plots the target domain 2D plane is shown with target domain modes marked with black x's. Colored points represent samples from domain A that are mapped to domain B, and each color denotes samples from each domain A mode. In this case, the task is to discover crossdomain relations between domains A and B, and translate samples from five domain A modes into domain B, which has ten modes spread around the arc of a circle.</p><p>We use a neural network with three linear layers each followed by a ReLU nonlinearity as the generator. For the discriminator we use five linear layers each followed by a ReLU, except for the last layer which is switched out with a sigmoid that outputs a scalar ∈ [0, 1]. The colored background shows the output value of the discriminator D B , which discriminates real target domain samples from synthetic translated samples from domain A. The contour lines show regions of same discriminator value.</p><p>50,000 iterations of training were performed, and due to the In the case of GAN with a reconstruction loss, the collapsing problem is less prevalent, but navy, green and light-blue points still overlap at a few modes. The contour plot also demonstrates the difference from baseline: regions around all domain B modes are leveled in a green colored plateau in the baseline, allowing translated samples to freely move between modes, whereas in the case with reconstruction loss the regions between domain B modes are clearly separated.</p><p>In addition, both this model and the standard GAN model fail to cover all modes in domain B since the mapping from domain A to domain B is injective. Our proposed Disco-GAN model, on the other hand, is able to not only prevent mode-collapse by translating into distinct well-bounded regions that do not overlap, but also generate domain B samples in all ten modes as the mappings in our model is bijective. It is noticeable that the discriminator for domain B is perfectly fooled by translated samples from domain A (white regions near B domain modes).</p><p>Although this experiment is limited due to its simplicity, the results clearly support the superiority of our proposed model over other variants of GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Real Domain Experiment</head><p>To evaluate whether our DiscoGAN successfully learns underlying relationships between domains, we trained and tested our model using several image-to-image translation tasks that require the use of discovered cross-domain relations between source and target domains.</p><p>In each real domain experiment, all input images and translated images were size of 64 × 64 × 3. For training, we employed learning rate of 0.0002 and used the Adam optimizer <ref type="bibr" target="#b10">(Kingma &amp; Ba, 2015)</ref> with β 1 = 0.5 and β 2 = 0.999. We applied Batch Normalization <ref type="bibr" target="#b8">(Ioffe &amp; Szegedy, 2015)</ref> to all convolution and deconvolution layers except the first and the last layers, and applied weight decay regularization coefficient of 10 −4 and minibatch of size 200. All computations were conducted on a single machine with an Nvidia Titan X Pascal GPU and an Intel(R) Xeon(R) E5-1620 CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">CAR TO CAR, FACE TO FACE</head><p>We used a Car dataset <ref type="bibr" target="#b5">(Fidler et al., 2012)</ref> which consists of rendered images of 3D car models with varying azimuth angles at 15</p><p>• intervals. We split the dataset into train set and test set, and again split the train set into two groups, each of which is used as A domain and B domain samples. In addition to training a standard GAN model, a GAN with a reconstruction model and a proposed DiscoGAN model, we also trained a regressor that predicts the azimuth angle of a car image using the train set. To evaluate, we translated images in the test set using each of the three trained models, and azimuth angles were predicted using the regressor for both input and translated images. <ref type="figure" target="#fig_5">Figure  5</ref> shows the predicted azimuth angles of input and translated images for each model. In standard GAN and GAN with reconstruction (5a and 5b), most of the red dots are grouped in a few clusters, indicating that most of the input images are translated into images with same azimuth angles, and that these models suffer from mode collapsing problem. Our proposed DiscoGAN (5c), on the other hand, shows strong correlation between predicted angles of input and translated images, indicating that our model successfully discovers azimuth relation between the two domains. In this experiment, the input and translated images either have positive correlation where they have the same range of azimuth angles (5b), or negative correlation where they have opposite range of angles (5a and 5c).</p><p>Next, we use a Face dataset <ref type="bibr" target="#b18">(Paysan et al., 2009)</ref> shown in <ref type="figure" target="#fig_6">Figure 6a</ref>, in which the face images vary in azimuth rotation from -90</p><p>• to +90</p><p>• . Similar to previous Car to Car experiment, input images in the -90</p><p>• to +90</p><p>• rotation range generated output images either in the same range, from -90</p><p>• to +90</p><p>• , or the opposite range, from +90</p><p>• to -90</p><p>• when our proposed model was used (6d). We also trained a standard GAN and a GAN with reconstruction loss for comparison. When a standard GAN and GAN with reconstruction loss were used, the generated images do not vary as much as the input images in terms of rotation. In this sense, similar to what has been shown in previous Car to Car experiment, the two models suffered from mode collapse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">FACE CONVERSION</head><p>We also applied the face attribute conversion task on CelebA and Facescrub dataset <ref type="bibr" target="#b12">(Liu et al., 2015;</ref><ref type="bibr" target="#b17">Ng &amp; Winkler, 2014)</ref>, where a specific feature, such as gender or hair color, varies between two domains and other facial features are preserved. The results are listed in <ref type="figure">Figure 7</ref>.</p><p>In <ref type="figure">Figure 7a</ref> • to +90</p><p>• (b) results from a standard GAN (c) results from GAN with a reconstruction loss (d) results from our Disco-GAN. Here our model generated images in the opposite range, from +90</p><p>• to -90 • .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">CHAIR TO CAR, CAR TO FACE</head><p>We also investigated the case where the discrepancy between two domains is large, and there is a single shared feature between two domains. 3D rendered images of chair <ref type="bibr" target="#b1">(Aubry et al., 2014)</ref> and the previously used car and face datasets <ref type="bibr" target="#b5">(Fidler et al., 2012;</ref><ref type="bibr" target="#b18">Paysan et al., 2009</ref>) were used in this task. All three datasets vary along the azimuth rotation. <ref type="figure">Figure 8</ref> shows the results of image-to-image translation from chair to car and from car to face datasets. The translated images clearly match the rotation features of the input images while preserving visual features of car and face domain respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">EDGES TO PHOTOS</head><p>Edges-to-photos is an interesting task as it is a 1-to-N problem, where a single edge image of items such as shoes and handbags can generate multiple colored images of such items. In fact, an edge image can be colored in infinitely many ways. We validated that our DiscoGAN performs very well on this type of image-to-image translation task and generate realistic photos of handbags  and shoes <ref type="bibr" target="#b25">(Yu &amp; Grauman, 2014)</ref>. The generated images are presented in <ref type="figure" target="#fig_8">Figure 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5.">HANDBAG TO SHOES, SHOES TO HANDBAG</head><p>Finally, we investigated the case with two domains that are visually very different, where shared features are not explicit even to humans. We trained a DiscoGAN using previously used handbags and shoes datasets, not assuming any specific relation between those two. In the translation results shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our proposed model discovers fashion style as a related feature between the two domains. Note that translated results not only have similar colors and patterns, but also have similar level of fashion formality as the input fashion item.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Recently, a novel method to train generative models named Generative Adversarial Networks (GANs) <ref type="bibr" target="#b6">(Goodfellow et al., 2014)</ref> has been developed. A GAN is composed of two modules -a generator G and a discriminator D. The generator's objective is to generate (synthesize) data samples whose distribution closely matches that of real data samples while the discriminator's objective is to distinguish between real and generated samples. The two models G and D, formulated as a two-player minimax game, are trained simultaneously.</p><p>Researchers have studied GANs vigorously in the past few years: network models such as LAPGAN <ref type="bibr" target="#b3">(Denton et al., 2015)</ref> and DCGAN  and improved training techniques <ref type="bibr" target="#b21">(Salimans et al., 2016;</ref><ref type="bibr" target="#b0">Arjovsky et al., 2017)</ref> has been explored. More recent GAN works are described in <ref type="bibr" target="#b7">(Goodfellow, 2017)</ref>.</p><p>Several methods were developed to generate images based on GANs. Conditional Generative Adversarial Nets (cGANs) <ref type="bibr" target="#b16">(Mirza &amp; Osindero, 2014)</ref> used MNIST digit class label as an additional information to both generator and discriminator and can generate digit images of the specified class. Similarly, <ref type="bibr" target="#b4">Dosovitskiy et al. (2015)</ref> showed that GAN can generate images of objects based on specified characteristic codes such as color and viewpoint. Other approaches instead used conditional features from a completely different domain for image generation. For example, <ref type="bibr" target="#b20">Reed et al. (2016)</ref> used encoded text description of images as the conditional information to generating images that match the description.</p><p>In addition to using conditional information such as class labels and text encodings, several works in the field of image-to-image translation used images of one domain to generate images in another domain. <ref type="bibr" target="#b9">(Isola et al., 2016)</ref> translated black-and-white images to colored images by training on paired black-and-white and colored image data. Similarly, <ref type="bibr" target="#b22">Taigman et al. (2016)</ref> translated face images to emojis by providing image features from pre-trained face recognition module as conditional input to a GAN.</p><p>Several recent papers <ref type="bibr" target="#b23">(Tong et al., 2017;</ref><ref type="bibr" target="#b2">Chen et al., 2016;</ref><ref type="bibr" target="#b14">Makhzani, 2015)</ref> used similar architectures which reconstruct the original input by successively applying a mapping and its inverse mapping, and encourage the reconstruction to match the original input.</p><p>The most similar work to ours <ref type="bibr" target="#b15">(Mathieu et al., 2016)</ref> proposed an architecture that combined variational autoencoder <ref type="bibr" target="#b11">(Kingma et al., 2014)</ref> with generative adversarial network for factor decomposition and domain transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents a learning method to discover crossdomain relations with a generative adversarial network called DiscoGAN. Our approach works without any explicit pair labels and learns the relations between datasets from very different domains. One possible future directions is to extend DiscoGAN to handle mixed modalities such as text and image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Our GAN-based model trains with two independently collected sets of images and learns how to map two domains without any extra label. In this paper, we reduce this problem into generating a new image of one domain given an image from the other domain. (a) shows a high-level overview of the training procedure of our model with two independent sets (e.g. handbag images and shoe images). (b) and (c) show results of our method. Our method takes a handbag (or shoe) image as an input, and generates its corresponding shoe (or handbag) image. Again, it is worth noting that our method does not take any extra annotated supervision and can self-discover relations between domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Three investigated models. (a) standard GAN (Goodfellow et al., 2014), (b) GAN with a reconstruction loss, (c) our proposed model (DiscoGAN) designed to discover relations between two unpaired, unlabeled datasets. Details are described in Section 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Illustration of our models on simplified one dimensional domains. (a) ideal mapping from domain A to domain B in which the two domain A modes map to two different domain B modes, (b) GAN model failure case, (c) GAN with reconstruction model failure case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>denote the input and output domains, and super- scripts denote the input and output image size. The discrim- inator network is denoted as D B : R h×w×c B → [0, 1], and the subscript B denotes that it discriminates images in do- main B. Notations G BA and D A are used similarly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Toy domain experiment results. The colored background shows the output value of the discriminator. 'x' marks denote different modes in domain B, and colored circles indicate samples of domain A mapped to domain B, where each color corresponds to a different mode in domain A. (a) ten target (domain B) modes and initial translations, (b) results from standard GAN model, (c) GAN with reconstruction loss, (d) our proposed DiscoGAN model. domain simplicity our model often converged much earlier. Illustrations of translated samples in Figure 4 show very different behavior depending on the model used. In the baseline (standard GAN) case, many translated points of different colors are located around the same domain B mode. For example, navy and light-blue colored points are located together, as well as green and orange colored points. This result illustrates the mode-collapse problem of GANs since points of multiple colors (multiple A domain modes) are mapped to the same domain B mode. The baseline model still oscillate around domain B modes throughout the iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Car to Car translation experiment. Horizontal and vertical axes in the plots indicate predicted azimuth angles of input and translated images, where the angle of input image ranges from -75 • to 75 • . RMSE with respect to ground truth (blue lines) are shown in each plot. Images in the second row are examples of input car images ranging from -75 • to 75 • at 15 • intervals. Images in the third row are corresponding translated images. (a) plot of standard GAN (b) GAN with reconstruction (c) DiscoGAN. The angles of input and output images are highly correlated when our proposed DiscoGAN model is used. Note the angles of input and translated car images are reversed with respect to 0 • (i.e. mirror images) for (a) and (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Face to Face translation experiment. (a) input face images from -90 • to +90 • (b) results from a standard GAN (c) results from GAN with a reconstruction loss (d) results from our Disco-GAN. Here our model generated images in the opposite range, from +90 • to -90 • .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Figure 7. (a) Translation of gender in Facescrub dataset. (b) Blond to black and black to blond hair color conversion in CelebA dataset. (c) Eyeglasses to no eyeglasses, no eyeglasses to eyeglasses conversion in CelebA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Edges to photos experiment. Our model is trained on a set of object sketches and colored images and learns to generate corresponding photos or sketches. (a) colored handbag images are generated from handbag sketches, (b) colored shoe images are generated from shoe sketches, (c) handbag sketches are generated from colored handbag images.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wasserstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Seeing 3d chairs: Exemplar part-based 2d-3d alignment using a large dataset of cad models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to generate chairs with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d object detection and viewpoint estimation with a deformable 3d cuboid model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">tutorial: Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nips</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1701.00160</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Batch normalization: Accerlerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Image-toimage translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1611.07004</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 2rd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Disentangling factors of variation in deep representation using adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">F</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Junbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1411.1784</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A data-driven approach to cleaning large face datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing (ICIP)</title>
		<meeting>the IEEE International Conference on Image Processing (ICIP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A 3d face model for pose and illumination invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th IEEE International Conference on Advanced Video and Signal based Surveillance (AVSS) for Security, Safety and Monitoring in Smart Environments</title>
		<meeting>the 6th IEEE International Conference on Advanced Video and Signal based Surveillance (AVSS) for Security, Safety and Monitoring in Smart Environments</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 4th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning (ICML)</title>
		<meeting>the 33rd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02200</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mode regularized generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5rd International Conference on Learning Representations (ICLR</title>
		<meeting>the 5rd International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno>arXiv:1505:00853</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fine-grained visual comparisons with local learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Philipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
