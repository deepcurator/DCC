<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimizing the Latent Space of Generative Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez Paz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
						</author>
						<title level="a" type="main">Optimizing the Latent Space of Generative Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Generative Adversarial Networks (GANs) have achieved remarkable results in the task of generating realistic natural images. In most successful applications, GAN models share two common aspects: solving a challenging saddle point optimization problem, interpreted as an adversarial game between a generator and a discriminator functions; and parameterizing the generator and the discriminator as deep convolutional neural networks. The goal of this paper is to disentangle the contribution of these two factors to the success of GANs. In particular, we introduce Generative Latent Optimization (GLO), a framework to train deep convolutional generators using simple reconstruction losses. Throughout a variety of experiments, we show that GLO enjoys many of the desirable properties of GANs: synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors; all of this without the adversarial optimization scheme.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b16">(Goodfellow et al., 2014)</ref> are a powerful framework to learn models capable of generating natural images. GANs learn these generative models by setting up an adversarial game between two learning machines. On the one hand, a generator plays to transform noise vectors into fake samples, which resemble real samples drawn from a distribution of natural images. On the other hand, a discriminator plays to distinguish between real and fake samples. During training, the generator and the discriminator functions are optimized in turns. First, the discriminator learns to assign high scores to real samples, and low scores to fake samples. Then, the generator learns to increase the scores of fake samples, so 1 Facebook AI Research. Correspondence to: Piotr Bojanowski &lt;bojanowski@fb.com&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the 35</head><p>th International Conference on Machine Learning, <ref type="bibr">Stockholm, Sweden, PMLR 80, 2018</ref><ref type="bibr">. Copyright 2018</ref> by the author(s).</p><p>as to "fool" the discriminator. After proper training, the generator is able to produce realistic natural images from noise vectors.</p><p>Recently, GANs have been used to produce high-quality images resembling handwritten digits, human faces, and house interiors <ref type="bibr" target="#b37">(Radford et al., 2015)</ref>. Furthermore, GANs exhibit three strong signs of generalization. First, the generator translates linear interpolations in the noise space into semantic interpolations in the image space. In other words, a linear interpolation in the noise space will generate a smooth interpolation of visually-appealing images. Second, the generator allows linear arithmetic in the noise space. Similarly to word embeddings <ref type="bibr" target="#b32">(Mikolov et al., 2013)</ref>, linear arithmetic indicates that the generator organizes the noise space to disentangle the nonlinear factors of variation of natural images into linear statistics. Third, the generator is able to to synthesize new images that resemble those of the data distribution. This allows for applications such as image in-painting <ref type="bibr" target="#b20">(Iizuka et al., 2017)</ref> and super-resolution <ref type="bibr" target="#b28">(Ledig et al., 2016)</ref>.</p><p>Despite their success, training and evaluating GANs is notoriously difficult. The adversarial optimization problem implemented by GANs is sensitive to random initialization, architectural choices, and hyper-parameter settings. In many cases, a fair amount of human care is necessary to find the correct configuration to train a GAN in a particular dataset. It is common to observe generators with similar architectures and hyper-parameters to exhibit dramatically different behaviors. Even when properly trained, the resulting generator may synthesize samples that resemble only a few localized regions (or modes) of the data distribution <ref type="bibr" target="#b15">(Goodfellow, 2017)</ref>. While several advances have been made to stabilize the training of GANs <ref type="bibr" target="#b38">(Salimans et al., 2016)</ref>, this task remains more art than science.</p><p>The difficulty of training GANs is aggravated by the challenges in their evaluation: since evaluating the likelihood of a GAN with respect to the data is an intractable problem, the current gold standard to evaluate the quality of GANs is to eyeball the samples produced by the generator. This qualitative evaluation gives little insight on the coverage of the generator, making the mode dropping issue hard to measure. The evaluation of discriminators is also difficult, since their visual features do not always transfer well to supervised tasks <ref type="bibr" target="#b12">(Donahue et al., 2016;</ref><ref type="bibr" target="#b13">Dumoulin et al., 2016)</ref>. Finally, the application of GANs to non-image data has been relatively limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Research question</head><p>To model natural images with GANs, the generator and discriminator are commonly parametrized as deep Convolutional Networks (convnets) <ref type="bibr" target="#b26">(LeCun et al., 1998)</ref>. Therefore, it is reasonable to hypothesize that the reasons for the success of GANs in modeling natural images come from two complementary sources: (A1) Leveraging the powerful inductive bias of deep convnets.</p><p>(A2) The adversarial training protocol.</p><p>This work attempts to disentangle the factors of success (A1) and (A2) in GAN models. Specifically, we propose and study one algorithm that relies on (A1) and avoids (A2), but still obtains competitive results when compared to a GAN.</p><p>Contributions. We investigate the importance of the inductive bias of convnets by removing the adversarial training protocol of GANs (Section 2). Our approach, called Generative Latent Optimization (GLO), maps one learnable noise vector to each of the images in our dataset by minimizing a simple reconstruction loss. Since we are predicting images from learnable noise, GLO borrows inspiration from recent methods to predict learnable noise from images <ref type="bibr" target="#b3">(Bojanowski &amp; Joulin, 2017)</ref>. Alternatively, one can understand GLO as an auto-encoder where the latent representation is not produced by a parametric encoder, but learned freely in a non-parametric manner. In contrast to GANs, we track the correspondence between each learned noise vector and the image that it represents. Hence, the goal of GLO is to find a meaningful organization of the noise vectors, such that they can be mapped to their target images. To turn GLO into a generative model, we observe that it suffices to learn a simple probability distribution on the learned noise vectors.</p><p>We study the efficacy of GLO to compress and decompress  hypothesize (and show evidence) that this is a capacity issue. It has been observed that GANs are prone to mode collapse, completely forgetting large parts of the training dataset. In the literature this is often described as a problem with the GAN training procedure. Our experiments suggest that this is more of a feature than a bug, as it allows relatively small models to generate realistic images by intelligently choosing which part of the data to ignore. We quantitatively measure the significance of this issue with a reconstruction criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Generative Latent Optimization</head><p>First, we consider a large set of images {x 1 , . . . , x N }, where each image x i ∈ X has dimensions 3 × w × h. Second, we initialize a set of d-dimensional random vectors {z 1 , . . . , z N }, where z i ∈ Z ⊆ R d for all i = 1, . . . N . Third, we pair the dataset of images with the random vectors, obtaining the dataset {(z 1 , x 1 ), . . . , (z N , x N )}. Finally, we jointly learn the parameters θ in Θ of a generator g θ : Z → X and the optimal noise vector z i for each image x i , by solving:</p><formula xml:id="formula_0">min θ∈Θ 1 N N i=1 min zi∈Z (g θ (z i ), x i ) ,<label>(1)</label></formula><p>In the previous, : X × X is a loss function measuring the reconstruction error from g(z i ) to x i . We call this model Generative Latent Optimization (GLO).</p><p>Learnable z i . In contrast to autoencoders <ref type="bibr" target="#b6">(Bourlard &amp; Kamp, 1988)</ref>, which assume a parametric model f : X → Z, usually referred to as the encoder, to compute the vector z from samples x, and minimize the reconstruction loss (g(f (x)), x), in GLO we jointly optimize the inputs z 1 , . . . , z N and the model parameter θ. Since the vector z is a free parameter, our model can recover all the solutions that could be found by an autoencoder, and reach some others. In a nutshell, GLO can be viewed as an "encoderless" autoencoder, or as a "discriminator-less" GAN.</p><p>Choice of Z. A common choice of Z in the GAN literature is from a Normal distribution on R d . Since random vectors z drawn from the d-dimensional Normal distribution are very unlikely to land far outside the (surface of) the sphere S(</p><formula xml:id="formula_1">√ d, d</formula><p>, 2), and since projection onto the sphere is easy and numerically pleasant, after each z update in GLO training we project onto the sphere. For simplicity, instead of using the √ d sphere, we use the unit sphere.</p><p>Choice of loss function. On the one hand, the squared-loss</p><formula xml:id="formula_2">function 2 (x, x ) = x − x 2</formula><p>2 is a simple choice, but leads to blurry (average) reconstructions of natural images. On the other hand, GANs use a convnet (the discriminator) as loss function. Since the early layers of convnets focus on edges, the samples from a GAN are sharper. Therefore, our experiments provide quantitative and qualitative comparisons between the 2 loss and the Laplacian pyramid Lap 1 loss</p><formula xml:id="formula_3">Lap 1 (x, x ) = j 2 2j |L j (x) − L j (x )| 1 ,</formula><p>where L j (x) is the j-th level of the Laplacian pyramid representation of x <ref type="bibr" target="#b29">(Ling &amp; Okada, 2006)</ref>. Therefore, the Lap 1 loss weights the details at fine scales more heavily. In order to preserve low-frequency content such as color information, we will use a weighted combination of the Lap 1 and the 2 costs.</p><p>Optimization. For any choice of differentiable generator, the objective (1) is differentiable with respect to z, and θ. Therefore, we will learn z and θ by Stochastic Gradient Descent (SGD). The gradient of (1) with respect to z can be obtained by backpropagating the gradients through the generator function <ref type="bibr" target="#b4">(Bora et al., 2017)</ref>. We project each z back to the representation space Z after each update. To have noise vectors laying on the unit 2 sphere, we project z after each update by dividing its value by max( z 2 , 1). We initialize z by sampling them from a Gaussian distribution.</p><p>Generator architecture. Among the multiple architectural variations explored in the literature, the most prominent is the Deep Convolutional Generative Adversarial Network (DCGAN) <ref type="bibr" target="#b37">(Radford et al., 2015)</ref>. Therefore, in this paper, to make the comparison with the GAN literature as straightforward as possible, we will use the generator function of DCGAN construct the generator of GLO across all of our experiments. <ref type="figure">Figure 2</ref>. Illustration of interpolations obtained with our model on the CelebA dataset. We construct a path between 3 images to verify that paths do not collapse to an "average" representation in the middle of the interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related work</head><p>Generative Adversarial Networks. GANs were introduced by <ref type="bibr" target="#b16">Goodfellow et al. (2014)</ref>, and refined in multiple recent works <ref type="bibr" target="#b11">(Denton et al., 2015;</ref><ref type="bibr" target="#b37">Radford et al., 2015;</ref><ref type="bibr" target="#b47">Zhao et al., 2016;</ref><ref type="bibr" target="#b38">Salimans et al., 2016)</ref>. As described in Section 1, GANs construct a generative model of a probability distribution P by setting up an adversarial game between a generator g and a discriminator d:</p><formula xml:id="formula_4">min G max D E x∼P log d(x) + E z∼Q (1 − log d(g(z))).</formula><p>In practice, most of the applications of GANs concern modeling distributions of natural images. In these cases, both the generator g and the discriminator d are parametrized as deep convnets <ref type="bibr" target="#b26">(LeCun et al., 1998)</ref>.</p><p>Autoencoders. In their simplest form, an Auto-Encoder (AE) is a pair of neural networks, formed by an encoder f : X → Z and a decoder g : Z → X . The role of an autoencoder is the compress the data {x 1 , . . . , x N } into the representation {z 1 , . . . , z N } using the encoder f (x i ), and decompress it using the decoder g(f (x i )). Therefore, autoencoders minimize E x∼P (g(f (x)), x), where : X × X is a simple loss function, such as the mean squared error. There is a vast literature on autoencoders, spanning three decades from their conception <ref type="bibr" target="#b6">(Bourlard &amp; Kamp, 1988;</ref><ref type="bibr" target="#b1">Baldi &amp; Hornik, 1989)</ref>, renaissance <ref type="bibr" target="#b19">(Hinton &amp; Salakhutdinov, 2006)</ref>, and recent probabilistic extensions <ref type="bibr" target="#b44">(Vincent et al., 2008;</ref><ref type="bibr" target="#b23">Kingma &amp; Welling, 2013)</ref>.</p><p>Several works have combined GANs with AEs. For instance, <ref type="bibr" target="#b47">Zhao et al. (2016)</ref> replace the discriminator of a GAN by an AE, and <ref type="bibr" target="#b43">Ulyanov et al. (2017)</ref> replace the decoder of an AE by a generator of a GAN. Similar to GLO, these works suggest that the combination of standard pipelines can lead to good generative models. In this work we attempt one step further, to explore if learning a generator alone is possible.</p><p>Inverting generators. Several works attempt at recovering the latent representation of an image with respect to a generator. In particular, <ref type="bibr" target="#b30">Lipton &amp; Tripathi (2017)</ref>; <ref type="bibr" target="#b48">Zhu et al. (2016)</ref> show that it is possible to recover z from a generated We show that by taking the average hidden representation of the first row (man with sunglasses), substracting the one of the second row (men without sunglasses) and adding the one of the third row (women without sunglasses), we obtain a coherent image.</p><p>sample. Similarly, <ref type="bibr" target="#b10">Creswell &amp; Bharath (2016)</ref> show that it is possible to learn the inverse transformation of a generator. These works are similar to <ref type="bibr" target="#b45">(Zeiler &amp; Fergus, 2014)</ref>, where the gradients of a particular feature of a convnet are backpropagated to the pixel space in order to visualize what that feature stands for. From a theoretical perspective, <ref type="bibr" target="#b7">Bruna et al. (2013)</ref> explore the theoretical conditions for a network to be invertible. All of these inverting efforts are instances of the pre-image problem, <ref type="bibr" target="#b24">(Kwok &amp; Tsang, 2004)</ref>. <ref type="bibr" target="#b4">Bora et al. (2017)</ref> have recently showed that it is possible to recover from a trained generator with compressed sensing. Similar to our work, they use a 2 loss and backpropagate the gradient to the low rank distribution. However, they do not train the generator simultaneously. Jointly learning the representation and training the generator allows us to extend their findings. <ref type="bibr" target="#b39">Santurkar et al. (2017)</ref> also use generative models to compress images.</p><p>Several works have used an optimization of a latent representation for the express purpose of generating realistic images, e.g. <ref type="bibr" target="#b35">(Portilla &amp; Simoncelli, 2000;</ref><ref type="bibr" target="#b33">Nguyen et al., 2017)</ref>. In these works, the total loss function optimized to generate is trained separately from the optimization of the latent representation (in the former, the loss is based on a complex wavelet transform, and in the latter, on separately trained autoencoders and classification convolutional networks). In this work we train the latent representations and the generator together from scratch; and show that at test time we may sample new z either using simple parametric distributions or interpolations in the latent space.</p><p>Learning representations. Arguably, the problem of learning representations from data in an unsupervised manner is one of the long-standing problems in machine learning <ref type="bibr" target="#b2">(Bengio et al., 2013;</ref><ref type="bibr" target="#b27">LeCun et al., 2015)</ref>. One of the earliest algorithms used to achieve is goal is Principal Component Analysis, or PCA <ref type="bibr" target="#b34">(Pearson, 1901;</ref><ref type="bibr" target="#b21">Jolliffe, 1986)</ref>. For instance, PCA has been used to learn low-dimensional representations of human faces <ref type="bibr" target="#b42">(Turk &amp; Pentland, 1991)</ref>, or to produce a hierarchy of features <ref type="bibr" target="#b8">(Chan et al., 2015)</ref>. The nonlinear extension of PCA is an autoencoder <ref type="bibr" target="#b1">(Baldi &amp; Hornik, 1989)</ref>, which is in turn one of the most extended algorithms to learn low-dimensional representations from data. Similar algorithms learn low-dimensional representations of data with certain structure. For instance, in sparse coding <ref type="bibr" target="#b0">(Aharon et al., 2006;</ref><ref type="bibr" target="#b31">Mairal et al., 2008)</ref>, the representation of one image is the linear combination of a very few elements from a dictionary of features. More recently, <ref type="bibr" target="#b46">Zhang et al. (2016)</ref> realized the capability of deep neural networks to map large collections of images to noise vectors, and Bojanowski &amp; Joulin (2017) exploited a similar procedure to learn visual features unsupervisedly. Similarly to us, <ref type="bibr" target="#b3">Bojanowski &amp; Joulin (2017)</ref> allow the noise vectors z to move in order to better learn the mapping from images to noise vectors. The proposed GLO is the analogous to these works, in the opposite direction: learn a map from noise vectors to images. Finally, the idea of mapping between images and noise to learn generative models is a well known technique <ref type="bibr" target="#b9">(Chen &amp; Gopinath, 2000;</ref><ref type="bibr" target="#b25">Laparra et al., 2011;</ref><ref type="bibr" target="#b40">Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b5">Bordes et al., 2017)</ref>.</p><p>Nuisance Variables. One might consider the generator parameters the variables of interest, and Z to be "nuisance variables". There is a classical literature on dealing with nuisance parameters while estimating the parameters of interest, including optimization methods as we have used <ref type="bibr" target="#b41">(Stuart &amp; Ord, 2010)</ref>. In this framing, it may be better to marginalize over the nuisance variables, but for the models and data we use this is intractable.</p><p>Speech and music generation. Optimizing a latent representation of a generative model has a long history in speech <ref type="bibr" target="#b36">(Rabiner &amp; Schafer, 2007)</ref>, both for fitting single examples in the context of fitting a generative model, and in the context of speaker adaptation. In the context of music generation and harmonazation, the first model was introduced by <ref type="bibr" target="#b14">Ebcioglu (1988)</ref>. Closer to our work, is the neural network-based model of <ref type="bibr" target="#b18">Hild et al. (1992)</ref>, which was later improved upon by <ref type="bibr" target="#b17">Hadjeres &amp; Pachet (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we compare GLO quantitatively and qualitatively against standard generative models on a variety of datasets. We consider several tasks to understand the strengths and weaknesses of each model: a qualitative analysis of the properties of the latent space typically observed with deep generative models and an image reconstruction problem to give some quantitative insights on the capability of GLO to cover a dataset. We selected datasets that are both small and large, uni-modal and multi-modal to stress the specificities of our models in different settings.  <ref type="table">Table 1</ref>. pSNR of reconstruction for different models. Below the line, the codes were found using Lap 1 loss (although the test error is still measured in pSNR). Above the line, the codes were found using mean square error. Note that the generators of the VAE and GLO models were trained to reconstruct in Lap 1 loss. pSNR of GAN reconstruction of images generated by GAN (not real images) is greater than 50.</p><p>Implementation details The generator of a GLO follows the same architecture as the generator of DCGAN. We use Stochastic Gradient Descent (SGD) to optimize both θ and z, setting the learning rate for θ at 1 and the learning rate of z at 10. After each update, the noise vectors z are projected to the unit 2 Sphere. In the sequel, we initialize the random vectors of GLO using a Gaussian distribution (for the CelebA dataset) or the top d principal components (for the LSUN dataset). We use the 2 + Lap 1 loss for all the experiments but MNIST where we use an MSE loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Baselines and datasets.</head><p>We consider three standard baselines: PCA, VAE, and GAN. PCA <ref type="bibr" target="#b34">(Pearson, 1901</ref>) is equivalent to a linear autoencoder <ref type="bibr" target="#b1">(Baldi &amp; Hornik, 1989)</ref>. We use for VAE and GAN the same generator architecture as for GLO, i.e., a DCGAN. We also set the number of principal components for PCA to be same as the dimensions of the latent spaces. We use 32 dimensions for MNIST, 64 dimensions for SVHN and 256 dimensions for CelebA and LSUN. We use the same 2 + Lap 1 loss for VAE as for GLO for all the experiments but MNIST where we use an MSE loss. For the rest, we train VAE with the default hyper-parameters for 25 epochs. We train the GAN baseline with the default hyper-parameters and many seeds.</p><p>For our empirical evaluation, we consider four varied image datasets. We select both "unimodal" and "multimodal" datasets to probe the difficulty of models in each setting. We carry out our experiments on MNIST 1 , SVHN 2 as well as more challenging datasets such as CelebA 3 and LSUNbedroom 4 . On smaller datasets (MNIST and SVHN), we keep the images 32 pixels large. For CelebA and LSUN we resize the images to either 64 and 128 pixels large. For each dataset, we set aside evenly-spaced images corresponding to 1 32 of the data, and consider these images as a test set. We train our models on the complement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Properties of the latent space</head><p>The latent space of GANs seems to linearize the space of images. That is: interpolations between a pair of z vectors in the latent space map through the generator to a semantically meaningful, smooth nonlinear interpolation in image space. <ref type="figure" target="#fig_0">Figure 1</ref> shows that the latent space of GLO seems to linearize the image space as well. For example, the model interpolates between examples that are geometrically quite different, reconstructing the rotation of the head from left to right, as well as interpolating between genders or different ages. It is important to note that these paths do not go through an "average" image of the dataset as the path interpolation between the 3 images of <ref type="figure">Figure 2</ref> shows. Linear arithmetic operations in the latent space of GANs can lead to meaningful image transformations. For example: (man with sunglasses -man + woman) produces an image of a woman with sunglasses. <ref type="figure" target="#fig_1">Figure 3</ref> shows that the latent space of GLO shares the same property.</p><p>Finally, GLO models have the attractive property that the principal vectors corresponding to the largest principal values are meaningful in image space. As shown in <ref type="figure" target="#fig_4">Figure 6</ref>, they carry information like background color, the orientation of the head and gender. Interestingly, the gender information is represented by two principal vectors, one for the female and one for the male.</p><p>These results suggest that the desirable linearization properties of generators are probably due to the structure of the model (convnets) rather than the training procedure. . Samples generated by VAE, DCGAN and GLO on the 4 datasets. For CelebA and LSUN, we consider images of size 64 and 128. On small datasets, the three models generate images of the comparable quality. On LSUN, images from VAE and GLO are nowhere close to those from DCGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Generation</head><p>Another celebrated aspect of GANs is the high quality of the examples they generate. To sample from a GLO model, we fit a single full-covariance Gaussian to the Z found by the training procedure; and then pass samples from that Gaussian through the generator. <ref type="figure" target="#fig_2">Figure 4</ref> shows a comparison between images generated by VAE, GAN and GLO models trained on different datasets, offering a few insights on the main difference between the methods: First, the images produced by VAE are often less sharp than GLO, in particular on large datasets like CelebA and LSUN bedroom. This observation suggests that the prior distribution on the latent space of a VAE may be too strong to fit many images, while vectors in the latent space of GLO move freely and use as much space as required to fit the images in the latent space.</p><p>On the other hand, on these datasets, the trained Z from GLO are Gaussian enough to produce decent generations when fit with a single (full-covariance) Gaussian.</p><p>Second, it is interesting to notice that on the LSUN bedrooms, VAE and GLO are much worse than GAN. While they seem to capture the general shape of the bedrooms, they fail to produce the same level of detail as is observed in the samples generated by a GAN. One possibility is that in these settings, the "mode dropping" problem commonly discussed in the GAN literature <ref type="bibr" target="#b15">(Goodfellow, 2017</ref>) is more a feature than a bug. Both VAE and GLO do not suffer from mode dropping by construction (since their loss forces them to reconstruct the whole dataset) and it is possible that as a result, they both generate poorly when the variability in the distribution increases relative to the model capacity. In other words, when confronted with more data variability than it can handle, a GAN can still be successful in generating (and well-organizing) a well-chosen subset of the data.</p><p>In the next section, we look at the reconstruction error of each method on the different datasets. This quantitative evaluation gives further insights on the differences between the approaches, and in particular, it gives evidence that GANs are not covering the training data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Image reconstruction</head><p>In this set of experiments, we evaluate the quality of image reconstructions for each method. In <ref type="table">Table 1</ref> we report the reconstruction error in pSNR, which for a given image I and the case of GANs trained with convnets on images is an exciting direction for future work.</p><p>a reconstruction R, is defined as:</p><p>pSNR(I, R) = −20 log 10 MAX(I)</p><formula xml:id="formula_5">MSE(I, R) ,<label>(2)</label></formula><p>where MAX corresponds to the maximal value the image I can attain, and MSE is the Mean Squared Error.</p><p>To reconstruct an image from the test set, we need to find its latent representation. For the PCA and VAE baselines, this  is straightforward. The latent codes for DCGAN and GLO can be found by backpropagating the reconstruction error to the code through the generator. Note that the generating functions of all the GLO and VAE models in the table were trained with Lap 1 cost. This discrepancy in training loss favors PCA and if we find codes using MSE for GLO and DCGAN instead of Lap 1 , the scores improve by 1−2 points, even though we did not train GLO with an MSE.</p><p>Measuring a reconstruction error favors VAE and GLO over DCGAN as they are trained to minimize such an error metric. However, it is interesting to notice that on small datasets, there is no clear difference with DCGAN. The difference in performance between DCGAN and the other methods increases with the size of the dataset. This result already suggests that as the dataset grows, GANs are probably focusing on a subset of it, while, by objective, VAE and GLO are forced to reconstruct the full dataset. It is not clear though what is the nature of this "subset", as the distribution of the pSNR scores of a DCGAN is not significantly different from those of VAE or GLO as shown in the supplementary material. Finally, we remark although it is a-priori possible that the process of finding codes via backpropagation is not succeeding with the GAN generators, we find in practice that when we reconstruct an image generated by the GAN, the results are nearly perfect (pSNR &gt; 50). This suggests that the difference in pSNR between the models is not due to poor optimization of the codes. <ref type="figure" target="#fig_3">Figure 5</ref> shows qualitative examples of reconstruction. As suggested by the quantitative results, the VAE reconstruction is much blurrier than GLO and the reconstruction quality of DCGAN quickly deteriorates with the size and variability of the dataset. More interestingly, on CelebA, we observe that, while DCGAN reconstructions of frontal faces look good, DCGAN struggles on side faces as well as rare examples, e.g., stylistic or blurry images. More important, they seem to be copy-pasting "faces" rather than reconstructing them. This effect is even more apparent on LSUN where it is almost impossible to find an entire well reconstructed image. However, even though the colors are off, the edges are sharp if they are reconstructed, suggesting that GANs are indeed focusing on some specificities of the image distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>The experimental results presented in this work suggest that, when working with images, we can recover many of the properties of GANs using convnets trained with a simple reconstruction losses. While this does not invalidate the promise of GANs as generic models of uncertainty or as methods for building generative models, our results suggest that, in order to further test the adversarial construction, research needs to move beyond images modeled using convnets. On the other hand, practitioners who care only about generating images for a particular application, and find that the parameterized discriminator does improve their results, can incorporate reconstruction losses in their models, alleviating some of the instability of adversarial training.</p><p>While the visual quality of our results are promising, especially on the CelebA dataset, they are not yet to the level of the results obtained by GANs on the LSUN bedrooms. This suggests that being able to cover the entire dataset is too onerous of a task if all that is required is to generate a few nice samples. In that respect, we see that GANs have trouble reconstructing randomly chosen images at the same level of fidelity as their generations. At the same time, GANs can produce good images after a single pass through the data with SGD, suggesting that the so-called "mode dropping" can be seen as a feature. In future work we hope to better understand the tension between these two observations, and clarify the definition of this phenomenon.</p><p>There are many possibilities for improving the quality of GLO samples beyond understanding the effects of coverage. For example other loss functions (e.g. a VGG metric, as in <ref type="bibr" target="#b33">(Nguyen et al., 2017)</ref>), model architectures, especially progressive generation <ref type="bibr" target="#b22">(Karras et al., 2017)</ref>, and more sophisticated sampling methods after training the model all may improve the visual quality GLO samples. Finally, because the methods keep track of the correspondence between samples and their representatives, we hope to be able to organize the Z in interesting ways as we train.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of interpolations obtained with our model on the CelebA dataset. Each row corresponds to an image pair, and the leftmost and rightmost images are actual images from the training set. Given two images i and j, we get interpolated latent vectors z between zi and zj and show the reconstruction g(z).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Illustration of feature arithmetic on the CelebA dataset. We show that by taking the average hidden representation of the first row (man with sunglasses), substracting the one of the second row (men without sunglasses) and adding the one of the third row (women without sunglasses), we obtain a coherent image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4</head><label>4</label><figDesc>Figure 4. Samples generated by VAE, DCGAN and GLO on the 4 datasets. For CelebA and LSUN, we consider images of size 64 and 128. On small datasets, the three models generate images of the comparable quality. On LSUN, images from VAE and GLO are nowhere close to those from DCGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Reconstruction results from PCA, VAE, DCGAN and GLO on the 4 datasets. The original images are on the top row. VAE reconstructions are blurrier than GLO . DCGAN fails to reconstruct images from large datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Interpolation from the average face along the principal vectors corresponding to the largest principal values. The principal vectors and values were computed on the (centered) Z vectors of the GLO model. The first vector seems to capture the brightness of the background, the second one the orientation of the face, while the third and forth capture the information about the gender. The average image is 4th from left.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://yann.lecun.com/exdb/mnist/ 2 http://ufldl.stanford.edu/housenumbers/ 3 http://mmlab.ie.cuhk.edu.hk/projects/ CelebA.html 4 http://lsun.cs.princeton.edu/2017/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Here "reduced" or "not covering" may be in the sense of missing some examples, for example dropping a cluster from a Gaussian mixture model, or more subtle retreats from the full data, for example projecting onto some complicated sub-manifold. We believe understanding precisely what reduction happens (if any) in</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">rmk-svd: An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on signal processing</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural networks and principal component analysis: Learning from examples without local minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised Learning by Predicting Noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Compressed Sensing using Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03208</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning to Generate Samples from Noise through Infusion Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06975</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Auto-association by multilayer perceptrons and singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.4025</idno>
		<title level="m">Signal recovery from pooling representations</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pcanet</surname></persName>
		</author>
		<title level="m">A Simple Deep Learning Baseline for Image Classification? IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaussianization</surname></persName>
		</author>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Inverting The Generator Of A Generative Adversarial Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Bharath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05644</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprints</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Adversarially learned inference. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An expert system for harmonizing four-part chorales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ebcioglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Music Journal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="43" to="51" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00160</idno>
		<title level="m">Tutorial: Generative Adversarial Networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deepbach: a steerable model for bach chorales generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hadjeres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pachet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Harmonet: A neural net for harmonizing chorales in the style of js bach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feulner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Menzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Globally and Locally Consistent Image Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
		<idno>107:1-107:14</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Principal component analysis and factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<title level="m">Progressive growing of gans for improved quality, stability, and variation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The pre-image problem in kernel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-H</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Iterative gaussianization: from ica to random rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Camps-Valls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep learning. Nature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Diffusion distance for histogram comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Okada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Precise Recovery of Latent Vectors from Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04782</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprints</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sparse representation for color image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Plug &amp; play generative networks: Conditional iterative generation of images in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clune</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On lines and planes of closest fit to systems of points in space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</title>
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A parametric texture model based on joint statistics of complex wavelet coefficients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Portilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="70" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Introduction to digital speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Schafer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="194" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shavit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01467</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">Generative compression. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03585</idno>
		<title level="m">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Kendall&apos;s Advanced Theory of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Face recognition using eigenfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02304</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">Adversarial Generator-Encoder Networks. arXiv preprints</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Energybased generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
