<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Dual Convolutional Neural Networks for Low-Level Vision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">City University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<addrLine>5 SenseTime</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<addrLine>5 SenseTime</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">Learning Dual Convolutional Neural Networks for Low-Level Vision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Residual learning algorithms usually take upsampled image as the base structures and learn the details, the difference between the upsampled and ground truth images. However, residual learning cannot correct low-frequency errors in the structures, e.g., the structure obtained by nearest neighbor interpolation in (c). In contrast, our algorithm is motivated by the decomposition of a signal into structures and details, which involves both structure and detail learning and thus leads to better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Motivated by the success of deep learning in high-level vision tasks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, numerous deep models have been developed for low-level vision tasks, e.g., image super-resolution <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22]</ref>, inpainting <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b23">24]</ref>, noise removal <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref>, image filtering <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b23">24]</ref>, image deraining <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40]</ref>, and dehazing <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b1">2]</ref>. Although achieving impressive performance, the network architectures of these models strongly resemble those developed for highlevel classification tasks.</p><p>Existing methods are based on either plain neural networks or residual learning networks. As demonstrated in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27]</ref>, plain neural networks cannot outperform state-ofthe-art traditional approaches on a number of low-level vision problems, e.g., super-resolution <ref type="bibr" target="#b33">[34]</ref>. Low-level vision tasks usually involve the estimation of two components, low-frequency structures and high-frequency details. It is challenging for a single network to learn both components simultaneously. As a result, going deeper with plain neural networks does not always lead to better performance <ref type="bibr" target="#b5">[6]</ref>.</p><p>Residual learning has been shown to be an effective approach to achieve performance gain with a deeper network. The residual learning algorithms (e.g., <ref type="bibr" target="#b16">[17]</ref>) assume that the main structure is given and mainly focus on estimating the residual (details) using a deep network. These methods work well when the recovered structures are perfect or near perfect. However, when the main structure is not well recovered, these methods do not perform well, because the final result is a combination of the structures and details. <ref type="figure">Figure 1</ref> shows the image super-resolution results by the VDSR method <ref type="bibr" target="#b16">[17]</ref> with structures recovered by different methods. The residual network cannot correct low-frequency errors in the structures <ref type="figure">(Figure 1(b)</ref>).</p><p>To address this issue, we propose a dual convolutional neural network (DualCNN) that can jointly estimate the structures and details. A DualCNN consists of two branches, one shallow sub-network to estimate the structures and one deep sub-network to estimate the details. The modular design of a DualCNN makes it a flexible framework for a variety of low-level vision problems. When trained end-toend, DualCNNs perform favorably against state-of-the-art methods that have been specially designed for each individual task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Numerous deep learning methods have been developed for low-level vision tasks. A comprehensive review is beyond the scope of this work and we discuss the most related ones in this section.</p><p>Super-resolution. The SRCNN <ref type="bibr" target="#b4">[5]</ref> method uses a threelayer plain convolutional neural network (CNN) for superresolution. As the SRCNN method is less effective in recovering image details, Kim et al. <ref type="bibr" target="#b16">[17]</ref> propose the residual learning <ref type="bibr" target="#b16">[17]</ref> algorithm based on a deeper network. The VDSR algorithm uses the bicubic interpolation of the lowresolution input as the structure of the high-resolution image and estimates the residual details using a 20-layer CNN. However, if the image structure is not well recovered, the generated result is likely to contain substantial artifacts, as shown in <ref type="figure">Figure 1</ref>.</p><p>Noise/artifacts removal. Numerous algorithms based on CNNs have been developed to remove noise/artifacts <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref> and unwanted components, e.g., rainy/dirty pixels <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40]</ref>. These methods are based on plain models, residual learning models or recurrent models. In addition, these methods estimate either the output using one plain network, or details using a residual network. However, plain networks cannot recover fine details <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17]</ref> and residual networks cannot correct structural errors.</p><p>Edge-preserving filtering. For edge-preserving filtering, Xu et al. <ref type="bibr" target="#b37">[38]</ref> develop a CNN model to approximate a number of filters. Liu et al. <ref type="bibr" target="#b23">[24]</ref> use a hybrid network to approximate a number of edge-preserving filters. These methods aim to preserve the main structures and remove details using a single network, but this imposes a difficult learning task. In this work, we show that it is critical to accurately estimate both the structures and the details for low-level vision tasks.</p><p>Image dehazing. In image dehazing, existing CNN-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28]</ref> mainly focus on estimating the transmission map from an input. Given an estimated transmission map, the atmospheric light can be computed using the air light model. As such, errors in the transmission maps are propagated into the light estimation process. For more accurate results, it is necessary to jointly estimate the transmission map and atmospheric light in one model, which DualCNNs are designed for.</p><p>A common theme is that we need to design a new network for every low-level vision task. In this paper, we show that low-level vision problems usually involve the estimation of two components: structures and details. Thus we develop a single framework, called DualCNN, that can be flexibly applied to a variety of low-level vision problems, including the four tasks discussed above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Algorithm</head><p>As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the proposed dual model consists of two branches, Net-S, and output of Net-D, which respectively estimate the structure and detail components of the target signals from the input. Take image super-resolution as an example. Given a low-resolution image, we first use the bicubic upsampled image as the input. Then, our dual network learns the details and structures according to the formulation model of the image decomposition. Dual composition loss function. Let X, S, and D denote the ground truth label, output of Net-S, and output of Net-D, respectively. The dual composition loss function enforces that the recovered structure S and detail D can generate the ground truth label X using the given formation model:</p><formula xml:id="formula_0">L x (S, D) = φ(S) + ϕ(D) − X 2 2 ,<label>(1)</label></formula><p>where the forms of the functions φ(·) and ϕ(·) are known and depend on the domain knowledge of each task. For example, the functions φ(·) and ϕ(·) are identity functions for image decomposition problems (e.g., filtering) and restoration problems (e.g., super-resolution, denoising, and deraining). We will show that φ(·) and ϕ(·) can take more general forms to deal with specific problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Regularization of the DualCNN Model</head><p>The proposed DualCNN model has two branches, which may cause instability if only the composition loss (1) is used. For example, if Net-S and Net-D have the same structure, symmetrical solutions exist. To obtain a stable solution, we use individual loss functions to regularize the two branches respectively. The loss functions for the Net-S and Net-D are defined as</p><formula xml:id="formula_1">L s (S) = S − S gt 2 2 ,<label>(2)</label></formula><formula xml:id="formula_2">L d (D) = D − D gt 2 2 ,<label>(3)</label></formula><p>where S gt and D gt are ground truths corresponding to the outputs of Net-S and Net-D. Consequently the overall loss function to train DualCNN is</p><formula xml:id="formula_3">L = αL x + λL s + γL d ,<label>(4)</label></formula><p>where α, λ and γ are non-negative trade-off weights. Our framework can also use other loss functions, e.g., perceptual loss for style transfer. We use the SGD method to minimize the loss function (4) and train a DualCNN. In the training stage, the gradients for Net-S and Net-D can be obtained by </p><formula xml:id="formula_4">∂L ∂S = 2αφ ′ (S)E + 2λ(S − S gt ),<label>(5a)</label></formula><formula xml:id="formula_5">∂L ∂D = 2αϕ ′ (D)E + 2γ(D − D gt ),<label>(5b)</label></formula><p>where</p><formula xml:id="formula_6">E = φ(S) + ϕ(D) − X, φ ′ (S) and ϕ ′ (D)</formula><p>are the derivatives with respect to S and D.</p><p>In the test stage, we compute the high-quality output X est using the outputs of Net-S and Net-D according to the formation model,</p><formula xml:id="formula_7">X est = φ(S) + ϕ(D).<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Generalization</head><p>Aside from image decomposition and restoration problems, the proposed model can handle other low-level vision problems by modifying the composition loss function (1). Here we use image dehazing as an example. Image dehazing. The image dehazing model can be described using the air light model,</p><formula xml:id="formula_8">I = JD + S(1 − D),<label>(7)</label></formula><p>where I is the hazy image, J is the haze-free image, S is the atmospheric light, and D is the medium transmission map, which describes the portion of the light that reaches the camera from scene surfaces. With the formulation model (7), we can set φ(S) = S(1 − D) and ϕ(D) = JD in (1) within the DualCNN framework. As a result, the composition loss function (1) for image dehazing becomes</p><formula xml:id="formula_9">L x (S, D) = JD + S(1 − D) − I 2 2 .<label>(8)</label></formula><p>The other two loss functions <ref type="formula" target="#formula_1">(2)</ref> and <ref type="formula" target="#formula_2">(3)</ref> remain the same. In the training phase, we use the same method <ref type="bibr" target="#b27">[28]</ref> to generate the atmospheric light S, the transmission map D and construct hazy/haze-free image pairs. The implementation details of the training stage are presented in Section 4.4.</p><p>In the test phase, the clear image J est can be reconstructed by the outputs of Net-D and Net-S, i.e.,</p><formula xml:id="formula_10">J est = I − S max{D, d 0 } + S,<label>(9)</label></formula><p>where d 0 is used to prevent division by zero and a typical value is 0.1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We evaluate DualCNNs on several low-level vision tasks including super-resolution, edge-preserving smoothing, deraining and dehazing. The main results are presented in this section and more results can be found in the supplementary material. The trained models are publicly available on the authors' websites. Network parameters. Motivated by the success of SRC-NN and VDSR for super-resolution, we use 3 convolution layers followed by the ReLU function for the network Net-S. The filter sizes of each layer are 9 × 9, 1 × 1, and 5 × 5, respectively. The depths of each layer are 64, 32, and 1, respectively. For the network Net-D, we use 20 convolution layers followed by the ReLU function. The filter size of each layer is 3 × 3 and the depth of each layer is 64. The batch size is set to be 64 and the learning rate is 10 −4 . Although each branch of the proposed model is similar to SRCNN or VDSR, both our analysis and experimental results show that the proposed model is significantly different from these methods and achieves better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Super-resolution</head><p>Training data. For image super-resolution, we generate the training data by randomly sampling 250 thousands 41 × 41 patches from 291 natural images in <ref type="bibr" target="#b24">[25]</ref>. We apply the Gaussian filter to each ground truth label X to obtain S gt . The ground truth D gt is the difference between the ground truth label X and the structure S gt .</p><p>For this application, we set φ(S) = S and ϕ(D) = D. The weights α, λ and γ in the loss function (4) are set to be 1, 0.001 and 0.01, respectively. To increase the accuracy, we use the pre-trained models of SRCNN and VDSR as the initializations of Net-S and Net-D.</p><p>We present quantitative and qualitative comparisons against the state-of-the-art methods including A+ <ref type="bibr" target="#b33">[34]</ref>, SelfEx <ref type="bibr" target="#b13">[14]</ref>, SRCNN <ref type="bibr" target="#b4">[5]</ref>, ESPCN <ref type="bibr" target="#b28">[29]</ref>, SRGAN <ref type="bibr" target="#b19">[20]</ref>, and VD-SR <ref type="bibr" target="#b16">[17]</ref>. <ref type="table" target="#tab_0">Table 1</ref> shows quantitative evaluations on benchmark datasets. Overall, the proposed method performs fa-    ing the effectiveness of the proposed dual model. <ref type="figure" target="#fig_1">Figure 3</ref> shows some super-resolution results by the evaluated methods. The proposed algorithm can well preserve the main structures than state-of-the-art methods. Running time. We benchmark the running time of all methods on a machine with an Intel Core i7-7700 CPU and an NVIDIA GTX 1080 GPU. <ref type="table">Table 2</ref> shows that the running time of the DualCNN model is comparable to VD-SR, which achieves state-of-the-art results on the superresolution benchmark dataset <ref type="bibr" target="#b16">[17]</ref>.</p><formula xml:id="formula_11">(a) GT (b) Bicubic (c) SelfEx (d) SRCNN (e) VDSR (f) ESPCN (g) SRGAN (h) Ours</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Edge-preserving Filtering</head><p>Similar to the methods in <ref type="bibr" target="#b37">[38]</ref> and <ref type="bibr" target="#b23">[24]</ref>, we apply the DualCNN to learn edge preserving image filters including L 0 smoothing <ref type="bibr" target="#b35">[36]</ref>, relative total variation (RTV) <ref type="bibr" target="#b38">[39]</ref>, and weighted median filter (WMF) <ref type="bibr" target="#b40">[41]</ref>. We generate the training data by randomly sampling 1 million patches (clear/filtered pairs) from 200 natural images in <ref type="bibr" target="#b24">[25]</ref>. Each image patch is of 64 × 64 pixels, and other settings of generating training data are the same as those used in <ref type="bibr" target="#b37">[38]</ref>.</p><p>For this application, as our goal is to learn the filtered image which does not contain rich details, we set weights α, λ, and γ in the loss function (4) to be 1, 10 −4 and 0, respectively. We further let S gt be the ground truth label X.</p><p>We evaluate the proposed DualCNN model against methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b23">24]</ref> using the dataset from <ref type="bibr" target="#b37">[38]</ref>. <ref type="table" target="#tab_1">Table 3</ref> summarizes the PSNR results. Note that Xu et al. <ref type="bibr" target="#b37">[38]</ref> use image gradients to train their model and the final results are reconstructed by solving a constrained optimization problem. Thus it performs better for approximating L 0 smoothing. However, our method does not need these additional steps and generates high quality filtered images with significant improvements over the state-of-the-art deep learning based methods, particularly on RTV and WMF.</p><p>We note that the architecture of Net-D is similar to that of VDSR. As such, we retrain the network of VDSR for these problems. The results in <ref type="table" target="#tab_1">Table 3</ref> suggest that only using residual learning does not always generate high-quality filtered images. <ref type="figure" target="#fig_2">Figure 4</ref> shows the filtering results of approximating RTV <ref type="bibr" target="#b38">[39]</ref>. The state-of-the-art methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b23">24]</ref> fail to smooth the structures (e.g., the eyes in the green boxes) that are supposed to be removed using the RTV filter <ref type="figure" target="#fig_2">(Figure 4(f)</ref>). In addition, the results with only one branch (i.e., Net-S) have lower PSNR values <ref type="table" target="#tab_1">(Table 3</ref>) and some remaining tiny structures <ref type="figure" target="#fig_2">(Figure 4(d)</ref>). In contrast, joint learning structures and details preserves more accurate results and the filtered images are significantly closer to the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Image Deraining</head><p>Deraining aims to recover clear contents from rainy images. This process can be regarded as recovering the clear details (rainy streaks) and structures (clear images) from inputs. We evaluate the proposed DualCNN on this task.</p><p>To train the proposed DualCNN for image deraining, we generate the training data by randomly sampling 1 million patches (rainly/clear pairs) from the rainy image dataset used in <ref type="bibr" target="#b39">[40]</ref>. The size of each image patch used in training stage is 64 × 64 pixels. Following settings used in learning image filtering, we let S gt be the ground truth label X (i.e., clear image patch). The weights α, λ and γ in the loss function (4) are set to be 1, 0.01, and 0, respectively. We use the test dataset <ref type="bibr" target="#b39">[40]</ref> to evaluate the effectiveness of the proposed method. <ref type="table" target="#tab_2">Table 4</ref> shows the average PSNR values of restored images on the test dataset <ref type="bibr" target="#b39">[40]</ref>. Overall, the proposed method generates the results with the highest PSNR values. <ref type="figure" target="#fig_3">Figure 5</ref> shows deraining results from the evaluated methods. The proposed algorithm can accurately estimate both clear details and structures from the input image. The plain CNN-based methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b39">[40]</ref> and Net-S all generate results with obvious rainy streaks, demonstrating the advantage of simultaneously recovering structures and details using the DualCNN.</p><p>We further evaluate DualCNN using real examples. <ref type="figure" target="#fig_4">Figure 6</ref> shows a real example. We note that the algorithm in <ref type="bibr" target="#b9">[10]</ref> develops a deep details network for image deraining. The derained images are obtained by extracting details from input. However, this method depends on whether    the image decomposition method is able to extract details or not. The results shown in <ref type="figure" target="#fig_4">Figure 6</ref>(c) demonstrate the algorithm in <ref type="bibr" target="#b9">[10]</ref> fails to generate clearer images. In contrast, our method generates much clearer results compared to state-of-the-art algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Image Dehazing</head><p>As discussed in Section 3.2, the proposed method can be applied to the image dehazing. Similar to the method in <ref type="bibr" target="#b27">[28]</ref>, we synthesize the hazy image dataset using the NYU depth dataset <ref type="bibr" target="#b29">[30]</ref> and generate the training data by randomly sampling 1 million patches including hazy/clear pairs (I/J), atmospheric light (S), transmission map (D). The size of each image patch used in training stage is 32×32 pixels. The weights α, λ and γ are set to be 0.1, 0.9, and 0.9, respectively.</p><p>We quantitatively evaluate our method on the synthetic hazy images <ref type="bibr" target="#b27">[28]</ref>. As summarized in <ref type="table" target="#tab_3">Table 5</ref>, the proposed method performs favorably against the state-of-theart methods for image dehazing. The dehazed images in <ref type="figure">Figure 7</ref> show that the proposed method can recover the atmospheric light <ref type="figure">(Figure 7</ref>(e)) and transmission map <ref type="figure">(Figure 7(f)</ref>) well, thereby facilitating to recover the clear image <ref type="figure">(Figure 7(g)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis and Discussion</head><p>In this section, we further analyze the proposed method and compare it with the most related methods. Effect of the architectures of DualCNN. Lin et al. <ref type="bibr" target="#b22">[23]</ref> develop a bilinear model to extract complementary features for fine-grained visual recognition. By contrast, the proposed DualCNN is motivated by the decomposition of a signal into structures and details. More importantly, the formulation of the proposed model facilitates incorporating the domain knowledge of each individual application. Thus, the DualCNN model can be effectively applied to numerous low-level vision problems, e.g., super-resolution, image filtering, deraining, and dehazing.</p><p>Numerous deep learning methods have been developed based on a single branch for low-level vision problems, e.g., SRCNN <ref type="bibr" target="#b4">[5]</ref> and VDSR <ref type="bibr" target="#b16">[17]</ref>. One natural question is why  <ref type="bibr" target="#b11">[12]</ref> (c) Tarel et al. <ref type="bibr" target="#b32">[33]</ref> (d) Cai et al. <ref type="bibr" target="#b1">[2]</ref> (d) Ren et al. <ref type="bibr" target="#b27">[28]</ref> (e) Estimated S (f) Estimated D (g) Ours <ref type="figure">Figure 7</ref>. Visual comparisons for image dehazing. In contrast to the CNN-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28]</ref> which additionally use conventional methods to estimate atmosphere light, the proposed method directly estimates the atmosphere light in (e) and transmission map in (f) and thus leads to comparable results.  <ref type="table" target="#tab_4">Table 6</ref>.</p><p>Net-D Net-S Input Output <ref type="figure">Figure 9</ref>. An alternative cascaded architecture that estimates the structure and details sequentially.  Sample results using the VDSR model are shown in <ref type="figure" target="#fig_6">Figure 8</ref>. While the residual learning (i.e., VDSR) approach performs better than the SRCNN, the generated images with the plain CNN model <ref type="bibr" target="#b4">[5]</ref> contain blurry boundaries or rainy streaks <ref type="figure" target="#fig_6">(Figure 8(d)</ref>).</p><p>Although the proposed DualCNN consists of two branches, an alternative is to combine the Net-S and Net-D in a cascaded manner as shown in <ref type="figure">Figure 9</ref>. In this cascade model, the first stage estimates the main structure while the second stage estimates details. This network architecture is similar to the ResNet <ref type="bibr" target="#b12">[13]</ref>. However, this cascaded architecture does not generate high-quality results compared to the proposed DualCNN <ref type="figure" target="#fig_6">(Figure 8</ref>(e) and <ref type="table" target="#tab_4">Table 6</ref>). Effect of the loss functions in DualCNN. We evaluate the effects of different loss functions on image dehazing. Table 7 shows that adding two regularization losses L s in <ref type="bibr" target="#b1">(2)</ref> and L d in (3) significantly improves the performance. Different architectures in DualCNN. We have used different network structures for the two branches of DualCNNs in the experiments in Section 4. It is interesting to test using the same structures for the two branches of a Dual-CNN. To this end, we set the two branches in a DualCNN using the network structures of SRCNN <ref type="bibr" target="#b4">[5]</ref> and train the DualCNN according to the same settings used in the image super-resolution experiment. The trained DualCNN generates the results with higher average PSNR/SSIM values (30.3690/0.8603) than those of SRCNN (30.1496/0.8551) for ×4 upsampling on the "Set5" dataset.</p><p>We further quantitatively evaluate the DualCNN when the two branches are the same on image deraining using synthetic rainy dataset <ref type="bibr" target="#b39">[40]</ref>. Similar to the image super-  <ref type="figure">Figure 10</ref>. Quantitative evaluation of the convergence property on the super-resolution dataset (Set5, ×2).</p><p>resolution experimental settings, the two branches in the DualCNN are set to be the network structures of SRCN-N <ref type="bibr" target="#b4">[5]</ref> (SDCNN-S) and the network structures of VDSR <ref type="bibr" target="#b16">[17]</ref> (SDCNN-D), respectively. <ref type="table" target="#tab_6">Table 8</ref> shows that DualCNN with deeper model generates better results when the architectures of two branches are the same. However, the DualCNN where one branch is SRCNN and the other one is VDSR performs better than SDCNN-D. This is mainly because the main structures of the input images are similar to those of output images. Deeper model used in "net-S" will introduce errors in the learning stage.</p><p>Convergence property. We quantitatively evaluate convergence properties of our method on the super-resolution dataset, i.e., Set5. Although the proposed network contains two branches compared to other methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17]</ref>, it has the similar convergency property to the SRCNN and VDSR as shown in <ref type="figure">Figure 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a novel dual convolutional neural network for low-level vision tasks, called DualCNN. From an input signal, the DualCNN recovers both the structure and detail components, which can generate the target signal according to the problem formulation for a specific task. We analyze the effect of the DualCNN and show that it is a generic framework and can be effectively and efficiently applied to numerous low-level vision tasks, including image super-resolution, filtering, image deraining, and image dehazing. Experimental results show that the DualC-NN performs favorably against state-of-the-art methods that have been specially designed for each task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Proposed DualCNN model. It contains two branches, Net-D and Net-S, and a problem formulation module. A DualCNN first estimates the structures and the details and then reconstructs the final results according to the formulation module. The whole network is end-to-end trainable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Visual comparisons for super-resolution (×2). The state-of-the-art methods do not preserve the main structures of the images, while the proposed method generates a better result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Visual comparisons for learning the relative total variation (RTV) image filters. Existing deep learning based methods are not able to remove the details and structures that are supposed to be removed (the green boxes in (a) and (b)). (c) and (d) show the outputs of the two branches of the proposed model. (f) is the result by the original implementation of RTV. Better enlarge and view on a screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Visual comparisons for image deraining. The proposed method is able to remove rainy streaks from the input image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Visual comparisons of deep learning-based methods for image deraining on real examples. The proposed method is able to remove rainy streaks from the input image and generates much better images with fine details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Effectiveness of the proposed dual model. (c)-(f) show the comparisons between existing CNNs (including plain net and ResNet) and the proposed net in edge-preserving filtering and image deraining. The plain net (i.e., (c)), ResNet and its deeper version (i.e., (d) and (e)) generate results with significant artifacts. Quantitative evaluations are included in Table 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Quantitative evaluations for the state-of-the-art super-resolution methods on the benchmark datasets (Set5, Set14, Urban100, and BSDS500) in terms of PSNR and SSIM.</figDesc><table>Dataset 
Scale 
Bicubic 
A+ 
SelfEx 
SRCNN 
ESPCN 
VDSR 
SRGAN 
Ours 
PSNR/SSIM 
PSNR/SSIM 
PSNR/SSIM 
PSNR/SSIM 
PSNR/SSIM 
PSNR/SSIM 
PSNR/SSIM 
PSNR/SSIM 

Set5 

×2 
33.6924/0.9308 
36.575/0.9546 
36.5392/0.9537 36.4191/0.9531 36.7315/0.9547 37.6173/0.9596 37.0098/0.9548 
37.7005/0.9600 
×3 
30.4396/0.8694 32.6866/0.9097 32.6759/0.9099 32.4957/0.9049 32.6880/0.9077 33.7571/0.9229 33.5384/0.9170 
33.8003/0.9234 
×4 
28.4528/0.8116 30.3471/0.8623 30.3458/0.8636 30.1496/0.8551 30.2730/0.8540 31.4657/0.8863 
31.3496/0.8797 
31.4778/0.8860 

Set14 

×2 
30.2660/0.8687 32.3497/0.9051 32.2657/0.9029 32.2934/0.9040 32.4020/0.9056 33.2037/0.9131 32.6889/0.9049 
33.2334/0.9131 
×3 
27.5556/0.7731 29.1602/0.8181 29.1743/0.8190 29.0676/0.8147 29.1161/0.8161 29.8720/0.8319 
29.5351/0.8227 
29.8822/0.8315 
×4 
26.0089/0.7006 27.3238/0.7481 27.3956/0.7509 27.2283/0.7402 27.1681/0.7401 28.0667/0.7671 
27.8353/0.7588 
28.0949/0.7669 

Urban100 

×2 
26.8621/0.8400 28.5485/0.8782 29.5317/0.8962 29.1009/0.8896 29.2381/0.8920 30.7897/0.9144 29.4390/0.8745 
30.8273/0.9145 
×3 
24.4375/0.7336 25.7907/0.7878 26.4188/0.8079 25.8549/0.7874 25.9170/0.7897 27.1297/0.8278 
26.6243/0.8159 
27.1318/0.8277 
×4 
23.1158/0.6551 24.1890/0.7119 24.7648/0.7361 24.1275/0.7030 24.1534/0.7031 25.1575/0.7515 
24.1516/0.7298 
25.1722/0.7510 

BSDS500 

×2 
29.6393/0.8622 30.8758/0.8929 31.3447/0.9016 31.3319/0.9013 31.4187/0.9030 32.2226/0.9136 31.3938/0.8889 
32.2458/0.9140 
×3 
27.1875/0.7626 28.1461/0.8024 28.2960/0.8073 28.2233/0.8033 28.2404/0.8048 28.8889/0.8229 28.6354/0.8159 
28.8979/0.8232 
×4 
25.8953/0.6931 26.6798/0.7324 26.7851/0.7368 26.6595/0.7278 26.6122/0.7278 27.2342/0.7525 26.9104/0.7423 
27.2454/0.7527 

Table 2. Average running time (seconds) of the evaluated methods on the test dataset [17]. 

Methods 
A+ 
SelfEx SRCNN VDSR Ours 

Average running time 
0.88 
99.04 
0.55 
4.85 
5.19 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 3 .</head><label>3</label><figDesc>PSNR results for learning various image filters on the test dataset [37].</figDesc><table>Xu et al. [38] 
Liu et al. [24] 
VDSR [17] 
Net-S Ours 

L 0 
32.8 
30.9 
31.5 
28.0 
31.4 
WMF 
31.4 
34.0 
38.5 
29.2 
39.1 
RTV 
32.1 
37.1 
41.6 
32.0 
42.1 

vorably against the state-of-the-art methods. Note that the 
architecture of one branch in a DualCNN is either similar 

to SRCNN or VDSR. However, the results generated by 
a DualCNN have highest average PSNR values, suggest-</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>Quantitative comparison using the synthetic rainy dataset [40]. 

Methods 
SPM [16] 
PRM [3] 
CNN [9] 
GMM [21] 
ID-CGAN [40] 
Net-S 
Ours 

Avg. PSNR 
18.88 
20.46 
19.12 
22.27 
22.73 
22.18 
24.11 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 .</head><label>5</label><figDesc>Quantitative comparison using the synthetic hazy image dataset [28].</figDesc><table>Methods 
He et al. [12] 
Meng et al. [26] 
Ren et al. [28] 
Ours 

Avg. PSNR 
15.86 
15.06 
18.38 
18.85 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 6 .</head><label>6</label><figDesc>Quantitative evaluation of different networks on the image filtering [38] and deraining [40] datasets in terms of PSNR.</figDesc><table>Different nets SRCNN VDSR Cascade Ours 

Filtering 
32.0 
41.6 
42.0 
42.1 
Deraining 
22.3 
23.9 
23.5 
24.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 7 .</head><label>7</label><figDesc>Quantitative evaluation of the proposed dual composition loss function on the validation data of image dehazing in terms of PSNR and SSIM.deeper architectures do not necessarily lead to better perfor- mance. In principle, a sufficiently deep neural network has sufficient capacity to solve any problem given enough train- ing data. However, it is non-trivial to learn very deep CNN models for these problems while ensuring high efficiency and simplicity. For experimental validation, we use the SRCNN and a deeper model, i.e., VDSR, for image filtering and deraining. The experimental settings are discussed in Section 4.</figDesc><table>(λ/α, γ/α) 
(0, 0) 
(1, 0) 
(0, 1) 
(9, 9) 

Avg. PSNR 
21.13 
26.27 
26.45 
26.43 
Avg. SSIM 
0.7449 
0.8987 
0.9139 
0.9108 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 8 .</head><label>8</label><figDesc>Quantitative evaluation of two branches in DualCNN us- ing the synthetic rainy dataset [40]. Two branches SDCNN-S SDCNN-D Ours</figDesc><table>Avg. PSNR 
22.42 
23.58 
24.11 

0 
20 
40 
60 
80 
100 

Training Epoch 

33 

34 

35 

36 

37 

Average PSNRs 

Ours 
VDSR 
SRCNN 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image denosing: Can plain neural networks compete with BM3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dehazenet: An end-to-end system for single image haze removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5187" to="5198" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A generalized low-rank appearance model for spatio-temporally correlated rain streaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-T</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1968" to="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image superresolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accelerating the superresolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Restoring an image taken through a window covered with dirt or rain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="633" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Clearing the skies: A deep network architecture for single-image rain removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Removing rain from single images via a deep detail network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1956" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Natural image denoising with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic singleimage-based rain streaks removal via image decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1742" to="1755" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rain streak removal using layer priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2736" to="2744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video superresolution via deep draft-ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="531" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bilinear CNN models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Roy</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning recursive filters for low-level vision via a hybrid neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="560" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient image dehazing with boundary constraint and contextual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="617" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Shepard convolutional neural networks. In NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Single image dehazing via multi-scale convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generalized deep transfer networks for knowledge propagation in heterogeneous domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno>68:1-68:22</idno>
	</analytic>
	<monogr>
		<title level="j">TOMCCAP</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4s</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Vision enhancement in homogeneous and heterogeneous fog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hautière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Caraffa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Halmaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gruyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Transport. Syst. Mag</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="6" to="20" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A+: adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">D</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="350" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image smoothing via L 0 gradient minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno>174:1-174:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1790" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep edgeaware filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1669" to="1678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Structure extraction from texture via relative total variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Image de-raining using a conditional generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno>abs/1701.05957</idno>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">100+ times faster weighted median filter (WMF)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2830" to="2837" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
