<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Face Age Progression: A Pyramid Architecture of GANs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
							<email>yhwang@buaa.edu.cnjain@cse.msu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Face Age Progression: A Pyramid Architecture of GANs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The two underlying requirements of face age progression, i.e. aging   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Age progression is the process of aesthetically rendering a given face image to present the effects of aging. It is often used in entertainment industry and forensics, e.g., forecasting facial appearances of young children when they grow up or generating contemporary photos for missing individuals.</p><p>The intrinsic complexity of physical aging, the interferences caused by other factors (e.g., PIE variations), and shortage of labeled aging data collectively make face age progression a rather difficult problem. The last few years have witnessed significant efforts tackling this issue, where aging accuracy and identity permanence are commonly regarded as the two underlying premises of its success <ref type="bibr" target="#b27">[29]</ref>[36] <ref type="bibr" target="#b24">[26]</ref> <ref type="bibr" target="#b12">[14]</ref>. The early attempts were mainly based on the skin's anatomical structure and they mechanically simulated the profile growth and facial muscle changes w.r.t. the elapsed time <ref type="bibr" target="#b29">[31]</ref>[35] <ref type="bibr" target="#b21">[23]</ref>. These methods provided the first insight into face aging synthesis. However, they generally worked in a complex manner, making it difficult to generalize. Data-driven approaches followed, where face age progression was primarily carried out by applying the prototype of aging details to test faces <ref type="bibr" target="#b11">[13]</ref> <ref type="bibr" target="#b27">[29]</ref>, or by modeling the dependency between longitudinal facial changes and corresponding ages <ref type="bibr" target="#b26">[28]</ref>[34] <ref type="bibr" target="#b18">[20]</ref>. Although obvious signs of aging were synthesized well, their aging functions usually could not formulate the complex aging mechanism accurately enough, limiting the diversity of aging patterns.</p><p>The deep generative networks have exhibited a remarkable capability in image generation <ref type="bibr" target="#b6">[8]</ref> <ref type="bibr" target="#b7">[9]</ref>[11] <ref type="bibr" target="#b28">[30]</ref> and have also been investigated for age progression <ref type="bibr" target="#b31">[33]</ref>[37] <ref type="bibr" target="#b16">[18]</ref> <ref type="bibr" target="#b17">[19]</ref>. These approaches render faces with more appealing aging effects and less ghosting artifacts compared to the previous conventional solutions. However, the problem has not been essentially solved. Specifically, these approaches focus more on modeling face transformation between two age groups, where the age factor plays a dominant role while the identity information plays a subordinate role, with the result that aging accuracy and identity permanence can hardly be simultaneously achieved, in particular for long-term age progression <ref type="bibr" target="#b16">[18]</ref> <ref type="bibr" target="#b17">[19]</ref>. Furthermore, they mostly require multiple face images of different ages of the same individual at the training stage, involving another intractable issue, i.e. intra-individual aging face sequence collection <ref type="bibr" target="#b31">[33]</ref> <ref type="bibr" target="#b13">[15]</ref>. Both the aforementioned facts indicate that current deep generative aging methods leave room for improvement.</p><p>In this study, we propose a novel approach to face age progression, which integrates the advantage of Generative Adversarial Networks (GAN) in synthesizing visually plausible images with prior domain knowledge in human aging. Compared with existing methods in literature, it is more capable of handling the two critical requirements in age progression, i.e. identity permanence and aging accuracy. To be specific, the proposed approach uses a Convolutional Neural Networks (CNN) based generator to learn age transformation, and it separately models different face attributes depending upon their changes over time. The training critic thus incorporates the squared Euclidean loss in the image space, the GAN loss that encourages generated faces to be indistinguishable from the elderly faces in the training set in terms of age, and the identity loss which minimizes the input-output distance by a high-level feature representation embedding personalized characteristics. It ensures that the resulting faces present desired effects of aging while the identity properties remain stable. By estimating the data density of each individual target age cluster, our method does not demand matching face pairs of the same person across two age domains as the majority of the counterpart methods do. Additionally, in contrast to the previous techniques that primarily operate on cropped facial areas (usually excluding foreheads), we emphasize that synthesis of the entire face is important since the parts of forehead and hair also significantly impact the perceived age. To achieve this and further enhance the aging details, our method leverages the intrinsic hierarchy of deep networks, and a discriminator of the pyramid architecture is designed to estimate high-level age-related clues in a fine-grained way. Our approach overcomes the limitations of single age-specific representation and handles age transformation both locally and globally. As a result, more photorealistic imageries are generated (see <ref type="figure" target="#fig_0">Fig. 1</ref> for an illustration of aging results).</p><p>The main contributions of this study include:</p><p>(1) We propose a novel GAN based method for age progression, which incorporates face verification and age estimation techniques, thereby addressing the issues of aging effect generation and identity cue preservation in a coupled manner; (2) We highlight the importance of the forehead and hair components of a face that are closely related to the perceived age but ignored in other studies; it indeed enhances the synthesized age accuracy; (3) We set up new validating experiments in addition to existent ones, including commercial face analysis tool based evaluation and insensitivity assessment to the changes in expression, pose, and makeup. Our method is not only shown to be effective but also robust to age progression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the initial explorations of face age progression, physical models were exploited to simulate the aging mechanisms of cranium and facial muscles. Todd et al. <ref type="bibr" target="#b29">[31]</ref> introduced a revised cardioidal-strain transformation where head growth was modeled in a computable geometric procedure. Based on skin's anatomical structure, Wu et al. <ref type="bibr" target="#b33">[35]</ref> proposed a 3-layered dynamic skin model to simulate wrinkles. Mechanical aging methods were also incorporated by Ramanathan and Chellappa <ref type="bibr" target="#b21">[23]</ref> and Suo et al. <ref type="bibr" target="#b26">[28]</ref>.</p><p>The majority of the subsequent approaches were datadriven, which did not rely much on the biological prior knowledge, and the aging patterns were learned from the training faces. Wang et al. <ref type="bibr" target="#b32">[34]</ref> built the mapping between corresponding down-sampled and high-resolution faces in a tensor space, and aging details were added on the latter. Kemelmacher-Shlizerman et al. <ref type="bibr" target="#b11">[13]</ref> presented a prototype based method, and further took the illumination factor into account. Yang et al. <ref type="bibr" target="#b34">[36]</ref> first settled the multi-attribute decomposition problem, and progression was achieved by transforming only the age component to a target age group. These methods did improve the results, however ghosting artifacts frequently appeared in the synthesized faces.</p><p>More recently, the deep generative networks have been attempted. In <ref type="bibr" target="#b31">[33]</ref>, Wang et al. transformed faces across different ages smoothly by modeling the intermediate transition states in an RNN model. But multiple face images of various ages of each subject were required at the training stage, and the exact age label of the probe face was needed during test, thus greatly limiting its flexibility. Under the framework of conditional adversarial autoencoder <ref type="bibr" target="#b35">[37]</ref>, facial muscle sagging caused by aging was simulated, whereas only rough wrinkles were rendered mainly due to the insufficient representation ability of the training discriminator. With the Temporal Non-Volume Preserving (TNVP) aging approach <ref type="bibr" target="#b16">[18]</ref>, the short-term age progression was accomplished by mapping the data densities of two consecutive age groups with ResNet blocks <ref type="bibr" target="#b8">[10]</ref>, and the long-term aging synthesis was finally reached by a chaining of short-term stages. Its major weakness, however, was that it merely considered the probability distribution of a set of faces without any individuality information. As a result, the synthesized faces in a complete aging sequence varied a lot in color, expression, and even identity.</p><p>Our study also makes use of the image generation ability of GAN, and presents a different but effective method, where the age-related GAN loss is adopted for age transformation, the individual-dependent critic is used to keep the identity cue stable, and a multi-pathway discriminator is applied to refine aging detail generation. This solution is more powerful in dealing with the core issues of age progression, i.e. age accuracy and identity preservation.   <ref type="figure">Figure 2</ref>. Framework of the proposed age progression method. A CNN based generator G learns the age transformation. The training critic incorporates the squared Euclidean loss in the image space, the GAN loss that encourages generated faces to be indistinguishable from the training elderly faces in terms of age, and the identity preservation loss minimizing the input-output distance in a high-level feature representation which embeds the personalized characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identity Preservation Loss</head><p>cess. The generative function G tries to capture the underlying data density and confuse the discriminative function D, while the optimization procedure of D aims to achieve the distinguishability and distinguish the natural face images from the fake ones generated by G. Both G and D can be approximated by neural networks, e.g., Multi-Layer Perceptron (MLP). The risk function of optimizing this minimax two-player game can be written as:</p><formula xml:id="formula_0">V(D, G) = min G max D E x∼P data (x) log[D(x)]+E z∼Pz (z) log[1−D(G(z))]<label>(1)</label></formula><p>where z is a noise sample from a prior probability distribution P z , and x denotes a real face image following a certain distribution P data . On convergence, the distribution of the synthesized images P g is equivalent to P data . Recently, more emphasis has been given to the conditional GANs (cGANs) where the generative model G approximates the dependency of the pre-images (or controlled attributes) and their corresponding targets. cGANs have shown promising results in video prediction <ref type="bibr" target="#b15">[17]</ref>, text to image synthesis <ref type="bibr" target="#b22">[24]</ref>, image-to-image translation <ref type="bibr" target="#b9">[11]</ref> <ref type="bibr" target="#b36">[38]</ref>, etc. In our case, the CNN based generator takes young faces as inputs, and learns a mapping to a domain corresponding to elderly faces. To achieve aging effects while simultaneously maintaining person-specific information, a compound critic is exploited, which incorporates the traditional squared Euclidean loss in the image space, the GAN loss that encourages generated faces to be indistinguishable from the training elderly faces in terms of age, and the identity loss minimizing the input-output distance in a high-level feature representation which embeds the personalized characteristics. See <ref type="figure">Fig. 2</ref> for an overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Generator</head><p>Synthesizing age progressed faces only requires a forward pass through G. The generative network is a combination of encoder and decoder. With the input young face, it first exploits three strided convolutional layers to encode it to a latent space, capturing the facial properties that tend to be stable w.r.t. the elapsed time, followed by four residual blocks <ref type="bibr" target="#b8">[10]</ref> modeling the common structure shared by the input and output faces, similar to the settings in <ref type="bibr" target="#b10">[12]</ref>. Age transformation to a target image space is finally achieved by three fractionally-strided convolutional layers, yielding the age progression result conditioned on the given young face. Rather than using the max-pooling and upsampling layers to calculate the feature maps, we employ the 3 × 3 convolution kernels with a stride of 2, ensuring that every pixel contributes and the adjacent pixels transform in a synergistic manner. All the convolutional layers are followed by Instance Normalization and ReLU non-linearity activation. Paddings are added to the layers to make the input and output have exactly the same size. The architecture of G is shown in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discriminator</head><p>The system critic incorporates the prior knowledge of the data density of the faces from the target age cluster, and a discriminative network D is thus introduced, which outputs a scalar D(x) representing the probability that x comes from the data. The distribution of the generated faces P g (we denote the distribution of young faces as x ∼ P young , then G(x) ∼ P g ) is supposed to be equivalent to the distribution P old when optimality is reached. Supposing that we follow the classic GAN <ref type="bibr" target="#b7">[9]</ref>, which uses a binary cross entropy classification, the process of training D amounts to minimizing the loss:</p><formula xml:id="formula_1">L GAN D = −E x∈Pyoung (x) log[1 − D(G(x))] − E x∈P old (x) log[D(x)]<label>(2)</label></formula><p>It is always desirable that G and D converge coherently; however, D frequently achieves the distinguishability faster in practice, and feeds back vanishing gradients for G to learn, since the JS divergence is locally saturated. Recent studies, i.e. the Wasserstein GAN <ref type="bibr" target="#b3">[5]</ref>, the Least Squares GAN <ref type="bibr" target="#b14">[16]</ref>, and the Loss-Sensitive GAN <ref type="bibr" target="#b20">[22]</ref>, reveal that the most fundamental issue lies in how exactly the distance between sequences of probability distributions is defined. Here, we use the least squares loss substituting for the negative log likelihood objective, which penalizes the samples depending on how close they are to the decision boundary in a metric space, minimizing the Pearson X 2 divergence. Further, to achieve more convincing and vivid age-specific facial details, both the actual young faces and the generated age-progressed faces are fed into D as negative samples while the true elderly images as positive ones. Accordingly, the training process alternately minimizes the following:</p><formula xml:id="formula_2">L GAN D = 1 2 E x∼P old (x) [(Dω(φage(x)) − 1) 2 ] + 1 2 E x∼Pyoung (x) [Dω(φage(G(x))) 2 + Dω(φage(x)) 2 ]<label>(3)</label></formula><formula xml:id="formula_3">L GAN G = E x∼Pyoung (x) [(Dω(φage(G(x))) − 1) 2 ]<label>(4)</label></formula><p>Note, in <ref type="formula" target="#formula_2">(3)</ref> and <ref type="formula" target="#formula_3">(4)</ref>, a function φ age bridges G and D, which is especially introduced to extract age-related features conveyed by faces, as shown in <ref type="figure">Fig. 2</ref>. Considering that human faces at diverse age groups share a common configuration and similar texture properties, a feature extractor φ age is thus exploited independently of D, which outputs high-level feature representations to make the generated faces more distinguishable from the true elderly faces in terms of age. In particular, φ age is pre-trained for a multilabel classification task of age estimation with the VGG-16 structure <ref type="bibr" target="#b25">[27]</ref>, and after convergence, we remove the fully connected layers and integrate it into the framework. Since natural images exhibit multi-scale characteristics and along the hierarchical architecture, φ age captures the properties gradually from exact pixel values to high-level agespecific semantic information, this study leverages the intrinsic pyramid hierarchy. The pyramid facial feature representations are jointly estimated by D at multiple scales, handling aging effect generation in a fine-grained way.</p><p>The outputs of the 2nd, 4th, 7th and 10th convolutional layers of φ age are used. They pass through the pathways of D and finally result in a concatenated 12 × 3 representation. In D, all convolutional layers are followed by Batch Normalization and LeakyReLU activation except the last one in each pathway. The detailed architecture of D can be found in the supplementary material, and the joint estimation on the high-level features is illustrated in <ref type="figure" target="#fig_2">Fig. 3.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Identity Preservation</head><p>One core issue of face age progression is keeping the person-dependent properties stable. Therefore, we incorpo- rate the associated constraint by measuring the input-output distance in a proper feature space, which is sensitive to the identity change while relatively robust to other variations. Specifically, the network of deep face descriptor <ref type="bibr" target="#b19">[21]</ref> is utilized, denoted as φ id , to encode the personalized information and further define the identity loss function. φ id is trained with a large face dataset containing millions of face images from thousands of individuals * . It is originally bootstrapped by recognizing N = 2, 622 unique individuals; and then the last classification layer is removed and φ id (x) is tuned to improve the capability of verification in the Euclidean space using a triplet-loss training scheme. In our case, φ id is clipped to have 10 convolutional layers, and the identity loss is then formulated as:</p><formula xml:id="formula_4">L identity = E x∈Pyoung (x) d(φ id (x), φ id (G(x)))<label>(5)</label></formula><p>where d is the squared Euclidean distance between feature representations. For more implementation details of deep face descriptor, please refer to <ref type="bibr" target="#b19">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Objective</head><p>Besides the specially designed age-related GAN critic and the identity permanence penalty, a pixel-wise L2 loss in the image space is also adopted for further bridging the input-output gap, e.g., the color aberration, which is formulated as:</p><formula xml:id="formula_5">L pixel = 1 W × H × C G(x) − x 2 2<label>(6)</label></formula><p>where x denotes the input face and W , H, and C correspond to the image shape. Finally, the system training loss can be written as:</p><formula xml:id="formula_6">L G = λaL GAN G + λpL pixel + λiL identity (7) L D = L GAN D<label>(8)</label></formula><p>We train G and D alternately until optimality, and finally G learns the desired age transformation and D becomes a reliable estimator. * The face images are collected via the Google Image Search using the names of 5K celebrities, purified by automatic and manual filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Collection</head><p>The sources of face images for training GANs are the MORPH mugshot dataset <ref type="bibr" target="#b23">[25]</ref> with standardized imaging and the Cross-Age Celebrity Dataset (CACD) <ref type="bibr" target="#b5">[7]</ref> involving PIE variations.</p><p>An extension of the MORPH aging database <ref type="bibr" target="#b23">[25]</ref> contains 52,099 color images with near-frontal pose, neutral expression, and uniform illumination (some minor pose and expression variations are indeed present). The subject age ranges from 16 to 77 years old, with the average age being approximately 33. The longitudinal age span of a subject varies from 46 days to 33 years. CACD is a public dataset <ref type="bibr" target="#b5">[7]</ref> collected via the Google Image Search, containing 163,446 face images of 2,000 celebrities across 10 years, with age ranging from 14 to 62. The dataset has the largest number of images with age changes, showing variations in pose, illumination, expression, etc., with less controlled acquisition than MORPH. We mainly use MORPH and CACD for training and validation. FG-NET <ref type="bibr" target="#b2">[4]</ref> is also adopted for testing to make a fair comparison with prior work, which is popular in aging analysis but only contains 1,002 images from 82 individuals. More properties of these databases can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Prior to feeding the images into the networks, the faces are aligned using the eye locations provided by the dataset itself (CACD) or detected by the online face recognition API of Face++ <ref type="bibr" target="#b1">[3]</ref> (MORPH). Excluding those images undetected in MORPH, 163,446 and 51,699 face images from the two datasets are finally adopted, respectively, and they are cropped to 224 × 224 pixels. Due to the fact that the number of faces older than 60 years old is quite limited in both databases and neither contains images of children, we only consider adult aging. We follow the time span of 10 years for each age cluster as reported in many previous studies <ref type="bibr" target="#b34">[36]</ref>[29] <ref type="bibr" target="#b35">[37]</ref>[33] <ref type="bibr" target="#b16">[18]</ref>, and apply age progression on the faces below 30 years old, synthesizing a sequence of ageprogressed renderings when they are in their 30s, 40s, and 50s. Therefore, there are three separate training sessions for different target age groups.</p><p>The architectures of the networks G and D are shown in the supplementary material. For MORPH, the trade-off parameters λ p , λ a , and λ i are empirically set to 0.10, 300.00 and 0.005, respectively; and they are set to 0.20, 750.00 and 0.005 for CACD. At the training stage, we use Adam with the learning rate of 1 × 10 −4 and the weight decay factor of 0.5 for every 2, 000 iterations. We (i) update the discriminator at every iteration, (ii) use the age-related and identity-related critics at every generator iteration, and (iii) employ the pixel-level critic for every 5 generator iterations. The networks are trained with a batch size of 8 for 50, 000 iterations in total, which takes around 8 hours on a GTX 1080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance Comparison 4.3.1 Experiment I: Age Progression</head><p>Five-fold cross validation is conducted. On CACD, each fold contains 400 individuals with nearly 10,079, 8,635, 7,964, and 6,011 face images from the four age clusters of <ref type="bibr" target="#b12">[14]</ref><ref type="bibr" target="#b13">[15]</ref><ref type="bibr" target="#b14">[16]</ref><ref type="bibr" target="#b15">[17]</ref><ref type="bibr" target="#b16">[18]</ref><ref type="bibr" target="#b17">[19]</ref><ref type="bibr" target="#b18">[20]</ref><ref type="bibr" target="#b19">[21]</ref><ref type="bibr" target="#b20">[22]</ref><ref type="bibr" target="#b21">[23]</ref><ref type="bibr" target="#b22">[24]</ref><ref type="bibr" target="#b23">[25]</ref><ref type="bibr" target="#b24">[26]</ref><ref type="bibr" target="#b25">[27]</ref><ref type="bibr" target="#b26">[28]</ref><ref type="bibr" target="#b27">[29]</ref><ref type="bibr" target="#b28">[30]</ref>, <ref type="bibr" target="#b29">[31]</ref><ref type="bibr" target="#b30">[32]</ref><ref type="bibr" target="#b31">[33]</ref><ref type="bibr" target="#b32">[34]</ref><ref type="bibr" target="#b33">[35]</ref><ref type="bibr" target="#b34">[36]</ref><ref type="bibr" target="#b35">[37]</ref><ref type="bibr" target="#b36">[38]</ref><ref type="bibr">[39]</ref><ref type="bibr">[40]</ref>, <ref type="bibr">[41]</ref><ref type="bibr">[42]</ref><ref type="bibr">[43]</ref><ref type="bibr">[44]</ref><ref type="bibr">[45]</ref><ref type="bibr">[46]</ref><ref type="bibr">[47]</ref><ref type="bibr">[48]</ref><ref type="bibr">[49]</ref><ref type="bibr">[50]</ref>, and [51-60], respectively; while on MORPH, each fold consists of nearly 2,586 subjects with 4,467, 3,030, 2,205, and 639 faces from the four age groups. For each run, four folds are utilized for training, and the remainder for evaluation. Examples of age progression results are depicted in <ref type="figure">Fig. 4</ref>. As we can see, although the examples cover a wide range of population in terms of race, gender, pose, makeup and expression, visually plausible and convincing aging effects are achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Experiment II: Aging Model Evaluation</head><p>We acknowledge that face age progression is supposed to aesthetically predict the future appearance of the individual, beyond the emerging wrinkles and identity preservation, therefore in this experiment a more comprehensive evaluation of the age progression results is provided with both the visual and quantitative analysis.</p><p>Experiment II-A: Visual Fidelity: <ref type="figure">Fig. 5</ref> (a) displays example face images with glasses, occlusions, and pose variations. The age-progressed faces are still photorealistic and true to the original inputs; whereas the previous prototyping based methods <ref type="bibr" target="#b27">[29]</ref>[32] are inherently inadequate for such circumstances, and the parametric aging models <ref type="bibr" target="#b24">[26]</ref>[28] may also lead to ghosting artifacts. In <ref type="figure">Fig. 5 (b</ref>  <ref type="bibr" target="#b13">[15]</ref> focus on cropped faces without considering hair aging, mainly because hair is not as structured as the face area. Further, hair is diverse in texture, shape, and color, thus difficult to model. Nevertheless, the proposed method takes the whole face as input, and, as expected, the hair grows wispy and thin in aging simulation. <ref type="figure">Fig. 5 (c)</ref> confirms the capability of preserving the necessary facial details during aging, and <ref type="figure">Fig. 5 (d)</ref> shows the smoothness and consistency of the aging changes, where the lips become thinner, the under-eye bags become more and more obvious, and wrinkles are deeper.</p><p>Experiment II-B: Aging Accuracy: Along with face aging, the estimated age is supposed to increase. Correspondingly, objective age estimation is conducted to measure the aging accuracy. We apply the online face analysis tool of Face++ <ref type="bibr" target="#b1">[3]</ref> to every synthesized face. Excluding those undetected, the age-progressed faces of <ref type="bibr" target="#b20">22</ref>  idation). <ref type="table" target="#tab_4">Table 1</ref> shows the results. The mean values are 42.84, 50.78, and 59.91 years old for the 3 age clusters, respectively. Ideally, they would be observed in the age range of <ref type="bibr" target="#b29">[31]</ref><ref type="bibr" target="#b30">[32]</ref><ref type="bibr" target="#b31">[33]</ref><ref type="bibr" target="#b32">[34]</ref><ref type="bibr" target="#b33">[35]</ref><ref type="bibr" target="#b34">[36]</ref><ref type="bibr" target="#b35">[37]</ref><ref type="bibr" target="#b36">[38]</ref><ref type="bibr">[39]</ref><ref type="bibr">[40]</ref> [41-50], and [51-60]. Admittedly, the lifestyle factors may accelerate or slow down the aging rates for the individuals, leading to deviations in the estimated age from the actual age, but the overall trends should be relatively robust. Due to such intrinsic ambiguities, objective age estimations are further conducted on all the faces in the dataset as benchmark. In <ref type="table" target="#tab_4">Table 1</ref> and <ref type="figure" target="#fig_4">Fig. 6</ref>(a) and 6(c), it can be seen that the estimated ages of the synthesized faces are well matched with those of the real images, and increase steadily with the elapsed time, clearly validating our method. On CACD, the aging synthesis results of 50, 222 young faces are used in this evaluation (average of 10,044 test faces in each run). Even though the age distributions of different clusters do not have a good separation as in MORPH, it still suggests that the proposed age progression method has indeed captured the data density of the given subset of faces in terms of age. See <ref type="table" target="#tab_4">Table 1</ref>   <ref type="bibr" target="#b20">22</ref>,318 young faces in MORPH and their age-progressed renderings are used in this evaluation, leading to a total of 22, 318 × 6 verifications. As shown in <ref type="table" target="#tab_5">Table 2</ref>, the obtained mean verification rates for the 3 age-progressed clusters are 100%, 98.91%, and 93.09%, respectively, and for CACD, there are 50, 222 × 6 verifications, and the mean verification rates are 99.99%, 99.91%, and 98.28%, respectively, which clearly confirm the ability of identity preservation of the proposed method. Additionally, in <ref type="table" target="#tab_5">Table 2</ref> and <ref type="figure">Fig. 7</ref>, face verification performance decreases as the time elapsed between two images increases, which conforms to the physical effect of face aging <ref type="bibr" target="#b4">[6]</ref>, and it may also explain the better performance achieved on CACD compared to MORPH in this evaluation.    of the discriminator D advances the generation of the aging effects, making the age-progressed faces more natural. Accordingly, we carry out comparison to the one-pathway discriminator, under which the generated faces are directly fed into the estimator rather than represented as feature pyramid first. The discriminator architecture in the contrast experiment is equivalent to a chaining of the network φ age and the first pathway in the proposed pyramid D. <ref type="figure">Fig. 8</ref> provides a demonstration. Visually, the synthesized aging details of the counterpart are not so evident. To make the comparison more specific and reliable, quantitative evaluations are further conducted with the similar settings as in Experiments II-B and II-C, and the statistical results are shown in <ref type="table" target="#tab_6">Table 3</ref>. In the table, the estimated ages achieved on MORPH and CACD are generally higher than the benchmark (shown in <ref type="table" target="#tab_4">Table 1</ref>), and the mean absolute errors over the three age clusters are 2.69 and 2.52 years for the two databases, respectively, exhibiting larger  deviation than 0.79 and 0.50 years obtained by using the pyramid architecture. It is perhaps because the synthesized wrinkles in this contrast experiment are less clear and the faces look relatively messy. It may also explain the decreased face verification confidence observed in <ref type="table" target="#tab_6">Table 3</ref> in the identity preservation evaluation. Based on both the visual fidelity and the quantitative estimations, we can draw an inference that compared with the pyramid architecture, the one-pathway discriminator, as widely utilized in previous GAN-based frameworks, is lagging behind in regard to modeling the sophisticated aging changes. Experiment II-E: Comparison to Prior Work: To compare with prior work, we conduct testing on the FG-NET and MORPH databases with CACD as the training set.</p><p>These prior studies are <ref type="bibr" target="#b24">[26]</ref>  <ref type="bibr" target="#b13">[15]</ref>, which signify the state-of-the-art. In addition, one of the most popular mobile aging applications, i.e. Agingbooth <ref type="bibr" target="#b0">[1]</ref>, and the online aging tool Face of the future [2] are also compared. <ref type="figure" target="#fig_6">Fig.  9</ref> displays some example faces. As can be seen, Face of the future and Agingbooth follow the prototyping-based method, where the identical aging mask is directly applied to all the given faces as most of the aging Apps do. While the concept of such methods is straightforward, the age-progressed faces are not photorealistic. Regarding the published works in the literature, ghosting artifacts are unavoidable for the parametric method <ref type="bibr" target="#b26">[28]</ref> and the dictionary reconstruction based solutions <ref type="bibr" target="#b34">[36]</ref> <ref type="bibr" target="#b24">[26]</ref>. Technological advancements can be observed in the deep generative models <ref type="bibr" target="#b31">[33]</ref>[37] <ref type="bibr" target="#b13">[15]</ref>, whereas they only focus on the cropped facial area, and the age-progressed faces lack necessary aging details. In a further experiment, we collect 138 paired images of 54 individuals from the published papers, and invite 10 human observers to evaluate which age-progressed face is better in the pairwise comparison. Among the 1,380 votes, 69.78% favor the proposed method, 20.80% favor the prior work, and 9.42% indicate that they are about the same. Besides, the proposed method does not require burdensome preprocessing as previous works do, and it only needs 2 landmarks for pupil alignment. To sum up, we can say that the proposed method outperforms the counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>Compared with the previous approaches to face age progression, this study shows a different but more effective solution to its key issues, i.e. age transformation accuracy and identity preservation, and proposes a novel GAN based method. This method involves the techniques on face verification and age estimation, and exploits a compound training critic that integrates the simple pixel-level penalty, the age-related GAN loss achieving age transformation, and the individual-dependent critic keeping the identity information stable. For generating detailed signs of aging, a pyramidal discriminator is designed to estimate high-level face representations in a finer way. Extensive experiments are conducted, and both the achieved aging imageries and the quantitative evaluations clearly confirm the effectiveness and robustness of the proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Demonstration of our aging simulation results (images in the first column are input faces of two subjects).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>A classic GAN contains a generator G and a discrimina- tor D, which are iteratively trained via an adversarial pro-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The scores of four pathways are finally concatenated and jointly estimated by the discriminator D (D is an estimator rather than a classifier; the Label does not need to be a single scalar).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. Aging effects obtained on the CACD (the first two rows) and MORPH (the last two rows) databases for 12 different subjects. The first image in each panel is the original face image and the subsequent 3 images are the age progressed visualizations for that subject in the [31-40], [41-50] and 50+ age clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Distributions of the estimated ages obtained by Face++. (a) MORPH, synthesized faces; (b) CACD, synthesized faces; (c) MORPH, actual faces; and (d) CACD, actual faces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Figure 7. Distributions of the face verification confidence on (a) MORPH and (b) CACD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Performance comparison with prior work (zoom in for a better view of the aging details).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Pre-trained deep face descriptor</figDesc><table>Pixel Level Loss 

Discriminator D 

Young? 
Senior? 
Synthesized? 

Fusion 

Age Transformation Loss 
Age progressed 
face 

Elderly faces 

Young faces 

Pyramid of age-specific features 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>), some examples of hair aging are demonstrated. As far as we know, almost all aging approaches proposed in the litera- ture [36][26][13][33][37]</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>,318 test samples in the MORPH dataset are investigated (aver- age of 4,464 test faces in each run under 5-fold cross val-</figDesc><table>19 years old 

22 years old 
30 years old 

29 years old 
27 years old 
26 years old 

27 years old 
30 years old 
28 years old 

26 years old 
28 years old 
20 years old 

Test face 31 -40 
41 -50 
50+ 
Test face 31 -40 
41 -50 
50+ 
Test face 31 -40 
41 -50 
50+ 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Objective face verification with Face++ is carried out to check if the original identity property is well preserved during age pro- gression. For each test face, we perform comparisons be- tween the input image and the corresponding aging simula- tion results: [test face, aged face 1], [test face, aged face 2], and [test face, aged face 3]; and statistical analyses among the synthesized faces are conducted, i.e. [aged face 1, aged face 2], [aged face 1, aged face 3], and [aged face 2, aged face 3]. Similar to Experiment II-B,</figDesc><table>and Figs. 6(b) and 6(d) 
for detailed results. 
Experiment II-C: Identity Preservation: </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Objective age estimation results (in years) on MORPH and CACD</figDesc><table>MORPH 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Objective face verification results on (a) MORPH and (b) CACD 

Aged face 1 
Aged face 2 
Aged face 3 
Aged face 1 
Aged face 2 
Aged face 3 

verification confidence 

a 

verification confidence 

a 

(a) 

Test face 
94.64 ± 0.03 
91.46 ± 0.08 
85.87 ± 0.25 

(b) 

94.13±0.04 
91.96±0.12 
88.60±0.15 
Aged face 1 
-
94.34 ± 0.06 
89.92 ± 0.30 
-
94.88±0.16 
92.63±0.09 
Aged face 2 
-
-
92.23 ± 0.24 
-
-
94.21±0.24 
verification confidence 

b 

verification confidence 

b 

Test face 
94.64 ± 1.06 
91.46 ± 3.65 
85.87 ± 5.53 
94.13±1.19 
91.96±2.26 
88.60±4.19 
Aged face 1 
-
94.34 ± 1.64 
89.92 ± 3.49 
-
94.88±0.87 
92.63±2.10 
Aged face 2 
-
-
92.23 ± 2.09 
-
-
94.21±1.25 
verification rate (threshold = 76.5, FAR = 1e -5) 
verification rate (threshold = 76.5, FAR = 1e -5) 
Test face 
100 ± 0 % 
98.91 ± 0.40 % 
93.09 ± 1.31 % 
99.99 ± 0.01 % 
99.91 ± 0.05 % 
98.28 ± 0.33 % 

a The standard deviation is calculated on the mean values of the 5 folds. 
b The standard deviation is calculated on all the synthesized faces. 

0 
10 
20 
30 
40 
50 
60 
70 
80 
90 

Estimated age with Face++ (yrs) 

0 

0.01 

0.02 

0.03 

0.04 

0.05 

0.06 

0.07 

Frequency 

Age cluster 1 
Age cluster 2 
Age cluster 3 

42.84 50.78 59.91 

0 
10 
20 
30 
40 
50 
60 
70 
80 
90 

Estimated age with Face++ (yrs) 

0 

0.01 

0.02 

0.03 

0.04 

0.05 

0.06 

0.07 

Frequency 

Age cluster 1 
Age cluster 2 
Age cluster 3 

44.29 
48.34 

52.02 

0 
10 
20 
30 
40 
50 
60 
70 
80 
90 

Estimated age with Face++ (yrs) 

0 

0.01 

0.02 

0.03 

0.04 

0.05 

0.06 

0.07 

Frequency 

Age cluster 1 
Age cluster 2 
Age cluster 3 

43.59 

48.12 

52.59 

0 
10 
20 
30 
40 
50 
60 
70 
80 
90 

Estimated age with Face++ (yrs) 

0 

0.01 

0.02 

0.03 

0.04 

0.05 

0.06 

0.07 

Frequency 

Age cluster 1 
Age cluster 2 
Age cluster 3 

42.46 51.30 
61.39 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Quantitative evaluation results using one-pathway discriminator on (a) MORPH and (b) CACD</figDesc><table>Aged face 1 
Aged face 2 
Aged face 3 
Aged face 1 
Aged face 2 
Aged face 3 

(a) 
Estimated age (yrs old) 
46.14 ± 7.79 
54.99 ± 7.08 
62.10 ± 6.74 
(b) 
45.89 ± 9.85 
51.44 ± 9.78 
54.52 ± 10.22 
Verification confidence 
93.66 ± 1.15 
89.94 ± 2.59 
84.51 ± 4.36 
92.98 ± 1.76 
87.55 ± 4.62 
84.61 ± 5.83 

Test face 

Our results 

Face of Future online tool [2] 

&amp; 

AgingBooth App [1] 

(50 years old +) 

Prior work 

21 
28 
26 
18 
22 
42 
35 
45 
35 

61-70 [36] 
51-60 [36] 
52 [28] 
48 [28] 
51-60 [36] 
51-60 [33] 
41-50 [33] 
51-60 [37] 
51-60 [37] 

MORPH 
FGNET 

41-50 [26] 

30 
29 
25 

41-50 [15] 
60+ [15] 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head></head><label></label><figDesc>[28][33][36][19][37][18][20]</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgment</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agingbooth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pivi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Co</surname></persName>
		</author>
		<ptr target="https://itunes.apple.com/us/app/agingbooth/id357467791?mt=8.8" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<ptr target="http://www.faceplusplus.com.5" />
	</analytic>
	<monogr>
		<title level="j">Face++ Research Toolkit. Megvii Inc</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<ptr target="http://www.fgnet.rsunit.com/andhttp://www-prima.inrialpes.fr/FGnet/.5" />
		<title level="m">The FG-NET Aging Database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Longitudinal study of automatic face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Best-Rowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="148" to="162" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face recognition and retrieval using cross-age reference coding with cross-age celebrity dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="804" to="815" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014-12" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07004</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Illumination-aware age progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3334" to="3341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evaluating the performance of face-aging algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lanitis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Face aging with contextural generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Smolley</surname></persName>
		</author>
		<idno>ArXiv:1611.04076</idno>
		<title level="m">Least squares generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05440</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Temporal non-volume preserving approach to facial age-progression and age-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Nhan</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Gia</forename><surname>Quach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3755" to="3763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Longitudinal face modeling via temporal deep restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Nhan</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Gia</forename><surname>Quach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Bui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5772" to="5780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Age-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="947" to="954" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06264</idno>
		<title level="m">Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling shape and textural variations in aging faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Morph: A longitudinal image database of normal adult age-progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ricanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tesafaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="341" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Personalized age progression with aging dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3970" to="3978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A concatenational graph evolution aging model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2083" to="2096" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A compositional and dynamic model for face aging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="385" to="401" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unsupervised crossdomain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02200</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The perception of human growth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Pittenger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">242</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">132</biblScope>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Age simulation for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="913" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recurrent face aging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2378" to="2386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Combining tensor space analysis and active appearance models for aging effect simulation on face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TSMC-B</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1107" to="1118" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A plastic-viscoelastic model for wrinkles in facial animation and skin aging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PG</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="201" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Face aging effect simulation using hidden factor analysis joint sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2493" to="2507" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Age progression/regression by conditional adversarial autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5810" to="5818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
