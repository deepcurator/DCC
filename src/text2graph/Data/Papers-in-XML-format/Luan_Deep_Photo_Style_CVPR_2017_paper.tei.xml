<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Photo Style Transfer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fujun</forename><surname>Luan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Sylvain</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adobe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><forename type="middle">Shechtman</forename><surname>Adobe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Photo Style Transfer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Given a reference style image (a) and an input image (b), we seek to create an output image of the same scene as the input, but with the style of the reference image. The Neural Style algorithm [5] (c) successfully transfers colors, but also introduces distortions that make the output look like a painting, which is undesirable in the context of photo style transfer. In comparison, our result (d) transfers the color of the reference style image equally well while preserving the photorealism of the output. On the right (e), we show 3 insets of (b), (c), and <ref type="table">(</ref> </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Photographic style transfer is a long-standing problem that seeks to transfer the style of a reference style photo onto another input picture. For instance, by appropriately choosing the reference style photo, one can make the input picture look like it has been taken under a different illumination, time of day, or weather, or that it has been artistically retouched with a different intent. So far, existing techniques are either limited in the diversity of scenes or transfers that they can handle or in the faithfulness of the stylistic match they achieve. In this paper, we introduce a deep-learning approach to photographic style transfer that is at the same time broad and faithful, i.e., it handles a large variety of image content while accurately transferring the reference style. Our approach builds upon the recent work on Neural Style transfer by Gatys et al. <ref type="bibr" target="#b4">[5]</ref>. However, as shown in <ref type="figure">Figure 1</ref>, even when the input and reference style images are photographs, the output still looks like a painting, e.g., straight edges become wiggly and regular textures wavy. One of our contributions is to remove these painting-like effects by preventing spatial distortion and constraining the transfer operation to happen only in color space. We achieve this goal with a transformation model that is locally affine in colorspace, which we express as a custom fully differentiable energy term inspired by the Matting Laplacian <ref type="bibr" target="#b8">[9]</ref>. We show that this approach successfully suppresses distortion while having a minimal impact on the transfer faithfulness. Our other key contribution is a solution to the challenge posed by the difference in content between the input and reference images, which could result in undesirable transfers between unrelated content. For example, consider an image with less sky visible in the input image; a transfer that ignores the difference in context between style and input may cause the style of the sky to "spill over" the rest of the picture. We show how to address this issue using semantic segmentation <ref type="bibr" target="#b2">[3]</ref> of the input and reference images. We demonstrate the effectiveness of our approach with satisfying photorealistic style transfers for a broad variety of scenarios including transfer of the time of day, weather, season, and artistic edits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Challenges and Contributions</head><p>From a practical perspective, our contribution is an effective algorithm for photographic style transfer suitable for many applications such as altering the time of day or weather of a picture, or transferring artistic edits from a photo to another. To achieve this result, we had to address two fundamental challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure preservation.</head><p>There is an inherent tension in our objectives. On the one hand, we aim to achieve very local drastic effects, e.g., to turn on the lights on individual skyscraper windows <ref type="figure">(Fig. 1)</ref>. On the other hand, these effects should not distort edges and regular patterns, e.g., so that the windows remain aligned on a grid. Formally, we seek a transformation that can strongly affect image colors while having no geometric effect, i.e., nothing moves or distorts. Reinhard et al. <ref type="bibr" target="#b11">[12]</ref> originally addressed this challenge with a global color transform. However, by definition, such a transform cannot model spatially varying effects and thus is limited in its ability to match the desired style. More expressivity requires spatially varying effects, further adding to the challenge of preventing spatial distortion. A few techniques exist for specific scenarios <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref> but the general case remains unaddressed. Our work directly takes on this challenge and provides a first solution to restricting the solution space to photorealistic images, thereby touching on the fundamental task of differentiating photos from paintings.</p><p>Semantic accuracy and transfer faithfulness. The complexity of real-world scenes raises another challenge: the transfer should respect the semantics of the scene. For instance, in a cityscape, the appearance of buildings should be matched to buildings, and sky to sky; it is not acceptable to make the sky look like a building. One plausible approach is to match each input neural patch with the most similar patch in the style image to minimize the chances of an inaccurate transfer. This strategy is essentially the one employed by the CNNMRF method <ref type="bibr" target="#b9">[10]</ref>. While plausible, we find that it often leads to results where many input patches get paired with the same style patch, and/or that entire regions of the style image are ignored, which generates outputs that poorly match the desired style.</p><p>One solution to this problem is to transfer the complete "style distribution" of the reference style photo as captured by the Gram matrix of the neural responses <ref type="bibr" target="#b4">[5]</ref>. This approach successfully prevents any region from being ignored. However, there may be some scene elements more (or less) represented in the input than in the reference image. In such cases, the style of the large elements in the reference style image "spills over" into mismatching elements of the input image, generating artifacts like building texture in the sky. A contribution of our work is to incorporate a semantic labeling of the input and style images into the transfer procedure so that the transfer happens between semantically equivalent subregions and within each of them, the mapping is close to uniform. As we shall see, this algorithm preserves the richness of the desired style and prevents spillovers. These issues are demonstrated in <ref type="figure">Figure 2.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Related Work</head><p>Global style transfer algorithms process an image by applying a spatially-invariant transfer function. These methods are effective and can handle simple styles like global color shifts (e.g., sepia) and tone curves (e.g., high or low contrast). For instance, Reinhard et al. <ref type="bibr" target="#b11">[12]</ref> match the means and standard deviations between the input and reference style image after converting them into a decorrelated color space. Pitié et al. <ref type="bibr" target="#b10">[11]</ref> describe an algorithm to transfer the full 3D color histogram using a series of 1D histograms. As we shall see in the result section, these methods are limited in their ability to match sophisticated styles.</p><p>Local style transfer algorithms based on spatial color mappings are more expressive and can handle a broad class of applications such as time-of-day hallucination <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>, transfer of artistic edits <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17]</ref>, weather and season change <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>, and painterly stylization <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13]</ref>. Our work is most directly related to the line of work initiated by Gatys et al. <ref type="bibr" target="#b4">[5]</ref> that employs the feature maps of discriminatively trained deep convolutional neural networks such as VGG-19 <ref type="bibr" target="#b15">[16]</ref> to achieve groundbreaking performance for painterly style transfer <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13]</ref>. The main difference with these techniques is that our work aims for photorealistic transfer, which, as we previously discussed, introduces a challenging tension between local changes and large-scale consistency. In that respect, our algorithm is related to the techniques that operate in the photo realm <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>. But unlike these <ref type="figure">Figure 2</ref>: Given an input image (a) and a reference style image (e), the results (b) of Gatys et al. <ref type="bibr" target="#b4">[5]</ref> (Neural Style) and (c) of Li et al. <ref type="bibr" target="#b9">[10]</ref> (CNNMRF) present artifacts due to strong distortions when compared to (d) our result. In (f,g,h), we compute the correspondence between the output and the reference style where, for each pixel, we encode the XY coordinates of the nearest patch in the reference style image as a color with (R, G, B) = (0, 255 × Y /height, 255 × X/width). The nearest neural patch is found using the L2 norm on the neural responses of the VGG-19 conv3_1 layer, similarly to CNNMRF. Neural Style computes global statistics of the reference style image which tends to produce texture mismatches as shown in the correspondence (f), e.g., parts of the sky in the output image map to the buildings from the reference style image. CNNMRF computes a nearest-neighbor search of the reference style image which tends to have many-to-one mappings as shown in the correspondence (g), e.g., see the buildings. In comparison, our result (d) prevents distortions and matches the texture correctly as shown in the correspondence (h).</p><p>techniques that are dedicated to a specific scenario, our approach is generic and can handle a broader diversity of style images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>Our algorithm takes two images: an input image which is usually an ordinary photograph and a stylized and retouched reference image, the reference style image. We seek to transfer the style of the reference to the input while keeping the result photorealistic. Our approach augments the Neural Style algorithm <ref type="bibr" target="#b4">[5]</ref> by introducing two core ideas.</p><p>• We propose a photorealism regularization term in the objective function during the optimization, constraining the reconstructed image to be represented by locally affine color transformations of the input to prevent distortions.</p><p>• We introduce an optional guidance to the style transfer process based on semantic segmentation of the inputs (similar to <ref type="bibr" target="#b1">[2]</ref>) to avoid the content-mismatch problem, which greatly improves the photorealism of the results.</p><p>Background. For completeness, we summarize the Neural Style algorithm by Gatys et al. <ref type="bibr" target="#b4">[5]</ref> that transfers the reference style image S onto the input image I to produce an output image O by minimizing the objective function:</p><formula xml:id="formula_0">L total = L ℓ=1 α ℓ L ℓ c + Γ L ℓ=1 β ℓ L ℓ s (1a)</formula><p>with:</p><formula xml:id="formula_1">L ℓ c = 1 2N ℓ D ℓ ij (F ℓ [O] − F ℓ [I]) 2 ij (1b) L ℓ s = 1 2N 2 ℓ ij (G ℓ [O] − G ℓ [S]) 2 ij (1c)</formula><p>where L is the total number of convolutional layers and ℓ indicates the ℓ-th convolutional layer of the deep convolutional neural network. In each layer, there are N ℓ filters each with a vectorized feature map of size</p><formula xml:id="formula_2">D ℓ . F ℓ [·] ∈ R N ℓ ×D ℓ</formula><p>is the feature matrix with (i, j) indicating its index and the</p><formula xml:id="formula_3">Gram matrix G ℓ [·] = F ℓ [·]F ℓ [·]</formula><p>T ∈ R N ℓ ×N ℓ is defined as the inner product between the vectorized feature maps. α ℓ and β ℓ are the weights to configure layer preferences and Γ is a weight that balances the tradeoff between the content (Eq. 1b) and the style (Eq. 1c).</p><p>Photorealism regularization. We now describe how we regularize this optimization scheme to preserve the structure of the input image and produce photorealistic outputs. Our strategy is to express this constraint not on the output image directly but on the transformation that is applied to the input image. Characterizing the space of photorealistic images is an unsolved problem. Our insight is that we do not need to solve it if we exploit the fact that the input is already photorealistic. Our strategy is to ensure that we do not</p><note type="other">lose this property during the transfer by adding a term to Equation 1a that penalizes image distortions. Our solution is to seek an image transform that is locally affine in color space, that is, a function such that for each output patch, there is an affine function that maps the input RGB values onto their output counterparts. Each patch can have a different affine function, which allows for spatial variations.</note><p>To gain some intuition, one can consider an edge patch. The set of affine combinations of the RGB channels spans a broad set of variations but the edge itself cannot move because it is located at the same place in all channels.</p><p>Formally, we build upon the Matting Laplacian of Levin et al. <ref type="bibr" target="#b8">[9]</ref> who have shown how to express a grayscale matte as a locally affine combination of the input RGB channels. They describe a least-squares penalty function that can be minimized with a standard linear system represented by a matrix M I that only depends on the input image I (We refer to the original article for the detailed derivation. Note that given an input image I with N pixels, M I is N × N ). We name V c [O] the vectorized version (N × 1) of the output image O in channel c and define the following regularization term that penalizes outputs that are not well explained by a locally affine transform:</p><formula xml:id="formula_4">L m = 3 c=1 V c [O] T M I V c [O]<label>(2)</label></formula><p>Using this term in a gradient-based solver requires us to compute its derivative w.r.t. the output image. Since M I is a symmetric matrix, we have:</p><formula xml:id="formula_5">dLm dVc[O] = 2M I V c [O].</formula><p>Augmented style loss with semantic segmentation. A limitation of the style term (Eq. 1c) is that the Gram matrix is computed over the entire image. Since a Gram matrix determines its constituent vectors up to an isometry <ref type="bibr" target="#b17">[18]</ref>, it implicitly encodes the exact distribution of neural responses, which limits its ability to adapt to variations of semantic context and can cause "spillovers". We address this problem with an approach akin to Neural Doodle <ref type="bibr" target="#b0">[1]</ref> and a semantic segmentation method <ref type="bibr" target="#b2">[3]</ref> to generate image segmentation masks for the input and reference images for a set of common labels (sky, buildings, water, etc.). We add the masks to the input image as additional channels and augment the neural style algorithm by concatenating the segmentation channels and updating the style loss as follows:</p><formula xml:id="formula_6">L ℓ s+ = C c=1 1 2N 2 ℓ,c ij (G ℓ,c [O] − G ℓ,c [S]) 2 ij (3a) F ℓ,c [O] = F ℓ [O]M ℓ,c [I] F ℓ,c [S] = F ℓ [S]M ℓ,c [S] (3b)</formula><p>where C is the number of channels in the semantic segmentation mask, M ℓ,c [·] denotes the channel c of the segmentation mask in layer ℓ, and G ℓ,c [·] is the Gram matrix corresponding to F ℓ,c <ref type="bibr">[·]</ref>. We downsample the masks to match the feature map spatial size at each layer of the convolutional neural network.</p><p>To avoid "orphan semantic labels" that are only present in the input image, we constrain the input semantic labels to be chosen among the labels of the reference style image. While this may cause erroneous labels from a semantic standpoint, the selected labels are in general equivalent in our context, e.g., "lake" and "sea". We have also observed that the segmentation does not need to be pixel accurate since eventually the output is constrained by our regularization.</p><p>Our approach. We formulate the photorealistic style transfer objective by combining all 3 components together:</p><formula xml:id="formula_7">L total = L l=1 α ℓ L ℓ c + Γ L ℓ=1 β ℓ L ℓ s+ + λL m (4)</formula><p>where L is the total number of convolutional layers and ℓ indicates the ℓ-th convolutional layer of the deep neural network. Γ is a weight that controls the style loss. α ℓ and β ℓ are the weights to configure layer preferences. λ is a weight that controls the photorealism regularization. L ℓ c is the content loss (Eq. 1b). L ℓ s+ is the augmented style loss (Eq. 3a). L m is the photorealism regularization (Eq. 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Implementation Details</head><p>This section describes the implementation details of our approach. We employed the pre-trained VGG-19 <ref type="bibr" target="#b15">[16]</ref> as the feature extractor. We chose conv4_2 (α ℓ = 1 for this layer and α ℓ = 0 for all other layers) as the content representation, and conv1_1, conv2_1, conv3_1, conv4_1 and conv5_1 (β ℓ = 1/5 for those layers and β ℓ = 0 for all other layers) as the style representation. We used these layer preferences and parameters Γ = 10 2 , λ = 10 4 for all the results. The effect of λ is illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>.</p><p>We use the original author's Matlab implementation of Levin et al. <ref type="bibr" target="#b8">[9]</ref> to compute the Matting Laplacian matrices and modified the publicly available torch implementation <ref type="bibr" target="#b6">[7]</ref> of the Neural Style algorithm. The derivative of the photorealism regularization term is implemented in CUDA for gradient-based optimization. We produce results using our method with different λ parameters. A too small λ value cannot prevent distortions, and thus the results have a non-photorealistic look in (b,c). Conversely, a too large λ value suppresses the style to be transferred yielding a half-transferred look in (e,f). We found the best parameter λ = 10 4 to be the sweet spot to produce our result (d) and all the other results in this paper.</p><p>We initialize our optimization using the output of the Neural Style algorithm (Eq. 1a) with the augmented style loss (Eq. 3a), which itself is initialized with a random noise. This two-stage optimization works better than solving for Equation 4 directly, as it prevents the suppression of proper local color transfer due to the strong photorealism regularization.</p><p>We use DilatedNet <ref type="bibr" target="#b2">[3]</ref> for segmenting both the input image and reference style image. As is, this technique recognizes 150 categories. We found that this fine-grain classification was unnecessary and a source of instability in our algorithm. We merged similar classes such as 'lake', 'river', 'ocean', and 'water' that are equivalent in our context to get a reduced set of classes that yields cleaner and simpler segmentations, and eventually better outputs. The merged labels are detailed in the supplemental material.</p><p>Our code is available at: https://github.com/ luanfujun/deep-photo-styletransfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Comparison</head><p>We have performed a series of experiments to validate our approach. We first discuss visual comparisons with previous work before reporting the results of two user studies.</p><p>We compare our method with Gatys et al. <ref type="bibr" target="#b4">[5]</ref> (Neural Style for short) and Li et al. <ref type="bibr" target="#b9">[10]</ref> (CNNMRF for short) across a series of indoor and outdoor scenes in <ref type="figure" target="#fig_1">Figure 4</ref>. Both techniques produce results with painting-like distortions, which are undesirable in the context of photographic style transfer. The Neural Style algorithm also suffers from spillovers in several cases, e.g., with the sky taking on the style of the ground. And as previously discussed, CNNMRF often generates partial style transfers that ignore significant portions of the style image. In comparison, our photorealism regularization and semantic segmentation prevent these artifacts from happening and our results look visually more satisfying.</p><p>In <ref type="figure">Figure 5</ref>, we compare our method with global style transfer methods that do not distort images, Reinhard et al. <ref type="bibr" target="#b11">[12]</ref> and Pitié et al. <ref type="bibr" target="#b10">[11]</ref>. Both techniques apply a global color mapping to match the color statistics between the input image and the style image, which limits the faithfulness of their results when the transfer requires spatially-varying color transformation. Our transfer is local and capable of handling context-sensitive color changes.</p><p>In <ref type="figure">Figure 6</ref>, we compare our method with the time-ofday hallucination of Shih et al. <ref type="bibr" target="#b14">[15]</ref>. The two results look drastically different because our algorithm directly reproduces the style of the reference style image whereas Shih's is an analogy-based technique that transfers the color change observed in a time-lapse video. Both results are visually satisfying and we believe that which one is most useful depends on the application. From a technical perspective, our approach is more practical because it requires only a single style photo in addition to the input picture whereas Shih's hallucination needs a full time-lapse video, which is a less common medium and requires more storage. Further, our algorithm can handle other scenarios beside time-of-day hallucination.</p><p>In <ref type="figure">Figure 7</ref>, we show how users can control the transfer results simply by providing the semantic masks. This use case enables artistic applications and also makes it possible to handle extreme cases for which semantic labeling cannot help, e.g., to match a transparent perfume bottle to a fireball.</p><p>In <ref type="figure" target="#fig_4">Figure 8</ref>, we show examples of failure due to extreme mismatch. These can be fixed using manual segmentation.</p><p>We provide additional results such as comparison against Wu et al. <ref type="bibr" target="#b18">[19]</ref>, results with only semantic segmentation or photorealism regularization applied separately, and a solution for handling noisy or high-resolution input in the supplemental material. All our results were generated using a two-stage optimization in 3~5 minutes on an NVIDIA Titan X GPU.</p><p>User studies. We conducted two user studies to validate our work. First, we assessed the photorealism of several techniques: ours, the histogram transfer of Pitié et al. <ref type="bibr" target="#b10">[11]</ref>, CNNMRF <ref type="bibr" target="#b9">[10]</ref>, and Neural Style <ref type="bibr" target="#b4">[5]</ref>. We asked users to score images on a 1-to-4 scale ranging from "definitely not photorealistic" to "definitely photorealistic". We used 8 different scenes for each of the 4 methods for a total of 32 questions. We collected 40 responses per question on average. <ref type="figure">Figure 9a</ref> shows that CNNMRF and Neural Style produce nonphotorealistic results, which confirms our observation that these techniques introduce painting-like distortions. It also shows that, although our approach scores below histogram transfer, it nonetheless produces photore-  <ref type="bibr" target="#b11">[12]</ref> (d) Pitié et al. <ref type="bibr" target="#b10">[11]</ref> (e) Our result <ref type="figure">Figure 5</ref>: Comparison of our method against Reinhard et al. <ref type="bibr" target="#b11">[12]</ref> and Pitié <ref type="bibr" target="#b10">[11]</ref>. Our method provides more flexibility in transferring spatially-variant color changes, yielding better results than previous techniques. alistic outputs. Motivated by this result, we conducted a second study to estimate the faithfulness of the style transfer techniques. We found that global methods consistently generated distortion-free results but with a variable level of style faithfulness. We compared against several global methods in our second study: Reinhard's statistics transfer <ref type="bibr" target="#b11">[12]</ref>,  Pitié's histogram transfer <ref type="bibr" target="#b10">[11]</ref>, and Photoshop Match Color. Users were shown a style image and 4 transferred outputs, the 3 previously mentioned global methods and our technique (randomly ordered to avoid bias), and asked to choose the image with the most similar style to the reference style image. We, on purpose, did not show the input image so that users could focus on the output images. We showed 20 comparisons and collected 35 responses per question on average. The study shows that our algorithm produces the most faithful style transfer results more than 80% of the time <ref type="figure">(Fig. 9b)</ref>. We provide the links to our user study websites in the supplemental material. <ref type="figure">Figure 9</ref>: User study results confirming that our algorithm produces photorealistic and faithful results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We introduce a deep-learning approach that faithfully transfers style from a reference image for a wide variety of image content. We use the Matting Laplacian to constrain the transformation from the input to the output to be locally affine in colorspace. Semantic segmentation further drives more meaningful style transfer yielding satisfying photorealistic results in a broad variety of scenarios, including transfer of the time of day, weather, season, and artistic edits.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Transferring the dramatic appearance of the reference style image ((a)-inset), onto an ordinary flat shot in (a) is challenging. We produce results using our method with different λ parameters. A too small λ value cannot prevent distortions, and thus the results have a non-photorealistic look in (b,c). Conversely, a too large λ value suppresses the style to be transferred yielding a half-transferred look in (e,f). We found the best parameter λ = 10 4 to be the sweet spot to produce our result (d) and all the other results in this paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of our method against Neural Style and CNNMRF. Both Neural Style and CNNMRF produce strong distortions in their synthesized images. Neural Style also entirely ignores the semantic context for style transfer. CNNMRF tends to ignore most of the texture in the reference style image since it uses nearest neighbor search. Our approach is free of distortions and matches texture semantically.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Reference style image (c) Reinhard et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Our method and the techique of Shih et al. [15] generate visually satisfying results. However, our algorithm requires a single style image instead of a full time-lapse video, and it can handle other scenarios in addition to time-of-day hallucination.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Failure cases due to extreme mismatch.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Leon Gatys, Frédo Durand, Aaron Hertzmann as well as the anonymous reviewers for their valuable discussions. We thank Fuzhang Wu for generating results using <ref type="bibr" target="#b18">[19]</ref>. This research is supported by a Google Faculty Research Award, and NSF awards IIS 1617861 and 1513967.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Two-scale tone management for photographic look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="637" to="645" />
			<date type="published" when="2006" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Semantic style transfer and turning twobit doodles into fine artworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Champandard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep manifold traversal: Changing labels with convolutional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Upchurch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<idno>abs/1511.06421</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="327" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="https://github.com/jcjohnson/neural-style" />
		<title level="m">neural-style</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transient attributes for high-level understanding and editing of outdoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Laffont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="242" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.04589</idno>
		<title level="m">Combining markov random fields and convolutional neural networks for image synthesis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">N-dimensional probability density function transfer and its application to color transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pitie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kokaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dahyot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth IEEE International Conference on Computer Vision (ICCV&apos;05</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1434" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Color transfer between images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Adhikhmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Painting style transfer for head portraits using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Doyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">129</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Style transfer for headshot portraits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data-driven hallucination of different times of day from a single outdoor photo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">200</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-scale image harmonization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">125</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Weisstein</surname></persName>
		</author>
		<ptr target="http://mathworld.wolfram.com/GramMatrix.html" />
		<title level="m">Gram matrix. MathWorld-A Wolfram Web Resource</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Content-based colour transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="190" to="203" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
