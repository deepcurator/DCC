<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeLiGAN : Generative Adversarial Networks for Diverse and Limited Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swaminathan</forename><surname>Gurumurthy</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Video Analytics Lab, CDS</orgName>
								<orgName type="institution">Indian Institute of Science Bangalore</orgName>
								<address>
									<postCode>560012</postCode>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><forename type="middle">Kiran</forename><surname>Sarvadevabhatla</surname></persName>
							<email>ravika@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Video Analytics Lab, CDS</orgName>
								<orgName type="institution">Indian Institute of Science Bangalore</orgName>
								<address>
									<postCode>560012</postCode>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh Babu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Video Analytics Lab, CDS</orgName>
								<orgName type="institution">Indian Institute of Science Bangalore</orgName>
								<address>
									<postCode>560012</postCode>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeLiGAN : Generative Adversarial Networks for Diverse and Limited Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative models for images have enjoyed a resurgence in recent years, particularly with the availability of large datasets <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref> and advent of deep neural networks <ref type="bibr" target="#b14">[15]</ref>. In particular, Generative Adversarial Networks (GANs) <ref type="bibr" target="#b7">[8]</ref> and Variational Auto-Encoders (VAE) <ref type="bibr" target="#b12">[13]</ref> have shown a lot of promise in this regard. In this paper, we focus on GANbased approaches.</p><p>A typical GAN framework consists of two components, a generator G and a discriminator D. The generator G is modelled so that it transforms a random vector z into an image I, i.e. I = G(z). z usually arises from an easy-to- * Equal contribution sample distribution (e.g. uniform). G is trained to generate images I which are indistinguishable from a sampling of the true distribution, i.e I ∼ p data , where p data is the true distribution of images. The discriminator D takes an image as input and outputs the probability that the image is from the true data distribution p data . In practice, D is trained to output a low probability p D when fed a "fake" (generated) image. D and G are trained adversarially to improve by competing with each other. A proper training regime ensures that at end of training, G generates images which are essentially indistinguishable from real images, i.e. p D (G(z)) = 0.5 <ref type="bibr" target="#b7">[8]</ref>.</p><p>In recent times, GAN-based approaches have been used to generate impressively realistic house-numbers <ref type="bibr" target="#b3">[4]</ref>, faces, bedrooms <ref type="bibr" target="#b16">[17]</ref> and a variety of other image categories <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21]</ref>. Usually, these image categories tend to have extremely complex underlying distributions. This complexity arises from two factors: (1) level of detail (e.g. color photos of objects have more detail than binary handwritten digit images) (2) diversity (e.g. inter and intra-category variability is larger for object categories compared to, say, house numbers). To be viable, generator G needs to have sufficient capacity for tackling these complexity-inducing factors. Typically, such capacity is attained by having deep networks for G <ref type="bibr" target="#b1">[2]</ref>. However, training high-capacity generators requires a large amount of training data. Therefore, existing GAN-based approaches are not viable when the amount of training data is limited.</p><p>Contributions:</p><p>• We propose DeLiGAN -a novel GAN-based framework which is especially suited for small-yet-diverse data scenarios (Section 4).</p><p>• We show that DeLiGAN enables generation of diverse images for a number of different modalities in limited data regimes. In particular, we construct modalityspecific models which generate images of handwritten digits (Section 5.3), photo objects (Section 5.4) and hand-drawn sketches (Section 5.5).</p><p>• To quantitatively characterize the intra-class diversity of generated samples, we also design a modified ver-sion of the "inception-score" <ref type="bibr" target="#b20">[21]</ref>, a measure which has been found to correlate well with human assessment of generated samples (Section 5.1). The rest of the paper is organised as follows: We give an overview of the related work in Section 2, review GAN in Section 3 and then go on to describe our model DeLiGAN in Section 4. In Section 5, we discuss experimental results which showcase the capabilities of our model. Towards the end of the paper, we discuss these results and the implications of our design decisions in Section 6. We conclude with some pointers for future work in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Generative Adversarial Networks (GANs) have recently gained a lot of popularity due to the relative sharpness of samples generated by these models compared to other approaches. The originally proposed baseline approach <ref type="bibr" target="#b7">[8]</ref> has been modified to incorporate deep convolutional networks without destabilizing the training scheme and achieving significant qualitative improvements in image quality <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17]</ref>. Further improvements were made by Salimans et al. <ref type="bibr" target="#b20">[21]</ref> by incorporating algorithmic tricks such as minibatch discrimination which stabilize training and provide better image quality. We incorporate some of these tricks in our work as well.</p><p>Our central idea -utilizing a mixture model for latent space -has been suggested in various papers, but mostly in the context of variational inference. For example, Gershman et al. <ref type="bibr" target="#b6">[7]</ref>, Jordan et al. <ref type="bibr" target="#b10">[11]</ref> and Jaakkola et al. <ref type="bibr" target="#b9">[10]</ref> model the approximate posterior of the inferred latent distribution as a mixture model to represent more complicated distributions. More recently, Renzede et al. <ref type="bibr" target="#b18">[19]</ref> and Kingma et al. <ref type="bibr" target="#b11">[12]</ref> propose 'normalizing flows' to transform the latent probability density through a series of invertible mappings to construct a complex distribution. In the context of GANs, no such approaches exist, to the best of our knowledge.</p><p>Our approach can be viewed as an attempt to modify the latent space to obtain samples in the high probability regions in the latent space. The notion of latent space modification has been explored in some recent works. For example, Han et al. <ref type="bibr" target="#b8">[9]</ref> propose to alternate between training the latent factors and the generator parameters. Arulkumaran et al. <ref type="bibr" target="#b0">[1]</ref> formulate an MCMC sampling process to sample from high probability regions of a learned latent space in variational or adversarial autoencoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Generative Adversarial Networks (GANs)</head><p>Although GANs were introduced in Section 1, we formally describe them below to establish continuity.</p><p>A typical GAN framework consists of two components, a generator G and a discriminator D. In practice, these two components are usually two neural networks. The generator G is modelled so that it transforms a random vector z into an image x G , i.e. x G = G(z). z typically arises from an easy-to-sample distribution, for e.g. z ∼ U(−1, 1) where U denotes a uniform distribution. G is trained to generate images which are indistinguishable from a sampling of the true distribution. In other words, while training G, we try to maximise p data (x G ), the probability that the generated samples belong to the data distribution.</p><formula xml:id="formula_0">p data (x G ) = z p(x G , z)dz (1) = z p data (x G |z)p z (z)dz<label>(2)</label></formula><p>The above equations make explicit the fact that GANs assume a fixed, easy to sample, prior distribution p z (z) and then maximize p data (x G |z) by training the generator network to produce samples from the data distribution.</p><p>The discriminator D takes an image I as input and outputs the probability p D (I) that the image is from the true data distribution. Typically, D is trained to output a low probability when fed a "fake" (generated) image. Thus, D is supposed to act as an expert, estimating the probability that the sample is from the true data distribution as opposed to the G's output.</p><p>D and G are trained adversarially to improve by competing with each other. This is achieved by alternating between the training phases of D and G. G tries to 'fool' D into thinking that its outputs are from the the true data distribution by maximizing its score D(G(z)). This is achieved by solving the following optimization problem in the generator phase of training:</p><formula xml:id="formula_1">min G V G (D, G) = min G E z∼pz [log(1 − D(G(z)))] (3)</formula><p>On the other hand, D tries to minimize the score it assigns to generated samples G(z) by minimising D(G(z)) and maximize the score it assigns to the real (training) data x by maximising D(x). Hence, the optimisation problem for D can be formulated as follows:</p><formula xml:id="formula_2">max D V D (D, G) = max D E x∼p data [logD(x)] + E z∼pz [log(1 − D(G(z)))]<label>(4)</label></formula><p>Hence the combined loss for the GAN can now be written as:</p><formula xml:id="formula_3">min G max D V (D, G) = min G max D E x∼p data [logD(x)] + E z∼pz [log(1 − D(G(z)))] (5) Generator Network P latent (z) Discriminator Network P data (x)</formula><p>Fake/Real  <ref type="figure">Figure 1</ref>. Baseline GAN framework (left of dotted line) and our model -DeLiGAN (right of dotted line). Dotted arrows indicate sampling from the source. Instead of sampling directly from a simple latent distribution as done for baseline model, we reparameterize the latent space using a mixture of Gaussian model in DeLiGAN. We randomly select one of the Gaussian components (depicted with a dark blue outline in the right-side figure) and employ the "reparameterization trick" <ref type="bibr" target="#b12">[13]</ref> to obtain a sample from the chosen Gaussian. See Section 4 for details.</p><p>In their work, Goodfellow et al. <ref type="bibr" target="#b7">[8]</ref> show that Equation 5 gives us Jensen-Shannon (JS) divergence between the model's distribution and data generating process. A proper training regime ensures that at the end of training, G generates images which are essentially indistinguishable from real images, i.e. p D (G(z)) = 0.5 and JS divergence achieves its lowest value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our model -DeLiGAN</head><p>In GAN training, we essentially attempt to learn a mapping from a simple latent distribution p z to the complicated data distribution (Equation 2). This mapping requires a deep generative network which can disentangle the underlying factors of variation in the data distribution and enable diversity in generated samples <ref type="bibr" target="#b1">[2]</ref>. In turn, this translates to the requirement of large amounts of data. Therefore, when data is limited yet originates from a diverse image modality, increasing the network depth becomes infeasible. Our solution to this conundrum is the following: Instead of increasing the model depth, we propose to increase the modelling power of the prior distribution. In particular, we propose a reparameterization of the latent space as a Mixtureof-Gaussians model (see <ref type="figure">Figure 1</ref>).</p><formula xml:id="formula_4">p z (z) = N i=1 φ i g(z|µ i , Σ i )<label>(6)</label></formula><p>where g(z|µ i , Σ i ) represents the probability of the sam-</p><formula xml:id="formula_5">ple z in the normal distribution, N (µ i , Σ i ).</formula><p>For reasons which will be apparent shortly (Section 4.1), we assume uniform mixture weights, φ i , i.e.</p><formula xml:id="formula_6">p z (z) = N i=1 g(z|µ i , Σ i ) N<label>(7)</label></formula><p>To obtain a sample from the above distribution, we randomly select one of the N Gaussian components and employ the "reparameterization trick" introduced by Kingma et al. <ref type="bibr" target="#b12">[13]</ref> to sample from the chosen Gaussian. We also assume that each Gaussian component has a diagonal covariance matrix. Suppose the i-th Gaussian is chosen. Let us denote the diagonal elements of the corresponding covariance matrix as</p><formula xml:id="formula_7">σ i = [σ i1 σ i2 . . . σ i K ]</formula><p>where K is the dimension of the latent space. For the "reparameterization trick", we represent the sample from the chosen i-th Gaussian as a deterministic function of µ i , σ i and an auxiliary noise variable ǫ.</p><formula xml:id="formula_8">z = µ i + σ i ǫ where ǫ ∼ N (0, 1)<label>(8)</label></formula><p>Therefore, obtaining a latent space sample translates to sampling ǫ ∼ N (0, 1) and calculating z according to Equation <ref type="bibr" target="#b7">8</ref>  <ref type="figure">Figure 2</ref>. Architectural details of Generator and Discriminator in GAN models experimentally evaluated for various image domains. Notation: FC=Fully Connected Layer, GP = Global Pooling, NIN = Network-in-Network, MD=mini-batch discrimination. Convolutional layers are specified in the format dimensions/ stride | number of filters.</p><formula xml:id="formula_9">p data (G(z)) = N i=1 p data (G(µ i + σ i ǫ)|ǫ)p(ǫ)dǫ N<label>(9)</label></formula><p>Let us define</p><formula xml:id="formula_10">µ = [µ 1 , µ 2 , . . . , µ N ] T and σ = [σ 1 , σ 2 , . . . , σ N ]</formula><p>T . Therefore, our new objective is to learn µ and σ (along with the GAN parameters) to maximise</p><formula xml:id="formula_11">p data (G(µ i + σ i ǫ)|ǫ).</formula><p>Next, we describe the procedure for learning µ and σ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Learning µ and σ</head><p>For each Gaussian component, we first need to initialise its parameters. For µ i , 1 i N , we sample from a simple prior -in our case, a uniform distribution U (−1, 1). For σ i , we assign a small, fixed non-zero initial value (0.2 in our case). Normally, the number of samples we generate from each Gaussian relative to the other Gaussians during training gives us a measure of the 'weight' π for that component. However, π is not a trainable parameter in our model since we cannot obtain gradients for π i s. Therefore, as mentioned before, we consider all components to be equally important.</p><p>To generate data, we randomly choose one of the N Gaussian components and sample a latent vector z from the chosen Gaussian (Equation 8). z is passed to G to obtain the output data (image). The generated sample z can now be used to train parameters of D or G using the standard GAN training procedure (Equation 5). In addition, µ and σ are also trained simultaneously along with G's parameters, using gradients arising from G's loss function.</p><p>However, we need to consider a subtle issue here involving σ. Since p data (G(z)) (Equation 9) has local maxima at the µ i s, G tries to decrease the σ i s in an effort to obtain more samples from the high probability regions. As a result σ i s can collapse to zero. Hence, we add a L 2 regularizer to the generator cost to prevent this from happening. The original formulation of loss function for G (Equation 3) now becomes:</p><formula xml:id="formula_12">min G V G (D, G) = min G E z∼pz [log(1 − D(G(z)))] +λ N i=1 (1 − σ i ) 2 N (10)</formula><p>Note that this procedure can be extended to generate a batch of images for mini-batch training. Indeed, increasing the number of samples per Gaussian increases the accuracy of the gradients used to update µ and σ since they are averaged out over p(ǫ) <ref type="bibr" target="#b2">[3]</ref>, thereby speeding up training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>For our DeLiGAN framework, the choice of N , the number of Gaussian components, is made empiricallymore complicated data distributions require more Gaussians. Larger values of N potentially help model with relatively increased diversity. However, increasing N also increases memory requirements. Our experiments indicate that increasing N beyond a point has little to no effect on the model capacity since the Gaussian components tend to 'crowd' and become redundant. We use a N between 50 and 100 for our experiments.</p><p>To quantitatively characterize the diversity of generated samples, we also design a modified version of the "inception-score", a measure which has been found to correlate well with human evaluation <ref type="bibr" target="#b20">[21]</ref>. We describe this score next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Modified Inception Score</head><p>Passing a generated image x = G(z) through a trained classifier with an "inception" architecture <ref type="bibr" target="#b21">[22]</ref> results in a conditional label distribution p(y|x). If x is realistic enough, it should result in a "peaky" label distribution i.e. p(y|x) should have low entropy. We also want all categories to be covered uniformly among the generated samples, i.e. p(y) where KL stands for KL-divergence and expectation E is taken over generated samples x.</p><formula xml:id="formula_13">= z p(y|x = G(z))p z (z)dz should have GAN DeliGAN GAN++ Ensemble-GAN Nx GAN MOE-GAN (a) (b) (c) (d) (e) (f) (m) (l) (k) (j) (i) (h) (g) (n) Data</formula><p>Our modification:</p><p>In its original formulation, "inception-score" assigns a higher score for models that result in a low entropy class conditional distribution p(y|x). However, it is desirable to have diversity within image samples of a particular category. To characterize this diversity, we use a cross-entropy style score −p(y|x i )log(p(y|x j )) where x j s are samples of the same class as x i as per the outputs of the trained inception model. We incorporate this cross-entropy style term into the original "inception-score" formulation and define the modified "inception-score" (m-IS) as a KL-divergence:</p><formula xml:id="formula_14">e Ex i [Ex j [(KL(P (y|xi)||P (y|xj ))]]</formula><p>. Essentially, m-IS can be viewed as a proxy for measuring intra-class sample diversity along with the sample quality. In our experiments, we report m-IS scores on a per-class basis and a combined m-IS score averaged over all classes.</p><p>We analyze the performance of DeLiGAN models trained on toy data, handwritten digits <ref type="bibr" target="#b15">[16]</ref>, photo objects <ref type="bibr" target="#b13">[14]</ref> and hand-drawn object sketches <ref type="bibr" target="#b5">[6]</ref> and compare with a regular GAN model. Specifically, we use a variant of DCGAN <ref type="bibr" target="#b16">[17]</ref> with mini-batch discrimination in the discriminator <ref type="bibr" target="#b20">[21]</ref>. We also need to note here that DeLi-GAN adds extra parameters over DCGAN. Therefore, we also compare DeliGAN with baseline models containing an increased number of learnable parameters. We start by describing a series of experiments on toy data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Toy Data</head><p>As a baseline GAN model for toy data, we set up a multi-layer perceptron with one hidden layer as G and D (see <ref type="figure">Figure 2)</ref>. For the DeLiGAN model, we incorporate the mixture of Gaussian layer as shown in <ref type="figure">Figure 1</ref>. We also compare DeLiGAN with four other baseline models -(i) GAN++ (instead of mixture of Gaussian layer, we add a fully connected layer containing N neurons between the input (z) and the generator) (ii) Ensemble-GAN (An ensemble-of-N -generators setting for DeLiGAN. During training, we randomly choose one of the generators G i for training and update its parameters along with µ i , σ i ) (iii) N x-GAN (We increase number of parameters in the generator network N times by having N times more neurons in the hidden layer) and (iv) MoE-GAN (This is short for Mixtureof-Experts GAN. In this model, we just append a uniform discrete variable via a N -dimensional one-hot encoding <ref type="bibr" target="#b3">[4]</ref> to the random input z).</p><p>For the first set of experiments, we design our generator network to output data samples originally belonging to a unimodal 2-D Gaussian data (see <ref type="figure" target="#fig_0">Figure 3(g)</ref>). <ref type="figure" target="#fig_0">Figures 3  (a)</ref>-(f) show samples generated by the respective GAN variants for this data. For the unimodal case, all models perform reasonably well in generating samples.</p><p>For the second set of experiments, we replace the unimodal distribution with a bi-modal distribution comprising two Gaussians <ref type="figure" target="#fig_0">(Figure 3(n)</ref>). The results in this case show that DeLiGAN is able to clearly model the two separate distributions whereas the baseline GAN frameworks struggle to model the void in between <ref type="figure" target="#fig_0">(Figure 3(h-m)</ref>). Although the other variants, containing more parameters, were able to model the two modes, they still struggle to model the local structure in the Gaussians properly. The generations produced by DeLiGAN look the most convincing. Although not obvious from the results, a recurring trend across all the baseline models was the relative difficulty in training due to instabilities. On the other hand, training DeliGAN was much easier in practice. As we shall soon see, this phenomenon of suboptimal baseline models and better performance by <ref type="bibr">DeLiGAN</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">MNIST</head><p>The MNIST dataset contains 60, 000 images of handwritten digits from 0 to 9 <ref type="bibr" target="#b15">[16]</ref>. We conduct experiments on a reduced training set of 500 images to mimic the lowdata scenario. The images are sampled randomly from the dataset, keeping the total number of images per digit constant. For MNIST, the generator network has a fully connected layer followed by 3 deconvolution layers while the discriminator network has 3 convolutional layers followed by a mini-batch discrimination layer (see <ref type="figure">Figure 2)</ref>.</p><p>In <ref type="figure" target="#fig_1">Figure 4</ref>, we show typical samples generated by both models, arranged in a 7 × 7 grid. For each model, the last column of digits (outlined in red), contains nearest-neighbor images (from the training set) to the samples present in the last (7th) column of the grid. For nearest neighborhood computation, we use L 2 distance between the images.</p><p>The samples produced by our model <ref type="figure" target="#fig_1">(Figure 4(b)</ref>, right) are visibly crisper compared to baseline GAN <ref type="figure" target="#fig_1">(Figure 4(a)</ref>, left). Also, some of the samples produced by the GAN model are almost identical to one other (shown as similarly colored boxes in <ref type="figure" target="#fig_1">Figure 4</ref>(a)) whereas our model produces more diverse samples. We also observe that some of the samples produced by the baseline GAN model are deformed and don't resemble any digit. This artifact is much less common in our model. Additionally, in practice, the baseline GAN model frequently diverges during training given the small data regime and the deformation artifact mentioned above becomes predominant, eventually leading to homogeneous non-digit like samples. In contrast, our model remains stable during training and generates samples with better diversity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">CIFAR 10</head><p>The CIFAR 10 dataset <ref type="bibr" target="#b13">[14]</ref> contains 60, 000 32×32 color images across 10 object classes. Once again, to mimic the diverse-yet-limited-data scenario, we compare the architectures on a reduced dataset of 2000 images. The images are drawn randomly from the entire dataset, keeping the number of images per category constant. For the experiments involving CIFAR dataset, we adopt the architecture proposed by Goodfellow et al. <ref type="bibr" target="#b7">[8]</ref>. The generator has a fully connected layer followed by 3 deconvolution layers with batch normalisation after each layer. The discriminator network has 9 convolutional layers with dropout and weight normalisation, followed by a mini-batch discrimination layer. <ref type="figure" target="#fig_2">Figure 5</ref> shows samples generated by our model and the baseline GAN model. As in the case of MNIST, some of the samples generated by the GAN, shown with similar colored bounding boxes, look nearly identical ( <ref type="figure" target="#fig_2">Figure 5(a)</ref>). Again, we observe that our model produces visibly diverse looking samples and provides more stability. The modified "inception-score" values for the models <ref type="table">(Table 1)</ref> attest to this observation as well. Note that there exist categories ('cat', 'dog') with somewhat better diversity scores for GAN. Since images belonging to these categories are similar, these kinds of images would be better represented in the data. As a result, GAN performs better for these categories, whereas DeLiGAN manages to capture even the other under-represented categories. <ref type="table">Table 1</ref> also shows the modified inception scores for the GAN++ and MoE-GAN models introduced in the toy experiments. We observe that the performance in this case is actually worse than the baseline GAN model, despite the increased number of parame-  ters. Moreover, adding fully connected layers in the generator in GAN++ also leads to increased instability in training. We hypothesize that the added set of extra parameters worsens the performance given our limited data scenario. In fact, for baseline models such as Ensemble-GAN and N x-GAN, the added set of parameters also makes computations prohibitively expensive. Overall, the CIFAR dataset experiments demonstrate that our model can scale to more complicated real life datasets and still outperform the traditional GANs in low data scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Freehand Sketches</head><p>The TU-Berlin dataset <ref type="bibr" target="#b5">[6]</ref>, contains 20, 000 hand-drawn sketches evenly distributed among 250 object categories, which amounts to 80 images per category. This dataset represents a scenario where the amount of training data is actually limited, unlike previous experiments where the quantity of training data was artificially restricted. For sketches, our network contains 5 convolutional layers in the discriminator with weight normalization and dropout followed by minibatch discrimination and 3 deconvolutional layers, followed by a fully connected layer in the generator. To demonstrate the capability of our model, we perform two sets of experiments.</p><p>For the first set of experiments, we select 4 sketch categories -apple, pear, tomato, candle. These categories have simple global contours, low sketch stroke density and are somewhat similar in appearance. During training, we augment the dataset using the flipped versions of the images. Once again, we compare the generated results of GAN and DeLiGAN. <ref type="figure" target="#fig_4">Figure 6</ref> shows the samples generated by DeLiGAN and GAN respectively, trained on the similar looking categories (left side of the dotted line). The samples generated by both the models look visually appealing. Our guess is that since the object categories are very similar, the data distribution can be easily modelled as a continuous distribution in the latent space. Therefore, the latent space doesn't need a multi-modal representation in this case. This is also borne out by the m-IS diversity scores in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>For the second set of experiments, we select 5 diverse looking categories -apple, wine glass, candle, canoe, cup -and compare the generation results for both the models. The corresponding samples are  shown in <ref type="figure" target="#fig_4">Figure 6</ref> (on the right side of the dotted line). In this case, DeLiGAN samples are visibly better, less hazy, and arise from a more stable training procedure. The samples generated by DeLiGAN also exhibit larger diversity, visibly and according to m-IS scores as well <ref type="table" target="#tab_5">(Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>The experiments described above demonstrate the benefits of modelling the latent space as a mixture of learnable Gaussians instead of the conventional unit Gaussian/uniform distribution. One reason for our performance is derived from the fact that mixture models can approximate arbitrarily complex latent distributions, given a sufficiently large number of Gaussian components.</p><p>In practice, we also notice that our mixture model approach also helps increase the model stability and is especially useful for diverse, low-data regimes where the latent distribution might not be continuous. Consider the following: The gradients on µ i s push them in the latent space in a direction which increases the discriminator score, D(G(z)) as per the gradient update (Equation 11). Thus, samples generated from the updated Gaussian components result in higher probability, p data (G(z)).</p><formula xml:id="formula_15">∂V ∂µ = − 1 1 − D(G(z)) ∂D(G(z)) ∂G(z) ∂G(z) ∂z * 1<label>(11)</label></formula><p>Hence, as training progresses, we find the µ i s in particular, even if initialised in the lower probability regions, slowly drift towards the regions that lead to samples of high probability, p data (G(z)). Hence, fewer points are sampled from the low probability regions. This is illustrated by (i) the locations of samples generated by our model in the toy experiments <ref type="figure" target="#fig_0">(Figure 3(d)</ref>) (ii) relatively small frequency of bad quality generations (that don't resemble any digit) for the MNIST experiments <ref type="figure" target="#fig_1">(Figure 4)</ref>. Our model successfully handles the low probability void between the two modes in the data distribution by emulating the void into its own latent distribution. As a result, no samples are produced in these regions. This can also be seen in the MNIST experiments -our model produces very few non-digit like samples compared to the baseline GAN <ref type="figure" target="#fig_1">(Figure 4)</ref>.</p><p>In complicated multi-modal settings, the data may be disproportionally distributed among the modes such that some of the modes contain relatively more data points. In this situation, the generator in baseline GAN tends to fit the latent distribution to the mode with maximum data as dictated by the Jensen-Shannon Divergence <ref type="bibr" target="#b22">[23]</ref>. This results in low diversity among the generated samples since a section of the data distribution is sometimes overlooked by the generator network. This effect is especially pronounced in low data regimes because the number of modes in the image space increase due to the non-availability of data connecting some of the modes. As a result, the generator tries to fit to a small fraction of the already limited data. This is consistent with our experimental results wherein the diversity and quality of samples produced by baseline GANs deteriorate with decreasing amounts of training data (MNIST - <ref type="figure" target="#fig_1">Figure  4</ref>, CIFAR - <ref type="figure" target="#fig_2">Figure 5</ref>) or increasing diversity of the training data (Sketches - <ref type="figure" target="#fig_4">Figure 6</ref>).</p><p>Our design decision of having a trainable mixture model for latent space can be viewed as an algorithmic "plug-in" that can be added to almost any GAN framework including recently proposed models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21]</ref> to obtain better performance on diverse data. Finally, it is also important to note that our model is still constrained by the modelling capacity of the underlying GAN framework itself. Hence, as we employ better GAN frameworks on top of our mixture of Gaussians layer, we can expect the model to generate realistic, high-quality samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions and Future Work</head><p>In this work, we have shown that reparameterizing the latent space in GANs as a mixture model can lead to a powerful generative model. Via experiments across a diverse set of modalities (digits, hand-drawn object sketches and color photos of objects), we have observed that this seemingly simple modification helps stabilize the model and produce diverse samples even in low data scenarios. Currently, our mixture model setup incorporates some simplifying assumptions (diagonal covariance matrix for each component, equally weighted mixture components) which limit the ability of our model to approximate more complex distributions. These parameters can be incorporated into our learning scheme to better approximate the underlying latent distribution. The source code for models and experiments described in the paper can be accessed at http://val.cds.iisc.ac.in/deligan/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Comparing the performance of baseline GANs and our model (DeLiGAN) for toy data. Refer to Section 5.2 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparing the performance of GAN and our model (DeLiGAN) for MNIST handwritten digits data. Refer to Section 5.3 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Comparing the performance of GAN and our model (DeLiGAN) for CIFAR-10 data. Refer to Section 5.4 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Comparing the performance of GAN and our model (DeLiGAN) for hand-drawn sketches for similar categories (left side of dotted line) and dissimilar categories (right side of dotted line). Panels outlined in red correspond to generated samples. Panels outlined in green correspond to the nearest training examples. Similarly colored boxes for GAN generations of dissimilar categories indicate samples which look 'similar'. Refer to Section 5.5 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>. Substituting Equations 7, 8 in RHS of Equation 2, we get:</figDesc><table>Generator 
Network 

Discriminator 
Network 

Toy 

FC | 32 
FC | 1 

FC | 32 
FC | 2 
Input | 2 

MNIST 

5x5/2 | 8 
5x5/2 | 16 
3x3/2 | 32 
GP 
MD 
FC | 1 

3x3/2 | 32 
5x5/2 | 16 
5x5/2 | 1 
Input | 30 
FC | 1024 

CIFAR 

FC | 8192 
5x5/2 | 256 
5x5/2 | 128 
5x5/2 | 3 
Input | 100 

3x3/1 | 96 
3x3/1 | 96 
Dropout 
3x3/1 | 192 
3x3/2 | 192 
Dropout 
3x3/1 | 192 

NIN 
GP 
3x3/2 | 96 
3x3/1 | 192 

NIN 
MD 
FC | 1 

Sketches 

FC | 2048 
5x5/2 | 64 
5x5/2 | 32 
5x5/2 | 1 
Input | 
50-100 

5x5/1 | 16 
5x5/2 | 16 
Dropout 
5x5/2 | 32 
Dropout 
3x3/2 | 64 
NIN 
GP 
MD 
FC | 1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>persists even for more complex data distributions (CIFAR-10, sketches etc.)Table 1. Comparing modified "inception-score" values for baseline GANs and DeLiGAN across the 10 categories of CIFAR-10 dataset. Larger scores are better. The entries represent score's mean value and standard deviation for the category.</figDesc><table>Plane 

Car 
Bird 
Cat 
Deer 
Dog 
Frog 
Horse 
Ship 
Truck 
Overall 
GAN 
2.72 ± 0.20 2.02 ± 0.18 2.21 ± 0.44 2.43 ± 0.19 2.06 ± 0.09 2.22 ± 0.23 1.82 ± 0.08 2.12 ± 0.55 1.19 ± 0.19 2.16 ± 0.15 
2.15 ± 0.25 
DeLiGAN 2.78 ± 0.02 2.36 ± 0.06 2.44 ± 0.07 2.17 ± 0.04 2.31 ± 0.02 1.27 ± 0.01 2.31 ± 0.02 3.63 ± 0.14 1.51 ± 0.03 2.00 ± 0.05 
2.28 ± 0.62 
MoE-GAN 2.69 ± 0.08 2.08 ± 0.05 2.01 ± 0.06 2.19 ± 0.04 2.16 ± 0.03 1.85 ± 0.09 1.84 ± 0.07 2.14 ± 0.08 1.60 ± 0.04 1.85 ± 0.05 
2.04 ± 0.28 
GAN++ 
2.44 ± 0.06 1.73 ± 0.04 1.68 ± 0.05 2.27 ± 0.06 2.23 ± 0.04 1.73 ± 0.03 1.56 ± 0.02 1.21 ± 0.04 1.25 ± 0.02 1.53 ± 0.02 
1.76 ± 0.40 

b) DeliGAN 
a) GAN 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Comparing modified "inception-score" values for GAN 
and DeLiGAN across sketches from the 4 'similar' categories. The 
entries represent score's mean value and standard deviation for the 
category. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Comparing modified "inception-score" values for GAN and DeLiGAN across sketches from the 5 'dissimilar' categories. The entries represent score's mean value and standard deviation for the category.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgements</head><p>We would like to thank our anonymous reviewers for their suggestions, NVIDIA for their contribution of Tesla K40 GPU, Qualcomm India for their support to Ravi Kiran Sarvadevabhatla via the Qualcomm Innovation Fellowship and Google Research India for their travel grant support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Improving sampling from generative autoencoders with markov chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Bharath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09296</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Better mixing via deep representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML (1), volume 28 of JMLR Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="552" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00519</idno>
		<title level="m">Importance weighted autoencoders</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03657</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How do humans sketch objects?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.4665</idno>
		<title level="m">Nonparametric variational inference</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Alternating back-propagation for generator network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08571</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving the mean field approximation via the use of mixture distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in graphical models</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="163" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04934</idno>
		<title level="m">Improving variational inference with inverse autoregressive flow</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02454</idno>
		<title level="m">Learning what and where to draw</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05770</idno>
		<title level="m">Variational inference with normalizing flows</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03498</idno>
		<title level="m">Improved techniques for training GANs</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A note on the evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.01844</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<title level="m">Energy-based generative adversarial network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
