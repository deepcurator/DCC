<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Task-based End-to-end Model Learning in Stochastic Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><forename type="middle">L</forename><surname>Donti</surname></persName>
							<email>pdonti@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computer Science Dept. of Engr. &amp; Public Policy</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep3">Dept. of Computer Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University Pittsburgh</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University Pittsburgh</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213, 15213, 15213</postCode>
									<region>PA, PA, PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Amos</surname></persName>
							<email>bamos@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computer Science Dept. of Engr. &amp; Public Policy</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep3">Dept. of Computer Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University Pittsburgh</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University Pittsburgh</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213, 15213, 15213</postCode>
									<region>PA, PA, PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
							<email>zkolter@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computer Science Dept. of Engr. &amp; Public Policy</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep3">Dept. of Computer Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University Pittsburgh</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University Pittsburgh</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213, 15213, 15213</postCode>
									<region>PA, PA, PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Task-based End-to-end Model Learning in Stochastic Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>With the increasing popularity of machine learning techniques, it has become common to see prediction algorithms operating within some larger process. However, the criteria by which we train these algorithms often differ from the ultimate criteria on which we evaluate them. This paper proposes an end-to-end approach for learning probabilistic machine learning models in a manner that directly captures the ultimate task-based objective for which they will be used, within the context of stochastic programming. We present three experimental evaluations of the proposed approach: a classical inventory stock problem, a real-world electrical grid scheduling task, and a real-world energy storage arbitrage task. We show that the proposed approach can outperform both traditional modeling and purely black-box policy optimization approaches in these applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While prediction algorithms commonly operate within some larger process, the criteria by which we train these algorithms often differ from the ultimate criteria on which we evaluate them: the performance of the full "closed-loop" system on the ultimate task at hand. For instance, instead of merely classifying images in a standalone setting, one may want to use these classifications within planning and control tasks such as autonomous driving. While a typical image classification algorithm might optimize accuracy or log likelihood, in a driving task we may ultimately care more about the difference between classifying a pedestrian as a tree vs. classifying a garbage can as a tree. Similarly, when we use a probabilistic prediction algorithm to generate forecasts of upcoming electricity demand, we then want to use these forecasts to minimize the costs of a scheduling procedure that allocates generation for a power grid. As these examples suggest, instead of using a "generic loss," we instead may want to learn a model that approximates the ultimate task-based "true loss." This paper considers an end-to-end approach for learning probabilistic machine learning models that directly capture the objective of their ultimate task. Formally, we consider probabilistic models in the context of stochastic programming, where the goal is to minimize some expected cost over the models' probabilistic predictions, subject to some (potentially also probabilistic) constraints. As mentioned above, it is common to approach these problems in a two-step fashion: first to fit a predictive model to observed data by minimizing some criterion such as negative log-likelihood, and then to use this model to compute or approximate the necessary expected costs in the stochastic programming setting. While this procedure can work well in many instances, it ignores the fact that the true cost of the system (the optimization objective evaluated on actual instantiations in the real world) may benefit from a model that actually attains worse overall likelihood, but makes more accurate predictions over certain manifolds of the underlying space.</p><p>We propose to train a probabilistic model not (solely) for predictive accuracy, but so that-when it is later used within the loop of a stochastic programming procedure-it produces solutions that minimize the ultimate task-based loss. This formulation may seem somewhat counterintuitive, given that a "perfect" predictive model would of course also be the optimal model to use within a stochastic programming framework. However, the reality that all models do make errors illustrates that we should indeed look to a final task-based objective to determine the proper error tradeoffs within a machine learning setting. This paper proposes one way to evaluate task-based tradeoffs in a fully automated fashion, by computing derivatives through the solution to the stochastic programming problem in a manner that can improve the underlying model.</p><p>We begin by presenting background material and related work in areas spanning stochastic programming, end-to-end training, and optimizing alternative loss functions. We then describe our approach within the formal context of stochastic programming, and give a generic method for propagating task loss through these problems in a manner that can update the models. We report on three experimental evaluations of the proposed approach: a classical inventory stock problem, a real-world electrical grid scheduling task, and a real-world energy storage arbitrage task. We show that the proposed approach outperforms traditional modeling and purely black-box policy optimization approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and related work</head><p>Stochastic programming Stochastic programming is a method for making decisions under uncertainty by modeling or optimizing objectives governed by a random process. It has applications in many domains such as energy <ref type="bibr" target="#b0">[1]</ref>, finance <ref type="bibr" target="#b1">[2]</ref>, and manufacturing <ref type="bibr" target="#b2">[3]</ref>, where the underlying probability distributions are either known or can be estimated. Common considerations include how to best model or approximate the underlying random variable, how to solve the resulting optimization problem, and how to then assess the quality of the resulting (approximate) solution <ref type="bibr" target="#b3">[4]</ref>.</p><p>In cases where the underlying probability distribution is known but the objective cannot be solved analytically, it is common to use Monte Carlo sample average approximation methods, which draw multiple iid samples from the underlying probability distribution and then use deterministic optimization methods to solve the resultant problems <ref type="bibr" target="#b4">[5]</ref>. In cases where the underlying distribution is not known, it is common to learn or estimate some model from observed samples <ref type="bibr" target="#b5">[6]</ref>.</p><p>End-to-end training Recent years have seen a dramatic increase in the number of systems building on so-called "end-to-end" learning. Generally speaking, this term refers to systems where the end goal of the machine learning process is directly predicted from raw inputs [e.g. <ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. In the context of deep learning systems, the term now traditionally refers to architectures where, for example, there is no explicit encoding of hand-tuned features on the data, but the system directly predicts what the image, text, etc. is from the raw inputs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. The context in which we use the term end-to-end is similar, but slightly more in line with its older usage: instead of (just) attempting to learn an output (with known and typically straightforward loss functions), we are specifically attempting to learn a model based upon an end-to-end task that the user is ultimately trying to accomplish. We feel that this concept-of describing the entire closed-loop performance of the system as evaluated on the real task at hand-is beneficial to add to the notion of end-to-end learning.</p><p>Also highly related to our work are recent efforts in end-to-end policy learning <ref type="bibr" target="#b13">[14]</ref>, using value iteration effectively as an optimization procedure in similar networks <ref type="bibr" target="#b14">[15]</ref>, and multi-objective optimization <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. These lines of work fit more with the "pure" end-to-end approach we discuss later on (where models are eschewed for pure function approximation methods), but conceptually the approaches have similar motivations in modifying typically-optimized policies to address some task(s) directly. Of course, the actual methodological approaches are quite different, given our specific focus on stochastic programming as the black box of interest in our setting.</p><p>Optimizing alternative loss functions There has been a great deal of work in recent years on using machine learning procedures to optimize different loss criteria than those "naturally" optimized by the algorithm. For example, Stoyanov et al. <ref type="bibr" target="#b19">[20]</ref> and Hazan et al. <ref type="bibr" target="#b20">[21]</ref> propose methods for optimizing loss criteria in structured prediction that are different from the inference procedure of the prediction algorithm; this work has also recently been extended to deep networks <ref type="bibr" target="#b21">[22]</ref>. Recent work has also explored using auxiliary prediction losses to satisfy multiple objectives <ref type="bibr" target="#b22">[23]</ref>, learning dynamics models that maximize control performance in Bayesian optimization <ref type="bibr" target="#b23">[24]</ref>, and learning adaptive predictive models via differentiation through a meta-learning optimization objective <ref type="bibr" target="#b24">[25]</ref>.</p><p>The work we have found in the literature that most closely resembles our approach is the work of Bengio <ref type="bibr" target="#b25">[26]</ref>, which uses a neural network model for predicting financial prices, and then optimizes the model based on returns obtained via a hedging strategy that employs it. We view this approach-of both using a model and then tuning that model to adapt to a (differentiable) procedure-as a philosophical predecessor to our own work. In concurrent work, Elmachtoub and Grigas <ref type="bibr" target="#b26">[27]</ref> also propose an approach for tuning model parameters given optimization results, but in the context of linear programming and outside the context of deep networks. Whereas Bengio <ref type="bibr" target="#b25">[26]</ref> and Elmachtoub and Grigas <ref type="bibr" target="#b26">[27]</ref> use hand-crafted (but differentiable) algorithms to approximately attain some objective given a predictive model, our approach is tightly coupled to stochastic programming, where the explicit objective is to attempt to optimize the desired task cost via an exact optimization routine, but given underlying randomness. The notions of stochasticity are thus naturally quite different in our work, but we do hope that our work can bring back the original idea of task-based model learning. (Despite Bengio <ref type="bibr" target="#b25">[26]</ref>'s original paper being nearly 20 years old, virtually all follow-on work has focused on the financial application, and not on what we feel is the core idea of using a surrogate model within a task-driven optimization procedure.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">End-to-end model learning in stochastic programming</head><p>We first formally define the stochastic modeling and optimization problems with which we are concerned. Let (x 2 X, y 2 Y) ⇠ D denote standard input-output pairs drawn from some (real, unknown) distribution D. We also consider actions z 2 Z that incur some expected loss</p><formula xml:id="formula_0">L D (z) = E x,y⇠D [f (x, y, z)].</formula><p>For instance, a power systems operator may try to allocate power generators z given past electricity demand x and future electricity demand y; this allocation's loss corresponds to the over-or under-generation penalties incurred given future demand instantiations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>If we knew D, then we could select optimal actions</head><formula xml:id="formula_1">z ? D = argmin z L D (z).</formula><p>However, in practice, the true distribution D is unknown. In this paper, we are interested in modeling the conditional distribution y|x using some parameterized model p(y|x; ✓) in order to minimize the real-world cost of the policy implied by this parameterization. Specifically, we find some parameters ✓ to parameterize p(y|x; ✓) (as in the standard statistical setting) and then determine optimal actions z ? (x; ✓) (via stochastic optimization) that correspond to our observed input x and the specific choice of parameters ✓ in our probabilistic model. Upon observing the costs of these actions z ? (x; ✓) relative to true instantiations of x and y, we update our parameterized model p(y|x; ✓) accordingly, calculate the resultant new z ? (x; ✓), and repeat. The goal is to find parameters ✓ such that the corresponding policy z ? (x; ✓) optimizes the loss under the true joint distribution of x and y. Explicitly, we wish to choose ✓ to minimize the task loss</p><formula xml:id="formula_2">L(✓) in the context of x, y ⇠ D, i.e. minimize ✓ L(✓) = E x,y⇠D [f (x, y, z ? (x; ✓))].<label>(1)</label></formula><p>Since in reality we do not know the distribution D, we obtain z ? (x; ✓) via a proxy stochastic optimization problem for a fixed instantiation of parameters ✓, i.e.</p><formula xml:id="formula_3">z ? (x; ✓) = argmin z E y⇠p(y|x;✓) [f (x, y, z)].<label>(2)</label></formula><p>The above setting specifies z ? (x; ✓) using a simple (unconstrained) stochastic program, but in reality our decision may be subject to both probabilistic and deterministic constraints. We therefore consider more general decisions produced through a generic stochastic programming problem</p><formula xml:id="formula_4">1 z ? (x; ✓) = argmin z E y⇠p(y|x;✓) [f (x, y, z)] subject to E y⇠p(y|x;✓) [g i (x, y, z)]  0, i = 1, . . . , n ineq h i (z) = 0, i = 1, . . . , n eq .<label>(3)</label></formula><p>In this setting, the full task loss is more complex, since it captures both the expected cost and any deviations from the constraints. We can write this, for instance, as</p><formula xml:id="formula_5">L(✓) = Ex,y⇠D[f (x, y, z ? (x; ✓))]+ n ineq X i=1 I{Ex,y⇠D[gi(x, y, z ? (x; ✓))]  0}+ neq X i=1 Ex[I{hi(z ? (x; ✓)) = 0}]<label>(4)</label></formula><p>(where I(·) is the indicator function that is zero when its constraints are satisfied and infinite otherwise). However, the basic intuition behind our approach remains the same for both the constrained and unconstrained cases: in both settings, we attempt to learn parameters of a probabilistic model not to produce strictly "accurate" predictions, but such that when we use the resultant model within a stochastic programming setting, the resulting decisions perform well under the true distribution.</p><p>Actually solving this problem requires that we differentiate through the "argmin" operator z ? (x; ✓) of the stochastic programming problem. This differentiation is not possible for all classes of optimization problems (the argmin operator may be discontinuous), but as we will show shortly, in many practical cases-including cases where the function and constraints are strongly convex-we can indeed efficiently compute these gradients even in the context of constrained optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Discussion and alternative approaches</head><p>We highlight our approach in contrast to two alternative existing methods: traditional model learning and model-free black-box policy optimization. In traditional machine learning approaches, it is common to use ✓ to minimize the (conditional) log-likelihood of observed data under the model p(y|x; ✓). This method corresponds to approximately solving the optimization problem minimize</p><formula xml:id="formula_6">✓ E x,y⇠D [ log p(y|x; ✓)] .<label>(5)</label></formula><p>If we then need to use the conditional distribution y|x to determine actions z within some later optimization setting, we commonly use the predictive model obtained from <ref type="formula" target="#formula_6">(5)</ref> directly. This approach has obvious advantages, in that the model-learning phase is well-justified independent of any future use in a task. However, it is also prone to poor performance in the common setting where the true distribution y|x cannot be represented within the class of distributions parameterized by ✓, i.e. where the procedure suffers from model bias. Conceptually, the log-likelihood objective implicitly trades off between model error in different regions of the input/output space, but does so in a manner largely opaque to the modeler, and may ultimately not employ the correct tradeoffs for a given task.</p><p>In contrast, there is an alternative approach to solving (1) that we describe as the model-free "black-box" policy optimization approach. Here, we forgo learning any model at all of the random variable y. Instead, we attempt to learn a policy mapping directly from inputs x to actions z ? (x;✓) that minimize the loss L(✓) presented in (4) (where here✓ defines the form of the policy itself, not a predictive model). While such model-free methods can perform well in many settings, they are often very data-inefficient, as the policy class must have enough representational power to describe sufficiently complex policies without recourse to any underlying model. sample (x, y) ⇠ D</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>compute z ? (x; ✓) via Equation <ref type="formula" target="#formula_4">(3)</ref> 6:</p><p>// step in violated constraint or objective</p><formula xml:id="formula_7">7: if 9i s.t. g i (x, y, z ? (x; ✓)) &gt; 0 then 8:</formula><p>update ✓ with r ✓ g i (x, y, z ? (x; ✓)) 9:</p><formula xml:id="formula_8">else 10:</formula><p>update ✓ with r ✓ f (x, y, z ? (x; ✓))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>end if 12: end for Our approach offers an intermediate setting, where we do still use a surrogate model to determine an optimal decision z ? (x; ✓), yet we adapt this model based on the task loss instead of any model prediction accuracy. In practice, we typically want to minimize some weighted combination of log-likelihood and task loss, which can be easily accomplished given our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimizing task loss</head><p>To solve the generic optimization problem (4), we can in principle adopt a straightforward (constrained) stochastic gradient approach, as detailed in Algorithm 1. At each iteration, we solve the proxy stochastic programming problem (3) to obtain z ? (x, ✓), using the distribution defined by our current values of ✓. Then, we compute the true loss L(✓) using the observed value of y. If any of the inequality constraints g i in L(✓) are violated, we take a gradient step in the violated constraint; otherwise, we take a gradient step in the optimization objective f . We note that if any inequality constraints are probabilistic, Algorithm 1 must be adapted to employ mini-batches in order to determine whether these probabilistic constraints are satisfied. Alternatively, because even the g i constraints are probabilistic, it is common in practice to simply move a weighted version of these constraints to the objective, i.e., we modify the objective by adding some appropriate penalty times the positive part of the function, g i (x, y, z) + , for some &gt; 0. In practice, this has the effect of taking gradient steps jointly in all the violated constraints and the objective in the case that one or more inequality constraints are violated, often resulting in faster convergence. Note that we need only move stochastic constraints into the objective; deterministic constraints on the policy itself will always be satisfied by the optimizer, as they are independent of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Differentiating the optimization solution to a stochastic programming problem</head><p>While the above presentation highlights the simplicity of the proposed approach, it avoids the issue of chief technical challenge to this approach, which is computing the gradient of an objective that depends upon the argmin operation z ? (x; ✓). Specifically, we need to compute the term @L @✓</p><formula xml:id="formula_9">= @L @z ? @z ? @✓<label>(6)</label></formula><p>which involves the Jacobian @z ? @✓ . This is the Jacobian of the optimal solution with respect to the distribution parameters ✓. Recent approaches have looked into similar argmin differentiations <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, though the methodology we present here is more general and handles the stochasticity of the objective.</p><p>At a high level, we begin by writing the KKT optimality conditions of the general stochastic programming problem <ref type="bibr" target="#b2">(3)</ref>. Differentiating these equations and applying the implicit function theorem gives a set of linear equations that we can solve to obtain the necessary Jacobians (with expectations over the distribution y ⇠ p(y|x; ✓) denoted E y ✓ , and where g is the vector of inequality constraints) 2 6 4</p><formula xml:id="formula_10">r 2 z E y✓ f (z) + nineq X i=1 i r 2 z E y✓ g i (z) (r z E y✓ g(z)) T A T diag( ) (r z E y✓ g(z)) diag(E y✓ g(z)) 0 A 0 0 3 7 5 2 6 4 @z @✓ @ @✓ @⌫ @✓ 3 7 5 = 2 6 4 @rzEy ✓ f (z) @✓ + @ Pn ineq i=1 i rzEy ✓ gi(z) @✓ diag( ) @Ey ✓ g(z) @✓ 0 3 7 5 .<label>(7)</label></formula><p>The terms in these equations look somewhat complex, but fundamentally, the left side gives the optimality conditions of the convex problem, and the right side gives the derivatives of the relevant functions at the achieved solution with respect to the governing parameter ✓. In practice, we calculate the right-hand terms by employing sequential quadratic programming <ref type="bibr" target="#b29">[30]</ref> to find the optimal policy z ? (x; ✓) for the given parameters ✓, using a recently-proposed approach for fast solution of the argmin differentiation for QPs <ref type="bibr" target="#b30">[31]</ref> to solve the necessary linear equations; we then take the derivatives at the optimum produced by this strategy. Details of this approach are described in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We consider three applications of our task-based method: a synthetic inventory stock problem, a real-world energy scheduling task, and a real-world battery arbitrage task. We demonstrate that the task-based end-to-end approach can substantially improve upon other alternatives. Source code for all experiments is available at https://github.com/locuslab/e2e-model-learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Inventory stock problem</head><p>Problem definition To highlight the performance of the algorithm in a setting where the true underlying model is known to us, we consider a "conditional" variation of the classical inventory stock problem <ref type="bibr" target="#b3">[4]</ref>. In this problem, a company must order some quantity z of a product to minimize costs over some stochastic demand y, whose distribution in turn is affected by some observed features x <ref type="figure" target="#fig_1">(Figure 1a</ref>). There are linear and quadratic costs on the amount of product ordered, plus different linear/quadratic costs on over-orders [z y] + and under-orders [y z] + . The objective is given by</p><formula xml:id="formula_11">f stock (y, z) = c 0 z + 1 2 q 0 z 2 + c b [y z] + + 1 2 q b ([y z] + ) 2 + c h [z y] + + 1 2 q h ([z y] + ) 2 , (8) where [v] + ⌘ max{v, 0}</formula><p>. For a specific choice of probability model p(y|x; ✓), our proxy stochastic programming problem can then be written as</p><formula xml:id="formula_12">minimize z E y⇠p(y|x;✓) [f stock (y, z)].<label>(9)</label></formula><p>To simplify the setting, we further assume that the demands are discrete, taking on values</p><formula xml:id="formula_13">d 1 , . . . , d k with probabilities (conditional on x) (p ✓ ) i ⌘ p(y = d i |x; ✓)</formula><p>. Thus our stochastic programming problem (9) can be written succinctly as a joint quadratic program</p><formula xml:id="formula_14">3 minimize z2R,z b ,z h 2R k c 0 z + 1 2 q 0 z 2 + k X i=1 (p ✓ ) i ✓ c b (z b ) i + 1 2 q b (z b ) 2 i + c h (z h ) i + 1 2 q h (z h ) 2 i ◆ subject to d z1  z b , z1 d  z h , z,z h , z b 0.<label>(10)</label></formula><p>Further details of this approach are given in the appendix.</p><p>Experimental setup We examine our algorithm under two main conditions: where the true model is linear, and where it is nonlinear. In all cases, we generate problem instances by randomly sampling some x 2 R n and then generating p(y|x; ✓) according to either p(y|x;</p><formula xml:id="formula_15">✓) / exp(⇥ T x) (linear true model) or p(y|x; ✓) / exp((⇥ T x) 2</formula><p>) (nonlinear true model) for some ⇥ 2 R n⇥k . We compare the following approaches on these tasks: 1) the QP allocation based upon the true model (which performs optimally); 2) MLE approaches (with linear or nonlinear probability models) that fit a model to the data, and then compute the allocation by solving the QP; 3) pure end-to-end policy-optimizing models (using linear or nonlinear hypotheses for the policy); and 4) our task-based learning models (with linear or nonlinear probability models). In all cases, we evaluate test performance by running on 1000 random examples, and evaluate performance over 10 folds of different true ✓ ? parameters. <ref type="figure" target="#fig_2">Figures 2(a) and (b)</ref> show the performance of these methods given a linear true model, with linear and nonlinear model hypotheses, respectively. As expected, the linear MLE approach performs best, as the true underlying model is in the class of distributions that it can represent and thus solving the stochastic programming problem is a very strong proxy for solving the true optimization problem under the real distribution. While the true model is also contained within the nonlinear MLE's generic nonlinear distribution class, we see that this method requires more data to converge, and when given less data makes error tradeoffs that are ultimately not the correct tradeoffs for the task at hand; our task-based approach thus outperforms this approach. The task-based approach also substantially outperforms the policy-optimizing neural network, highlighting the fact that it is more data-efficient to run the learning process "through" a reasonable model. Note that here it does not make a difference whether we use the linear or nonlinear model in the task-based approach. <ref type="figure" target="#fig_2">Figures 2(c) and (d)</ref> show performance in the case of a nonlinear true model, with linear and nonlinear model hypotheses, respectively. Case (c) represents the "non-realizable" case, where the true underlying distribution cannot be represented by the model hypothesis class. Here, the linear MLE, as expected, performs very poorly: it cannot capture the true underlying distribution, and thus the resultant stochastic programming solution would not be expected to perform well. The linear policy model similarly performs poorly. Importantly, the task-based approach with the linear model performs much better here: despite the fact that it still has a misspecified model, the task-based nature of the learning process lets us learn a different linear model than the MLE version, which is Cost is evaluated over 1000 testing samples (lower is better). The linear MLE performs best for a true linear model. In all other cases, the task-based models outperform their MLE and policy counterparts.</p><p>particularly tuned to the distribution and loss of the task. Finally, also as to be expected, the non-linear models perform better than the linear models in this scenario, but again with the task-based non-linear model outperforming the nonlinear MLE and end-to-end policy approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Load forecasting and generator scheduling</head><p>We next consider a more realistic grid-scheduling task, based upon over 8 years of real electrical grid data. In this setting, a power system operator must decide how much electricity generation z 2 R 24 to schedule for each hour in the next 24 hours based on some (unknown) distribution over electricity demand <ref type="figure" target="#fig_1">(Figure 1b)</ref>. Given a particular realization y of demand, we impose penalties for both generation excess ( e ) and generation shortage ( s ), with s e . We also add a quadratic regularization term, indicating a preference for generation schedules that closely match demand realizations. Finally, we impose a ramping constraint c r restricting the change in generation between consecutive timepoints, reflecting physical limitations associated with quick changes in electricity output levels. These are reasonable proxies for the actual economic costs incurred by electrical grid operators when scheduling generation, and can be written as the stochastic programming problem</p><formula xml:id="formula_16">minimize z2R 24 24 X i=1 E y⇠p(y|x;✓)  s [y i z i ] + + e [z i y i ] + + 1 2 (z i y i ) 2 subject to |z i z i 1 |  c r 8i,<label>(11)</label></formula><p>where</p><formula xml:id="formula_17">[v] + ⌘ max{v, 0}</formula><p>. Assuming (as we will in our model), that y i is a Gaussian random variable with mean µ i and variance 2 i , then this expectation has a closed form that can be computed via analytically integrating the Gaussian PDF. <ref type="bibr" target="#b3">4</ref> We then use sequential quadratic programming (SQP) to iteratively approximate the resultant convex objective as a quadratic objective, iterate until convergence, and then compute the necessary Jacobians using the quadratic approximation at the solution, which gives the correct Hessian and gradient terms. Details are given in the appendix.</p><p>To develop a predictive model, we make use of a highly-tuned load forecasting methodology. Specifically, we input the past day's electrical load and temperature, the next day's temperature forecast, and additional features such as non-linear functions of the temperatures, binary indicators of weekends or holidays, and yearly sinusoidal features. We then predict the electrical load over all 24 (Lower loss is better.) As expected, the RMSE net achieves the lowest RMSE for its predictions. However, the task net outperforms the RMSE net on task loss by 38.6%, and the cost-weighted RMSE on task loss by 8.6%.</p><p>hours of the next day. We employ a 2-hidden-layer neural network for this purpose, with an additional residual connection from the inputs to the outputs initialized to the linear regression solution. An illustration of the architecture is shown in <ref type="figure" target="#fig_4">Figure 3</ref>. We train the model to minimize the mean squared error between its predictions and the actual load (giving the mean prediction µ i ), and compute Using the (mean and variance) predictions of this base model, we obtain z ? (x; ✓) by solving the generator scheduling problem <ref type="bibr" target="#b10">(11)</ref> and then adjusting network parameters to minimize the resultant task loss. We compare against a traditional stochastic programming model that minimizes just the RMSE, as well as a cost-weighted RMSE that periodically reweights training samples given their task loss. <ref type="bibr" target="#b4">5</ref> (A pure policy-optimizing network is not shown, as it could not sufficiently learn the ramp constraints. We could not obtain good performance for the policy optimizer even ignoring this infeasibility.) <ref type="figure" target="#fig_3">Figure 4</ref> shows the performance of the three models. As expected, the RMSE model performs best with respect to the RMSE of its predictions (its objective). However, the task-based model substantially outperforms the RMSE model when evaluated on task loss, the actual objective that the system operator cares about: specifically, we improve upon the performance of the traditional stochastic programming method by 38.6%. The cost-weighted RMSE's performance is extremely variable, and overall, the task net improves upon this method by 8.6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Price forecasting and battery storage</head><p>Finally, we consider a battery arbitrage task, based upon 6 years of real electrical grid data. Here, a grid-scale battery must operate over a 24 hour period based on some (unknown) distribution over future electricity prices <ref type="figure" target="#fig_1">(Figure 1c)</ref>. For each hour, the operator must decide how much to charge (z in 2 R 24 ) or discharge (z out 2 R 24 ) the battery, thus inducing a particular state of charge in the battery (z state 2 R 24 ). Given a particular realization y of prices, the operator optimizes over: 1) profits, 2) flexibility to participate in other markets, by keeping the battery near half its capacity B (with weight ), and 3) battery health, by discouraging rapid charging/discharging (with weight ✏,  <ref type="table">Table 1</ref>: Task loss results for 10 runs each of the battery storage problem, given a lithium-ion battery with attributes B = 1, eff = 0.9, c in = 0.5, and c out = 0.2. (Lower loss is better.) Our task-based net on average somewhat improves upon the RMSE net, and demonstrates more reliable performance.</p><p>✏ &lt; ). The battery also has a charging efficiency ( eff ), limits on speed of charge (c in ) and discharge (c out ), and begins at half charge. This can be written as the stochastic programming problem </p><p>Assuming (as we will in our model) that y i is a random variable with mean µ i , then this expectation has a closed form that depends only on the mean. Further details are given in the appendix.</p><p>To develop a predictive model for the mean, we use an architecture similar to that described in Section 4.2. In this case, we input the past day's prices and temperature, the next day's load forecasts and temperature forecasts, and additional features such as non-linear functions of the temperatures and temporal features similar to those in Section 4.2. We again train the model to minimize the mean squared error between the model's predictions and the actual prices (giving the mean prediction µ i ), using about 5 years of data to train the model and 1 subsequent year for testing. Using the mean predictions of this base model, we then solve the storage scheduling problem by solving the optimization problem <ref type="bibr" target="#b11">(12)</ref>, again learning network parameters by minimizing the task loss. We compare against a traditional stochastic programming model that minimizes just the RMSE. <ref type="table">Table 1</ref> shows the performance of the two models. As energy prices are difficult to predict due to numerous outliers and price spikes, the models in this case are not as well-tuned as in our load forecasting experiment; thus, their performance is relatively variable. Even then, in all cases, our task-based model demonstrates better average performance than the RMSE model when evaluated on task loss, the objective most important to the battery operator (although the improvements are not statistically significant). More interestingly, our task-based method shows less (and in some cases, far less) variability in performance than the RMSE-minimizing method. Qualitatively, our task-based method hedges against perverse events such as price spikes that could substantially affect the performance of a battery charging schedule. The task-based method thus yields more reliable performance than a pure RMSE-minimizing method in the case the models are inaccurate due to a high level of stochasticity in the prediction task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>input: D // samples from true distribution 2: initialize ✓ // some initial parameterization 3: for t = 1, . . . , T do 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Features x, model predictions y, and policy z for the three experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Inventory problem results for 10 runs over a representative instantiation of true parameters (c 0 = 10, q 0 = 2, c b = 30, q b = 14, c h = 10, q h = 2). Cost is evaluated over 1000 testing samples (lower is better). The linear MLE performs best for a true linear model. In all other cases, the task-based models outperform their MLE and policy counterparts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results for 10 runs of the generation-scheduling problem for representative decision parameters e = 0.5, s = 50, and c r = 0.4. (Lower loss is better.) As expected, the RMSE net achieves the lowest RMSE for its predictions. However, the task net outperforms the RMSE net on task loss by 38.6%, and the cost-weighted RMSE on task loss by 8.6%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3</head><label>3</label><figDesc>Figure 3: 2-hidden-layer neural network to predict hourly electric load for the next day.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 i</head><label>2</label><figDesc>as the (constant) empirical variance between the predicted and actual values. In all cases we use 7 years of data to train the model, and 1.75 subsequent years for testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>z state,i+1 = z state,i z out,i + eff z in,i 8i, z state,1 = B/2, 0  z in  c in , 0  z out  c out , 0  z state  B.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">It is standard to presume in stochastic programming that equality constraints depend only on decision variables (not random variables), as non-trivial random equality constraints are typically not possible to satisfy.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This distinction is roughly analogous to the policy search vs. model-based settings in reinforcement learning. However, for the purposes of this paper, we consider much simpler stochastic programs without the multiple rounds that occur in RL, and the extension of these techniques to a full RL setting remains as future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This is referred to as a two-stage stochastic programming problem (though a very trivial example of one), where first stage variables consist of the amount of product to buy before observing demand, and second-stage variables consist of how much to sell back or additionally purchase once the true demand has been revealed.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Part of the philosophy behind applying this approach here is that we know the Gaussian assumption is incorrect: the true underlying load is neither Gaussian distributed nor homoskedastic. However, these assumptions are exceedingly common in practice, as they enable easy model learning and exact analytical solutions. Thus, training the (still Gaussian) system with a task-based loss retains computational tractability while still allowing us to modify the distribution's parameters to improve actual performance on the task at hand.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">It is worth noting that a cost-weighted RMSE approach is only possible when direct costs can be assigned independently to each decision point, i.e. when costs do not depend on multiple decision points (as in this experiment). Our task-based method, however, accommodates the (typical) more general setting.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and future work</head><p>This paper proposes an end-to-end approach for learning machine learning models that will be used in the loop of a larger process. Specifically, we consider training probabilistic models in the context of stochastic programming to directly capture a task-based objective. Preliminary experiments indicate that our task-based learning model substantially outperforms MLE and policy-optimizing approaches in all but the (rare) case that the MLE model "perfectly" characterizes the underlying distribution. Our method also achieves a 38.6% performance improvement over a highly-optimized real-world stochastic programming algorithm for scheduling electricity generation based on predicted load. In the case of energy price prediction, where there is a high degree of inherent stochasticity in the problem, our method demonstrates more reliable task performance than a traditional predictive method. The task-based approach thus demonstrates promise in optimizing in-the-loop predictions. Future work includes an extension of our approach to stochastic learning models with multiple rounds, and further to model predictive control and full reinforcement learning settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE1252522, and by the Department of Energy Computational Science Graduate Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stochastic programming models in energy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stein-Erik</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fleten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbooks in operations research and management science</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="637" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stochastic optimization models in finance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">G</forename><surname>Ziemba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vickson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Scientific</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Stochastic models of manufacturing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J George</forename><surname>Buzacott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shanthikumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Prentice Hall</publisher>
			<biblScope unit="volume">4</biblScope>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A tutorial on stochastic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Philpott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Manuscript. Available at www2.isye.gatech.edu/ashapiro/publications.html, 17</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The empirical behavior of sampling methods for stochastic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Linderoth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="215" to="241" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>R Tyrrell Rockafellar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-B</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wets</surname></persName>
		</author>
		<title level="m">Scenarios and policy aggregation in optimization under uncertainty. Mathematics of operations research</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="119" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Off-road obstacle avoidance through end-to-end learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urs</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Cosatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beat</forename><surname>Flepp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="739" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cognitive networks: adaptation and learning to achieve end-to-end performance objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Friend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen B</forename><surname>Dasilva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mackenzie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Magazine</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="51" to="57" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end text recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2012 21st International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3304" to="3308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep speech 2: End-toend speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Diamos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02595</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end training of deep visuomotor policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Value iteration networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">I</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2146" to="2154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Local search for multiobjective function optimization: pareto descent method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sakuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigenobu</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th annual conference on Genetic and evolutionary computation</title>
		<meeting>the 8th annual conference on Genetic and evolutionary computation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="659" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-objective reinforcement learning using sets of pareto dominating policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristof</forename><surname>Van Moffaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Nowé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3483" to="3512" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossam</forename><surname>Mossalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Roijers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whiteson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02707</idno>
		<title level="m">Multiobjective deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Model-based multi-objective reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maikel</forename><surname>Marco A Wiering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Withagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mȃdȃlina M Drugan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ropson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<idno>15324435</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="725" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Direct loss minimization for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Keshet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1594" to="1602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Training deep neural networks via direct loss minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2169" to="2177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Reinforcement learning with unsupervised auxiliary tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">Marian</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05397</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somil</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Calandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><forename type="middle">J</forename><surname>Tomlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09260</idno>
		<title level="m">Goal-driven dynamics learning via bayesian optimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Using a financial training criterion rather than a prediction criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Neural Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="433" to="443" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Elmachtoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grigas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.08005</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Smart &quot;predict, then optimize</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">On differentiating parameterized argmin and argmax problems with application to bi-level optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">Santa</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edison</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.05447</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Zico</forename><surname>Kolter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07152</idno>
		<title level="m">Input convex neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">W</forename><surname>Boggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sequential quadratic programming. Acta numerica</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="51" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00443</idno>
		<title level="m">Optnet: Differentiable optimization as a layer in neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiV preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
