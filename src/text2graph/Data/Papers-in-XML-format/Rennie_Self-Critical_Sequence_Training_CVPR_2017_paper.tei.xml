<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-critical Sequence Training for Image Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Watson Multimodal Algorithms and Engines Group IBM T.J. Watson Research Center</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Marcheret</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Watson Multimodal Algorithms and Engines Group IBM T.J. Watson Research Center</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Mroueh</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Watson Multimodal Algorithms and Engines Group IBM T.J. Watson Research Center</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerret</forename><surname>Ross</surname></persName>
							<email>rossja@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Watson Multimodal Algorithms and Engines Group IBM T.J. Watson Research Center</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Goel</surname></persName>
							<email>vaibhavagoel@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Watson Multimodal Algorithms and Engines Group IBM T.J. Watson Research Center</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-critical Sequence Training for Image Captioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Recently it has been shown that policy-gradient   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image captioning aims at generating a natural language description of an image. Open domain captioning is a very challenging task, as it requires a fine-grained understanding of the global and the local entities in an image, as well as their attributes and relationships. The recently released MSCOCO challenge <ref type="bibr" target="#b0">[1]</ref> provides a new, larger scale platform for evaluating image captioning systems, complete with an evaluation server for benchmarking competing methods. Deep learning approaches to sequence model-ing have yielded impressive results on the task, dominating the task leaderboard. Inspired by the recently introduced encoder/decoder paradigm for machine translation using recurrent neural networks (RNNs) <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr">[3]</ref>, and <ref type="bibr" target="#b3">[4]</ref> use a deep convolutional neural network (CNN) to encode the input image, and a Long Short Term Memory (LSTM) <ref type="bibr" target="#b4">[5]</ref> RNN decoder to generate the output caption. These systems are trained end-to-end using back-propagation, and have achieved state-of-the-art results on MSCOCO. More recently in <ref type="bibr" target="#b5">[6]</ref>, the use of spatial attention mechanisms on CNN layers to incorporate visual context-which implicitly conditions on the text generated so far-was incorporated into the generation process. It has been shown and we have qualitatively observed that captioning systems that utilize attention mechanisms lead to better generalization, as these models can compose novel text descriptions based on the recognition of the global and local entities that comprise images.</p><p>As discussed in <ref type="bibr" target="#b7">[7]</ref>, deep generative models for text are typically trained to maximize the likelihood of the next ground-truth word given the previous ground-truth word using back-propagation. This approach has been called "Teacher-Forcing" <ref type="bibr" target="#b8">[8]</ref>. However, this approach creates a mismatch between training and testing, since at test-time the model uses the previously generated words from the model distribution to predict the next word. This exposure bias <ref type="bibr" target="#b7">[7]</ref>, results in error accumulation during generation at test time, since the model has never been exposed to its own predictions.</p><p>Several approaches to overcoming the exposure bias problem described above have recently been proposed. In <ref type="bibr" target="#b8">[8]</ref> they show that feeding back the model's own predictions and slowly increasing the feedback probability p during training leads to significantly better test-time performance. Another line of work proposes "Professor-Forcing" <ref type="bibr" target="#b9">[9]</ref>, a technique that uses adversarial training to encourage the dynamics of the recurrent network to be the same when training conditioned on ground truth previous words and when sampling freely from the network.</p><p>While sequence models are usually trained using the cross entropy loss, they are typically evaluated at test time using discrete and non-differentiable NLP metrics such as BLEU <ref type="bibr" target="#b10">[10]</ref>, ROUGE <ref type="bibr" target="#b11">[11]</ref>, METEOR <ref type="bibr" target="#b12">[12]</ref> or CIDEr <ref type="bibr" target="#b2">[13]</ref>. Ideally sequence models for image captioning should be trained to avoid exposure bias and directly optimize metrics for the task at hand.</p><p>Recently it has been shown that both the exposure bias and non-differentiable task metric issues can be addressed by incorporating techniques from Reinforcement Learning (RL) <ref type="bibr" target="#b14">[14]</ref>. Specifically in <ref type="bibr" target="#b7">[7]</ref>, Ranzato et al. use the REINFORCE algorithm <ref type="bibr" target="#b15">[15]</ref> to directly optimize nondifferentiable, sequence-based test metrics, and overcome both issues. REINFORCE as we will describe, allows one to optimize the gradient of the expected reward by sampling from the model during training, and treating those samples as ground-truth labels (that are re-weighted by the reward they deliver). The major limitation of the approach is that the expected gradient computed using mini-batches under REINFORCE typically exhibit high variance, and without proper context-dependent normalization, is typically unstable. The recent discovery that REINFORCE with proper bias correction using learned "baselines" is effective has led to a flurry of work in applying REINFORCE to problems in RL, supervised learning, and variational inference <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b18">18]</ref>. Actor-critic methods <ref type="bibr" target="#b14">[14]</ref> , which instead train a second "critic" network to provide an estimate of the value of each generated word given the policy of an actor network, have also been investigated for sequence problems recently <ref type="bibr" target="#b19">[19]</ref>. These techniques overcome the need to sample from the policy's (actor's) action space, which can be enormous, at the expense of estimating future rewards, and training multiple networks based on one another's outputs, which as <ref type="bibr" target="#b19">[19]</ref> explore, can also be unstable.</p><p>In this paper we present a new approach to sequence training which we call self-critical sequence training (SCST), and demonstrate that SCST can improve the performance of image captioning systems dramatically. SCST is a REINFORCE algorithm that, rather than estimating the reward signal, or how the reward signal should be normalized, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. As a result, only samples from the model that outperform the current test-time system are given positive weight, and inferior samples are suppressed. Using SCST, attempting to estimate the reward signal, as actor-critic methods must do, and estimating normalization, as REINFORCE algorithms must do, is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure. Empirically we find that directly optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly effective. Our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task, improving the best result in terms of CIDEr from 104.9 to 114.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Captioning Models</head><p>In this section we describe the recurrent models that we use for caption generation.</p><p>FC models. Similarly to <ref type="bibr">[3,</ref><ref type="bibr" target="#b3">4]</ref>, we first encode the input image F using a deep CNN, and then embed it through a linear projection W I . Words are represented with one hot vectors that are embedded with a linear embedding E that has the same output dimension as W I . The beginning of each sentence is marked with a special BOS token, and the end with an EOS token. Under the model, words are generated and then fed back into the LSTM, with the image treated as the first word W I CN N (F ). The following updates for the hidden units and cells of an LSTM define the model <ref type="bibr" target="#b4">[5]</ref>:</p><formula xml:id="formula_0">x t = E1 wt−1 for t &gt; 1, x 1 = W I CN N (F ) i t = σ (W ix x t + W ih h t−1 + b i ) (Input Gate) f t = σ (W f x x t + W f h h t−1 + b f ) (Forget Gate) o t = σ (W ox x t + W oh h t−1 + b o ) (Output Gate) c t = i t ⊙ φ(W ⊗ zx x t + W ⊗ zh h t−1 + b ⊗ z ) + f t ⊙ c t−1 h t = o t ⊙ tanh(c t ) s t = W s h t ,</formula><p>where φ is a maxout non-linearity with 2 units (⊗ denotes the units) and σ is the sigmoid function. We initialize h 0 and c 0 to zero. The LSTM outputs a distribution over the next word w t using the softmax function:</p><formula xml:id="formula_1">w t ∼ softmax (s t )<label>(1)</label></formula><p>In our architecture, the hidden states and word and image embeddings have dimension 512. Let θ denote the parameters of the model. Traditionally the parameters θ are learned by maximizing the likelihood of the observed sequence. Specifically, given a target ground truth sequence (w * 1 , . . . , w * T ), the objective is to minimize the cross entropy loss (XE):</p><formula xml:id="formula_2">L(θ) = − T t=1 log(p θ (w * t |w * 1 , . . . w * t−1 )),<label>(2)</label></formula><p>where p θ (w t |w 1 , . . . w t−1 ) is given by the parametric model in Equation <ref type="bibr" target="#b0">(1)</ref>.</p><p>Attention Model (Att2in). Rather than utilizing a static, spatially pooled representation of the image, attention models dynamically re-weight the input spatial (CNN) features to focus on specific regions of the image at each time step.</p><p>In this paper we consider a modification of the architecture of the attention model for captioning given in <ref type="bibr" target="#b5">[6]</ref>, and input the attention-derived image feature only to the cell node of the LSTM.</p><formula xml:id="formula_3">x t = E1 wt−1 for t ≥ 1 w 0 = BOS i t = σ (W ix x t + W ih h t−1 + b i ) (Input Gate) f t = σ (W f x x t + W f h h t−1 + b f ) (Forget Gate) o t = σ (W ox x t + W oh h t−1 + b o ) (Output Gate) c t = i t ⊙ φ(W ⊗ zx x t + W ⊗ zI I t + W ⊗ zh h t−1 + b ⊗ z ) + f t ⊙ c t−1 h t = o t ⊙ tanh(c t ) s t = W s h t ,</formula><p>where I t is the attention-derived image feature. This feature is derived as in <ref type="bibr" target="#b5">[6]</ref> as follows: given CNN features at</p><formula xml:id="formula_4">N locations {I 1 , . . . I N }, I t = N i=1 α i t I i , where α t = softmax(a t + b α ), and a i t = W tanh(W aI I i + W ah h t−1 + b a ).</formula><p>In this work we set the dimension of W to 1 × 512, and set c 0 and h 0 to zero. Let θ denote the parameters of the model. Then p θ (w t |w 1 , . . . w t−1 ) is again defined by <ref type="bibr" target="#b0">(1)</ref>. The parameters θ of attention models are also traditionally learned by optimizing the XE loss <ref type="bibr" target="#b1">(2)</ref>. Attention Model (Att2all). The standard attention model presented in <ref type="bibr" target="#b5">[6]</ref> also feeds then attention signal I t as an input into all gates of the LSTM, and the output posterior. In our experiments feeding I t to all gates in addition to the input did not boost performance, but feeding I t to both the gates and the outputs resulted in significant gains when ADAM <ref type="bibr" target="#b20">[20]</ref> was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Reinforcement Learning</head><p>Sequence Generation as an RL problem. As described in the previous section, captioning systems are traditionally trained using the cross entropy loss. To directly optimize NLP metrics and address the exposure bias issue, we can cast our generative models in the Reinforcement Learning terminology as in <ref type="bibr" target="#b7">[7]</ref>. Our recurrent models (LSTMs) introduced above can be viewed as an "agent" that interacts with an external "environment" (words and image features). The parameters of the network, θ, define a policy p θ , that results in an "action" that is the prediction of the next word. After each action, the agent (the LSTM) updates its internal "state" (cells and hidden states of the LSTM, attention weights etc). Upon generating the end-of-sequence (EOS) token, the agent observes a "reward" that is, for instance, the CIDEr score of the generated sentence-we denote this reward by r. The reward is computed by an evaluation metric by comparing the generated sequence to corresponding ground-truth sequences. The goal of training is to minimize the negative expected reward:</p><formula xml:id="formula_5">L(θ) = −E w s ∼p θ [r(w s )] ,<label>(3)</label></formula><p>where w s = (w </p><formula xml:id="formula_6">L(θ) ≈ −r(w s ), w s ∼ p θ .</formula><p>Policy Gradient with REINFORCE. In order to compute the gradient ∇ θ L(θ), we use the REINFORCE algorithm <ref type="bibr" target="#b15">[15]</ref>(See also Chapter 13 in <ref type="bibr" target="#b14">[14]</ref>). REINFORCE is based on the observation that the expected gradient of a nondifferentiable reward function can be computed as follows:</p><formula xml:id="formula_7">∇ θ L(θ) = −E w s ∼p θ [r(w s )∇ θ log p θ (w s )] .<label>(4)</label></formula><p>In practice the expected gradient can be approximated using a single Monte-Carlo sample w s = (w </p><formula xml:id="formula_8">∇ θ L(θ) ≈ −r(w s )∇ θ log p θ (w s ).</formula><p>REINFORCE with a Baseline. The policy gradient given by REINFORCE can be generalized to compute the reward associated with an action value relative to a reference reward or baseline b:</p><formula xml:id="formula_9">∇ θ L(θ) = −E w s ∼p θ [(r(w s ) − b)∇ θ log p θ (w s )] . (5)</formula><p>The baseline can be an arbitrary function, as long as it does not depend on the "action" w s <ref type="bibr" target="#b14">[14]</ref>, since in this case:</p><formula xml:id="formula_10">E w s ∼p θ [b∇ θ log p θ (w s )] = b ws ∇ θ p θ (w s ) = b∇ θ ws p θ (w s ) = b∇ θ 1 = 0.<label>(6)</label></formula><p>This shows that the baseline does not change the expected gradient, but importantly, it can reduce the variance of the gradient estimate. For each training case, we again approximate the expected gradient with a single sample w s ∼ p θ :</p><formula xml:id="formula_11">∇ θ L(θ) ≈ −(r(w s ) − b)∇ θ log p θ (w s ).<label>(7)</label></formula><p>Note that if b is function of θ or t as in <ref type="bibr" target="#b7">[7]</ref>, equation <ref type="formula" target="#formula_10">(6)</ref> still holds and b(θ) is a valid baseline.</p><p>Final Gradient Expression. Using the chain rule, and the parametric model of p θ given in Section 2 we have:</p><formula xml:id="formula_12">∇ θ L(θ) = T t=1 ∂L(θ) ∂s t ∂s t ∂θ ,</formula><p>where s t is the input to the softmax function. Using RE-INFORCE with a baseline b the estimate of the gradient of</p><formula xml:id="formula_13">∂L(θ)</formula><p>∂st is given by <ref type="bibr" target="#b17">[17]</ref>:</p><formula xml:id="formula_14">∂L(θ) ∂s t ≈ (r(w s ) − b)(p θ (w t |h t ) − 1 w s t ).<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Self-critical sequence training (SCST)</head><p>The central idea of the self-critical sequence training (SCST) approach is to baseline the REINFORCE algorithm with the reward obtained by the current model under the inference algorithm used at test time. The gradient of the negative reward of a sample w s from the model w.r.t. to the softmax activations at time-step t then becomes:</p><formula xml:id="formula_15">∂L(θ) ∂s t = (r(w s ) − r(ŵ))(p θ (w t |h t ) − 1 w s t ).<label>(9)</label></formula><p>where r(ŵ) again is the reward obtained by the current model under the inference algorithm used at test time. Accordingly, samples from the model that return higher reward thanŵ will be "pushed up", or increased in probability, while samples which result in lower reward will be suppressed. Like MIXER <ref type="bibr" target="#b7">[7]</ref>, SCST has all the advantages of REINFORCE algorithms, as it directly optimizes the true, sequence-level, evaluation metric, but avoids the usual scenario of having to learn a (context-dependent) estimate of expected future rewards as a baseline. In practice we have found that SCST has much lower variance, and can be more effectively trained on mini-batches of samples using SGD.</p><p>Since the SCST baseline is based on the test-time estimate under the current model, SCST is forced to improve the performance of the model under the inference algorithm used at test time. This encourages training/test time consistency like the maximum likelihood-based approaches "Data as Demonstrator" <ref type="bibr" target="#b8">[8]</ref>, "Professor Forcing" <ref type="bibr" target="#b9">[9]</ref>, and E2E <ref type="bibr" target="#b7">[7]</ref>, but importantly, it can directly optimize sequence metrics. Finally, SCST is self-critical, and so avoids all the inherent training difficulties associated with actor-critic methods, where a second "critic" network must be trained to estimate value functions, and the actor must be trained on estimated value functions rather than actual rewards. In this paper we focus on scenario of greedy decoding, where:ŵ</p><formula xml:id="formula_16">t = arg max wt p(w t | h t )<label>(10)</label></formula><p>This choice, depicted in <ref type="figure" target="#fig_3">Figure 1</ref>, minimizes the impact of baselining with the test-time inference algorithm on training time, since it requires only one additional forward pass, and trains the system to be optimized for fast, greedy decoding at test-time.</p><p>Generalizations. The basic SCST approach described above can be generalized in several ways. One generalization is to condition the baseline on what has been generated (i.e. sampled) so far, which makes the baseline word-dependent, and further reduces the variance of the reward signal by making it dependent only on future rewards. This is achieved by baselining the reward for word w s t at timestep t with the reward obtained by the word sequencew = {w s 1:t−1 ,ŵ t:T }, which is generated by sampling tokens for timesteps 1 : t − 1, and then executing the inference algorithm to complete the sequence. The resulting reward signal, r(w s ) − r(w), is a baselined future reward (advantage) signal that conditions on both the input image and the sequence w </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>this variant time-dependent SCST (TD-SCST).</head><p>Another important generalization is to utilize the inference algorithm as a critic to replace the learned critic of traditional actor-critic approaches. Like for traditional actorcritic methods, this biases the learning procedure, but can be used to trade off variance for bias. Specifically, the primary reward signal at time t can be based on a sequence that samples only n future tokens, and then executes the inference algorithm to complete the sequence. The primary reward is then based onw = {w s 1:t+n ,ŵ t+n+1:T }, and can further be baselined in a time-dependent manner using TD-SCST. The resulting reward signal in this case is r(w) − r(w). We call this variant True SCST.</p><p>We have experimented with both TD-SCST and "True" SCST as described above on the MSCOCO task, but found that they did not lead to significant additional gain. We have also experimented with learning a control-variate for the SCST baseline on MSCOCO to no avail. Nevertheless, we anticipate that these generalizations will be important for other sequence modeling tasks, and policy-gradient-based RL more generally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Dataset.</p><p>We evaluate our proposed method on the MSCOCO dataset <ref type="bibr" target="#b0">[1]</ref>. For offline evaluation purposes we used the data splits from <ref type="bibr" target="#b21">[21]</ref>. The training set contains 113, 287 images, along with 5 captions each. We use a set of 5K image for validation and report results on a test set of 5K images as well, as given in <ref type="bibr" target="#b21">[21]</ref>. We report four widely used automatic evaluation metrics, BLEU-4, ROUGEL, METEOR, and CIDEr. We prune the vocabulary and drop any word that has count less then five, we end up with a vocabulary of size 10096 words.</p><p>Image Features 1) FC Models. We use two type of Features: a) (FC-2k) features, where we encode each image with Resnet-101 (101 layers) <ref type="bibr" target="#b22">[22]</ref>. Note that we do not rescale or crop each image. Instead we encode the full image with the final convolutional layer of resnet, and apply average pooling, which results in a vector of dimension 2048. b) (FC-15K) features where we stack average pooled 13 layers of Resnet-101 (11 × 1024 and 2 × 2048). These 13 layers are the odd layers of conv4 and conv5, with the exception of the 23rd layer of conv4, which was omitted. This results in a feature vector of dimension 15360. 2) Spatial CNN features for Attention models: (Att2in) We encode each image using the residual convolutional neural network Resnet-101 <ref type="bibr" target="#b22">[22]</ref>. Note that we do not rescale or crop the image. Instead we encode the full Implementation Details. The LSTM hidden, image, word and attention embeddings dimension are fixed to 512 for all of the models discussed herein. All of our models are trained according to the following recipe, except where otherwise noted. We initialize all models by training the model under the XE objective using ADAM <ref type="bibr" target="#b20">[20]</ref> optimizer with an initial learning rate of 5 × 10 −4 . We anneal the learning rate by a factor of 0.8 every three epochs, and increase the probability of feeding back a sample of the word posterior by 0.05 every 5 epochs until we reach a feedback probability 0.25 <ref type="bibr" target="#b8">[8]</ref>. We evaluate at each epoch the model on the development set and select the model with best CIDEr score as an initialization for SCST training. We then run SCST training initialized with the XE model to optimize the CIDEr metric (specifically, the CIDEr-D metric) using ADAM with a learning rate 5 × 10 −5 1 . Initially when experimenting with FC-2k and FC-15k models we utilized curriculum learning (CL) during training, as proposed in <ref type="bibr" target="#b7">[7]</ref>, by increasing the number of words that are sampled and trained under CIDEr by one each epoch (the prefix of the sentence remains under <ref type="bibr" target="#b0">1</ref> In the case of the Att2all models, the XE model was trained for only 20 epochs, and the learning rate was also annealed during RL training. the XE criterion until eventually being subsumed). We have since realized that for the MSCOCO task CL is not required, and provides little to no boost in performance. The results reported here for the FC-2K and FC-15K models are trained with CL, while the attention models were trained directly on the entire sentence for all epochs after being initialized by the XE seed models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>Evaluation  <ref type="bibr" target="#b7">[7]</ref> on the test portion of the Karpathy splits when trained to optimize the CIDEr metric (FC-2K models). Both improve the seed cross-entropy trained model, but SCST significantly outperforms MIXER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Offline Evaluation</head><p>Evaluating different RL training strategies. <ref type="table">Table 1</ref> compares the performance of SCST to MIXER <ref type="bibr" target="#b7">[7]</ref> (test set, Karpathy splits). In this experiment, we utilize "curriculum learning" (CL) by optimizing the expected reward of the metric on the last n words of each training sentence, optimizing XE on the remaining sentence prefix, and slowly increasing n. The results reported were generated with the optimized CL schedule reported in <ref type="bibr" target="#b7">[7]</ref>.  <ref type="table">Table 2</ref>: mean/std. performance of SCST versus REIN-FORCE with learned baseline (MIXER less CL; Att2all models, 4 seeds; Karpathy test set; CIDEr optimized).</p><p>We found that CL was not necessary to train both SCST and REINFORCE with a learned baseline on MSCOCOturning off CL sped up training and yielded equal or better results. The gain of SCST over learned baselines was consistently &gt; 3 CIDEr points, regardless of the CL schedule and the initial seed (c.f.  Optimizing the CIDEr metric increases the overall performance under the evaluation metrics the most significantly. The performance of the seed cross-entropy (XE) model is also depicted. All models were decoded greedily, with the exception of the XE beam search result, which was optimized to beam 3 on the validation set.</p><p>Training on different metrics. We experimented with training directly on the evaluation metrics of the MSCOCO challenge. Results for FC-2K models are depicted in table 3. In general we can see that optimizing for a given metric during training leads to the best performance on that same metric at test time, an expected result. We experimented with training on multiple test metrics, and found that we were unable to outperform the overall performance of the model trained only on the CIDEr metric, which lifts the performance of all other metrics considerably. For this reason most of our experimentation has since focused on optimizing CIDEr.</p><p>Single FC-Models Versus Attention Models. We trained FC models (2K and 15 K), as well as attention models  (Att2in and Att2all) using SCST with the CIDEr metric. We trained 4 different models for each FC and attention type, starting the optimization from four different random seeds 2 . We report in <ref type="table" target="#tab_4">Table 4</ref>, the system with best performance for each family of models on the test portion of Karpathy splits <ref type="bibr" target="#b21">[21]</ref>. We see that the FC-15K models outperform the FC-2K models. Both FC models are outperformed by the attention models, which establish a new state of the art for a single model performance on Karpathy splits. Note that this quantitative evaluation favors attention models is inline with our observation that attention models tend to generalize better and compose outside of the context of the training of MSCOCO, as we will see in Section 6. Model Ensembling. In this section we investigate the performance of ensembling over 4 random seeds of the XE and SCST-trained FC models and attention models. We see in <ref type="table">Table 5</ref> that ensembling improves performance and confirms the supremacy of attention modeling, and establishes  <ref type="table">Table 5</ref>: Performance of Ensembled XE and SCST-trained models on the Karpathy test split (ensembled over 4 random seeds). The models learned using self-critical sequence training (SCST) were trained to optimize the CIDEr metric. MIXER less CL results (MIXER-) are also included.</p><p>yet another state of the art result on Karpathy splits <ref type="bibr" target="#b21">[21]</ref>.</p><p>Note that in our case we ensemble only 4 models and we don't do any fine-tuning of the Resnet. NIC <ref type="bibr" target="#b23">[23]</ref>, in contrast, used an ensemble of 15 models with fine-tuned CNNs. <ref type="table">Table 6</ref> reports the performance of two variants of 4 ensembled attention models trained with self-critical sequence training (SCST) on the official MSCOCO evaluation server. The previous best result on the leaderboard (as of April 10, 2017) is also depicted. We outperform the previous best system on all evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Online Evaluation on MS-COCO Server</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Example of Generated Captions</head><p>Here we provide a qualitative example of the captions generated by our systems for the image in <ref type="figure">figure 2</ref>. This picture is taken from the objects out-of-context (OOOC) dataset of images <ref type="bibr" target="#b24">[24]</ref>. It depicts a boat situated in an unusual context, and tests the ability of our models to compose descriptions of images that differ from those seen during  <ref type="table">Table 6</ref>: Performance of 4 ensembled attention models trained with self-critical sequence training (SCST) on the official MSCOCO evaluation server (5 reference captions). The previous best result on the leaderboard (as of 04/10/2017) is also depicted ( http://mscoco.org/dataset/#captions-leaderboard, <ref type="table">Table C5</ref>, Watson Multimodal).</p><p>training. The top 5 captions returned by the XE and SCSTtrained FC-2K, FC-15K, and attention model ensembles when deployed with a decoding "beam" of 5 are depicted in <ref type="figure" target="#fig_5">figure 3</ref> 3 . On this image the FC models fail completely, and the SCST-trained ensemble of attention models is the only system that is able to correctly describe the image. In general we found that the performance of all captioning systems on MSCOCO data is qualitatively similar, while on images containing objects situated in an uncommon context <ref type="bibr" target="#b24">[24]</ref> (i.e. unlike the MSCOCO training set) our attention models perform much better, and SCST-trained attention models output yet more accurate and descriptive captions. In general we qualitatively found that SCST-trained attention models describe images more accurately, and with higher confidence, as reflected in <ref type="figure" target="#fig_5">Figure 3</ref>, where the average of the log-likelihoods of the words in each generated caption are also depicted. Additional examples, including an example with the corresponding heat-maps for the SCST-trained Att2in ensemble can be found in the supplementary material ( <ref type="figure">figure 8 of section D)</ref>. We found that Att2in attention models performed better than our Att2all models when applied to images "from the wild", so here we focus on demonstrating the performance of our Att2in systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion and Future Work</head><p>In this paper we have presented a simple and efficient approach to more effectively baselining the REINFORCE algorithm for policy-gradient based RL, which allows us to more effectively train on non-differentiable metrics, and leads to significant improvements in captioning performance on MSCOCO-our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task. The self-critical approach, which normalizes the reward obtained by sampled sentences with the reward obtained by the model under the test-time inference algorithm is intuitive, and avoids having to estimate both action-dependent and action-independent reward functions.  <ref type="figure">Figure 2</ref> by the various models discussed in the paper. Beside each caption we report the average log probability of the words in the caption. On this image, which presents an object situated in an atypical context <ref type="bibr" target="#b24">[24]</ref>, the FC models fail to give an accurate description, while the attention models handle the previously unseen image composition well. The models trained with SCST return a more accurate and more detailed summary of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-critical Sequence Training for Image Captioning:</head><p>Supplementary Material</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Beam search</head><p>Throughout the paper and in this supplementary material we often refer to caption results and evaluation metric results obtained using "beam search". This section briefly summarizes our beam search procedure. While decoding the image to generate captions that describe it, rather than greedily selecting the most probable word (N = 1), we can maintain a list of the N most probable sub-sequences generated so far, generate posterior probabilities for the next word of each of these subsequences, and then again prune down to the N -best sub-sequences. This approach is widely referred to as a beam search, where N is the width of the decoding "beam". In our experiments we additionally prune away hypotheses within the N -best list that have a log probability that is below that of the maximally probable partial sentence by more than ∆ log = 5. For all reported results, the value of N is tuned on a per-model basis on the validation set (of the Karpathy splits). On MSCOCO data, N = 2 is typically optimal for cross-entropy (XE) trained models and SCST-trained models, but in the latter case beam search provides only a very small boost in performance. For our captioning demonstrations we set N = 5 for all models for illustrative purposes, and because we have qualitatively observed that for test images that are substantially different from those encountered during training, beam search is important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance of XE versus SCST trained models</head><p>In tables 4 and 5 of the main text we compared the performance of models trained to optimize the CIDEr metric with self-critical sequence training (SCST) with that of their corresponding bootstrap models, which were trained under the crossentropy (XE) criterion using scheduled sampling <ref type="bibr" target="#b8">[8]</ref>. We provide some additional details about these experiments here. For all XE models, the probability p f of feeding forward the maximally probable word rather than the ground-truth word was increased by 0.05 every 5 epochs until reaching a maximum value of 0.25. The XE model with the best performance on the validation set of the Karpathy splits was then selected as the bootstrap model for SCST (with the exception of the Att2all attention models, where CE training was intentionally terminated prematurely to encourage more exploration during early epochs of RL training).</p><p>For all models, the performance of greedily decoding each word at test time is reported, as is the performance of beam search as described in the previous section. As reported in <ref type="bibr" target="#b7">[7]</ref>, we found that beam search using RL-trained models resulted in very little performance gain. <ref type="figure">Figure 5</ref> depicts the performance of our best Att2in model, which is trained to directly optimize the CIDEr metric, as a function of training epoch and evaluation metric, on the validation portion of the Karpathy splits. Optimizing CIDEr clearly improves all of the MSCOCO evaluation metrics substantially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance of MIXER versus SCST trained models</head><p>SCST consistently outperforms MIXER by more than four CIDER points (c.f. <ref type="figure" target="#fig_3">figure 4 and table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Examples of Generated Captions</head><p>Figures 6-14 depict demonstrations of the captioning performance of all systems. In general we found that the performance of all captioning systems on MSCOCO data is qualitatively similar, while on images containing objects situated in an uncommon context <ref type="bibr" target="#b24">[24]</ref> (i.e. unlike the MSCOCO training set) our attention models perform much better, and SCST-trained attention models output yet more accurate and descriptive captions. Attention heat-maps for the image and corresponding captions depicted in <ref type="figure">figure 6</ref> and 7 are given in <ref type="figure">figure 8</ref>. The heatmaps of the attention weights are reasonably inline with the predicted words in both cases, and the SCST attention weights are spatially sharper here, and in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Further details and analysis of SCST training</head><p>One detail that was crucial to optimizing CIDEr to produce better models was to include the EOS tag as a word. When the EOS word was omitted, trivial sentence fragments such as "with a" and "and a" were dominating the metric gains, despite the "gaming" counter-measures (sentence length and precision clipping) that are included in CIDEr-D <ref type="bibr" target="#b2">[13]</ref>, which is what we optimized. Including the EOS tag substantially lowers the reward allocated to incomplete sentences, and completely resolved this issue. Another more obvious detail that is important is to associate the reward for the sentence with the first EOS encountered. Omitting the reward from the first EOS fails to reward sentence completion which leads to run-on, and rewarding any words that follow the first EOS token is inconsistent with the decoding procedure.</p><p>This work has focused on optimizing the CIDEr metric, since, as discussed in the paper, optimizing CIDER substantially improves all MSCOCO evaluation metrics, as was shown in tables 4 and 5 and is depicted in <ref type="figure">figure 5</ref>. Nevertheless, directly optimizing another metric does lead to higher evaluation scores on that same metric as shown, and so we have started to experiment with including models trained on Bleu, Rouge-L, and METEOR in our Att2in ensemble to attempt to improve it further. So far we have not been able to substantially improve performance w.r.t. the other metrics without more substantially degrading CIDEr.   Captions generated for the image depicted in <ref type="figure" target="#fig_7">Figure 9</ref> by various models discussed in the paper. Beside each caption we report the average log probability of the words in the caption. All models perform well on this test image from the MSCOCO distribution. More generally we have observed that qualitatively, all models perform comparably on the MSCOCO test images.  Captions generated for the image depicted in <ref type="figure" target="#fig_3">Figure 11</ref> by the various models discussed in the paper. Beside each caption we report the average log probability of the words in the caption. On this image, which presents an object situated in an atypical context <ref type="bibr" target="#b24">[24]</ref>, the FC models fail to give an accurate description, while the attention models handle the previously unseen image composition well. The models trained with SCST return a more accurate and more detailed summary of the image.  Captions generated for the image depicted in <ref type="figure" target="#fig_3">Figure 13</ref> by the various models discussed in the paper. Beside each caption we report the average log probability of the words in the caption. On this image, which presents an object situated in an atypical context <ref type="bibr" target="#b24">[24]</ref>, the FC models fail to give an accurate description, while the attention models handle the previously unseen image composition well. The models trained with SCST return a more accurate and more detailed summary of the image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>1 , . . . w s T ) and w s t is the word sampled from the model at the time step t. In practice L(θ) is typically estimated with a single sample from p θ :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>T</head><label></label><figDesc>) from p θ , for each training example in the minibatch:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Self-critical sequence training (SCST). The weight put on words of a sampled sentence from the model is determined by the difference between the reward for the sampled sentence and the reward obtained by the estimated sentence under the test-time inference procedure (greedy inference depicted). This harmonizes learning with the inference procedure, and lowers the variance of the gradients, improving the training procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Captions generated for the image depicted in Figure 2 by the various models discussed in the paper. Beside each caption we report the average log probability of the words in the caption. On this image, which presents an object situated in an atypical context [24], the FC models fail to give an accurate description, while the attention models handle the previously unseen image composition well. The models trained with SCST return a more accurate and more detailed summary of the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :Figure 5 :Figure 8 :</head><label>458</label><figDesc>Figure 4: SCST vs. MIXER (FC models) over multiple random seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: An image from the MSCOCO test set (Karpathy splits).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Captions generated for the image depicted in Figure 9 by various models discussed in the paper. Beside each caption we report the average log probability of the words in the caption. All models perform well on this test image from the MSCOCO distribution. More generally we have observed that qualitatively, all models perform comparably on the MSCOCO test images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: An image from the objects out-of-context (OOOC) dataset of images from [24].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Captions generated for the image depicted in Figure 11 by the various models discussed in the paper. Beside each caption we report the average log probability of the words in the caption. On this image, which presents an object situated in an atypical context [24], the FC models fail to give an accurate description, while the attention models handle the previously unseen image composition well. The models trained with SCST return a more accurate and more detailed summary of the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: An image from the objects out-of-context (OOOC) dataset of images from [24].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Captions generated for the image depicted in Figure 13 by the various models discussed in the paper. Beside each caption we report the average log probability of the words in the caption. On this image, which presents an object situated in an atypical context [24], the FC models fail to give an accurate description, while the attention models handle the previously unseen image composition well. The models trained with SCST return a more accurate and more detailed summary of the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>table 2 and graph in supp. material).</figDesc><table>Training 
Evaluation Metric 
Metric 
CIDEr BLEU4 ROUGEL METEOR 
XE 
90.9 
28.6 
52.3 
24.1 
XE (beam) 
94.0 
29.6 
52.6 
25.2 
CIDEr 
106.3 
31.9 
54.3 
25.5 
BLEU 
94.4 
33.2 
53.9 
24.6 
ROUGEL 
97.7 
31.6 
55.4 
24.5 
METEOR 
80.5 
25.3 
51.3 
25.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Performance on the test portion of the Karpathy splits [21] as a function of training metric ( FC-2K models).</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Performance of the best XE and corr. SCST-trained single models on the Karpathy test split (best of 4 random seeds). The results obtained via the greedy decoding and optimized beam search are depicted. Models learned using SCST were trained to directly optimize the CIDEr metric. MIXER less CL results (MIXER-) are also included.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Authors Etienne Marcheret and Vaibhava Goel were at IBM while the work was being completed.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">please consult the supplementary material for additional details regarding how the models were trained.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">pls. consult the the supp. material for further details on beam search.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 2</ref><p>: An image from the objects out-of-context (OOOC) dataset of images from <ref type="bibr" target="#b24">[24]</ref>.  1. a scooter parked in front of a building 0.326988 2. a group of a motorcycle parked in front of a building 0.366700 3. a group of surfboards in front of a building 0.386932 4. a scooter parked in front of a building with a clock 0.429441 5. a scooter parked in front of a building with a building 0.433893 <ref type="figure">Figure 6</ref>: Picture of a common object in MSCOCO (a giraffe) situated in an uncommon context (out of COCO domain) <ref type="bibr" target="#b24">[24]</ref>.    <ref type="figure">Figure 7</ref>: Captions generated by various models discussed in the paper to describe the image depicted in <ref type="figure">figure 6</ref>. Beside each caption we report the average of the log probabilities of each word, normalized by the sentence length. Notice that the attention models trained with SCST give an accurate description of this image with high confidence. Attention models trained with XE are less confident about the correct description. FC models trained with CE or SCST fail at giving an accurate description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Caption Generation Demo With Visual Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Caption Generation Demo With Visual Attention</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Microsoft COCO: common objects in context. EECV, 2014. 1, 4</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoderdecoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bart Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Aglar Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Professor forcing: A new algorithm for training recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS) 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out: Proceedings of the ACL-04 workshop</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Ramakrishna Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">High-dimensional continuous control using generalized advantage estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02438</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reinforcement learning neural turing machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.0030</idno>
		<title level="m">Neural variational inference and learning in belief networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An actor-critic algorithm for sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<title level="m">Show and tell: Lessons learned from the 2015 MSCOCO image captioning challenge. PAMI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Context models and out-of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung</forename><surname>Jinchoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note>context objects. 7, 8, 10, 13</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Image Caption Generation Demo With Visual Attention</title>
		<ptr target="http://dccxc026.pok.ibm.com:60000/upload" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Image Caption Generation Demo With Visual Attention</title>
		<ptr target="http://dccxc026.pok.ibm.com:60000/upload" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Attention heat-maps for the best model in the XE-trained ensemble of attention models, for the image depicted in figure 6</title>
		<idno>11/15/2016</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Image Caption Generation Demo With Visual Attention</title>
		<ptr target="http://dccxc027.pok.ibm.com:60000/upload2/2" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
