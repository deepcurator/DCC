<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Feature Augmentation for Unsupervised Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Volpi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Pattern Analysis &amp; Computer Vision -Istituto Italiano di Tecnologia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Morerio</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Pattern Analysis &amp; Computer Vision -Istituto Italiano di Tecnologia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Stanford Vision and Learning Lab</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
							<email>vittorio.murino@iit.it</email>
							<affiliation key="aff0">
								<orgName type="department">Pattern Analysis &amp; Computer Vision -Istituto Italiano di Tecnologia</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Università di Verona</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Feature Augmentation for Unsupervised Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Recent works showed that Generative Adversarial Networks (GANs) can be successfully applied in unsupervised domain adaptation, where, given a labeled source dataset and an unlabeled target dataset, the goal is to train powerful classifiers for the target samples. In particular, it was shown that a GAN objective function can be used to learn target features indistinguishable from the source ones. In this work, we extend this framework by (i) forcing the learned feature extractor to be domain-invariant, and (ii) training it through data augmentation in the feature space, namely performing feature augmentation. While data augmentation in the image space is a well established technique in deep learning, feature augmentation has not yet received the same level of attention. We accomplish it by means of a feature generator trained by playing the GAN minimax game against source features. Results show that both enforcing domain-invariance and performing feature augmentation lead to superior or comparable performance to state-of-the-art results in several unsupervised domain adaptation benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative adversarial networks (GANs <ref type="bibr" target="#b9">[10]</ref>) are models capable of mapping noise vectors into realistic samples from a data distribution. GANs are defined by two neural networks, a generator and a discriminator, and the training procedure is a minimax game where the generator is optimized to fool the discriminator, and the discriminator is optimized to correctly classify generated samples from actual training samples. Recently, this framework proved to be able to generate images with impressive accuracy <ref type="bibr" target="#b23">[24]</ref>, to generate videos from static frames <ref type="bibr" target="#b36">[37]</ref>, and to translate images from one style to another <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>Furthermore, GANs have been exploited in the context of unsupervised domain adaptation. Here, a source (labeled) dataset and a target (unlabeled) dataset are considered, which are separated by the so-called domain shift <ref type="bibr" target="#b32">[33]</ref>, i.e., they are drawn from two different data distributions. Unsupervised domain adaptation aims at building models that are able to correctly classify target samples, despite the domain shift. In this framework, adversarial training has been used (i) to learn feature extractors that map target samples in a feature space indistinguishable from the one where source samples are mapped <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b33">34]</ref>, and (ii) to develop image-to-image translation algorithms <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b16">17]</ref> aimed at converting source images in a style that resembles that of the target image domain.</p><p>In this paper, we build on the work by Tzeng et al. <ref type="bibr" target="#b33">[34]</ref>, which proposes to use a GAN objective to learn target features that are indistinguishable from the source ones, leading to a pair of feature extractors, one for the source and one for the target samples. We extend this approach in two directions: (a) we force domain-invariance in a single feature extractor trained through GANs, and (b) we perform data augmentation in the feature space (i.e., feature augmentation), by defining a more complex minimax game. More specifically, we perform feature augmentation by devising a feature generator trained with a Conditional GAN (CGAN <ref type="bibr" target="#b20">[21]</ref>). The minimax game is here played with features instead of images, allowing to generate features conditioned to the desired classes. The CGAN generator is thus able to learn the class distribution in the feature space, and therefore to generate an arbitrary number of labeled feature vectors. Our results show that forcing domain-invariance and augmenting features are both valuable approaches in the unsupervised domain adaptation setting, leading to higher classification accuracies.</p><p>In summary, the main contributions of this paper are the following:</p><p>1. Introducing for the first time the use of GANs to perform data augmentation in the feature space.</p><p>2. Proposing a new method for unsupervised domain adaptation, based on feature augmentation and (source/target) feature domain-invariance.</p><p>3. Evaluating the proposed method on unsupervised domain adaptation benchmarks (cross-dataset digit classification and cross-modal object classification), obtaining results which are superior or comparable to current state-of-the-art in most of the addressed tasks.</p><p>The remaining of the paper is organized as follows. Section 2 is dedicated to the related work. The models and the training procedure are presented in Section 3. In Section 4, the datasets used for the analysis and method's validation are described. The experiments and associated results are detailed in Section 5. Finally, conclusive remarks are drawn in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The work related to our proposed method is focused on GAN research and on modern domain adaptation techniques (i.e., based on deep learning).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generative adversarial networks.</head><p>In the original formulation by Goodfellow et al. <ref type="bibr" target="#b9">[10]</ref>, a GAN model is trained through a minimax game between a generator, that maps noise vectors in the image space, and a discriminator, trained to discriminate generated images from real ones. Several other papers address ways to control what GANs generate <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b25">26]</ref>. In particular, CGANs <ref type="bibr" target="#b20">[21]</ref> allow to condition on the desired classes, from which samples are generated. Other works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref> propose to learn inference by playing a minimax game against features. In these works, trained models are feature extractors that map images into the feature space, not feature generators, which is our primary goal.</p><p>Performing feature augmentation through GANs is one of the original aspects of our approach. We propose a generator able to generate features from noise vectors and label codes, via a CGAN <ref type="bibr" target="#b20">[21]</ref> framework, playing a minimax game with features extracted from a pre-trained model instead of images.</p><p>Unsupervised domain adaptation. Ganin and Lempitsky <ref type="bibr" target="#b7">[8]</ref> propose a neural network (Domain-Adversarial Neural Network, DANN) where a ConvNet-based <ref type="bibr" target="#b15">[16]</ref> feature extractor is optimized to both correctly classify source samples and have domain-invariant features, through adversarial training. Different works <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b18">19]</ref> aim at minimizing the Maximum Mean Discrepancy <ref type="bibr" target="#b10">[11]</ref> between features extracted from source and target samples, training a classifier to correctly classify source samples while minimizing this measure. Bousmalis et al. <ref type="bibr" target="#b1">[2]</ref> propose to learn image representations divided in two components, one shared across domains and one private, following the hypothesis that modeling unique elements in each domain can help to extract features which are domain-invariant. Tzeng et al. <ref type="bibr" target="#b33">[34]</ref> use GANs to train an encoder for target samples, by making the features extracted with this model indistinguishable from the ones extracted through an encoder trained with source samples. The last layer of the latter can then be used for both encoders to infer labels. Saito et al. <ref type="bibr" target="#b27">[28]</ref> propose an asymmetric tri-training where pseudo-labels are inferred and exploited for target samples during training. In particular, two networks are trained to assign labels to target samples and one to obtain target-discriminative features. Haeusser et al. <ref type="bibr" target="#b12">[13]</ref> propose to exploit associations between source and target features during training, to maximize the domain-invariance of the learned features while minimizing the error on source samples.</p><p>Recently, several image-to-image translation methods have been proposed to solve unsupervised domain adaptation tasks. Taigman et al. <ref type="bibr" target="#b31">[32]</ref> propose the Domain Transfer Network (DTN), that allows to translate images from a source domain to a target one, under a f -constancy constraint, where f is a generic function that maps images in a feature space. Translated images result portrayed in the target images' style, while maintaining the content of the images fed in input. Liu and Tuzel <ref type="bibr" target="#b17">[18]</ref> introduce Coupled GAN (CoGAN), an extension of GAN that allows to model a joint distribution P (X, Y ) and to generate couples of images from noise vectors, one belonging to P (X) and one to P (Y ). This model can be applied to image-toimage translation tasks: fixing one image, the noise vector that most likely could have generated that picture can be inferred and, feeding it to the model, the second image is generated. Bousmalis et al. <ref type="bibr" target="#b0">[1]</ref> propose to train an image-toimage translation network relying on both a GAN loss and a task-specific loss (and in problems with prior knowledge, also a content-specific loss). The resulting network takes in input both an image and a noise vector, that allows to generate a potentially infinite number of target images. Liu et al. <ref type="bibr" target="#b16">[17]</ref> propose UNIT, an extension of CoGAN that relies on both GANs and Variational Auto-Encoders, and makes the assumption of a shared latent space. Image-to-image translation methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b16">17]</ref> are applied to unsupervised domain adaptation by generating target images and training classifiers directly on them.</p><p>The domain-invariant feature extractor we designed is inspired by Tzeng et al. <ref type="bibr" target="#b33">[34]</ref>, with two main differences. First, we play the minimax game against features which are generated by a pre-trained model, thus performing feature augmentation. Second, we train the feature extractor in order to make it work for both source and target samples (thus achieving domain-invariance), avoiding catastrophic forgetting. Both modifications lead to higher accuracies in classifying target samples, as we will show in Section 5. Domain-invariance also allows to use the same feature extractor for both source and target samples, while in Tzeng et al. <ref type="bibr" target="#b33">[34]</ref> two different encoders are required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head><p>Our goal is to train a domain-invariant feature extractor (E I ), whose training procedure is made more robust by data augmentation in the space of source features. The training procedure we designed to accomplish our intent is based on three different steps, depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. First, we need to train a feature extractor on source data (C • E s ). This step is necessary because we need a reference feature space and a reference classifier that performs well on it. Secondly, we need to train a feature generator (S) to perform data augmentation in the source feature space. We can train it by playing a GAN minimax game against features extracted through E S . Finally, we can train a domaininvariant feature extractor (E I ) by playing a GAN minimax game against features generated through S. This module can then be combined with the softmax layer previously trained (C • E I ) to perform inference on both source and target samples. All modules are neural networks trained by backpropagation <ref type="bibr" target="#b26">[27]</ref>. In the following sections, we detail how each Step is performed, how new features can be generated, and how source/target labels can be inferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training</head><p>Step 0. The model C • E s is trained to classify source samples. E s represents a ConvNet feature extractor and C represents a fully connected softmax layer, with a size that depends on the problem. The optimization problem consists in the minimization of the following cross-entropy loss (CE Loss in <ref type="figure" target="#fig_0">Figure 1</ref>):</p><formula xml:id="formula_0">min θ Es ,θ C ℓ 0 = E (xi,yi)∼(Xs,Ys) H(C • E s (x i ), y i ),<label>(1)</label></formula><p>where θ Es and θ C indicate the parameters of E s and C, respectively, X s , Y s are the distributions of source samples (x i ) and source labels (y i ), respectively, and H represents the softmax cross-entropy function.</p><p>Step 1. The model S is trained to generate feature samples that resemble the source features. Exploiting the CGAN framework, the following minimax game is defined:</p><formula xml:id="formula_1">min θ S max θ D 1 ℓ 1 = E (z,yi)∼(pz(z),Ys) D 1 (S(z||y i )||y i ) − 1 2 + E (xi,yi)∼(Xs,Ys) |D 1 (E s (x i )||y i ) 2 ,<label>(2)</label></formula><p>where θ S and θ D1 indicate the parameters of S and D 1 , respectively, p z (z) is the distribution 1 from which noise samples are drawn, and denotes a concatenation operation. In this and the following steps, we relied on Least Squares GANs <ref type="bibr" target="#b19">[20]</ref> since we observed more stability during training. number of new feature samples, we only need S, which takes as input the concatenation of a noise vector and a one-hot label code, and outputs a feature vector from the desired class:</p><formula xml:id="formula_2">F (z|y) = S(z||y)<label>(3)</label></formula><p>where z ∼ p z (z) and F is a feature vector belonging to the class label associated with y (dashed box in <ref type="figure" target="#fig_0">Figure 1</ref>, left).</p><p>Step 2. The domain-invariant encoder E I is trained via the following minimax game, after being initialized with weights optimized on Step 0 (note that E S and E I have the same architecture), a requirement to reach optimal convergence:</p><formula xml:id="formula_3">min θ E I max θ D 2 ℓ 2 = E xi∼Xs∪Xt D 2 (E I (x i )) − 1 2 (4) + E (z,yi)∼(pz(z),Ys) D 2 (S(z||y i )) 2 ,</formula><p>where θ E I and θ D2 indicate the parameters of E I and D 2 , respectively. Since the model E I is trained using both source and target domains, the feature extractor results domain-invariant. In particular, it maps both source and target samples in a common feature space, where features are indistinguishable from the ones generated through S. Being the latter trained to produce features indistinguishable from the source ones, the feature extractor E I can be combined with the classification layer of Step 0 (C) and used for inference (as in Tzeng et al. <ref type="bibr" target="#b33">[34]</ref>):</p><formula xml:id="formula_4">y i = C • E I (x i ),<label>(5)</label></formula><p>where x i is a generic image from the source or the target data distribution andỹ i is the inferred label (dashed box in <ref type="figure" target="#fig_0">Figure 1</ref>, right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Datasets</head><p>To evaluate our approach, we used several benchmark splits of public source/target datasets adopted in domain adaptation.</p><p>MNIST ↔ USPS. Both datasets consist of white digits on a solid black background. We tested two different protocols: the first one (P1) consists in sampling 2, 000 MNIST <ref type="bibr" target="#b15">[16]</ref> images and 1, 800 USPS <ref type="bibr" target="#b4">[5]</ref> images. The second one (P2) consists in using the whole MNIST training set, 50, 000 images, and dividing USPS in 6, 562 images for training, 2.007 for testing, and 729 for validation. For P1, we tested the two directions of the split (MNIST → USPS and MNIST ← USPS). For P2, we tested only MNIST → USPS, and we avoided to use the validation set in this case, too. In both experimental protocols, we resized USPS digits to 28 × 28 pixels, which is the MNIST images' size. SVHN → MNIST. SVHN <ref type="bibr" target="#b22">[23]</ref> is built with real images of Street View House Numbers. We used the whole training sets of both datasets, following the standard protocol for unsupervised domain adaptation (SVHN training set contains 73, 257 images), and tested on MNIST test set. We resized MNIST images to 32 × 32 pixels and converted SVHN to grayscale. We did not use the extra set of SVHN.</p><p>SYN DIGITS → SVHN. This split represents a syntheticto-real domain adaptation problem, of great interest for research in computer vision since, quite often, generating labeled synthetic data requires less effort than obtaining large labeled dataset with real examples. SYN DIGITS <ref type="bibr" target="#b7">[8]</ref> contains 500, 000 images belonging to the same SVHN classes. We tested on SVHN test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NYUD (RGB → D)</head><p>. This modality adaptation problem was proposed by Tzeng et al. <ref type="bibr" target="#b33">[34]</ref>. The dataset is gathered by cropping out tight bounding boxes around instances of 19 object classes present in the NYUD <ref type="bibr" target="#b28">[29]</ref> dataset. It comprises 2,186 labeled source (RGB) images and 2,401 unlabeled target (HHA-encoded <ref type="bibr" target="#b11">[12]</ref>) depth images. Note that these are obtained from two different splits of the original dataset, to ensure that the same instance is not seen in both domains. The adaptation task is extremely challenging, due to the very different domains, the limited number of examples (especially for some classes), and the low resolution of the cropped bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we evaluate our approach. First, we show that our model S is able to generate consistent and discriminant feature vectors conditioned on the desired classes. Second, we report an ablation study to figure out the benefits brought by the different steps that compose our approach. Finally, we compare our method with competing algorithms on unsupervised domain adaptation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Architectures</head><p>A detailed description of architectures and hyperparameters used (learning rate, batch sizes, etc.) is reported in the Supplementary Material. We provide here the details necessary for a basic understanding of the experiments.</p><p>2 . S is built by the repetition of two blocks, each defined by a fully connected layer, a Batch Normalization layer <ref type="bibr">[14]</ref>, and a Dropout layer <ref type="bibr" target="#b30">[31]</ref>, followed by a fully connected layer with tanh activation functions. D 1 is a one-hiddenlayer neural network, with a sigmoid hidden unit as output layer. We defined E S and E I following standard architectures used in unsupervised domain adaptation <ref type="bibr" target="#b7">[8]</ref>. In partic- ular, for SVHN → MNIST, MNIST → USPS and USPS → MNIST, we defined the network as conv-pool-conv-poolfc-fc-softmax (with Dropout <ref type="bibr" target="#b30">[31]</ref> on fully connected layers for MNIST ↔ USPS experiments). For SYN → SVHN, conv-pool-conv-pool-conv-fc-fc-softmax. For the NYUD experiment, in order to be comparable with <ref type="bibr" target="#b33">[34]</ref>, we used a VGG-16 <ref type="bibr" target="#b29">[30]</ref> pretrained on ImageNet <ref type="bibr" target="#b3">[4]</ref>. The final feature dimensionality (e.g., the size of the feature vector fed to the softmax layer) was set to 128 for all experiments, except for SYN → SVHN (256). D 2 is built with two or three fully connected layers (depending on the experiment) with a sigmoid unit on top. Note that for the NYUD experiment we used three hidden layers, while Tzeng et al. <ref type="bibr" target="#b33">[34]</ref> built the discriminator with two, since our method requires an additional one to reach convergence. For all our experiments, we used Adam optimizer <ref type="bibr" target="#b14">[15]</ref> with momentum set to 0.95. ReLU <ref type="bibr" target="#b21">[22]</ref> units were used throughout the architectures, except for last layers of discriminators, defined as sigmoid units, last layer of S, whose activation functions are tanh, and D 2 , which was built with Leaky ReLU units, in agreement with the findings of Radford et al. <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Generating features</head><p>We qualitatively show with t-SNE <ref type="bibr" target="#b35">[36]</ref> that we can generate feature vectors from the desired classes, after having trained S as described in Section 3.1. <ref type="figure" target="#fig_2">Figure 2</ref> shows comparisons between real and generated features for different datasets. For each dataset, two identical point clouds are represented: the bi-color side (at the left of each panel), highlights real and generated samples (in red and blue, respectively); the multi-color side (at the right of each panel) highlights instead the different classes. From a <ref type="table">Table 1</ref>. Second column: number of activation patterns (APs) among the features extracted from training data. Third column: number of APs that S is able to generate. Fourth column: classification accuracy of the generated features, accordingly to given labels.  qualitative point of view, real and generate features appear indistinguishable, and class structure is preserved. To quantitatively measure the quality of the features generated, we fed them to the classifier C trained with the original samples for class estimation. <ref type="table">Table 1 (fourth column)</ref> shows that such features are also quantitatively reliable, and this is valid for all the datasets considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset #APs E S (x) #APs S(z y)</head><p>Feature augmentation. Finally, we are interested in evaluating the variability of the features generated through S to figure out whether (i) the model is memorizing the features from the training set, and (ii) it is realistic to assume that we are performing data augmentation in the feature space. To shed light on these two questions, we decided to perform the following empirical test: we counted the number of activation patterns (APs) that S is able to generate, and compared it with the ones intrinsically available in the original dataset. An activation pattern is defined by thresholding the output of the activation functions of the hidden state of a network. Raghu et al. <ref type="bibr" target="#b24">[25]</ref> defined this concept for ReLUs <ref type="bibr" target="#b21">[22]</ref>, where values greater than zero are set to one, the others to zero. For our purposes, we can apply the same rule even if we are using tanh activation functions. For example, SVHN has 73, 257 samples that -with the feature extractor we used for our experiments -correspond to 69, 625 activation patterns. S can instead generate a number of activation patterns in the order of 10 6 (counted empirically, feeding noise to S till saturation), indistinguishable from the original ones due to the training procedure defined in Section 3.1. <ref type="table">Table 1</ref> reports the results associated with the other datasets considered. Interestingly, activation patterns associated with the 2, 186 source samples of NYUD are only 19: each pattern is associated with a different class. This is most likely due to overfitting: the network is already explicitly encoding classes at feature level. However, the generator S can enrich the feature set to a broad extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation study</head><p>We carried out an ablation study to evaluate the benefit brought by the introduced modifications to the current way of using GAN objectives in unsupervised domain adaptation. Since the Least Squares GAN <ref type="bibr" target="#b19">[20]</ref> framework is required to solve Step 1 and Step 2 of our method (Section 3.1), we re-designed the ADDA algorithm <ref type="bibr" target="#b33">[34]</ref> in this framework as a baseline, and from this point we imple- <ref type="table">Table 2</ref>. Comparison of our method with competing algorithms. The row LS-ADDA lists results obtained by our implementation of Least Squares ADDA. The row Ours (DI) refers to our approach in which only domain-invariance is imposed. The row Ours (DIFA) refers to our full proposed method, which includes feature augmentation. (*) DTN <ref type="bibr" target="#b31">[32]</ref> and UNIT <ref type="bibr" target="#b16">[17]</ref> use extra SVHN data (531, 131 images). (**) Protocols P1 and P2 are mixed in the results section of Bousmalis et al. <ref type="bibr" target="#b0">[1]</ref>. Convergence not reached is indicated as no conv.  <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> 0.739 0.771 ± 0.018 <ref type="bibr" target="#b33">[34]</ref> -0.730 ± 0.020 <ref type="bibr" target="#b33">[34]</ref> 0.911 -DDC <ref type="bibr" target="#b33">[34]</ref> 0.681 ± 0.003 0.791 ± 0.005 -0.665 ± 0.033 --DSN <ref type="bibr" target="#b1">[2]</ref> 0.827 ---0.912 -ADDA <ref type="bibr" target="#b33">[34]</ref> 0.760 ± 0.018 0.894 ± 0.002 -0.901 ± 0.008 -0.211 Tri <ref type="bibr" target="#b27">[28]</ref> 0.862 ---0.931 -DTN <ref type="bibr" target="#b31">[32]</ref> 0.844* -----PixelDA** <ref type="bibr" target="#b0">[1]</ref> --0.959 ---UNIT <ref type="bibr" target="#b16">[17]</ref> 0.905* -0.960 ---CoGANs <ref type="bibr" target="#b17">[18]</ref> no conv. <ref type="bibr" target="#b33">[34]</ref> 0.912 ± 0.008 0.957 <ref type="bibr" target="#b16">[17]</ref> 0.891 ± 0. 468 <ref type="bibr" target="#b33">[34]</ref> mented our peculiar contributions, showing that each one favourably concurs to improve performance. We term it LS-ADDA, and it is defined by the following minimax game:</p><formula xml:id="formula_5">min θ E t max θ D ℓ = E xi∼Xt D(E t (x i )) − 1 2 (6) + E xi∼Xs D(E s (x i )) 2 ,</formula><p>where E s is the feature extractor trained on source samples (as the one pre-trained in Step 0, <ref type="figure" target="#fig_0">Figure 1)</ref>, and E t is the encoder for the target samples that is being trained. D is the discriminator, as those described in this work. The second analysis stage lies in imposing domaininvariance, and this is carried out by solving the following minimax problem:</p><formula xml:id="formula_6">min θ E I max θ D ℓ = E xi∼Xs∪Xt D(E I (x i )) − 1 2 (7) + E xi∼Xs D(E s (x i )) 2 ,</formula><p>where E I is the shared encoder for the source and target samples that is being trained, and the rest of the modules are the same described above. This represents our first notable contribution, which we call DI (short for DI LS-ADDA, as this architecture introduces domain-invariance to LS-ADDA). Finally, the third analysis stage is constituted by our complete proposed approach, in which the minimax game also embeds the feature augmentation procedure (described in Step 2 of Section 3.1). We term it DIFA (DomainInvariance + Feature Augmentation). For each of the three architectures proposed in this ablation study, we finally end up with an encoder that can be combined with the module C trained in Step 0 (see <ref type="figure" target="#fig_0">Figure 1)</ref>. We tested these algorithms on the benchmark splits detailed in Section 4. <ref type="figure" target="#fig_3">Figure 3</ref> shows the evolution of the performance of these three frameworks throughout the minimax games: green curves are associated with LS-ADDA, orange curves are associated with DI, and blue curves are associated with DIFA. The values reported in the bottom part of the plots indicate the average and the standard deviation calculated over the final stages of training, i.e., when the minimax game reaches a stability point, despite oscillations. For the splits SVHN → MNIST and SYN → SVHN, we averaged over three different runs, due to some instability in the equilibriums reached, that can be observed in <ref type="figure" target="#fig_3">Figure 3</ref>. The general trend is that enforcing domain-invariance (DI) brings a first improvement (except in the MNIST → USPS (P1) experiment), and feature augmentation (DIFA) adds a further increment. In NYUD, LS-ADDA cannot converge.</p><p>The only exception is USPS → MNIST, where LS-ADDA is the best performing method. Note that we did not report experiments related to embedding feature augmentation without domain-invariance because it performs poorly, due to high instability. <ref type="table">Table 2</ref> reports our findings and results obtained by the other works in the literature. The first row reports accuracies on target data achieved with non-adapted classifiers trained on source data, and the last row reports accuracies on target data achieved with classifiers trained on target data (oracle). Our main contributions lie in forcing the domaininvariance in the GAN minimax game (DI) and further improving it with feature augmentation (DIFA). A difficulty in unsupervised domain adaptation is determine the fair accuracy reached by each method, since cross-validation is not feasible (target labels should be used only to evaluate the method at the end of the training procedure). We believe that a fair way is the one we proposed in the previous section (mean ± std calculated over the last iterations), since choosing a single value would be arbitrary and unfair in stochastic training procedures (e.g., see SVHN → MNIST and SYN → SVHN in <ref type="figure" target="#fig_3">Figure 3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparisons with other methods</head><p>Results show that our approach based on domaininvariance and feature augmentation leads to accuracies comparable or higher to current state-of-the-art in several unsupervised domain adaptation benchmarks. Among the splits we tested, the only exception is USPS → MNIST, where ADDA <ref type="bibr" target="#b33">[34]</ref> and our implementation of it (LS-ADDA) perform better -with the drawback of having two different feature extractors for source/target samples. In SVHN → MNIST, our approach gives results comparable to current state-of-the-art (UNIT <ref type="bibr" target="#b16">[17]</ref>), but it must be noted that the latter was achieved by making use of extra SVHN set (531, 131 images), making the result difficult to interpret. In MNIST → USPS (P2) we perform better or comparably to any other method that was tested on it. Also note that all those methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17]</ref> rely on the generation of target images to perform adaptation, and that <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref> rely on additional hyperparameters -a severe drawback in unsupervised domain adaptation, where cross-validation is not applicable. In SYN → SVHN, our method is statistically comparable with the one proposed by Saito et al. <ref type="bibr" target="#b27">[28]</ref>. In this case, it is also worth noting that the adapted feature extractor performs better than a neural network trained on SVHN (target) training set (see <ref type="table">Table 2</ref>, last row). This opens a wide range of possibility of using synthetic data, which are much easier to obtain than labeled, real data in real-world applications. In NYUD (RGB → Depth), we perform better than ADDA <ref type="bibr" target="#b33">[34]</ref> by a large margin. In particular, embedding both domain-invariance and feature augmentation leads to an improvement &gt; 10%. We did not include the work by Haeusser et al. <ref type="bibr" target="#b12">[13]</ref> in <ref type="table">Table 2</ref> because it makes use of a much more powerful feature extractor (conv-conv-poolconv-conv-pool-conv-conv-pool-fc-softmax), which makes their method hard to compare with other works.</p><p>Finally, <ref type="table" target="#tab_2">Table 3</ref> shows the difference of performance on classifying source samples using C • E s or C • E I . As it can be observed, the encoder E I (trained following Step 2) works well on source samples, too. This allows to use the same encoder for both target and source data, a very useful feature in an application setting where we might not know the source of the data. The worst results on source samples, achieved on SVHN dataset, are most likely due to the large difference between the source and the target domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Limitations</head><p>The main limit of the domain-invariant feature extractor we designed is the same that can be detected in the works by Tzeng et al. <ref type="bibr" target="#b33">[34]</ref> and by Ganin and Lempitsky <ref type="bibr" target="#b7">[8]</ref>. Practically, all these approaches encourage source and target features to be indistinguishable, but this does not guarantee that target samples will be mapped in the correct regions of the feature space. In our case and in ADDA's one, this strongly depends on the feature extractor trained on source samples: if the representation is far from being good, the results will be sub-optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and future work</head><p>In this work, we proposed two techniques to improve the current usage of GAN objectives in the unsupervised domain adaptation framework. First, we induced domaininvariance through a straightforward extension of the original algorithm. Second, we proposed to perform data augmentation in the feature space through GANs <ref type="bibr" target="#b9">[10]</ref>, a novel application. An exhaustive evaluation was carried out on standard domain adaptation benchmarks, and results confirmed that both approaches lead to higher accuracies on target data. Also, we showed that the obtained feature extractors can be used on source data, too.</p><p>Results showed that our approach is comparable or superior to current state-of-the-art methods, with the exception of a single benchmark. In particular, we performed better than recent, more complex methods that rely on generating target images to tackle unsupervised domain adaptation tasks. This achievement re-opens the debate on the necessity of generating images belonging to the target distribution: recent results <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref> seemed to suggest it.</p><p>For future work, we plan to test our approach on more complex unsupervised domain adaptation problems, as well as investigate if feature augmentation can be applied to different frameworks, e.g., the contexts where traditional data augmentation proved to be successful.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Training procedure, representing the steps described in Section 3.1. Solid lines indicate that the module is being trained, dashed lines indicate that the module is already trained (from previous steps). All modules are neural networks, whose architectures are detailed in Section 5.1. Smaller, dashed panels in the bottom indicate how to generate features (left) and how to infer source or target labels (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Feature generation .</head><label>generation</label><figDesc>In order to generate an arbitrary 1 Uniform in the range [−1, 1] throughout this work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. t-SNE plots of features associated with different adopted datasets (MNIST, SVHN, SYN, USPS). For each dataset, in the left part of the panels, red and blue dots indicate real and generated features, respectively. In the right part of the panels, different colors indicate different classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Accuracies on target samples evaluated throughout the training of the feature extractors of LS-ADDA (green), DI (orange) and DIFA (blue). Inference was performed by combining the feature extractor being learned with C of Step 0, Section 3.1. In the NYUD experiment the green curve is missing due to non-convergence of LS-ADDA. SVHN → MNIST and SYN → SVHN plots were obtained averaging over three different runs; confidence bands are portrayed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Difference in accuracy between training and test source data, by classifying with C • ES and C • EI . Source test data is not provided for NYUD [34]. EI does not experience catastrophic forgetting and generalizes well on unseen source data (test).</figDesc><table>Dataset 
E S → E I (training) E S → E I (test) 
USPS 
0.975 → 0.973 
0.980 → 0.979 
MNIST (P1) 
1.000 → 0.997 
0.960 → 0.961 
MNIST (P2) 
0.997 → 0.986 
0.992 → 0.984 
SVHN 
0.982 → 0.883 
0.905 → 0.856 
SYN 
0.998 → 0.996 
0.995 → 0.994 
NYUD 
1.000 → 1.000 
test set n.a. 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Models were implemented using Tensorflow, and training procedures were performed on a NVIDIA Titan X GPU. Code will be made available at https://github.com/ricvolpi</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Lyne P. Tchapmi for helpful suggestions on an early draft. The research reported in this publication was supported by funding from MURI (1186514-1-TBCJE).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Erhan ; D</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<title level="m">Advances in neural information processing systems 1. chapter Neural Network Recognizer for Handwritten Zip Code Digits</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="323" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1605.09782</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno>abs/1606.00704</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML 2015</title>
		<meeting>the 32nd International Conference on Machine Learning, ICML 2015<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domainadversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Associative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haeusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Frerix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Unsupervised image-toimage translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno>abs/1703.00848</idno>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32Nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multiclass generative adversarial networks with the L2 loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y K</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1611.04076</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial nets. CoRR, abs/1411.1784</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<editor>J. Frnkranz and T. Joachims</editor>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the expressive power of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohldickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>D. Precup and Y. W. Teh</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
<note type="report_type">PMLR. 6</note>
	<note>International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning what and where to draw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="217" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parallel distributed processing: Explorations in the microstructure of cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">chapter Learning Internal Representations by Error Propagation</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="318" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Asymmetric tri-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2988" to="2997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support infer-ence from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised crossdomain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;11</title>
		<meeting>the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;11<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3474</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visualizing highdimensional data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="613" to="621" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
