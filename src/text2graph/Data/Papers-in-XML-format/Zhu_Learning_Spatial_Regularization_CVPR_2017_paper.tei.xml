<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Spatial Regularization with Image-level Supervisions for Multi-label Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
							<email>zhufengx@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Spatial Regularization with Image-level Supervisions for Multi-label Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-label image classification is an important task in computer vision with various applications, such as scene recognition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>, multi-object recognition <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18]</ref>, human attribute recognition <ref type="bibr" target="#b23">[24]</ref>, etc. Compared to single-label image classification <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12]</ref>, which has been extensively studied, multi-label problem is more practical and challenging, as real-world images are usually associated with multiple labels, such as objects or attributes.</p><p>Binary relevance method <ref type="bibr" target="#b33">[34]</ref> is an easy way to extend single-label algorithms to solve multi-label classification, which simply trains one binary classifier for each label. Various loss functions have been investigated in <ref type="bibr" target="#b10">[11]</ref>. To cope with the problem that labels may relate to different visual regions over the whole image, proposal-based approaches <ref type="bibr" target="#b37">[38]</ref> are proposed to transform multi-label classification problem into multiple single-label classification tasks. However, these modifications of existing single-label algorithms ignored semantic relations of labels.</p><p>Recent progress on multi-label image classification mainly focused on capturing semantic relations between labels. Such relations or dependency can be modeled by probabilistic graphical models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref>, structured inference neural network <ref type="bibr" target="#b15">[16]</ref>, or Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b35">[36]</ref>. Despite the great improvements achieved by exploiting semantic relations, existing methods cannot capture spatial relations of labels, because their spatial locations are not annotated for training. In this paper, we propose to capture both semantic and spatial relations of labels by a Spatial Regularization Network in a unified framework <ref type="figure" target="#fig_0">(Figure 1</ref>), which can be trained end-to-end with only image-level supervisions, thus requires no additional annotations.</p><p>Deep Convolution Neural Networks (CNNs) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13]</ref> have achieved great success on single-label image classification in recent years. Because of their strong capability in learning discriminative features, deep CNN models pretrained on large datasets can be easily transferred to solve other tasks and boost their performance. However, the feature representations might not be optimal for images with multiple labels, since a ground truth label might semantically relate to only a small region of the image. The diverse and complex contents in multi-label images make it difficult to learn effective feature representations and classifiers.</p><p>Inspired by recent success of attention mechanism in many vision tasks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b14">15]</ref>, we propose a deep neural network for multi-label classification, which consists of a sub-network, Spatial Regularization Net (SRN), to learn spatial regularizations between labels with only image-level supervisions. The SRN learns an attention map for each label, which associates related image regions to each label. By performing learnable convolutions on the attention maps of all labels, the SRN captures the underlying semantic and spatial relations between labels and act as spatial regularizations for multi-label classification.</p><p>The contribution of this paper is as follows. 1) We propose an end-to-end deep neural network for multi-label image classification, which exploits both semantic and spatial relations of labels by training learnable convolutions on the attention maps of labels. Such relations are learned with only image-level supervisions. Investigation and visualization of learned models demonstrate that our model can effectively capture semantic and spatial relations of labels. 2) Our proposed algorithm has great generalization capability and works well on data with different types of labels. We comprehensively evaluate our method on 3 publicly available datasets, NUS-WIDE <ref type="bibr" target="#b4">[5]</ref> (81 concept labels), MS-COCO <ref type="bibr" target="#b24">[25]</ref> (80 object labels), and WIDER-Attribute <ref type="bibr" target="#b23">[24]</ref> (14 human attribute labels), showing significant improvements over state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Multi-label classification has applications in many areas, such as document topic categorization <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b9">10]</ref>, music annotation and retrieval <ref type="bibr" target="#b34">[35]</ref>, scene recognition <ref type="bibr" target="#b3">[4]</ref>, and gene functional analysis <ref type="bibr" target="#b1">[2]</ref>. Comprehensive reviews for general multi-label classification methods can be found in <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b43">44]</ref>. In this work, we focus on multi-label image classification methods with deep learning techniques.</p><p>A simple way of adapting existing single-label methods to multi-label is to learn an independent classifier for each label <ref type="bibr" target="#b33">[34]</ref>. Recent success of deeply-learned features <ref type="bibr" target="#b20">[21]</ref> for single-label image classification have boosted the accuracy of multi-label classification. Based on such deep features, Gong et al. <ref type="bibr" target="#b10">[11]</ref> evaluated various loss functions and found that weighted approximate ranking loss worked best with CNNs. Proposal-based approaches showed promising performance in object detection <ref type="bibr" target="#b7">[8]</ref>. Similar ideas have also been explored for multi-label image classification. Wei et al. <ref type="bibr" target="#b37">[38]</ref> converted multi-label problems into a set of multiclass problems over region proposals. Classification results for the whole images were obtained by max-pooling label confidences over all proposals. Yang et al. <ref type="bibr" target="#b41">[42]</ref> treated images as a bag of instances/proposals, and solved a multiinstance learning problem. The above approaches ignored label relations in multi-label images.</p><p>Approaches that learn to capture label relations were also proposed. Read et al. <ref type="bibr" target="#b27">[28]</ref> extended the binary relevance method by training a chain of binary classifiers, where each classifier makes predictions based on both image features and previously predicted labels. A more common way of modeling label relations is to use probabilistic graphical models <ref type="bibr" target="#b19">[20]</ref>. There were also methods on determining structures of the label relation graphs. Xue et al. <ref type="bibr" target="#b40">[41]</ref> directly thresholded the label correlation matrix to obtain the label structure. Li et al. <ref type="bibr" target="#b22">[23]</ref> used a maximum spanning tree over mutual information matrix of labels to create the graph. Li et al. <ref type="bibr" target="#b21">[22]</ref> proposed to learn image-dependent conditional label structures base on Graphical Lasso framework <ref type="bibr" target="#b26">[27]</ref>. Recently, deep neural networks have also been explored for learning label relations. Hu et al. <ref type="bibr" target="#b15">[16]</ref> proposed a structured inference neural network that transfers predictions across multiple concept layers. Wang et al. <ref type="bibr" target="#b35">[36]</ref> treated multi-label classification as a sequential prediction problem, and solved label dependency by Recurrent Neural Networks (RNN). Although classification accuracy has been greatly improved by learning semantic relations of labels, the above mentioned approaches fail to explore the underlying spatial relations between labels.</p><p>Attention mechanism was proven to be beneficial in many vision tasks, such as visual tracking <ref type="bibr" target="#b2">[3]</ref>, object recognition <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b0">1]</ref>, image captioning <ref type="bibr" target="#b39">[40]</ref>, image question answering <ref type="bibr" target="#b42">[43]</ref>, and segmentation <ref type="bibr" target="#b14">[15]</ref>. The spatial attention mechanism adaptively focuses on related regions of the image when the deep networks are trained with spatiallyrelated labels. In this paper, we utilize attention mechanism for improving multi-label image classification, which captures the underlying spatial relations of labels and provides spatial regularization for the final classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We propose a deep neural network for multi-label classification, which utilizes image-level supervisions for learning spatial regularizations on multiple labels. The overall framework of our approach is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The main net has the same network structure as ResNet-101 <ref type="bibr" target="#b12">[13]</ref>. The proposed Spatial Regularization Net (SRN) takes visual features from the main net as inputs and learns to regularize spatial relations between labels. Such relations are exploited based on the learned attention maps for the multiple labels. Label confidences from both main net and SRN are aggregated to generate final classification confidences. The whole network is a unified framework and is trained in an end-to-end manner.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Main Net for Multi-label Classification</head><p>The main net follows the structure of ResNet-101 <ref type="bibr" target="#b12">[13]</ref> which is composed of repetitive building blocks with different output dimensions. Specifically, the block structure proposed in <ref type="bibr" target="#b13">[14]</ref> is adopted. The 14 × 14 feature map (for 224 × 224 input images) from layer "res4b22 relu" of the main net is used as input for SRN, which is of sufficient resolution to learn spatial regularizations in our experiments.</p><p>Let I denote an input image with ground-truth labels y = [y 1 , y 2 , ..., y C ] T , where y l is a binary indicator. y l = 1 if image I is tagged with label l, and y l = 0 otherwise. C is the number of all possible labels in the dataset. The main net conducts binary classification for each of the C labels,</p><formula xml:id="formula_0">X = f cnn (I; θ cnn ), X ∈ R 14×14×1024 ,<label>(1)</label></formula><formula xml:id="formula_1">y cls = f cls (X; θ cls ),ŷ cls ∈ R C ,<label>(2)</label></formula><p>where X is the feature map from layer "res4b22 relu",</p><formula xml:id="formula_2">y cls = [ŷ 1 cls · · · ,ŷ C cls ]</formula><p>T is predicted label confidences by the main net. Prediction errors of the main net is measured based onŷ cls and ground-truth labels y.</p><p>The proposed SRN is composed of two successive subnetworks, where the first sub-network f att (X; θ att ) learns label attention maps with image-level supervisions (Section 3.2), and the second sub-network f sr (U; θ sr ) captures spatial regularizations of labels (Section 3.3) based on the learned label attention maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Label Attention from Image-level Supervisions</head><p>Multi-label image is composed of multiple image regions that are semantically related to different labels. Although the region locations are generally not provided by the image-level supervisions, when predicting one label's existence, it is desirable that more attention is paid to the related regions. In our work, our neural network learn to predict such related image regions for each label with image-level supervisions using the attention mechanism. The learned attention maps could then be used to learn spatial regularizations for the labels.</p><p>Given input visual features X ∈ R 14×14×1024 from layer "res4b22 relu" of the main net, we aim to automatically generate label attention values for each individual labels,</p><formula xml:id="formula_3">Z = f att (X; θ att ), Z ∈ R 14×14×C ,<label>(3)</label></formula><p>where Z is the unnormalized label attention values by f att (·) with each channel corresponding to one label. Following <ref type="bibr" target="#b39">[40]</ref>, Z is spatially normalized with the softmax function to obtain the final label attention maps A,</p><formula xml:id="formula_4">a l i,j = exp (z l i,j ) i,j exp (z l i,j ) , A ∈ R 14×14×C ,<label>(4)</label></formula><p>where z l i,j and a l i,j represent the unormalized and normalized attention values at (i, j) for label l. Intuitively, if label l is tagged to the input image, the image regions related to it should be assigned with higher attention values. The attention estimator f att (·) is modeled as 3 convolution layers with 512 kernels of 1×1, 512 kernels of 3×3, and C kernels of 1×1, respectively. The ReLU nonlinearity operations are performed following the first two convolution layers.</p><p>Since ground-truth annotations of attention maps are not available, f att (X; θ att ) is learned with only image-level multi-label supervisions. Let x i,j ∈ R 1024 denote the visual feature vector at location (i, j) of X. In the original ResNet, the visual features is averaged across all spatial locations for classification as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">7×7</head><p>i,j x i,j . Since we expect the attention map A l for each label l to have higher values at the label-related regions, and i,j a l i,j = 1 for all l, the attention maps could be used to weightedly average the visual features X for each label l as,</p><formula xml:id="formula_5">v l = i,j x i,j a l i,j , v l ∈ R 1024 .<label>(5)</label></formula><p>Compared with the original averaged visual features shared by all labels, the weightedly-averaged visual feature vector v l is more related to image regions corresponding to label l. Each such feature vector is then used to learn a linear classifier for estimating label l's confidence,</p><formula xml:id="formula_6">y l att = W l v l + b l ,<label>(6)</label></formula><p>where W l and b l are classifier parameters for label l. For all labels,</p><formula xml:id="formula_7">ŷ att = [ŷ 1 att , · · · ,ŷ C att ]</formula><p>T . Using only the imagelevel supervisions y for training, the attention estimator parameters are learned by minimizing the cross-entropy loss betweenŷ att and y (see the dashed lines in <ref type="figure" target="#fig_1">Figure 2)</ref>.</p><p>The attention estimator network f att (·) can effectively learn attention maps for each label. Learned attention maps for an image are illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. It shows that the weakly-supervised attention model could effectively capture related visual regions for each label. For example, "sunglass" focuses on the face region, while "longPants" pays more attention to legs. The negative labels also focus on reasonable regions, for example, "Hat" tries to find hat in the region of head.</p><p>For efficient learning of the attention maps, recall that we have i,j a l i,j = 1, and Eq. (6) can be rewritten aŝ</p><formula xml:id="formula_8">y l att = i,j a l i,j (W l x i,j + b l ).<label>(7)</label></formula><p>This equation can be viewed as applying label-specific linear classifier at every location of the feature map X, and then spatially aggregating label confidences based on attention maps. In our implementation, the linear classifiers are modeled as a convolution layer with C kernels of size 1 × 1 ("conv1" in <ref type="figure" target="#fig_1">Figure 2</ref>). The output of this layer is a confidence map S ∈ R 14×14×C , where its lth channel is S l = W l * X + b l , with * denoting convolution operation. The label attention map A and confidence map S are element-wisely multiplied, and then spatially sum-pooled to obtain the label confidence vectorŷ att . This formulation leads to an easy-to-implement network for learning label attentions, and generates confidence maps for weighting the attention maps in SRN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Spatial Regularizations from Attention Maps</head><p>Label attention maps encode rich spatial information of labels. They can be used to generate more robust spatial regularizations for labels. However, the attention map for each label always sum up to 1 (see <ref type="figure" target="#fig_2">Figure 3)</ref>, which may highlight wrong locations. Learning from label-not-existing attention maps might lead to wrong spatial regularizations. Therefore, we propose to learn spatial regularizations from weighted attention maps U ∈ R 14×14×C , where σ(x) = 1/(1+e −x ) is the sigmoid function that converts label confidences S to the range [0, 1], and • indicates element-wise multiplication. The weighted attention maps U encode both local confidences of attention and global visibility of each label, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><formula xml:id="formula_9">U = σ(S) • A,<label>(8)</label></formula><p>Given the weighted attention maps U, a label regularization function is required to estimate the label confidences based on label spatial information from U,</p><formula xml:id="formula_10">y sr = f sr (U; θ sr ),ŷ sr ∈ R C ,<label>(9)</label></formula><formula xml:id="formula_11">whereŷ sr = [ŷ 1 sr ,ŷ 2 sr , ...,ŷ C sr ]</formula><p>T is predicted label confidences by the label regularization function.</p><p>Since the weighted attention maps for all labels are spatially aligned, it is easy to capture their relative relations with stacked convolution operations. The convolutions should have large enough receptive fields to capture the complex spatial relations between the labels. However, a naive implementation might be problematic. If we only use one convolution layer with 2048 filters of size 14 × 14, then the total number of additional parameters would be 0.4C million. For a dataset that has 80 different labels, the actual number of additional parameter would be 32 million, In contrast, the original ResNet-101 only contains approximately 40 million parameters. Such large number of additional parameters would make the network difficult to train.</p><p>We propose to decouple semantic relation learning and spatial relation learning in different convolution layers. The intuition is that one label may only semantically relate to a small number of other labels, and measuring spatial relations with those unrelated attention maps is unnecessary. f sr (U; θ sr ) is implemented as three convolution layers with ReLU nonlinearity followed by one fully-connected layer as shown in <ref type="figure">Figure 4</ref>. The first two layers capture semantic relations of labels with 2 layers of 1 × 1 convolutions, and the third layer explores spatial relations using 2048 14 × 14 kernels. The filters of the third convolution layer are grouped, with each group of 4 kernels corresponding to one feature channel of the input feature map. The 4 kernels in each group convolve the same feature channel independently. Different kernels in one group capture different spatial relations of semantically related labels. Experimental results show that the proposed SRN provides effective regularization to the classification results based on semantic and spatial relations of labels, with only about 6 million additional parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Overall Network and Training Scheme</head><p>The final label confidences are aggregation of the outputs of main net and SRN,ŷ = αŷ cls + (1 − α)ŷ sr , where α is a weighting factor. Though the factor can also be learned, we fix α = 0.5 and do not observe performance drop. The whole network is trained with the cross-entropy loss with the ground truth labels y,</p><formula xml:id="formula_12">F loss (y,ŷ) = C l=1 y l log σ(ŷ l ) + (1 − y l ) log(1 − σ(ŷ l )).</formula><p>(10) We train the network in multiple steps. First, we finetune only the main net on the target dataset, which is pretrained on 1000-classification task of ImageNet dataset <ref type="bibr" target="#b5">[6]</ref>. Both f cnn (I; θ cnn ) and f cls (X; θ cls ) are learned with crossentropy loss F loss (y,ŷ cls ). Secondly, we fix f cnn and f cls , and focus on training f att (X; θ att ) and "conv1" (see dashed lines in <ref type="figure" target="#fig_1">Figure 2</ref>) with loss F loss (y,ŷ att ). Thirdly, we train f sr (U; θ sr ) with cross-entropy loss F loss (y,ŷ sr ), by fixing all other sub-networks. Finally, the whole network is jointly fine-tuned with loss F loss (y,ŷ) + F loss (y,ŷ att ).</p><p>Our deep neural network is implemented with Caffe library <ref type="bibr" target="#b16">[17]</ref>. To avoid over-fitting, we adopt image augmentation strategies suggested in <ref type="bibr" target="#b36">[37]</ref>. The input images are first resized to 256 × 256, and then cropped at four corners and the center. The width and height of cropped patches are randomly chosen from the set {256, 224, 192, 168, 128}. Finally, the cropped images are all resized to 224 × 224. We employ stochastic gradient descend algorithm for training, with a batch size of 96, a momentum of 0.9, and weight decay of 0.0005. The initial learning rate is set as 10 −3 , and decreased to 1/10 of the previous value whenever validation loss gets saturated, until 10 −5 . We train our model with 4 NVIDIA Titan X GPUs. For MS-COCO, training costs about 16 hours for all steps. For testing, we simply resize all images to 224 × 224 and conduct single-crop evaluation.  <ref type="figure">Figure 4</ref>. Detailed network structure of fsr(·) for learning spatial regularizations from weighted attention maps. The first two layers ("conv2" and "conv3") are convolution layers with multi-channel filters, while, "conv4" is composed of single-channel filters. Every 4 filters convolve with the same feature channel by "conv3" to limit the parameter size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our approach is evaluated with three benchmark datasets with different types of labels: NUS-WIDE <ref type="bibr" target="#b4">[5]</ref> with 81 concept labels, MS-COCO <ref type="bibr" target="#b24">[25]</ref> with 80 object labels, and WIDER-Attribute <ref type="bibr" target="#b23">[24]</ref> with 14 human attribute labels. Experimental results show that our approach significantly outperforms state-of-the-arts on all the three datasets <ref type="bibr" target="#b0">1</ref> , and has strong generalization capability to different types of labels. Analysis of the learned deep models demonstrates that our proposed approach can effectively capture both semantic and spatial relations of labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation Metrics and Compared Methods Evaluation Metrics.</head><p>A comprehensive study of evaluation metrics for multi-label classification is presented in <ref type="bibr" target="#b38">[39]</ref>.</p><p>We employ macro/micro precision, macro/micro recall, macro/micro F1-measure, and Mean Average Precision (mAP) for performance comparison. For precision/recall/F1-measure, if the estimated label confidences for any label are greater than 0.5, the labels are predicted as positive. Macro precision (denoted as "P-C") is evaluated by averaging per-class precisions, while micro precision (denoted as "P-O") is an overall measure which counts true predictions for all images over all classes. Similarly, we can also evaluate macro/micro recall ("R-C"/"R-O") and macro/micro F1-measure ("F1-C"/"F1-O"). Mean Average Precision is the mean value of per-class average precisions. The above metrics do not require a fixed number of labels for each image. Generally, the mAP, F1-C and F1-O are of more importance. To fairly compare with state-of-the-arts, we also evaluate precision, recall and F1-measure under the constraints that each image is predicted with top-3 labels. To obtain such top-3 labels in our approach, the 3 labels with highest confidences are obtained  <ref type="table">Table 1</ref>. Quantitative results by our proposed ResNet-SRN and compared methods on NUS-WIDE dataset. "mAP", "F1-C", "P-C", and "R-C" are evaluated for each class before averaging. "F1-O", "P-O", and "R-O" are averaged over all sample-label pairs.</p><formula xml:id="formula_13">Method All top-3 mAP F1-C P-C R-C F1-O P-O R-O F1-C P-C R-C F1-O P-O R-O KNN [5] - - - - - - -</formula><p>for each image even if their confidence values are lower than 0.5. However, we argue that outputting a variable number of labels for each image is more practical for real-world applications. Therefore, we report both our results with and without the top-3 label constraint.</p><p>Compared Methods. For NUS-WIDE and MS-COCO datasets, we compare with state-of-the-art methods on the datasets including CNN-RNN <ref type="bibr" target="#b35">[36]</ref>, WARP <ref type="bibr" target="#b10">[11]</ref>, and KNN <ref type="bibr" target="#b4">[5]</ref>. CNN-RNN explored semantic relations of labels, while other methods did not. For WIDER-Attribute dataset, RCNN <ref type="bibr" target="#b7">[8]</ref>, R*CNN <ref type="bibr" target="#b8">[9]</ref>, and DHC <ref type="bibr" target="#b23">[24]</ref> are compared. Both R*CNN and DHC explored spatial context surrounding human bounding boxes. For our approach (denoted as "ResNet-SRN"), one variant is also explored, which learns spatial regularizations from unweighted attention maps A instead of U to evaluate the necessity of weighting the attention maps. It is denoted as "ResNet-SRN-att".</p><p>We also design three baseline methods to further validate the effectiveness of our proposed Spatial Regularization Net. The first baseline is the original ResNet-101 (denoted as "ResNet-101") fine-tuned on each of the datasets. For the second baseline, since the proposed SRN has about 6 million additional parameters compared with ResNet-101, which is approximately equal to two ResNet building blocks with 2048 output feature channels, we add two such residual blocks following the last block of ResNet-101 (the layer "res5c relu") to create a "ResNet-107" model. For the third baseline, we investigate learning semantic relations of labels based on initial label confidences by ResNet-101. The initial confidences are concatenated with the visual features from the "pool5" layer to encode label relations. Two 2048-neuron and one C-neuron fully-connected layers try to capture label semantic relations from the concatenated features to generate final label confidences. We refer this model as "ResNet-101-semantic" in our experiments. <ref type="bibr" target="#b4">[5]</ref>. This dataset contains 269,648 images and associated tags from Flickr. The dataset is manually annotated by 81 concepts, with 2.4 concept labels per image on average. The concepts include events/activities (e.g., "swimming", "running"), scene/location (e.g., "airport", "ocean"), objects (e.g., "animal", "car"). We trained our approach to predict the 81 concept labels. Official train/test split is used, i.e. 161,789 images for training/validation, and 107,859 image for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NUS-WIDE</head><p>Experimental results on this dataset are shown in Table 1. Our proposed ResNet-SRN and its variant ResNet-SRN-att outperform all state-of-the-arts and baseline models. With the advances of deep network structures, even our baseline ResNet-101 has achieved better performance than existing state-of-the-arts. It mainly results from the learning capability of ResNet-101 with deep learnable layers. When adding more layers to match the parameter size of our proposed SRN, ResNet-107 shows very close performance with ResNet-101, which suggests that the capacity of ResNet-101 is sufficient on NUS-WIDE, and adding more parameters does not lead to performance increase. Utilizing predicted labels as context (ResNet-101-semantic) dose not improve performance on this dataset. In contrast, by exploring spatial and semantic relations of labels, our proposed ResNet-SRN model outperforms all baseline methods by ∼ 2 percent. It indicates that learned spatial relations of labels provide good regularizations for multi-label image classification. The performance gain of our ResNet-SRN over ResNet-SRN-att suggests that the weighted attention map U is more informative for learning spatial regularizations than the unweighted attention map A.</p><p>Forcing the algorithm to predict a fixed number (k = 3 is proposed in state-of-the-art methods) of labels for each image may not fully reflect the algorithm's actual performance. When removing the constraint (Section 4.1), we can observe significant performance improvements (e.g., from 48.9 to 58.5 for the F1-C metric of ResNet-SRN).</p><p>MS-COCO <ref type="bibr" target="#b24">[25]</ref>. This dataset is primarily built for object recognition task in the context of scene understanding. The training set is composed of 82,783 images, which contain common objects in the scenes. The objects are categorized into 80 classes, with about 2.9 object labels per image. Since the ground-truth labels of test set is not available, we evaluated all methods on the validation set (40,504 images). The number of labels for each image varies considerably on this MS-COCO. Following <ref type="bibr" target="#b35">[36]</ref>, when evaluating with top-3 label predictions, we filtered out labels with probability  <ref type="table">Table 2</ref>. Quantitative results by our proposed ResNet-SRN and compared methods on MS-COCO validation set. "mAP", "F1-C", "P-C", and "R-C" are evaluated for each class before averaging. "F1-O", "P-O", and "R-O" are averaged over all sample-label pairs.</p><formula xml:id="formula_14">Method All top-3 mAP F1-C P-C R-C F1-O P-O R-O F1-C P-C R-C F1-O P-O R-O WARP [11] - - - - - - -</formula><p>lower than 0.5 for each image, thus the image may return less than k = 3 labels. Quantitative results on MS-COCO are presented in <ref type="table">Ta</ref> WIDER-Attribute <ref type="bibr" target="#b23">[24]</ref>. This dataset contains 13,789 images and 57,524 human bounding boxes. The task is to predict existence of 14 human attributes for each annotated person. Each image is also labeled by an event label from 30 event classes for context learning. For our approach, human is cropped from the full image based on bounding box annotations, and then used for training and testing. The training/validation and testing set contain 28,340 and 29,177 person, respectively. WIDER-Attribute also contains unspecified labels. We treated these unspecified labels as negative labels during training. Unspecified labels are excluded from evaluation in testing following the settings of <ref type="bibr" target="#b23">[24]</ref>.</p><p>Experimental results are shown in <ref type="table">Table 3</ref>. All ResNet models outperform state-of-the-arts, R-CNN <ref type="bibr" target="#b7">[8]</ref>, R*CNN <ref type="bibr" target="#b8">[9]</ref>, and DHC <ref type="bibr" target="#b23">[24]</ref>, and our proposed ResNet-SRN performs best. It is important to note that R*CNN and DHC explore visual context surrounding the target human by taking full images and human bounding boxes as input. Event labels associated with each image are also used for training in DHC. In contrast, our approach and baselines only utilize cropped image patches without using the event labels. Nevertheless, the ResNet-SRN and ResNet-SRN-att show consistent improvement over state-of-the-arts and baseline methods. This result indicates that the proposed SRN could capture the spatial relations of human attributes with imagelevel supervisions, and these learned spatial regularizations could help predicting human attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>All mAP F1-C F1-O R-CNN <ref type="bibr" target="#b7">[8]</ref> 80.0 --R*CNN <ref type="bibr" target="#b8">[9]</ref> 80.5 --DHC <ref type="bibr" target="#b23">[24]</ref> 81.3 --ResNet-101 <ref type="bibr" target="#b12">[13]</ref> 85  <ref type="table">Table 3</ref>. Quantitative results by our proposed ResNet-SRN and compared methods on WIDER-Attribute dataset. "mAP"and "F1-C" are evaluated for each class before averaging. "F1-O" is averaged over all sample-label pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Visualization and Analysis</head><p>The effectiveness of our approach has been quantitatively evaluated in <ref type="table">Table 1</ref>, 2, and 3, we visualize and analyze the learned neurons from the conv4 layer of our SRN to illustrate its capability of learning spatial regularizations for labels. We observe that the learned neurons capture two types of label spatial information. One type of neurons capture the spatial locations of individual labels, while the other type of neurons are only activated when several labels have specific relative position patterns.</p><p>We calculate correlations between learned neuron responses and label locations in images, and find some neuron highly correlates to individual label's spatial locations. In <ref type="figure">Figure 5</ref>, we show two such examples. In (a), the response of neuron #425 of "conv4" in SRN highly correlates the vertical location of the label "longHair" in WIDERAttribute dataset. In (b) the activation of neuron #1199 of "conv4" highly correlates the vertical location of the label "flag". It demonstrates that the two neurons focuses on spatial locations of certain labels.</p><p>In <ref type="figure">Figure 6</ref>, we show three images from WIDERAttribute dataset that have highest activations on neuron #786 of "conv4" in SRN. The images have common labels ("Male", "longSleeve", "formal", "longPants") with similar relative label positions. It suggests that this neuron is trained to capture semantic and spatial relations of the four labels, and favors specific relative positions between them. We also analyzed AP improvements for all classes in COCO. As shown in <ref type="figure" target="#fig_5">Figure 7</ref>, our approach is more effective for classes having more co-existing labels in the same images so that spatial relations can be better utilized to regularize the results. For the class toaster, it was not improved much because of its limited number of training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we aim to improve multi-label image classification by exploring spatial relations of labels. This is achieved by learning attention maps for all labels with only image-level supervisions, and then capturing both semantic and spatial relations of labels base on weighted attention maps. Extensive evaluations on NUS-WIDE, MS-COCO, and WIDER-Attribute datasets show that our proposed Spatial Regularization Net significantly outperforms state-ofthe-arts. Visualization of learned models also shows that our approach could effectively capture both semantic and spatial relations for labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgment</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of using our proposed Spatial Regularization Net (SRN) for improving multi-label image classification. The SRN learns semantic and spatial label relations from label attention maps with only image-level supervisions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overall framework of our approach. (Top) The main net follows the structure of ResNet-101 and learns one independent classifier for each label. "Res-2048" stands for one ResNet building block with 2048 output channels. (Bottom) The proposed SRN captures spatial and semantic relations of labels with attention mechanism. Dashed lines indicate weakly-supervised pre-training for attention maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Examples of learned attention maps from WIDERAttribute dataset. Labels in red are ground-truth labels. "Weighted Attention" is the attention map weighted by corresponding label confidence (Eq. (8)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>- ble 2 .</head><label>2</label><figDesc>The comparison results are similar to those on NUS- WIDE. Based on ResNet-101 network, all baseline mod- els perform better than state-of-the-art approaches. ResNet- 107 shows a minor improvement over ResNet-101. Due to more labels per image (3.5 labels on MS-COCO compared with 2.4 labels on NUS-WIDE), exploring label semantic relations by ResNet-101-semantic is helpful, but the im- provement is limited (e.g., from 75.2 to 75.5 in terms of mAP). Both ResNet-SRN and ResNet-SRN-att show supe- rior performance over baseline models, while the spatial regularizations learned from weighted attention maps per- form better (e.g., ResNet-SRN boosts mAP to 77.1, as com- pared with 76.1 of ResNet-SRN-att).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Figure 5. Correlation between neuron activations and label locations. These two neurons are sensitive to the location variations of corresponding labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Top: improvement in AP for each class in COCO. Bottom: average number of concurrent labels for true positive images of each class. All sorted according to improvements in AP.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code and trained models available at https://github.com/ zhufengx/SRN_multilabel.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work is supported in part by National Natural Sci- </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical multi-label prediction of gene function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Barutcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">G</forename><surname>Troyanskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="830" to="836" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning attentional policies for tracking and recognition in video with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning multi-label scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Boutell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1757" to="1771" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nus-wide: a real-world web image database from national university of singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM international conference on image and video retrieval</title>
		<meeting>the ACM international conference on image and video retrieval</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="70" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Contextual action recognition with r*cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative methods for multi-labeled classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Godbole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep convolutional ranking for multilabel image annotation. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning transferrable knowledge for semantic segmentation with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<title level="m">Learning structured inference neural networks with label relations. CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02532</idno>
		<title level="m">Tubelets with convolutional neural networks for object detection from videos</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Object detection from video tubelets with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="817" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Probabilistic Graphical Models: Principles and Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conditional graphical lasso for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-label image classification with a probabilistic label enhancement model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Uncertainty in Artificial Intell</title>
		<meeting>Uncertainty in Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Human attribute recognition by deep hierarchical contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Highdimensional ising model selection using 1-regularized logistic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1287" to="1319" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Classifier chains for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="333" to="359" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Boostexter: A boosting-based system for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="135" to="168" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deeply learned attributes for crowded scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4657" to="4666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Slicing convolutional neural network for crowd video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5620" to="5628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-label classification: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Warehousing and Mining</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic annotation and retrieval of music and sound effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turnbull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="467" to="476" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Cnn-rnn: A unified framework for multi-label image classification. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Towards good practices for very deep two-stream convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno>abs/1507.02159</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5726</idno>
		<title level="m">Cnn: Single-label to multi-label</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A unified view of multi-label performance measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.00288</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Correlative multi-label multi-instance image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Exploit bounding box annotations for multi-label object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A review on multi-label learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1819" to="1837" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
