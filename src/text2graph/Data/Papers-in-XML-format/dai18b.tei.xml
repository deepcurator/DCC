<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Attack on Graph Structured Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
						</author>
						<title level="a" type="main">Adversarial Attack on Graph Structured Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Deep learning on graph structures has shown exciting results in various applications. However, few attentions have been paid to the robustness of such models, in contrast to numerous research work for image or text adversarial attack and defense. In this paper, we focus on the adversarial attacks that fool deep learning models by modifying the combinatorial structure of data. We first propose a reinforcement learning based attack method that learns the generalizable attack policy, while only requiring prediction labels from the target classifier. We further propose attack methods based on genetic algorithms and gradient descent in the scenario where additional prediction confidence or gradients are available. We use both synthetic and real-world data to show that, a family of Graph Neural Network models are vulnerable to these attacks, in both graph-level and node-level classification tasks. We also show such attacks can be used to diagnose the learned classifiers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph structure plays an important role in many real-world applications. Representation learning on the structured data with deep learning methods has shown promising results in various applications, including drug screening <ref type="bibr" target="#b6">(Duvenaud et al., 2015)</ref>, protein analysis <ref type="bibr" target="#b9">(Hamilton et al., 2017)</ref>, knowledge graph completion <ref type="bibr" target="#b22">(Trivedi et al., 2017)</ref>, etc..</p><p>Despite the success of deep graph networks, the lack of interpretability and robustness of these models make it risky for some financial or security related applications. As analyzed in <ref type="bibr" target="#b0">Akoglu et al. (2015)</ref>, the graph information is proven to be important in the area of risk management. A graph sensitive evaluation model will typically take the user-user relationship into consideration: a user who connects with many high-credit users may also have high credit. Such heuristics learned by the deep graph methods would often 1 Georgia Institute of Technology 2 Ant Financial 3 Tsinghua University.</p><p>Correspondence to: Hanjun Dai &lt;hanjundai@gatech.edu&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the 35</head><p>th International Conference on Machine Learning, <ref type="bibr">Stockholm, Sweden, PMLR 80, 2018</ref><ref type="bibr">. Copyright 2018</ref> by the author(s).</p><p>yield good predictions, but could also put the model in a risk. A criminal could try to disguise himself by connecting other people using Facebook or Linkedin. Such 'attack' to the credit prediction model is quite cheap, but the consequence could be severe. Due to the large number of transactions happening every day, even if only one-millionth of the transactions are fraudulent, fraudsters can still obtain a huge benefit. However, few attentions have been put on domains involving graph structures, despite the recent advances in adversarial attacks and defenses for other domains like images <ref type="bibr" target="#b8">(Goodfellow et al., 2014</ref>) and text <ref type="bibr" target="#b10">(Jia &amp; Liang, 2017)</ref>.</p><p>So in this paper, we focus on the graph adversarial attack for a set of graph neural network(GNN) <ref type="bibr" target="#b18">(Scarselli et al., 2009)</ref> models. These are a family of supervised <ref type="bibr" target="#b4">(Dai et al., 2016)</ref> models that have achieved state-of-the-art results in many transductive tasks <ref type="bibr" target="#b11">(Kipf &amp; Welling, 2016)</ref> and inductive tasks <ref type="bibr" target="#b9">(Hamilton et al., 2017)</ref>. Through experiments in both node classification and graph classification problems, we will show that the adversarial samples do exist for such models. And the GNN models can be quite vulnerable to such attacks.</p><p>However, effectively attacking graph structures is a nontrivial problem. Different from images where the data is continuous, the graphs are discrete. Also the combinatorial nature of the graph structures makes it much more difficult than text. Inspired by the recent advances in combinatorial optimization <ref type="bibr" target="#b1">(Bello et al., 2016;</ref><ref type="bibr" target="#b5">Dai et al., 2017)</ref>, we propose a reinforcement learning based attack method that learns to modify the graph structure with only the prediction feedback from the target classifier. The modification is done by sequentially add or drop edges from the graph. A hierarchical method is also used to decompose the quadratic action space, in order to make the training feasible. <ref type="figure">Figure 1</ref> illustrates this approach. We show that such learned agent can also propose adversarial attacks for new instances without access to the classifier.</p><p>Several different adversarial attack settings are considered in our paper. When more information from the target classifier is accessible, a variant of the gradient based method and a genetic algorithm based method are also presented. Here we mainly focus on the following three settings:</p><p>• white box attack (WBA): in this case, the attacker is allowed to access any information of the target classifier, including the prediction, gradient information, etc.. • practical black box attack (PBA): in this case, only the prediction of the target classifier is available. When the prediction confidence is accessible, we denote this setting as PBA-C; if only the discrete prediction label is allowed, we denote the setting as PBA-D.</p><formula xml:id="formula_0">! " ($) ! &amp; (') …… ! " (() ! &amp; (<label>() )</label></formula><p>• restrict black box attack (RBA): this setting is one step further than PBA. In this case, we can only do black-box queries on some of the samples, and the attacker is asked to create adversarial modifications to other samples.</p><p>As we can see, regarding the amount of information the attacker can obtain from the target classifier, we can sort the above settings as WBA &gt; PBA-C &gt; PBA-D &gt; RBA. For simplicity, we focus on the non-targeted attack, though it is easy to extend to the targeted attack scenario.</p><p>In Sec 2, we first present the background about GNNs and two supervised learning tasks. Then in Sec 3 we formally define the graph adversarial attack problem. Sec 3.1 presents the attack method RL-S2V that learns the generalizable attack policy over the graph structure. We also propose other attack methods with different levels of access to the target classifier in Sec 3.2. We experimentally show the vulnerability of GNN models in Sec 4, and also present a way of doing defense against such attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>A set of graphs is denoted by</p><formula xml:id="formula_1">G = {G i } N i=1 , where |G| = N . Each graph G i = (V i , E i ) is represented by the set of nodes V i = {v (i) j } |Vi| j=1</formula><p>and edges E i = {e</p><formula xml:id="formula_2">(i) j } |Ei| j=1 . Here the tuple e (i) j = (e (i) j,1 , e (i) j,2 ) ∈ V i × V i represents the edge between node e (i)</formula><p>j,1 and e (i) j,2 . In this paper, we focus on undirected graphs, but it is straightforward to extend to directed ones. Optionally, the nodes or edges can have associated features. We denote them as x(v</p><formula xml:id="formula_3">(i) j ) ∈ R D node and w(e (i) j ) = w(e (i) j,1 ,e (i) j,2 ) ∈ R D edge , respectively.</formula><p>This paper works on attacking the graph supervised classification algorithms. Here two different supervised learning settings are considered:</p><p>Inductive Graph Classification: We associate each graph G i with a label y i ∈ Y = {1, 2, ... , Y }, where Y is the number of categories. The dataset</p><formula xml:id="formula_4">D (ind) = {(G i ,y i )} N i=1</formula><p>is represented by pairs of graph instances and graph labels. This setting is inductive since the test instances will never be seen during training. Examples of such task including classifying the drug molecule graphs according to their functionality. In this case, the classifier f (ind) ∈ F (ind) : G → Y is optimized to minimize the following loss:</p><formula xml:id="formula_5">L (ind) = 1 N N i=1 L(f (ind) (G i ),y i )<label>(1)</label></formula><p>where L(·,·) is the cross entropy by default.</p><p>Transductive Node Classification: In node classification setting, a target node c i ∈ V i of graph G i is associated with a corresponding node label y i ∈ Y. The classification is on the nodes, instead of the entire graph. We here focus on the transductive setting, where only a single graph</p><formula xml:id="formula_6">G 0 = (V 0 ,E 0 ) is considered in the entire dataset. That is to say, G i = G 0 ,∀G i ∈ G.</formula><p>It is transductive since test nodes (but not their labels) are also observed during training. Examples in this case include classifying papers in a citation database like Citeseer, or entities in a social network like Facebook.</p><p>Here the dataset is represented as</p><formula xml:id="formula_7">D (tra) = {(G 0 ,c i ,y i )} N i=1</formula><p>, and the classifier f (tra) (·;G 0 ) ∈ F (tra) : V 0 → Y minimizes the following loss:</p><formula xml:id="formula_8">L (tra) = 1 N N i=1 L(f (tra) (c i ;G 0 ),y i )<label>(2)</label></formula><p>When not causing confusion, we will overload the notations</p><formula xml:id="formula_9">D = {(G i , c i , y i )} N i=1</formula><p>to represent the dataset, and f ∈ F in either settings. In this case, c i is implicitly omitted in inductive graph classification setting; While in transductive node classification setting, G i always refers to G 0 implicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GNN family models</head><p>The Graph Neural Networks (GNNs) define a general architecture for neural network on graph G = (V,E). This architecture obtains the vector representation of nodes through an iterative process:</p><formula xml:id="formula_10">µ (k) v = h (k) {w(u,v),x(u),µ (k−1) u } u∈N (v) , x(v),µ (k−1) v ,k ∈ {1,2,...,K}<label>(3)</label></formula><p>where N (v) specifies the neighborhood of node v ∈ V . The initial node embedding µ</p><p>v ∈ R d is set to zero. For simplicity, we denote the outcome node embedding as µ v = µ</p><p>v . To obtain the graph-level embedding from node embeddings, a global pooling is applied over the node embeddings. The vanilla GNN model runs the above iteration until convergence. But recently, people find a fixed number of propagation steps T with various different parameterizations <ref type="bibr" target="#b13">(Li et al., 2015;</ref><ref type="bibr" target="#b4">Dai et al., 2016;</ref><ref type="bibr" target="#b7">Gilmer et al., 2017;</ref><ref type="bibr" target="#b12">Lei et al., 2017)</ref> work quite well in various applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Graph adversarial attack</head><p>Given a learned classifier f and an instance from the dataset (G,c,y) ∈ D, the graph adversarial attacker g(·,·) : G ×D → G asks to modify the graph G = (V,E) intoG = (Ṽ ,Ẽ), such that</p><formula xml:id="formula_13">max G I(f (G,c) = y) s.t.G = g(f,(G,c,y)) I(G,G,c) = 1.<label>(4)</label></formula><p>Here I(·,·,·) : G ×G ×V → {0,1} is an equivalency indicator that tells whether two graphs G andG are equivalent under the classification semantics.</p><p>In this paper, we focus on the modifications to the discrete structures. The attacker g is allowed to add or delete edges from G to construct the new graph. Such type of actions are rich enough, since adding or deleting nodes can be performed by a series of modifications to the edges. Also modifying the edges is harder than modifying the nodes, since choosing a node only requires O(|V |) complexity, while naively choosing an edge requires O(|V | 2 ).</p><p>Since the attacker is aimed at fooling the classifier f , instead of actually changing the true label of the instance, the equivalency indicator should be defined first to restrict the modifications an attacker can perform. We use two ways to define the equivalency indicator:</p><p>1) Explicit semantics. In this case, a gold standard classifier f * is assumed to be accessible. Thus the equivalency indicator I(·,·,·) is defined as:</p><formula xml:id="formula_14">I(G,G,c) = I(f * (G,c) = f * (G,c)),<label>(5)</label></formula><p>where I(·) ∈ {0,1} is an indicator function. 2) Small modifications. In many cases when explicit semantics is unknown, we will ask the attacker to make as few modifications as possible within a neighborhood graph:</p><formula xml:id="formula_15">I(G,G,c) =I(|(E −Ẽ)∪(Ẽ −E)| &lt; m) ·I(Ẽ ⊆ N (G,b))).<label>(6)</label></formula><p>In the above equation, m is the maximum number of edges that allowed to modify, and</p><formula xml:id="formula_16">N (G,b) = {(u,v) : u,v ∈ V,d (G) (u,v) &lt;= b} defines the b-hop neighborhood graph, where d (G) (u,v) ∈ {1,2,.</formula><p>..} is the distance between two nodes in graph G.</p><p>Take an example in friendship networks, a suspicious behavior would be adding or deleting many friends in a short period, or creating the friendship with someone who doesn't share any common friend. The "small modification" constraint eliminates the possibility of above two possibilities, so as to regulate the behavior of g. With either of the two realizations of robust classifier r, it is easy to enforce the attacker. Each time when an invalid modification proposed, the classifier can simply ignore such move.</p><p>Below we first introduce our main algorithm, RL-S2V, for learning attacker g in Section 3.1. Then in Section 3.2, we present other possible attack methods under different scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Attacking as hierarchical reinforcement learning</head><p>Given an instance (G,c,y) and a target classifier f , we model the attack procedure as a Finite Horizon Markov Decision Process M (m) (f,G,c,y). The definition of such MDP is as follows:</p><p>• Action As we mentioned in Sec 3, the attacker is allowed to add or delete edges in the graph. So a single action at time step t is a t ∈ A ⊆ V × V . However, simply performing actions in O(|V | 2 ) space is too expensive. We will shortly show how to use hierarchical action to decompose this action space.</p><p>• State The state s t at time t is represented by the tuple (Ĝ t ,c), whereĜ t is a partially modified graph with some of the edges added/deleted from G.</p><p>• Reward The purpose of the attacker is to fool the target classifier. So the non-zero reward is only received at the end of the MDP, with reward being</p><formula xml:id="formula_17">r (G,c) = 1 : f (G,c) = y −1 : f (G,c) = y<label>(7)</label></formula><p>In the intermediate steps of modification, no reward will be received. That is to say, r(s t ,a t ) = 0,∀t = 1,2,...,m−1.</p><p>In PBA-C setting where the prediction confidence of the target classifier is accessible, we can also use r (G,c) = L(f (G,c),y) as the reward.</p><p>• Terminal Once the agent modifies m edges, the process stops. For simplicity, we focus on the MDP with fixed length. In the case when fewer modification is enough, we can simply let the agent to modify the dummy edges.</p><p>Given the above settings, a sample trajectory from this MDP will be: (s 1 ,a 1 ,r 1 ,...,s m ,a m ,r m ,s m+1 ), where s 1 = (G,c), s t = (Ĝ t ,c),∀t ∈ {2,...,m} and s m+1 = (G,c). The last step will have reward r m = r(s m ,a m ) = r (G,c) and all other intermediate rewards are zero: r t = 0,∀t ∈ {1,2,...,m − 1}. Since this is a discrete optimization problem with a finite horizon, we use Q-learning to learn the MDPs. In our preliminary experiments we also tried with policy optimization methods like Advantage Actor Critic, but found Q-learning works more stable. So below we focus on the modeling with Q-learning.</p><p>Q-learning is an off-policy optimization where it fits the Bellman optimality equation directly as below:</p><formula xml:id="formula_18">Q * (s t ,a t ) = r(s t ,a t )+γmax a Q * (s t+1 ,a ).<label>(8)</label></formula><p>This implicitly suggests a greedy policy:</p><formula xml:id="formula_19">π(a t |s t ;Q * ) = argmax at Q * (s t ,a t ).<label>(9)</label></formula><p>In our finite horizon case, γ is fixed to 1. Note that directly operating the actions in O(|V | 2 ) space is too expensive for large graphs. Thus we propose to decompose the action</p><formula xml:id="formula_20">a t ∈ V ×V into a t = (a (1) t ,a (2) t ), where a (1) t ,a (2) t ∈ V .</formula><p>Thus a single edge action a t is decomposed into two ends of this edge. The hierarchical Q-function is then modeled as below:</p><formula xml:id="formula_21">Q 1 * (s t ,a (1) t ) = max a (2) t Q 2 * (s t ,a (1) t ,a<label>(2)</label></formula><p>t )</p><formula xml:id="formula_22">Q 2 * (s t ,a (1) t ,a<label>(2)</label></formula><p>t ) = r s t ,a t = (a</p><formula xml:id="formula_23">(1) t ,a<label>(2)</label></formula><formula xml:id="formula_24">t ) + max a (1) t+1 Q 1 * (s t ,a (1) t+1 ).<label>(10)</label></formula><p>In the above formulation, Q 1 * and Q 2 * are two functions that implement the original Q</p><p>* . An action is considered as completed only when a pair of (a</p><formula xml:id="formula_25">(1) t ,a<label>(2)</label></formula><p>t ) is chosen. Thus the reward will only be valid after a <ref type="formula" target="#formula_0">(2)</ref> t is made. It is easy to see that such decomposition has the same optimality structure as in Eq <ref type="formula" target="#formula_0">(8)</ref> Take a further look at Eq (10), since only the reward in last time step is non-zero, and also the budget of modification m is given, we can explicitly unroll the Bellman equations as:</p><formula xml:id="formula_26">Q * 1,1 (s 1 ,a (1) 1 ) = max a (2) 1 Q * 1,2 (s 1 ,a (1) 1 ,a (2) 1 ) Q * 1,2 (s 1 ,a (1) 1 ,a (2) 1 ) = max a (1) 2 Q * 2,1 (s 2 ,a (1) 2 ) ... Q * m,1 (s m ,a (1) m ) = max a (2) m Q * m,2 (s m ,a (1) m ,a (2) m ) Q * m,2 (s m ,a (1) m ,a (2) m ) = r(G,c)<label>(11)</label></formula><p>To make notations compact, we still use</p><formula xml:id="formula_27">Q * = {Q * t,1|2 } m t=1</formula><p>to denote the Q-function. Since each sample in the dataset defines an MDP, it is possible to learn a separate Q function</p><formula xml:id="formula_28">for each MDP M (m) i (f, G i , c i , y i ), i = 1, .</formula><p>.. , N . However, we here focus on a more practical and challenging setting, where only one Q * is learned. The learned Q-function is thus asked to generalize or transfer over all the MDPs:</p><formula xml:id="formula_29">max θ N i=1 E t,a=argmaxa t Q * (at|st;θ) [r (G i ,c i ) ],<label>(12)</label></formula><p>where Q * is parameterized by θ. Below we present the parameterization for such Q * that generalizes over MDPs. Since the Q function is scoring the nodes in the state graph, it is natural to use GNN family models for parameterization, in order to learn a generalizable attacker. Specifically, Q 1 * is parameterized as:</p><formula xml:id="formula_30">Q 1 * (s t ,a<label>(1)</label></formula><formula xml:id="formula_31">t ) = W (1) Q1 σ W (2) Q1 [µ a (1) t ,µ(s t )] ,<label>(13)</label></formula><p>where µ a</p><p>(1) t is the embedding of node a</p><p>(1) t in graphĜ t , obtained by structure2vec (S2V) <ref type="bibr" target="#b4">(Dai et al., 2016)</ref>:</p><formula xml:id="formula_32">µ (k) v = relu(W (3) Q1 x(v)+W (4) Q1 u∈N (v) µ (k−1) u ),<label>(14)</label></formula><p>where</p><formula xml:id="formula_33">µ v = µ (K) v and µ (0) v = 0. Also µ(s t ) = µ(Ĝ t = (V t ,Ê t ),c</formula><p>) is the representation of entire state tuple:</p><formula xml:id="formula_34">µ(s t ) = v∈V µ v : graph attack [ v∈NĜ t (c,b) µ v ,µ c ] : node attack (15)</formula><p>In node attack scenario, the state embedding is taken from the b-hop neighborhood of node c, denoted as NĜ</p><formula xml:id="formula_35">t (c, b).</formula><p>The parameter set of Q 1 * is θ 1 = {W</p><formula xml:id="formula_36">(i) Q1 } 4 i=1</formula><p>. Q 2 * is parameterized similarly with parameter θ 2 , with an extra consideration of the chosen node a (1) t :</p><formula xml:id="formula_37">Q 2 * (s t ,a (1) t ,a<label>(2)</label></formula><formula xml:id="formula_38">t ) = W (1) Q2 σ W (2) Q2 [µ a (1) t ,µ a (2) t ,µ(s t )]<label>(16)</label></formula><p>We denote this method as RL-S2V since it learns a Q-function parameterized by S2V to perform attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Other attacking methods</head><p>The RL-S2V is suitable for black-box attack and transfer. However, for different attack scenarios, other algorithms might be preferred. We first introduce RandSampling that requires least information in Sec 3.2.1; Then in Sec 3.2.2, a white-box attack GradArgmax is proposed; Finally the GeneticAlg, which is a kind of evolutionary computing, is proposed in Sec 3.2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">RANDOM SAMPLING</head><p>This is the simplest attack method that randomly adds or deletes edges from graph G. When an edge modification action a t = (u,v) is sampled, we will only accept it when it satisfies the semantic constraint I(·,·,·). It requires the least information for attack. Despite its simplicity, sometimes it can get good attack rate. Figure 2. Illustration of graph structure gradient attack. This white-box attack adds/deletes the edges with maximum gradient (with respect to α) magnitudes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">GRADIENT BASED WHITE BOX ATTACK</head><p>Gradients have been successfully used for modifying continuous inputs, e.g., images. However, taking gradient with respect to a discrete structure is non-trivial. Recall the general iterative embedding process defined in Eq (3), we associate a coefficient α u,v for each pair of (u,v) ∈ V ×V :</p><formula xml:id="formula_39">µ (k) v = h (k) {α u,v w(u,v),x(u),µ (k−1) u } u∈N (v) ∪ {α u ,v w(u ,v),x(u ),µ (k−1) u } u / ∈N (v) , x(v),µ (k−1) v ,k ∈ {1,2,...,K}<label>(17)</label></formula><p>Let α u,v = I(u ∈ N (v)). That is to say, α itself is the binary adjacency matrix. It is easy to see that the above formulation has the same effect as in Eq (3). However, such additional coefficients give us the gradient information with respect to each edge (either existing or non-existing):</p><formula xml:id="formula_40">∂L ∂α u,v = K k=1 ∂L µ k · ∂µ k ∂α u,v .<label>(18)</label></formula><p>In order to attack the model, we could perform the gradient ascent, i.e., α u,v ← α u,v + η ∂L ∂αu,v . However, the attack is on a discrete structure, where only m edges are allowed to be added or deleted. So here we need to solve a combinatorial optimization problem:</p><formula xml:id="formula_41">max {ut,vt} m t=1 m t=1 | ∂L ∂α ut,vt | s.t.G = Modify(G,{α ut,vt } m t=1 ) I(G,G,c) = 1.<label>(19)</label></formula><p>We simply use a greedy algorithm to solve the above optimization. Here the modification of G given a set of coefficients {α ut,vt } modifying edges (u t ,v t ) of graphĜ t :</p><formula xml:id="formula_42">G t+1 = (V t ,Ê t \(u t ,v t )) : ∂L ∂αu t ,v t &lt; 0 (V t ,Ê t ∪{(u t ,v t )}) : ∂L ∂αu t ,v t &gt; 0<label>(20)</label></formula><p>That is to say, we modify the edges who are most likely to cause the change to the objective. Depending on the sign of the gradient, we either add or delete the edge. We name it as GradArgmax since it does the greedy selection based on gradient information.</p><p>The attack procedure is shown in <ref type="figure">Figure 2</ref>. Since this approach requires the gradient information, we consider it as a white-box attack method. Also, the gradient considers all pairs of nodes in a graph, the computation cost is at least O(|V | 2 ), excluding the back-propagation of gradients in Eq (18). Without further approximation, this approach cannot scale to large graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">GENETIC ALGORITHM</head><p>Evolution computing has been successfully applied in many zero-order optimization scenarios, including neural architecture search <ref type="bibr" target="#b17">(Real et al., 2017;</ref><ref type="bibr" target="#b14">Miikkulainen et al., 2017)</ref> and adversarial attack for images <ref type="bibr" target="#b20">(Su et al., 2017)</ref>. We here propose a black-box attack method that implements a type of genetic algorithms. Given an instance (G,c,y) and the target classifier f , Such algorithm involves five major components, as elaborated below:</p><p>• Population: the population refers to a set of candidate solutions. Here we denote it as P (r) = {Ĝ</p><formula xml:id="formula_43">(r) j } |P (r) | j=1 , where eachĜ (r)</formula><p>j is a valid modification solution to the original graph G. r = 1,2,...,R is the index of generation and R is the maximum numbers of evolutions allowed.</p><p>• Fitness: each candidate solution in current population will get a score that measures the quality of the solution.</p><p>We use the loss function of target model L(f (Ĝ (r) j ,c),y) </p><formula xml:id="formula_44">RandSampling √ √ O(1) GradArgmax √ O(|V | 2 ) GeneticAlg √ O(|V |+|E|) RL-S2V √ √ √ O(|V |+|E|)</formula><p>as the score function. A good attack solution should increase such loss. Since the fitness is a continuous score, it is not applicable in PBA-D setting, where only classification label is accessible.</p><p>• Selection: Given the fitness scores of current population, we can either do weighted sampling or greedy selection to select the 'breeding' population P and do the crossover by mixing the edges from these two candidates:</p><formula xml:id="formula_45">G = (V,(Ê 1 ∩Ê 2 )∪rp(Ê 1 \Ê 2 )∪rp(Ê 2 \Ê 1 )). (21)</formula><p>Here rp(·) means randomly picking a subset.</p><p>• Mutation: the mutation process is also biology inspired.</p><p>For a candidate solutionĜ ∈ P (r) , suppose the modified edges are δE = {(u t ,v t )} m t=1 . Then for each edge (u t ,v t ), we have a certain probability to change it to either (u t ,v ) or (u ,v t ).</p><p>The population size |P (r) |, the probability of crossover used in rp(·), the mutation probability and the number of evolutions R are all hyper-parameters that can be tuned. Due to the limitation of the fitness function, this method can only be used in the PBA-C setting. Also since we need to execute the target model f to get fitness scores, the computation cost of such genetic algorithm is O(|V |+|E|), which is mainly made up by the computation cost of GNNs. The overall procedure is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. We simply name it as GeneticAlg since it is an instantiation of general genetic algorithm framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>For GeneticAlg, we set the population size |P| = 100 and the number of rounds R = 10. We tune the crossover rate and mutation rate in {0.1,...,0.5}. For RL-S2V, we tune the number of propagations of its S2V model K = {1,...,5}. There is no parameter tuning for GradArgmax and RandSampling.</p><p>We use the proposed attack methods to attack the graph classification model in Sec 4.1 and node classification model in Sec 4.2. In each scenario, we first show the attack rate when queries are allowed for target model, then we show the generalization ability of the RL-S2V for RBA setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Graph-level attack</head><p>In this set of experiments, we use synthetic data, where the gold classifier f * is known. Thus the explicit semantics is used for the equivalency indicator I. The dataset D we constructed contains 15,000 graphs, generated with Erdos-Renyi random graph model. It is a three class graph classification task, where each class contains 5,000 graphs. The classifier is asked to tell how many connected components are there in the corresponding undirected graph G. The label set Y = {1,2,3}. So there could be up to 3 components in a graph. See <ref type="figure" target="#fig_4">Figure 4</ref> for illustration. The gold classifier f * is obtained by performing a one-time traversal of the entire graph. The dataset is divided into training and two test sets. The test set I contains 1,500 graphs, while test set II contains 150 graphs. Each set contains the same number of instances from different classes.</p><p>We choose structure2vec as the target model for attack. We also tune its number of propagation parameter K = {2,...,5}. <ref type="table">Table 2</ref> shows the results with different settings. For test set I, we can see the structure2vec achieves very high accuracy on distinguishing the number of connected components. Also increasing K seems to improve the generalization in most cases. However, we can see under the practical black-box attack scenario, the GeneticAlg and RL-S2V can bring down the accuracy to 40% ∼ 60%. In attacking the graph classification algorithm, the GradArgmax seems not to be very effective. One reason could be the last pooling step in S2V when obtaining graph-level embedding. During back propagation, the pooling operation will dispatch the gradient to every other node embeddings, which makes the ∂L ∂α looks similar in most entries. For restrict black-box attack on test set II (see the lower half of Table 2), the attacker is asked to propose adversarial samples without any access to the target model. Since RL-S2V is learned on test set I, it is able to transfer its learned policy to test set II. This suggests that the target classifier makes some form of consistent mistakes.</p><p>This experiment shows that, (1) the adversarial examples do exist for supervised graph problems; (2) a model with good generalization ability can still suffer from adversarial attacks; (3) RL-S2V can learn the transferrable adversarial policy to attack unseen graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Node-level attack</head><p>In this experiment, we want to inspect the adversarial attack to the node classification problems. Different from Sec 4.1, here the setting is transductive, where the test samples (but not their labels) are also seen during training. Here we use four real-world datasets, namely the Citeseer, Cora, Pubmed and Finance. The first three are small-scaled citation <ref type="table">Table 2</ref>. Attack graph classification algorithm. We report the 3-class classification accuracy of target model on the vanilla test set I and II, as well as adversarial samples generated. The upper half of the table reports the attack results on test set I, with different levels of access to the information of target classifier. The lower half reports the results of RBA setting on test set II where only RandSampling and RL-S2V can be used. K is the number of propagation steps used in GNN family models (see Eq (3)).</p><p>attack test set I 15-20 nodes 40-50 nodes 90-100 nodes  networks commonly used for node classification, where each node is a paper with corresponding bag-of-words features. The last one is a large-scale dataset that contains transactions from an e-commerce within one day, where the node set contains buyers, sellers and credit cards. The classifier is asked to distinguish the normal transactions from abnormal ones. The statistics of each dataset is shown in <ref type="table" target="#tab_4">Table 3</ref>. The nodes also contain features with different dimensions. For the full table please refer to <ref type="bibr" target="#b11">Kipf &amp; Welling (2016)</ref>. We use GCN <ref type="bibr" target="#b11">(Kipf &amp; Welling, 2016)</ref> as the target model to attack. Here the "small modifications" is used to regulate the attacker. That is to say, given a graph G and target node c, the adversarial samples are limited to delete single edge within 2-hops of node c. <ref type="table" target="#tab_5">Table 4</ref> shows the results. We can see although deleting a single edge is the minimum modification one can do to the graph, the attack rate is still about 10% on those small graphs, and 4% in the Finance dataset. We also ran an exhaustive attack as sanity check, which is the best any algorithm can do under the attack budget. The classifier accuracy will reduce to 60% or lower if two-edge modification is allowed. However, consider the average degree in the graph is not large, deleting two or more edges would violate the "small modification" constraints. We need to be careful to only create adversarial samples, instead of actually changing the true label of that sample.</p><formula xml:id="formula_46">Settings Methods K = 2 K = 3 K = 4 K = 5 K = 2 K = 3 K = 4 K = 5 K = 2 K = 3 K = 4 K = 5<label>(</label></formula><p>In this case, the GradArgmax performs quite good, which is different from the case in graph-level attack. Here the gradient with respect to the adjacency matrix α is no longer averaged, which makes it easier to distinguish the useful modifications. For the restrict black-box attack on test set II, the RL-S2V still learns an attack policy that generalizes to unseen (a) pred = 2 (b) pred = 1 (c) pred = 2 samples. Though we do not have gold classifier in real-world datasets, it is highly possible that the adversarial samples proposed are valid: (1) the structure modification is tiny and within 2-hop; (2) we did not modify the node features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Inspection of adversarial samples</head><p>In this section, we visualize the adversarial samples proposed by different attackers. The solutions proposed by RL-S2V</p><p>(a) pred = 2 (b) pred = 1 (c) pred = 2 <ref type="figure">Figure 6</ref>. Attack solutions proposed by GradArgmax on node classification problem. Attacked node is colored orange. Nodes from the same class as the attacked node are marked black, otherwise white. Target classifier is GCN with K = 2. for graph-level classification problem are shown in <ref type="figure" target="#fig_5">Figure 5</ref>. The ground truth labels are 1, 2, 3, while the target classifier mistakenly predicts 2, 1, 2, respectively. In <ref type="figure" target="#fig_5">Figure 5</ref>(b) and (c), the RL agent connects two nodes who are 4 hops away from each other (before the red edge is added). This shows that, although the target classifier structure2vec is trained with K = 4, it didn't capture the 4-hop information efficiently. Also <ref type="figure" target="#fig_5">Figure 5</ref>(a) shows that, even connecting nodes who are just 2-hop away, the classifier makes mistake on it. <ref type="figure">Figure 6</ref> shows the solutions proposed by GradArgmax. Orange node is the target node for attack. Edges with blue color are suggested to be added by GradArgmax, while black ones are suggested to be deleted. Black nodes have the same node label as the orange node, while while nodes do not. The thicker the edge, the larger the magnitude of the gradient is. <ref type="figure">Figure 6</ref>(b) deletes one neighbor with the same label, but still have other black nodes connected. In this case, the GCN is over-sensitive. The mistake made in <ref type="figure">Figure 6</ref>(c) is reasonable, since although the red edge does not connect two nodes with the same label, it connects to a large community of nodes from the same class in 2-hop distance. In this case, the prediction made by GCN is reasonable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Defense against attacks</head><p>Different from the images, here the possible number of graph structures is finite given the number of nodes. So by adding the adversarial samples back for further training, the improvement of the target model's robustness can be expected. For example, in the experiment of Sec 4.1, adding adversarial samples for training is equivalent to increasing the size of the training set, which will definitely be helpful. So here we seek to use a cheap method for adversarial training -simply doing edge drop during training for defense.</p><p>Dropping the edges during training is different from Dropout <ref type="bibr" target="#b19">(Srivastava et al., 2014)</ref>. Dropout operates on the neurons in the hidden layers, while edge drop modifies the discrete structure. It is also different from simply drop the entire hidden vector, since deleting a single edge can affect more than just one edge. For example, GCN computes the normalized graph Laplacian. So after deleting a single edge, the normalized graph Laplacian needs to be recomputed for some entries. This approach is similar to <ref type="bibr" target="#b9">Hamilton et al. (2017)</ref>, who samples a fixed number of neighborhoods during training for the efficiency. Here we drop the edges globally at random, during each training step.</p><p>The new results after adversarial training are presented in <ref type="table" target="#tab_6">Table 5</ref>. We can see from the table that, though the accuracy of target model remains similar, the attack rate of various methods decreases about 1%. Though the scale of the improvement is not significant, it shows some effectiveness with such cheap adversarial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related work</head><p>Adversarial attack in continuous and discrete space: In recent years, the adversarial attacks to the deep learning models have raised increasing attention from researchers. Some methods focus on the white-box adversarial attack using gradient information, like box constrained L-BFGS <ref type="bibr" target="#b21">(Szegedy et al., 2013)</ref>, Fast Gradient Sign <ref type="bibr" target="#b8">(Goodfellow et al., 2014)</ref>, deep fool (Moosavi-Dezfooli et al., 2016), etc.. When the full information of target model is not accessible, one can train a substitute model <ref type="bibr" target="#b16">(Papernot et al., 2017)</ref>, or use zero-order optimization method <ref type="bibr" target="#b3">(Chen et al., 2017)</ref>. There are also some works working on the attack in discrete data space. The one-pixel attack <ref type="bibr" target="#b20">(Su et al., 2017)</ref> modifies the image by only several pixels using differential evolution; <ref type="bibr" target="#b10">Jia &amp; Liang (2017)</ref> attacks the text reading comprehension system with the help of rules and human efforts. <ref type="bibr" target="#b23">Zügner et al. (2018)</ref> studies the problem of adversarial attack over graphs in parallel to our work, although with very different methods.</p><p>Combinatorial optimization: Modifying the discrete structure to fool the target classifier can be treated as a combinatorial optimization problem. Recently, there are some exciting works using reinforcement learning to learn to solve the general sequential decision problems <ref type="bibr" target="#b1">(Bello et al., 2016)</ref> or graph combinatorial problems . These are closely related to RL-S2V. The RL-S2V extends the previous approach using hierarchical way to decompose the quadratic action space, in order to make the training feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we study the adversarial attack on graph structured data. To perform the efficient attack, we proposed three methods, namely RL-S2V, GradArgmax and GeneticAlg for three different attack settings, respectively. We show that a family of GNN models are vulnerable to such attack. By visualizing the attack samples, we can also inspect the target classifier. We also discussed about defense methods through experiments. Our future work includes developing more effective defense algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>, but making an action would only require O(2×|V |) = O(|V |) complexity. Figure 1 illustrates this process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Illustration of attack using genetic algorithm. The population evolves with selection, crossover and mutation operations. Fitness is measured by the loss function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Example graphs for classification. Here we show three graphs with 1, 2, or 3 components, with 40-50 nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Attack solutions proposed by RL-S2V on graph classification problem. Target classifier is structure2vec with K = 4. The ground truth # components are: (a) 1 (b) 2 (c) 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Figure 1. Illustration of applying hierarchical Q-function to propose adversarial attack solutions. Here adding a single edge at is decomposed into two decision steps a</figDesc><table>' *  

+ ,-

(',/) 

…… 

) / *  

+ ,0 

(',/) 

S2V Module 
) ' *  value 
S2V 
) / *  value 
argmax 

7 

) ' *  (8 9 , :) 

: 9 

(') 

argmax 

7 

) 
/ *  (8 9 , : 9 
' , :) 

: 9 

(/) 

: 9 

8 9 
8 9;' 

(1) 

t and a 

(2) 

t , with two Q-functions Q 
1 *  and Q 
2 *  , respectively. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Application scenarios for different proposed graph attack methods. Cost is measured by the time complexity for proposing a single attack.</figDesc><table>WBA PBA-C PBA-D RBA 
Cost 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Statistics of the graphs used for node classification.</figDesc><table>Dataset 
Nodes 
Edges 
Classes Train/Test I/Test II 

Citeseer 
3,327 
4,732 
6 
120/1,000/500 
Cora 
2,708 
5,429 
7 
140/1,000/500 
Pubmed 
19,717 
44,338 
3 
60/1,000/500 
Finance 2,382,980 8,101,757 
2 
317,041/812/800 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>Attack node classification algorithm. In the upper half of 
the table, we report target model accuracy before/after the attack 
on the test set I, with various settings and methods. In the lower 
half, we report accuracy on test set II with RBA setting only. In 
this second part, only RandSampling and RL-S2V can be used. 
Method 
Citeseer 
Cora 
Pubmed Finance 

(unattacked) 
71.60% 81.00% 79.90% 88.67% 

RBA, RandSampling 67.60% 78.50% 79.00% 87.44% 

WBA, GradArgmax 
63.00% 71.30% 
72.4% 
86.33% 

PBA-C, GeneticAlg 
63.70% 71.20% 72.30% 85.96% 

PBA-D, RL-S2V 
62.70% 71.20% 72.80% 85.43% 

Exhaust 
62.50% 70.70% 71.80% 85.22% 

Restricted black-box attack on test set II 

(unattacked) 
72.60% 80.20% 80.40% 91.88% 

RandSampling 
68.00% 78.40% 79.00% 90.75% 

RL-S2V 
66.00% 75.00% 74.00% 89.10% 

Exhaust 
62.60% 70.80% 71.00% 88.88% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 5 .</head><label>5</label><figDesc>Results after adversarial training by random edge drop.</figDesc><table>Method 
Citeseer 
Cora 
Pubmed Finance 

(unattacked) 
71.30% 81.70% 79.50% 88.55% 

RBA, RandSampling 67.70% 79.20% 78.20% 87.44% 

WBA, GradArgmax 
63.90% 72.50% 72.40% 87.32% 

PBA-C, GeneticAlg 
64.60% 72.60% 72.50% 86.45% 

PBA-D, RL-S2V 
63.90% 72.80% 72.90% 85.80% 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph based anomaly detection and description: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Danai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="626" to="688" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Irwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Hieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09940</idno>
		<title level="m">Neural combinatorial optimization with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Thermometer encoding: One hot way to resist adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Buckman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aurko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho-Jui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
		<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning combinatorial optimization algorithms over graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><forename type="middle">B</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bistra</forename><surname>Dilkina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01665</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2215" to="2223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02216</idno>
		<title level="m">Inductive representation learning on large graphs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adversarial examples for evaluating reading comprehension systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07328</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deriving neural architectures from sequence and graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Wengong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09037</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<title level="m">Gated graph sequence neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miikkulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Risto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Elliot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Francon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Navruzyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arshak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Duffy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hodjat</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00548</idno>
		<title level="m">Babak. Evolving deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seyed-Mohsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2574" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Somesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berkay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security</title>
		<meeting>the 2017 ACM on Asia Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saurabh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kurakin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01041</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vasconcellos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kouichi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sakurai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.08864</idno>
		<title level="m">One pixel attack for fooling deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wojciech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumitru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Rob. Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Know-evolve: Deep temporal reasoning for dynamic knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakshit</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hanjun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
