<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Statistically-motivated Second-order Pooling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">⋆</forename><surname>Kaicheng</surname></persName>
							<email>kaicheng.yu@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CVLab</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<postCode>1015</postCode>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CVLab</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<postCode>1015</postCode>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
							<email>mathieu.salzmann@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CVLab</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<postCode>1015</postCode>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Statistically-motivated Second-order Pooling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Second-order descriptors</term>
					<term>convolutional neural networks</term>
					<term>image classiication</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Second-order pooling, a.k.a. bilinear pooling, has proven effective for deep learning based visual recognition. However, the resulting second-order networks yield a inal representation that is orders of magnitude larger than that of standard, irst-order ones, making them memory-intensive and cumbersome to deploy. Here, we introduce a general, parametric compression strategy that can produce more compact representations than existing compression techniques, yet outperform both compressed and uncompressed second-order models. Our approach is motivated by a statistical analysis of the network's activations, relying on operations that lead to a Gaussian-distributed inal representation, as inherently used by irst-order deep networks. As evidenced by our experiments, this lets us outperform the state-of-the-art irst-order and second-order models on several benchmark recognition datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual recognition is one of the fundamental goals of computer vision. Over the years, second-order representations, i.e., region covariance descriptors, have proven more efective than their irst-order counterparts <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38]</ref> for many tasks, such as pedestrian detection <ref type="bibr" target="#b45">[46]</ref>, material recognition <ref type="bibr" target="#b8">[9]</ref> and semantic segmentation <ref type="bibr" target="#b5">[6]</ref>. More recently, convolutional neural networks (CNNs) have achieved unprecedented performance in a wide range of image classiication problems <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref>. Inspired by the past developments in handcrafted features, several works have proposed to replace the fully-connected layers with second-order pooling strategies, essentially utilizing covariance descriptors within CNNs <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b28">29]</ref>. This has led to second-order or bilinear CNNs whose representation power surpasses that of standard, irst-order ones.</p><p>One drawback of these second-order CNNs is that vectorizing the covariance descriptor to pass it to the classiication layer, as done in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>, yields a vector representation that is orders of magnitude larger than that of irst-order CNNs, thus making these networks memory-intensive and subject to overitting. While compression strategies have been proposed <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref>, they are either nonparametric <ref type="bibr" target="#b12">[13]</ref>, thus limiting the representation power of the network, Dist. <ref type="figure" target="#fig_0">Fig. 1</ref>. Statistically-Motivated Second-Order (SMSO) pooling. Top: Our parametric compression strategy vectorizes a covariance matrix and normalizes the resulting vector. Bottom: Each of these operations yields a well-deined distribution of the data, thus resulting in a consistent framework, whose inal representation follows a Gaussian distribution, as state-of-the-art irst-order deep networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SMSO Pooling</head><p>or designed for a speciic classiication formalism <ref type="bibr" target="#b26">[27]</ref>, thus restricting their applicability.</p><p>In this paper, we introduce a general, parametric compression strategy for second-order CNNs. As evidenced by our results, our strategy can produce more compact representations than <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref>, with as little as 10% of their parameters, yet signiicantly outperforming these methods, as well as the state-of-the-art irst-order <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42]</ref> and uncompressed second-order pooling strategies <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Unlike most deep learning architectures, our approach is motivated by a statistical analysis of the network's activations. In particular, we build upon the observation that irst-order networks inherently exploit Gaussian distributions for their feature representations. This is due to the fact that, as discussed in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref> and explained by the Central Limit Theorem, the outputs of linear layers, and thus of operations such as global average pooling, follow a multivariate Gaussian distribution. The empirical success of such Gaussian distributions of feature representations in irst-order deep networks motivated us to design a compression strategy such that the inal representation also satisies this property.</p><p>To this end, as illustrated by <ref type="figure" target="#fig_0">Fig. 1</ref>, we exploit the fact that the covariance matrices resulting from second-order pooling follow a Wishart distribution <ref type="bibr" target="#b25">[26]</ref>. We then introduce a parametric vectorization (PV) layer, which compresses the second-order information while increasing the model capacity by relying on trainable parameters. We show that our PV layer outputs a vector whose elements follow χ 2 distributions, which motivates the use of a square-root normalization that makes the distribution of the resulting representation converge to a Gaussian, as veriied empirically in Section 3.4. These operations rely on basic algebraic transformations, and can thus be easily integrated into any deep architecture and optimized with standard backpropagation.</p><p>We demonstrate the beneits of our statistically-motivated second-order (SMSO) pooling strategy on standard benchmark datasets for second-order models, in-cluding the Describing Texture Dataset (DTD) <ref type="bibr" target="#b8">[9]</ref>, the Material in Context (MINC) dataset <ref type="bibr" target="#b4">[5]</ref> and the scene recognition MIT-Indoor dataset <ref type="bibr" target="#b39">[40]</ref>. Our approach consistently outperforms the state-of-the-art second-order pooling strategies, independently of the base network used (i.e., VGG-D <ref type="bibr" target="#b41">[42]</ref> or ResNet-50 <ref type="bibr" target="#b19">[20]</ref>), as well as these base networks themselves. Our code is publicly available at https://github.com/kcyu2014/smsop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Visual recognition has a long history in computer vision. Here, we focus on the methods that, like us, make use of representations based on second-order information to tackle this task. In this context, the region covariance descriptors (RCDs) of <ref type="bibr" target="#b45">[46]</ref> constitute the irst attempt at leveraging second-order information. Similarly, Fisher Vectors <ref type="bibr" target="#b1">[2]</ref> also efectively exploit second-order statistics. Following this success, several metrics have been proposed to compare RCDs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref>, and they have been used in various classiication frameworks, such as boosting <ref type="bibr" target="#b11">[12]</ref>, kernel Support Vector Machines <ref type="bibr" target="#b46">[47]</ref>, sparse coding <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b48">49]</ref> and dictionary learning <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b17">18]</ref>. In all these works, however, while the classiier was trained, no learning was involved in the computation of the RCDs.</p><p>To the best of our knowledge, <ref type="bibr" target="#b16">[17]</ref>, and its extension to the log-Euclidean metric <ref type="bibr" target="#b20">[21]</ref>, can be thought of as the irst attempts to learn RCDs. This, however, was achieved by reducing the dimensionality of input RCDs via a single transformation, which has limited learning capacity. In <ref type="bibr" target="#b21">[22]</ref>, the framework of <ref type="bibr" target="#b16">[17]</ref> was extended to learning multiple transformations of input RCDs. Nevertheless, this approach still relied on RCDs as input. The idea of incorporating second-order descriptors in a deep, end-to-end learning paradigm was introduced concurrently in <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b35">[36]</ref>. The former introduced the DeepO 2 P operation, consisting of computing the covariance matrix of convolutional features. The latter proposed the slightly more general idea of bilinear pooling, which, in principle, can exploit inner products between the features of corresponding spatial locations from different layers in the network. In practice, however, the use of cross-layer bilinear features does not bring a signiicant boost in representation power <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34]</ref>, and bilinear pooling is therefore typically achieved by computing the inner products of the features within a single layer, thus becoming essentially equivalent to second-order pooling.</p><p>A key to the success of second-order pooling is the normalization, or transformation, of the second-order representation. In <ref type="bibr" target="#b23">[24]</ref>, the matrix logarithm was employed, motivated by the fact that covariance matrices lie on a Riemannian manifold, and that this operation maps a matrix to its tangent space, thus producing a Euclidean representation. By contrast, <ref type="bibr" target="#b35">[36]</ref> was rather inspired by previous normalization strategies for handcrafted features <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b37">38]</ref>, and proposed to perform an element-wise square-root and ℓ 2 normalization after vectorization of the matrix representation. More recently, <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34]</ref> introduced a matrix square-root normalization strategy that was shown to outperform the other transformation techniques.</p><p>All the above-mentioned methods simply vectorize the second-order representation, i.e., covariance matrix. As such, they produce a inal representation whose size scales quadratically with the number of channels in the last convolutional feature map, thus being typically orders of magnitude larger than the inal representation of irst-order CNNs. To reduce the resulting memory cost and parameter explosion, several approaches have been proposed to compress second-order representations while preserving their discriminative power. The irst attempt at compression was achieved by <ref type="bibr" target="#b12">[13]</ref>, which introduced two strategies, based on the idea of random projection, to map the covariance matrices to vectors. These projections, however, were not learned, thus not increasing the capacity of the network and producing at best the same accuracy as the bilinear CNN of <ref type="bibr" target="#b35">[36]</ref>. In <ref type="bibr" target="#b26">[27]</ref>, a parametric strategy was employed to reduce the dimensionality of the bilinear features. While efective, this strategy was speciically designed to be incorporated in a bilinear Support Vector Machine.</p><p>By contrast, here, we introduce a parametric compression approach that can be incorporated into any standard deep learning framework. Furthermore, our strategy is statistically motivated so as to yield a inal representation whose distribution is of the same type as that inherently used by irst-order deep networks. As evidenced by our experiments, our method can produce more compact representations than existing compression techniques, yet outperforms the stateof-the-art irst-order and second-order models.</p><p>Note that higher-order information has also been exploited in the past <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref>. While promising, we believe that developing statistically-motivated pooling strategies for such higher-order information goes beyond the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we irst introduce our second-order pooling strategy while explaining the statistical motivation behind it. We then provide an alternative interpretation of our approach yielding a lower complexity, study and display the empirical distributions of our network's representations, and inally discuss the relation of our model to the recent second-order pooling techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SMSO Pooling</head><p>Our goal is to design a general, parametric compression strategy for second-order deep networks. Furthermore, inspired by the fact that irst-order deep networks inherently make use of Gaussian distributions for their feature representations, we reason about the statistical distribution of the network's intermediate representations so as to produce a inal representation that is also Gaussian. Note that, while we introduce our SMSO pooling strategy within a CNN formalism, it applies to any method relying on second-order representations.</p><p>Formally, let X ∈ R n×c be a data matrix, consisting of n sample vectors of dimension c. For example, in the context of CNNs, X contains the activations of the last convolutional layer, with n = w × h corresponding to the spatial resolution of the corresponding feature map. Here, we assume x i ∈ R c to follow a multivariate Gaussian distribution N c (µ, Σ). In practice, as discussed in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref> and explained by the Central Limit Theorem, this can be achieved by using a linear activation after the last convolutional layer, potentially followed by batch normalization <ref type="bibr" target="#b22">[23]</ref>. Covariance Computation. Given the data matrix X, traditional secondorder pooling consists of computing a covariance matrix</p><formula xml:id="formula_0">Y ∈ R c×c as Y = 1 n − 1 n ∑ i=1 (x i − µ)(x i − µ) T = 1 n − 1X TX ,<label>(1)</label></formula><p>whereX denotes the mean-subtracted data matrix.</p><p>The following deinition, see, e.g., <ref type="bibr" target="#b25">[26]</ref>, determines the distribution of Y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deinition 1. If the elements</head><formula xml:id="formula_1">x i ∈ R c of a data matrix X ∈ R n×c follow a zero mean multivariate Gaussian distribution x i ∼ N c (0, Σ), then the covariance matrix Y of X is said to follow a Wishart distribution, denoted by Y = X T X ∼ W c (Σ, n) .<label>(2)</label></formula><p>Note that, in the bilinear CNN <ref type="bibr" target="#b35">[36]</ref>, the mean is typically not subtracted from the data. As such, the corresponding bilinear matrix follows a form of non-central Wishart distribution <ref type="bibr" target="#b24">[25]</ref>. Second-order Feature Compression. The standard way to use a secondorder representation is to simply vectorize it <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b23">24]</ref>, potentially after some form of normalization <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b31">32]</ref>. This, however, can yield very high-dimensional vectors that are cumbersome to deal with in practice. To avoid this, motivated by <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref>, we propose to compress the second-order representation during vectorization. Here, we introduce a simple, yet efective, compression technique that, in contrast with <ref type="bibr" target="#b12">[13]</ref>, is parametric, and, as opposed to <ref type="bibr" target="#b26">[27]</ref>, amenable to general classiiers.</p><p>Speciically, we develop a parametric vectorization (PV) layer, which relies on trainable weights W ∈ R c×p , with p the dimension of the resulting vector. Each dimension j of the vector z output by this PV layer can be expressed as</p><formula xml:id="formula_2">z j = w T j Yw j ,<label>(3)</label></formula><p>where w j is a column of W. The distribution of each dimension z j is deined by the following theorem. From this theorem, we can see that each output dimension of our PV layer follows a scaled χ 2 distribution γχ 2 n , where γ = w T j Σw j , with Σ the covariance matrix of the original multivariate Gaussian distribution. Transformation and normalization. As shown above, each dimension of our current vector representation follows a χ 2 distribution. However, as discussed above, irst-order deep networks inherently exploit Gaussian distributions for their feature representations. To make our inal representation also satisfy this property, we rely on the following theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 2 ([50]). If</head><formula xml:id="formula_3">z ∼ χ 2 n with degree freedom n, then z ′ = √ 2z (5)</formula><p>converges to a Gaussian distribution with mean √ 2n − 1 and standard deviation σ = 1 when n is large, i.e., z ′ ∼ N ( √ 2n − 1, 1). Following this theorem, we therefore deine our normalization as the transformation</p><formula xml:id="formula_4">z ′ j = √ αz j − √ 2n − 1 ,<label>(6)</label></formula><p>for each dimension j, where we set α = 2/(w T j Σw j ) to correspond to Theorem 2, while accounting for the factor γ arising from our parametric vectorization above. Note that other transformations, such as log(z) and (z/n) <ref type="bibr">1/3</ref> , are known to also converge to Gaussian distributions as n increases <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b49">50]</ref>. We show that these operations yield similar results to the one above in Section 4.4.</p><p>Note that, according to Theorem 2, the mean and variance of the resulting Gaussian distribution are determined by the degree of freedom n, which, in our case, corresponds to the number of samples used to compute the covariance matrix in Eq. 1. Such pre-determined values, however, might limit the discriminative power of the resulting representation. To tackle this, we further rely on trainable scale and bias parameters, yielding a inal representation</p><formula xml:id="formula_5">z ′′ j = β j + γ j z ′ j ,<label>(7)</label></formula><p>where γ j &gt; 0, β j ∈ R. Note that this transformation is also exploited by batch normalization. However, here, we do not need to compute the batch statistics during training, since Theorem 2 tells us that the batches follow a consistent distribution. Altogether, our SMSO pooling strategy, deined by the operations discussed above, yields a p-dimensional vector. This representation can then be passed to a classiier. It can easily be veriied that the above-mentioned operations are diferentiable, and the resulting deep network can thus be trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Alternative Computation</head><p>Here, we derive an equivalent way to perform our SMSO pooling, with a lower complexity when p is small, as shown in the supplementary material. Note, however, that our statistical reasoning is much clearer for the derivation of Section 3.1 and was what motivated our approach.</p><p>To derive the alternative, we note that</p><formula xml:id="formula_6">1 √ α z ′ j = √ w T j Yw j (8) = w T j ( n ∑ i=1 (x i − µ)(x i − µ) T ) w j (9) = n ∑ i=1 ( w T j (x i − µ) ) ((x i − µ) T w j ) (10) = n ∑ i=1 (w T jx i ) 2 ,<label>(11)</label></formula><formula xml:id="formula_7">wherex i = x i − µ.</formula><p>So, in essence, given X, z ′ can be computed by performing a 1×1 convolution, with weights shaped as (1, 1, c, p) and without bias, followed by a global ℓ 2 pooling operation, and a scaling by the constant √ α. Note that ℓ 2 pooling was introduced several years ago <ref type="bibr" target="#b40">[41]</ref>, but has been mostly ignored in the recent deep learning advances. By contrast, feature reduction with 1×1 convolutions is widely utilized in irst-order network designs <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b19">20]</ref>. In essence, this mathematically equivalent formulation shows that our second-order compression strategy can be achieved without explicitly computing covariance matrices. Yet, our statistical analysis based on these covariance matrices remains valid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Relation to Other Methods</head><p>In this section, we discuss the connections between our method and the other recent second-order pooling strategies in CNNs. In the supplementary material, we compare the computational complexity of diferent second-order methods with that of ours. Normalization. Bilinear pooling (BP) <ref type="bibr" target="#b35">[36]</ref> also proposed to make use of a square-root as normalization operation. An important diference with our approach, however, is that BP directly vectorizes the matrix representation Y. It is easy to see that the diagonal elements of Y follow a χ 2 distribution, e.g., by taking w in Theorem 1 to be a vector with a single value 1 and the other values 0. Therefore, after normalization, some of the dimensions of the BP representation also follow a Gaussian distribution. However, the of-diagonal elements follow a variance-gamma distribution, and, after square-root normalization, will not be Gaussian, thus making the diferent dimensions of the inal representation follow inconsistent distributions.</p><p>In <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b31">[32]</ref>, normalization was performed on the matrix Y directly, via a matrix logarithm and a matrix power normalization, respectively. As such, it is diicult to understand what distribution the elements of the inal representation, obtained by standard vectorization, follow.  <ref type="bibr" target="#b41">[42]</ref>. Note that, as discussed in the text, these empirical distributions match the theoretical ones derived in Section 3.1, and our inal representation does exploit the same type of distribution as irst-order networks.</p><p>Compression. The compact bilinear pooling (CBP) of <ref type="bibr" target="#b12">[13]</ref> exploits a compression scheme that has a form similar to ours in Eq. 3. However, in <ref type="bibr" target="#b12">[13]</ref>, the projection vectors w j are random but ixed. Making them trainable, as in our PV layer, increases the capacity of our model, and, as shown in Section 4, allows us to signiicantly outperform CBP.</p><p>In <ref type="bibr" target="#b26">[27]</ref>, a model is developed speciically for a max-margin bilinear classiier. The parameter matrix of this classiier is approximated by a low-rank factorization, which translates to projecting the initial features to a lower-dimensional representation. As with our alternative formulation of Section 3.2, the resulting bilinear classiier can be obtained without having to explicitly compute Y. This classiier is formulated in terms of quantities of the form ∥U T X i ∥ 2 F , where U is a trainable low-rank weight matrix. In essence, this corresponds to removing the square-root operation from Eq. 11 and summing over all dimensions j. By contrast, our representation, ignoring the scale and bias of Eq. 7, is passed to a separate classiication layer that computes a linear combination of the diferent dimensions with trainable weights, thus increasing the capacity of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Empirical distributions of SMSO pooling</head><p>Our SMSO pooling strategy was motivated by considering the distribution of the representation at various stages in the network. Here, we study the empirical distributions of these features using the MINC dataset, discussed in Section 4, and with a model based on VGG-D. To this end, in <ref type="figure" target="#fig_1">Fig. 2</ref>, we provide a visualization of the distributions after the initial batch normalization (i.e., before computing the covariance matrix, see Section 4.1 for details), after our PV layer, and after square-root transformation with trainable scaling and bias. Speciically, for the initial features, because visualizing a Gaussian distribution in hundreds of dimensions is not feasible, we plot the distribution along the irst 2 principal components. For our representations, where each dimension follows an independent Gaussian, we randomly select four dimensions and plot stacked histograms.</p><p>As expected from the theory, the initial features are close to Gaussian, and the features after our PV layer therefore follow a long-tailed χ 2 distribution. The inal features, after square-root normalization, scaling and bias, are much less skewed, and thus much closer to a Gaussian distribution, thus matching the type of distribution that the inal representations of state-of-the-art deep networks follow, as shown in <ref type="figure" target="#fig_1">Fig. 2(d)</ref>. To further verify this, we conducted a Shapiro-Wilk test on the inal representation. This resulted in a p-value of 0.19 &gt; 0.05, which means that the Gaussian assumption is not rejected, sustaining our claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Here, we irst provide implementation details and introduce the baseline models. We then compare our approach to these baselines on four standard benchmark datasets, and provide an ablation study of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>We evaluate our method on two popular network architectures: the VGG-D network of <ref type="bibr" target="#b41">[42]</ref> (a.k.a. VGG-16) and the ResNet-50 of <ref type="bibr" target="#b19">[20]</ref>. For all second-order models discussed below, i.e., ours and the baselines, we remove all the fullyconnected layers and the last max pooling layer from VGG-D, that is, we truncate the model after the ReLU activation following conv5-3. For ResNet-50, we remove the last global average pooling layer and take our initial features from the last residual block. As in <ref type="bibr" target="#b31">[32]</ref>, we add a 1 × 1 convolutional layer to project the initial features to c = 256 for all the experiments. Note that this is a linear layer, and thus makes the resulting features satisfy our Gaussian assumption.</p><p>Following common practice <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b31">32]</ref>, we rely on weights pre-trained on ImageNet and use stochastic gradient descent with an initial learning rate 10 times smaller than the one used to learn from scratch, i.e., 0.001 for VGG-D and 0.01 for ResNet-50. We then divide this weight by 10 when the validation loss has stopped decreasing for 8 epochs. We initialize the weights of the new layers, i.e., the 1 × 1 convolution, the PV layer and the classiier, with the strategy of <ref type="bibr" target="#b13">[14]</ref>, i.e., random values drawn from a Gaussian distribution. We implemented our approach using Keras <ref type="bibr" target="#b7">[8]</ref> with TensorFlow <ref type="bibr" target="#b0">[1]</ref> as backend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Models</head><p>We now describe the diferent baseline models that we compare our approach with. Note that the classiier is deined as a k-way softmax layer for all these models, as for ours, except for low-rank bilinear pooling, which was speciically designed to make use of a low-rank hinge loss. Original model: This refers to the original, irst-order, models, i.e., either VGG-D or ResNet-50, pre-trained on ImageNet and ine-tuned on the new data. Other than replacing the 1000-way ImageNet classiication layer with a k-way one, we keep the original model settings described in <ref type="bibr" target="#b41">[42]</ref> and <ref type="bibr" target="#b19">[20]</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilinear Pooling (BP) [36]:</head><p>This corresponds to the original, uncompressed bilinear pooling strategy, with signed square-root and ℓ 2 normalization after vanilla vectorization. In this case, we set c = 512, as in the original paper, as the feature dimension before computing the second-order representation. If the original feature dimension does not match this value, i.e., with ResNet-50, we make use of an additional 1 × 1 convolutional layer. Note that we observed that using either 512 or 256 as feature dimension made virtually no diference on the results. We therefore used c = 512, which matches the original paper. DeepO 2 P [24]: This refers to the original, uncompressed covariance-based model, with matrix logarithm and vanilla vectorization. Again, as in the original paper, we set c = 512 as the feature dimension before computing the covariance matrix, by using an additional 1 × 1 convolutional layer when necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matrix Power Normalization (MPN) [32]:</head><p>This model relies on a matrix square-root operation acting on the second-order representation. Following the original paper, we set c = 256 by making use of an additional 1 × 1 convolutional layer before second-order pooling. Note that the improved bilinear pooling of <ref type="bibr" target="#b33">[34]</ref> has the same structure as MPN, and we do not report it as a separate baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compact bilinear pooling (CBP) [13]:</head><p>We report the results of both versions of CBP: the Random Maclaurin (RM) one and the Tensor Sketch (TS) one. For both versions, we set the projection dimension to d = 8, 192, which was shown to achieve the same accuracy as BP, i.e., the best accuracy reported in <ref type="bibr" target="#b12">[13]</ref>. As in the original paper, we apply the same normalization as BP <ref type="bibr" target="#b35">[36]</ref>. Low rank bilinear pooling (LRBP) <ref type="bibr" target="#b26">[27]</ref>: This corresponds to the compression method dedicated to the bilinear SVM classiier. Following <ref type="bibr" target="#b26">[27]</ref>, we set the projection dimension to m = 100 and its rank to r = 8, and initialize the dimensionality reduction layer using the SVD of the Gram matrix computed from the entire validation set. Following the authors' implementation, we apply a scaled square-root with factor 2 × 10 5 after the conv5-3 ReLU, which seems to prevent the model from diverging. Furthermore, we found that training LRBP from the weights of BP ine-tuned on each dataset also helped convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison to the Baselines</head><p>Let us now compare the results of our model with those of the baselines described in Section 4.2. To this end, we make use of four diverse benchmark image classiication datasets, thus showing the versatility of our approach. These datasets are the Describing Texture Dataset (DTD) <ref type="bibr" target="#b8">[9]</ref> for texture recognition, the challenging Material In Context (MINC-2500) dataset <ref type="bibr" target="#b4">[5]</ref> for large-scale material recognition in the wild, the MIT-Indoor dataset <ref type="bibr" target="#b39">[40]</ref> for indoor scene understanding and the Caltech-UCSD Bird (CUB) dataset <ref type="bibr" target="#b47">[48]</ref> for ine-grained classiication. DTD contains 47 classes for a total of 5,640 images, mostly capturing the texture itself, with limited surrounding background. By contrast, MINC-2500, consisting of 57,500 images of 23 classes, depicts materials in their real-world environment, thus containing strong background information and making it more challenging. MIT-Indoor contains 15,620 images of 67 diferent indoor scenes, and, with   <ref type="table">Table 1</ref>. Comparison of VGG-D based models. We report the top 1 classiication accuracy (in %) of the original VGG-D model, uncompressed second-order models with diferent normalization strategies (BP, DeepO2P, MPN), second-order compression methods (CBP-TS, CBP-RM, LRBP), and our approach (SMSO) with diferent PV dimensions. Note that our approach signiicantly outperforms all the baselines despite a more compact inal representation (Feature dim.) and much fewer parameters (# param is the number of trainable parameters after the last convolutional layer).</p><p>DTD, has often been used to demonstrate the discriminative power of secondorder representations. The CUB dataset contains 11,788 images of 200 diferent bird species. In <ref type="figure" target="#fig_2">Fig. 3</ref>, we provide a few samples from each dataset. For our experiments, we make use of the standard train-test splits released with these datasets. For DTD, MIT-Indoor and CUB, we deine the input size as 448 × 448 for all the experiments. For the large-scale MINC-2500 dataset, we use 224 × 224 images for all models to speed up training. Note that a larger input size could potentially result in higher accuracies <ref type="bibr" target="#b4">[5]</ref>. For all datasets and all models, we use the same data augmentation strategy as in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b33">34]</ref>. Experiments with VGG-D. We irst discuss the results obtained with the VGG-D architecture as base model. These results are reported in <ref type="table">Table 1</ref> for all models and all datasets. In short, our SMSO framework with PV dimension p = 2, 048 outperforms all the baselines by a signiicant margin on all three datasets. In particular, our accuracy is 7% to 19% higher than the original VGG-D, with much fewer parameters, thus showing the beneits of exploiting secondorder features. MPN is the best-performing baseline, but, besides the fact that we consistently outperform it, has a much higher computational complexity and run time, as shown in the supplementary material. The second-order compression methods (CBP and LRBP) underperform the uncompressed models on average. By contrast, even with p = 64, we outperform most baselines, with a model that corresponds to 10% of the parameters of the most compact baseline.</p><p>In <ref type="figure" target="#fig_3">Fig. 4</ref>, we compare the training and validation loss curves of our approach with those of the best-performing baselines, BP and MPN, on DTD and MINC. Note that our model converges much faster than BP and tends to be more stable than MPN, particularly on DTD. This, we believe, is due to the fact that we rely on basic algebraic operations, instead of the eigenvalue decomposition involved in MPN whose gradient can be diicult to compute, particularly in the presence of small or equal eigenvalues <ref type="bibr" target="#b23">[24]</ref>.</p><p>During these VGG-D based experiments, we have observed that, in practice, LRBP was diicult to train, being very sensitive to the learning rate, which we had to manually adapt throughout training. Because of this, and the fact that LRBP yields lower accuracy than uncompressed models, we do not include this baseline in the remaining experiments. We also exclude DeepO 2 P from the next experiments, because of its consistently lower accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments with ResNet-50.</head><p>To further show the generality of our approach, we make use of the more recent, very deep ResNet-50 <ref type="bibr" target="#b19">[20]</ref> architecture as base network. <ref type="table">Table 2</ref> provides the results of our SMSO framework with p = 64 and p = 2, 048, and of the baselines. In essence, the conclusions remain unchanged; we outperform all the baselines for p = 2, 048. Note that, here, however, the second-order baselines typically do not even outperform the original ResNet-50, whose results are signiicantly higher than the VGG-D ones. By contrast, our model is able to leverage this improvement of the base model and to further increase its accuracy by appropriately exploiting second-order features.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>We evaluate the inluence of diferent components of our model on our results.</p><p>Inluence of the PV dimension. In our experiments, we proposed to set p = 2, 048 or p = 64. We now investigate the inluence of this parameter on our results. To this end, we vary p in the range [2 4 , 2 13 ] by steps corresponding to a factor 2. The curves for this experiment on the validation data of the three datasets with VGG-D based models are provided in <ref type="figure" target="#fig_4">Fig. 5</ref>. Note that our model is quite robust to the exact value of this parameter, with stable accuracies outperforming the best compression baseline for each dataset over large ranges. More importantly, even with p = 64, our model yields as good results as the best compression method, CBP-TS, with only 10% of its parameters.  <ref type="figure" target="#fig_0">N (µ, 1) N (β, 1) N</ref>  Comparison of diferent distributions and transformations. We conduct experiments to compare diferent inal feature distributions on MINC-2500 with a VGG-D based model. The results are provided in <ref type="table">Table 3</ref>. Without our PV compression and without transformation or normalization, the resulting features follow a Wishart distribution, yielding an accuracy of 75.97%, which is comparable to BP <ref type="bibr" target="#b35">[36]</ref>. Adding our PV layer p = 2, 048, but not using any transformation or normalization, yields χ 2 -distributed features and an accuracy similar to the previous one. This suggests that our parametric compression is efective, since we obtain similar accuracy with much fewer parameters. Including the square-root transformation, but without the additional scale and bias of Eq. 7, increases the accuracy to 76.32%. Additionally learning the scale and bias boosts the accuracy to 78.00%, thus showing empirically the beneits of Gaussian-distributed features over other distributions.</p><p>In the last two columns of <ref type="table">Table 3</ref>, we report the results of diferent transformations that bring the χ 2 -distributed features to a Gaussian distribution, i.e., the cubic-root and the element-wise logarithm. Note that these two transformations yield accuracies similar to those obtained with the square-root. More importantly, all transformations yield higher accuracies than not using any (76.14%), which further evidences the beneits of Gaussian-distributed features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have introduced a general and parametric compression strategy for secondorder deep networks, motivated by a statistical analysis of the distribution of the network's intermediate representations. Our SMSO pooling strategy outperforms the state-of-the-art irst-order and second-order models, with higher accuracies than other compression techniques for up to 90% parameter reduction. With a ResNet-50 base architecture, it is the only second-order model to consistently outperform the original one. While Gaussian distributions have proven efective here and for irst-order models, there is no guarantee that they are truly optimal. In the future, we will study if other transformations yielding non-Gaussian distributions can help further improve second-order models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem 1 (</head><label>1</label><figDesc>Theorem 5.6 in [26]). If Y ∈ R c×c follows a Wishart distribu- tion W c (Σ, n), and w ∈ R c and w ̸ = 0,degree of freedom n, i.e., z ∼ χ 2 n .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Histograms of SMSO intermediate feature vectors. We plot the distribution of (a) the initial features X, (b) the features after our PV layer z, (c) the inal representation z ′′ and, for comparison, (d) irst-order features after the last fullyconnected layer in VGG-D [42]. Note that, as discussed in the text, these empirical distributions match the theoretical ones derived in Section 3.1, and our inal representation does exploit the same type of distribution as irst-order networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Sample images from DTD, MINC-2500, MIT-Indoor and CUB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Training and validation loss curves. We plot the training (dashed) and validation (solid) loss values as a function of the number of training epochs for our SMSO pooling strategy (orange), BP (green) and MPN (blue) on DTD (a) and MINC-2500 (b). Our models clearly converge faster than BP, and tend to be more stable than MPN, particularly on the smaller-scale DTD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Inluence of the PV dimension p. We plot the top 1 accuracy as a function of the value p in logarithmic scale on MIT (left), MINC (middle) and DTD (right). Note that accuracy is quite stable over large ranges of p values, yielding as good results as the best-performing compression baseline (CBP-TS) with as few as p = 64 dimensions, corresponding to only 10% of the parameters of CBP-TS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Model name Feature dim. # param. DTD [9] MIT [40] MINC [5] CUB [48]Table 2. Comparison of ResNet-50 based models. We report the top 1 classi- ication accuracy (in %) of the original ResNet-50 model, uncompressed second-order models with diferent normalization strategies (BP, MPN), second-order compression methods (CBP-TS, CBP-RM), and our approach (SMSO). Note that, as in the VGG-D case, our model outperforms all the baselines, including the original ResNet-50, which is not the case of most second-order baselines. It also yields much more compact models than the second-order baselines. (# params. refers to the same quantity as in Table 1.)</figDesc><table>ResNet-50 [20] 
2,048 
4K 
71.45 
76.45 
79.12 
74.51 
BP [36] 
32, 896 
752K 
69.37 
68.35 
79.05 
82.70 
MPN [32] 
32, 896 
752K 
71.10 
72.12 
79.83 
85.43 

CBP-TS [13] 
8,192 
189K 
65.30 
72.60 
75.91 
77.35 
CBP-RM [13] 
8,192 
189K 
62.35 
67.81 
74.15 
-

SMSO (Ours) 
64 
13K 
71.03 
76.31 
79.17 
81.98 
SMSO (Ours) 
2,048 
57K 
72.51 79.68 
81.33 
85.77 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Table 3. Comparison of diferent inal feature distributions. We report the results of diferent combinations of vectorization (vec.), transformation (trans.) and normalization (norm.) strategies, yielding diferent inal feature distributions. Here, µ = √ 2n − 1 from Theorem 2. Ultimately, these results show that bringing the data back to a Gaussian distribution with a trainable scale and bias yields higher accuracies.</figDesc><table>(µ, γ 
2 ) γχ 

2 

n + β 
N (β, γ 
2 ) 
Acc. 
75.97 75.32 
76.32 
77.12 
76.47 
76.14 
78.00 77.86 77.17 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/,softwareavailablefromtensorlow.org" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">All About VLAD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1578" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Log-Euclidean metrics for fast and simple calculus on difusion tensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Arsigny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic Resonance in Medicine</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The statistical analysis of variance-heterogeneity and the logarithmic transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Kendall</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/2983618" />
	</analytic>
	<monogr>
		<title level="j">Supplement to the Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="128" to="138" />
			<date type="published" when="1946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Material Recognition in the Wild with the Materials in Context Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Upchurch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semantic Segmentation with Second-Order Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Riemannian Sparse Coding for Positive Deinite Matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Describing Textures in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Kernel Pooling for Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Histograms of Oriented Gradients for Human Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<title level="m">Compact Bilinear Pooling. In: CVPR. pp</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Understanding the diiculty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Action Recognition Using Sparse Representation on Covariance Manifolds of Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">From manifold to manifold: Geometryaware dimensionality reduction for SPD matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Riemannian coding and dictionary learning: Kernels to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
		<title level="m">Sparse Coding and Dictionary Learning for Symmetric Positive Deinite Matrices -A Kernel Approach</title>
		<imprint>
			<publisher>ECCV</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Toward User-Speciic Tracking by Detection of Human Shapes in Multi-Cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D C</forename><surname>Angonese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iofe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">ICML</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vantzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Matrix backpropagation for Deep Networks with Structured Layers</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The non-central wishart distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>James</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/99771" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences</title>
		<imprint>
			<biblScope unit="volume">229</biblScope>
			<biblScope unit="page" from="364" to="366" />
			<date type="published" when="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Wichern</surname></persName>
		</author>
		<title level="m">Applied multivariate statistical analysis</title>
		<imprint>
			<publisher>Prentice-Hall New Jersey</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Low-Rank Bilinear Pooling for Fine-Grained Classiication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Domain Adaptation by Mixture of Alignments of Second-or Higher-Order Scatter Tensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A deeper look at power normalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5774" to="5783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ImageNet Classiication with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS. pp</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<title level="m">Is Second-Order Information Helpful for LargeScale Visual Recognition? In: ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Log-Euclidean Kernels for Sparse Representation and Dictionary Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">ICCV</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Improved Bilinear Pooling with CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Second-order democratic aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Bilinear Cnn Models for Fine-Grained Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A Riemannian Framework for Tensor Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Improving the Fisher Kernel for LargeScale Image Classiication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Log-Hilbert-Schmidt metric between positive deinite operators on Hilbert spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Quang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>San-Biagio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recognizing Indoor Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="413" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Convolutional Neural Networks Applied to House Numbers Digit Classiication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ICPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A new metric on the manifold of kernel matrices with application to matrix geometric means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Generalized Dictionary Learning for Symmetric Positive Definite Matrices with Application to Nearest Neighbor Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Human Detection via Classiication on Riemannian Manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Caltech-UCSD Birds</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">RAID-G -Robust Estimation of Approximate Ininite Dimensional Gaussian with Application to Material Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The distribution of chi-square</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Hilferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="684" to="688" />
			<date type="published" when="1931" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
