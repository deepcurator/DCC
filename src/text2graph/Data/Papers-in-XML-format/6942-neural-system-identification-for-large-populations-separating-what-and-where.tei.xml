<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural system identification for large populations separating &quot;what&quot; and &quot;where&quot;</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Klindt</surname></persName>
							<email>klindt.david@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Integrative Neuroscience</orgName>
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute for Ophthalmic Research</orgName>
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
							<email>alexander.ecker@uni-tuebingen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Integrative Neuroscience</orgName>
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Bernstein Center for Computational Neuroscience</orgName>
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute for Theoretical Physics</orgName>
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Center for Neuroscience and Artificial Intelligence</orgName>
								<orgName type="institution">Baylor College of Medicine</orgName>
								<address>
									<settlement>Houston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Euler</surname></persName>
							<email>thomas.euler@cin.uni-tuebingen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Integrative Neuroscience</orgName>
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute for Ophthalmic Research</orgName>
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
							<email>matthias.bethge@bethgelab.org</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Integrative Neuroscience</orgName>
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Bernstein Center for Computational Neuroscience</orgName>
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute for Theoretical Physics</orgName>
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Center for Neuroscience and Artificial Intelligence</orgName>
								<orgName type="institution">Baylor College of Medicine</orgName>
								<address>
									<settlement>Houston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Max Planck Institute for Biological Cybernetics</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural system identification for large populations separating &quot;what&quot; and &quot;where&quot;</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Neuroscientists classify neurons into different types that perform similar computations at different locations in the visual field. Traditional methods for neural system identification do not capitalize on this separation of "what" and "where". Learning deep convolutional feature spaces that are shared among many neurons provides an exciting path forward, but the architectural design needs to account for data limitations: While new experimental techniques enable recordings from thousands of neurons, experimental time is limited so that one can sample only a small fraction of each neuron's response space. Here, we show that a major bottleneck for fitting convolutional neural networks (CNNs) to neural data is the estimation of the individual receptive field locations -a problem that has been scratched only at the surface thus far. We propose a CNN architecture with a sparse readout layer factorizing the spatial (where) and feature (what) dimensions. Our network scales well to thousands of neurons and short recordings and can be trained end-to-end. We evaluate this architecture on ground-truth data to explore the challenges and limitations of CNN-based system identification. Moreover, we show that our network model outperforms current state-of-the art system identification models of mouse primary visual cortex.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In neural system identification, we seek to construct quantitative models that describe how a neuron responds to arbitrary stimuli <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. In sensory neuroscience, the standard way to approach this problem is with a generalized linear model (GLM): a linear filter followed by a point-wise nonlinearity <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. However, neurons elicit complex nonlinear responses to natural stimuli even as early as in the retina <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> and the degree of nonlinearity increases as ones goes up the visual hierarchy. At the same time, neurons in the same brain area tend to perform similar computations at different positions in the visual field. This separability of what is computed from where it is computed is a key idea underlying the notion of functional cell types tiling the visual field in a retinotopic fashion.</p><p>For early visual processing stages like the retina or primary visual cortex, several nonlinear methods have been proposed, including energy models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, spike-triggered covariance methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, linear-nonlinear (LN-LN) cascades <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, convolutional subunit models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> and GLMs based on handcrafted nonlinear feature spaces <ref type="bibr" target="#b14">[15]</ref>. While these models outperform the simple GLM, they still cannot fully account for the responses of even early visual processing stages (i.e. retina, V1), let alone higher-level areas such as V4 or IT. The main problem is that the expressiveness of the model (i.e. number of parameters) is limited by the amount of data that can be collected for each neuron.</p><p>The recent success of deep learning in computer vision and other fields has sparked interest in using deep learning methods for understanding neural computations in the brain <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, including promising first attempts to learn feature spaces for neural system identification <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. In this study, we would like to achieve a better understanding of the possible advantages of deep learning methods over classical tools for system identification by analyzing their effectiveness on ground truth models. Classical approaches have traditionally been framed as individual multivariate regression problems for each recorded neuron, without exploiting computational similarities between different neurons for regularization. One of the most obvious similarities between different neurons, however, is that the visual system simultaneously extracts similar features at many different locations. Because of this spatial equivariance, the same nonlinear subspace is spanned at many nearby locations and many neurons share similar nonlinear computations. Thus, we should be able to learn much more complex nonlinear functions by combining data from many neurons and learning a common feature space from which we can linearly predict the activity of each neuron.</p><p>We propose a convolutional neural network (CNN) architecture with a special readout layer that separates the problem of learning a common feature space from estimating each neuron's receptive field location and cell type, but can still be trained end-to-end on experimental data. We evaluate this model architecture using simple simulations and show its potential for developing a functional characterization of cell types. Moreover, we show that our model outperforms the current state-ofthe-art on a publicly available dataset of mouse V1 responses to natural images <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Using artificial neural networks to predict neural responses has a long history <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. Recently, two studies <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> fit two-layer models with a convolutional layer and a pooling layer. They do find marked improvements over GLMs and spike-triggered covariance methods, but like most other previous studies they fit their model only to individual cells' responses and do not exploit computational similarities among neurons.</p><p>Antolik et al. <ref type="bibr" target="#b18">[19]</ref> proposed learning a common feature space to improve neural system identification. They outperform GLM-based approaches by fitting a multi-layer neural network consisting of parameterized difference-of-Gaussian filters in the first layer, followed by two fully-connected layers. However, because they do not use a convolutional architecture, features are shared only locally. Thus, every hidden unit has to be learned 'from scratch' at each spatial location and the number of parameters in the fully-connected layers grows quadratically with population size.</p><p>McIntosh et al. <ref type="bibr" target="#b19">[20]</ref> fit a CNN to retinal data. The bottleneck in their approach is the final fullyconnected layer that maps the convolutional feature space to individual cells' responses. The number of parameters in this final readout layer grows very quickly and even for their small populations represents more than half of the total number of parameters.</p><p>Batty et al. <ref type="bibr" target="#b20">[21]</ref> also advocate feature sharing and explore using recurrent neural networks to model the shared feature space. They use a two-step procedure, where they first estimate each neuron's location via spike-triggered average, then crop the stimulus accordingly for each neuron and then learn a model with shared features. The performance of this approach depends critically on the accuracy of the initial location estimate, which can be problematic for nonlinear neurons with a weak spike-triggered average response (e. g. complex cells in primary visual cortex).</p><p>Our contribution is a novel network architecture consisting of a number of convolutional layers followed by a sparse readout layer factorizing the spatial and feature dimensions. Our approach has two main advantages over prior art. First, it reduces the effective number of parameters in the readout layer substantially while still being trainable end-to-end. Second, our readout forces all computations to be performed in the convolutional layers while the factorized readout layer provides an estimate of the receptive field location and the cell type of each neuron.</p><p>In addition, our work goes beyond the findings of these previous studies by providing a systematic evaluation, on ground truth models, of the advantages of feature sharing in neural system identification -in particular in settings with many neurons and few observations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning a common feature space</head><p>We illustrate why learning a common feature space makes much more efficient use of the available data by considering a simple thought experiment. Suppose we record from ten neurons that all compute exactly the same function, except that they are located at different positions. If we know each neuron's position, we can pool their data to estimate a single model by shifting the stimulus such that it is centered on each neuron's receptive field. In this case we have effectively ten times as much data as in the single-neuron case <ref type="figure">(Fig. 1</ref>, red line) and we will achieve the same model performance with a tenth of the data <ref type="figure">(Fig. 1, solid blue line)</ref>. In contrast, if we treat each neuron as an individual regression problem, the performance will on average be identical to the single-neuron case <ref type="figure">(Fig. 1</ref>, dashed blue line). Although this insight has been well known from transfer learning in machine learning, it has so far not been applied widely in a neuroscience context.</p><p>In practice we neither know the receptive field locations of all neurons a priori nor do all neurons implement exactly the same nonlinear function. However, the improvements of learning a shared feature space can still be substantial. First, estimating the receptive field location of an individual neuron is a much simpler task than estimating its entire nonlinear function from scratch. Second, we expect the functional response diversity within a cell type to be much smaller than the overall response diversity across cell types <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. Third, cells in later processing stages (e. g. V1) share the nonlinear computations of their upstream areas (retina, LGN), suggesting that equipping them with a common feature space will simplify learning their individual characteristics <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Feature sharing in a simple linear ground-truth model</head><p>We start by investigating the possible advantages of learning a common feature space with a simple ground truth model -a population of linear neurons with Poisson-like output noise:</p><formula xml:id="formula_0">r n = a T n s y n ∼ N r n , |r n | (1)</formula><p>Here, s is the (Gaussian white noise) stimulus, r n the firing rate of neuron n, a n its receptive field kernel and y n its noisy response. In this simple model, the classical GLM-based approch reduces to (regularized) multivariate linear regression, which we compare to a convolutional neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Convolutional neural network model</head><p>Our neural network consists of a convolutional layer and a readout layer <ref type="figure">(Fig. 2)</ref>. The first layer convolves the image with a number of kernels to produce K feature maps, followed by batch normalization <ref type="bibr" target="#b28">[29]</ref>. There is no nonlinearity in the network (i.e. activation function is the identity). Batch normalization ensures that the output has fixed variance, which is important for the regularization in the second layer. The readout layer pools the output, c, of the convolutional layer by applying a sparse mask, q, for each neuron:r</p><formula xml:id="formula_1">n = i,j,k c ijk q ijkn (2)</formula><p>Here,r n is the predicted firing rate of neuron n. The mask q is factorized in the spatial and feature dimension:</p><formula xml:id="formula_2">q ijkn = m ijn w kn ,<label>(3)</label></formula><p>where m is a spatial mask and w is a set of K feature weights for each neuron. The spatial mask and feature weights encode each neuron's receptive field location and cell type, respectively. As we expect them to be highly sparse, we regularize both by an L1 penalty (with strengths λ m and λ w ).</p><p>. . . Figure 2: Our proposed CNN architecture in its simplest form. It consists of a feature space module and a readout layer. The feature space is extracted via one or more convolutional layers (here one is shown). The readout layer computes for each neuron a weighted sum over the entire feature space.</p><p>To keep the number of parameters tractable and facilitate interpretability, we factorize the readout into a location mask and a vector of feature weights, which are both encouraged to be sparse by regularizing with L1 penalty.</p><p>By factorizing the spatial and feature dimension in the readout layer, we achieve several useful properties: first, it reduces the number of parameters substantially compared to a fully-connected layer <ref type="bibr" target="#b19">[20]</ref>; second, it limits the expressiveness of the layer, forcing the 'computations' down to the convolutional layers, while the readout layer performs only the selection; third, this separation of computation from selection facilitates the interpretation of the learned parameters in terms of functional cell types.</p><p>We minimize the following penalized mean-squared error using the Adam optimizer <ref type="bibr" target="#b29">[30]</ref>:</p><formula xml:id="formula_3">L = 1 B b,n (y bn −r bn ) 2 + λ m i,j,n |m ijn | + λ w k,n |w kn |<label>(4)</label></formula><p>where b denotes the sample index and B = 256 is the minibatch size. We use an initial learning rate of 0.001 and early stopping based on a separate validation set consisting of 20% of the training set. When the validation error has not improved for 300 consecutive steps, we go back to the best parameter set and decrease the learning rate once by a factor of ten. After the second time we end the training. We find the optimal regularization weights λ m and λ w via grid search.</p><p>To achieve optimal performance, we found it to be useful to initialize the masks well. Shifting the convolution kernel by one pixel in one direction while shifting the mask in the opposite direction in principle produces the same output. However, because in practice the filter size is finite, poorly initialized masks can lead to suboptimal solutions with partially cropped filters (cf. <ref type="figure" target="#fig_2">Fig. 3C, CNN 10</ref> ).</p><p>To initialize the masks, we calculated the spike-triggered average for each neuron, smoothed it with a large Gaussian kernel and took the pixel with the maximum absolute value as our initial guess for the neurons' location. We set this pixel to the standard deviation of the neuron's response (because the output of the convolutional layer has unit variance) and initialized the rest of the mask randomly from a Gaussian N (0, 0.001). We initialized the convolution kernels randomly from N (0, 0.01) and the feature weights from N (1/K, 0.01).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline models</head><p>In the linear example studied here, the GLM reduces to simple linear regression. We used two forms of regularization: lasso (L1) and ridge (L2). To maximize the performance of these baseline models, we cropped the stimulus around each neuron's receptive field. Thus, the number of parameters these models have to learn is identical to those in the convolution kernel of the CNN. Again, we cross-validated over the regularization strength.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance evaluation</head><p>To measure the models' performance we compute the fraction of explainable variance explained:  which is evaluated on the ground-truth firing rates r without observation noise. A perfect model would achieve FEV = 1. We evaluate FEV on a held-out test set not seen during model fitting and cross-validation.</p><formula xml:id="formula_4">FEV = 1 − (r − r) 2 /Var(r)<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Single cell type, homogeneous population</head><p>We first considered the idealized situation where all neurons share the same 17 × 17 px on-center/offsurround filter, but at different locations <ref type="figure" target="#fig_2">(Fig. 3A)</ref>. In other words, there is only one feature map in the convolutional layer (K = 1). We used a 48 × 48 px Gaussian white noise stimulus and scaled the neurons' output such that |r| = 0.1, mimicking a neurally-plausible signal-to-noise ratio at firing rates of 1 spike/s and an observation window of 100 ms. We simulated populations of N = 1, 10, 100 and 1000 neurons and varied the amount of training data.</p><p>The CNN model consistently outperformed the linear regression models <ref type="figure" target="#fig_2">(Fig. 3B)</ref>. The ridgeregularized linear regression explained around 60% of the explainable variance with 4,000 samples (i. e. pairs of stimulus and N-dimensional neural response vector). A CNN model pooling over 10 neurons achieved the same level of performance with less than a quarter of the data. The margin in performance increased with the number of neurons pooled over in the model, although the relative improvement started to level off when going from 100 to 1,000 neurons.</p><p>With few observations, the bottleneck appears to be estimating each neuron's location mask. Two observations support this hypothesis. First, the CNN 1000 model learned much 'cleaner' weights with 256 samples than ridge regression with 4,096 <ref type="figure" target="#fig_2">(Fig. 3C)</ref>, although the latter achieved a higher predictive performance (FEV = 55% vs. 65%). This observation suggests that the feature space can be learned efficiently with few samples and many neurons, but that the performance is limited by the estimation of neurons' location masks. Second, when using the ground-truth kernel and optimizing solely the location masks, performance was only marginally better than for 1,000 neurons <ref type="figure" target="#fig_2">(Fig. 3B</ref>, blue dotted line), indicating an upper performance bound by the problem of estimating the location masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Functional classification of cell types</head><p>Our next step was to investigate whether our model architecture can learn interpretable features and obtain a functional classification of cell types. Using the same simple linear model as above, we simulated two cell types with different filter kernels. To make the simulation a bit more realistic, we made the kernels heterogeneous within a cell type <ref type="figure" target="#fig_3">(Fig. 4A</ref>). We simulated a population of 1,000 neurons (500 of each type).</p><p>With sparsity on the readout weights every neuron has to select one of the two convolutional kernels. As a consequence, the feature weights represent more or less directly the cell type identity of each neuron <ref type="figure" target="#fig_3">(Fig. 4C</ref>). This in turn forces the kernels to learn the average of each type <ref type="figure" target="#fig_3">(Fig. 4B)</ref>. However, any other set of kernels spanning the same subspace would have achieved the same predictive performance. Thus, we find that sparsity on the feature weights facilitates interpretability: each neuron chooses one feature channel which represents the essential computation of this type of neuron.</p><p>5 Learning nonlinear feature spaces</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ground truth model</head><p>Next, we investigated how our approach scales to more complex, nonlinear neurons and natural stimuli. To keep the benefits of having ground truth data available, we chose our model neurons from the VGG-19 network <ref type="bibr" target="#b30">[31]</ref>, a popular CNN trained on large-scale object recognition. We selected four random feature maps from layer conv2_2 as 'cell types'. For each cell type, we picked 250 units with random locations (32 × 32 possible locations). We computed ground-truth responses for all 1000 cells on 44 × 44 px image patches obtained by randomly cropping images from the ImageNet (ILSVRC2012) dataset. As before, we rescaled the output to produce sparse, neurally plausible mean responses of 0.1 and added Poisson-like noise.</p><p>We fit a CNN with three convolutional layers consisting of 32, 64 and 4 feature maps (kernel size 5 × 5), followed by our sparse, factorized readout layer <ref type="figure" target="#fig_4">(Fig. 5A</ref>). Each convolutional layer was followed by batch normalization and a ReLU nonlinearity. We trained the model using Adam with a batch-size of 64 and the same initial step size, early stopping, cross-validation and initialization of the masks as described above. As a baseline, we fit a ridge-regularized GLM with ReLU nonlinearity followed by an additional bias.</p><p>To show that our sparse, factorized readout layer is an important feature of our architecture, we also implemented two alternative ways of choosing the readout, which have been proposed in previous work on learning common feature spaces for neural populations. The first approach is to estimate the receptive field location in advance based on the spike-triggered average of each neuron <ref type="bibr" target="#b20">[21]</ref>. <ref type="bibr" target="#b0">1</ref> To do so, we determined the pixel with the strongest spike-triggered average. We then set this pixel to one in the location mask and all other pixels to zero. We then kept the location mask fixed while optimizing convolution kernels and feature weights. The second approach is to use a fully-connected readout tensor <ref type="bibr" target="#b19">[20]</ref> and regularize the activations of all neurons with L1 penalty. In addition, we regularized the fully-connected readout tensor with L2 weight decay. We fit both models to populations of 1,000 neurons.</p><p>Our CNN with the factorized readout outperformed all three baselines <ref type="figure" target="#fig_4">(Fig. 5B)</ref>. <ref type="bibr" target="#b1">2</ref> The performance of the GLM saturated at ≈20% FEV <ref type="figure" target="#fig_4">(Fig. 5B)</ref>, highlighting the high degree of nonlinearity of our model neurons. Using a fully-connected readout <ref type="bibr" target="#b19">[20]</ref> incurred a substantial performance penalty when the number of samples was small and only asymptotically (for a large number of samples) reached the same performance as our factorized readout. Estimating the receptive field location in advance <ref type="bibr" target="#b20">[21]</ref> led to a drop in performance -even for large sample sizes. A likely explanation for this finding is the fact that the responses are quite nonlinear and, thus, estimates of the receptive field location via spike-triggered average (a linear method) are not very reliable, even for large sample sizes.</p><p>Note that the fact that we can fit the model is not trivial, although ground truth is a CNN. We have observations of noise-perturbed VGG units whose locations we do not know. Thus, we have to infer both the location of each unit as well as the complex, nonlinear feature space simultaneously. Our results show that our model solves this task more efficiently than both simpler (GLM) and equally expressive <ref type="bibr" target="#b19">[20]</ref> models when the number of samples is relatively small.</p><p>In addition to fitting the data well, the model also recovered both the cell types and the receptive field locations correctly <ref type="figure" target="#fig_4">(Fig. 5C, D)</ref>. When fit using 2 16 samples (2 <ref type="bibr" target="#b9">10</ref> for validation/test and the rest for training), the readout weights of the four cell types clustered nicely <ref type="figure" target="#fig_4">(Fig. 5C</ref>) and it successfully recovered the location masks <ref type="figure" target="#fig_4">(Fig. 5D)</ref>. In fact, all cells were classified correctly based on their largest feature weight.</p><p>Next, we investigated how our model and its competitors <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> fare when scaling up to large recordings with many types of neurons. To simulate this scenario, we sampled again VGG units (from the same layer as above), taking 64 units with random locations from up to 16 different feature maps (i.e. cell types). Correspondingly we increased the number of feature maps in the last convolutional layer of the models. We fixed the number of training samples to 2 <ref type="bibr" target="#b11">12</ref> to compare models in a challenging regime (cf. <ref type="figure" target="#fig_4">Fig. 5B</ref>) where performance can be high but is not yet asymptotic.</p><p>Our CNN model scales gracefully to more diverse neural populations <ref type="figure" target="#fig_4">(Fig. 5E)</ref>, remaining roughly at the same level of performance. Similarly, the CNN with the fixed location masks estimated in advance scales well, although with lower overall performance. In contrast, the performance of the fully-connected readout drops fast, because the number of parameters in the readout layer grows very quickly with the number of feature maps in the final convolutional layer. In fact, we were unable to fit models with more than 16 feature maps with this approach, because the size of the read-out tensor became prohibitively large for GPU memory. Finally, we asked how far we can push our model with long recordings and many neurons. We tested our model with 2 <ref type="bibr" target="#b15">16</ref> training samples from 128 different types of neurons (again 64 units each). On this large dataset with ≈ 60.000 recordings from ≈ 8.000 neurons we were still able to fit the model on a single GPU and perform at 90% FEV (data not shown). Thus, we conclude that our model scales well to large-scale problems with thousands of nonlinear and diverse neurons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Application to data from primary visual cortex</head><p>To test our approach on real data and going beyond the previously explored retinal data <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, we used the publicly available dataset from Antolik et al. <ref type="bibr" target="#b18">[19]</ref>. <ref type="bibr" target="#b2">3</ref> The dataset has been obtained by two-photon imaging in the primary visual cortex of sedated mice viewing natural images. It contains three scans with 103, 55 and 102 neurons, respectively, and their responses to static natural images. Each scan consists of a training set of images that were each presented once (1800, 1260 and 1800 images, respectively) as well as a test set consisting of 50 images (each image repeated 10, 8 and 12 times, respectively). We use the data in the same form as the original study <ref type="bibr" target="#b18">[19]</ref>, to which we refer the reader for full details on data acquisition, post-processing and the visual stimulation paradigm.</p><p>To fit this dataset, we used the same basic CNN architecture described above, with three small modifications. First, we replaced the ReLU activation functions by a soft-thresholding nonlinearity, f (x) = log(1 + exp(x)). Second, we replaced the mean-squared error loss by a Poisson loss (because neural responses are non-negative and the observation noise scales with the mean response). Third, we had to regularize the convolutional kernels, because the dataset is relatively limited in terms of recording length and number of neurons. We used two forms of regularization: smoothness and group sparsity. Smoothness is achieved by an L2 penalty on the Laplacian of the convolution kernels:</p><formula xml:id="formula_5">L laplace = λ laplace i,j,k,l (W :,:,kl * L) 2 ij , L = 0.5 1 0.5 1 −6 1 0.5 1 0.5<label>(6)</label></formula><p>where W ijkl is the 4D tensor representing the convolution kernels, i and j depict the two spatial dimensions of the filters and k, l the input and output channels. Group sparsity encourages filters to pool from only a small set of feature maps in the previous layer and is defined as:</p><formula xml:id="formula_6">L group = λ group i,j kl W 2 ijkl .<label>(7)</label></formula><p>We fit CNNs with one, two and three layers. After an initial exploration of different CNN architectures (filter sizes, number of feature maps) on the first scan, we systematically cross-validated over different filter sizes, number of feature maps and regularization strengths via grid search on all three scans. We fit all models using 80% of the training dataset for training and the remaining 20% for validation using Adam and early stopping as described above. For each scan, we selected the best model based on the likelihood on the validation set. In all three scans, the best model had 48 feature maps per layer and 13 × 13 px kernels in the first layer. The best model for the first two scans had 3 × 3 kernels in the subsequent layers, while for the third scan larger 8 × 8 kernels performed best.</p><p>We compared our model to four baselines: (a) the Hierarchical Structural Model from the original paper publishing the dataset <ref type="bibr" target="#b18">[19]</ref>, (b) a regularized linear-nonlinear Poisson (LNP) model, (c) a CNN with fully-connected readout (as in <ref type="bibr" target="#b19">[20]</ref>) and (d) a CNN with fixed spatial masks, inferred from the spike-triggered averages of each neuron (as in <ref type="bibr" target="#b20">[21]</ref>). We used a separate, held-out test set to compare the performance of the models. On the test set, we computed the correlation coefficient between the response predicted by each model and the average observed response across repeats of the same image. <ref type="bibr" target="#b3">4</ref> Our CNN with factorized readout outperformed all four baselines on all three scans ( <ref type="table" target="#tab_2">Table 1)</ref>. The other two CNNs, which either did not use a factorized readout (as in <ref type="bibr" target="#b19">[20]</ref>) or did not jointly optimize feature space and readout (as in <ref type="bibr" target="#b20">[21]</ref>), performed substantially worse. Interestingly, they did not even reach the performance of <ref type="bibr" target="#b18">[19]</ref>, which uses a three-layer fully-connected neural network instead of a CNN. Thus, our model is the new state of the art for predicting neural responses in mouse V1 and the factorized readout was necessary to outperform an earlier (and simpler) neural network architecture that also learned a shared feature space for all neurons <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Our results show that the benefits of learning a shared convolutional feature space can be substantial. Predictive performance increases, however, only until an upper bound imposed by the difficulty of estimating each neuron's location in the visual field. We propose a CNN architecture with a sparse, factorized readout layer that separates these two problems effectively. It allows scaling up the complexity of the convolutional layers to many parallel channels (which are needed to describe diverse, nonlinear neural populations), while keeping the inference problem of each neuron's receptive field location and type identity tractable.</p><p>Furthermore, our performance curves (see <ref type="figure" target="#fig_2">Figs. 3</ref> and 5) may inform experimental designs by determining whether one should aim for longer recordings or more neurons. For instance, if we want to explain at least 80% of the variance in a very homogenous population of neurons, we could choose to record either ≈ 2,000 responses from 10 cells or ≈ 500 responses from 1,000 cells.</p><p>Besides making more efficient use of the data to infer their nonlinear computations, the main promise of our new regularization scheme for system identification with CNNs is that the explicit separation of "what" and "where" provides us with a principled way to functionally classify cells into different types: the feature weights of our model can be thought of as a "barcode" identifying each cell type. We are currently working on applying this approach to large-scale data from the retina and primary visual cortex. Later processing stages, such as primary visual cortex could additionally benefit from similarly exploiting equivariance not only in the spatial domain, but also (approximately) in the orientation or direction-of-motion domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of code</head><p>The code to fit the models and reproduce the figures is available online at: https://github.com/david-klindt/NIPS2017</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure</head><label></label><figDesc>Figure 1: Feature sharing makes more efficient use of the available data. Red line: System identification performance with one recorded neuron. Blue lines: Performance for a hypothetical population of 10 neurons with identical receptive field shapes whose locations we know. A shared model (solid blue) is equivalent to having 10× as much data, i. e. the performance curve shifts to the left. If we fit all neurons independently (dashed blue), we do not benefit from their similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Feature sharing in homogeneous linear population. A, Population of homogeneous spatially shifted on-center/off-surround neurons. B, Model comparison: Fraction of explainable variance explained vs. the number of samples used for fitting the models. Ordinary least squares (OLS), L1 (Lasso) and L2 (Ridge) regularized regression models are fit to individual neurons. CNN N are convolutional models with N neurons fit jointly. The dashed line shows the performance (for N → ∞) of estimating the mask given the ground truth convolution kernel.C, Learned filters for different methods and number of samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A, Example receptive fields of two types of neurons, differing in their average size. B, Learned filters of the CNN model. C, Scatter plot of the feature weights for the two cell types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Inferring a complex, nonlinear feature space. A, Model architecture. B, Dependence of model performance (FEV) on number of samples used for training. C, Feature weights of the four cell types for CNN 1000 with 2 15 samples cluster strongly. D, Learned location masks for four randomly chosen cells (one per type). E, Dependence of model performance (FEV) on number of types of neurons in population, number of samples fixed to 2 12 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>1: Feature sharing makes more efficient use of the available data. Red line: System identification per- formance with one recorded neuron. Blue lines: Per- formance for a hypothetical population of 10 neurons with identical receptive field shapes whose locations we know. A shared model (solid blue) is equivalent to having 10× as much data, i. e. the performance curve shifts to the left. If we fit all neurons independently (dashed blue), we do not benefit from their similarity.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Application to data from primary visual cortex (V1) of mice [19]. The table shows average correlations between model predictions and neural responses on the test set.</figDesc><table>Scan 
1 
2 
3 Average 

Antolik et al. 2016 [19] 
0.51 0.43 0.46 
0.47 
LNP 
0.37 0.30 0.38 
0.36 
CNN with fully connected readout 
0.47 0.34 0.43 
0.43 
CNN with fixed mask 
0.45 0.38 0.41 
0.42 

CNN with factorized readout (ours) 0.55 0.45 0.49 
0.50 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="31">st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that they used a recurrent neural network for the shared feature space. Here we only reproduce their approach to defining the readout. 2 It did not reach 100% performance, since the feature space we fit was smaller and the network shallower than the one used to generate the ground truth data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">See [22, 23] for concurrent work on primate V1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We used the correlation coefficient for evaluation (a) to facilitate comparison with the original study [19] and (b) because estimating FEV on data with a small number of repetitions per image is unreliable.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Philipp Berens, Katrin Franke, Leon Gatys, Andreas Tolias, Fabian Sinz, Edgar Walker and Christian Behrens for comments and discussions. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Do we know what the early visual system does?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Carandini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">B</forename><surname>Demb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerio</forename><surname>Mante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Tolhurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">L</forename><surname>Gallant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><forename type="middle">C</forename><surname>Rust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">46</biblScope>
			<biblScope unit="page" from="10577" to="10597" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Complete functional characterization of sensory neurons by system identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">V</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="477" to="505" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The two-dimensional spatial structure of simple receptive fields in cat striate cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judson</forename><forename type="middle">P</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">A</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1187" to="1211" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Capturing the dynamical repertoire of single neurons with generalized linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alison</forename><forename type="middle">I</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">W</forename><surname>Pillow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07389</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>q-bio</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Eye smarter than scientists believed: neural computations in circuits of the retina</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Gollisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Meister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="164" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Testing pseudo-linear models of responses to natural scenes in primate retina. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Heitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nora</forename><surname>Brackbill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Greschner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">M</forename><surname>Litke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Chichilnisky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">45336</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction and functional architecture in the cat&apos;s visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><forename type="middle">N</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">106</biblScope>
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatiotemporal energy models for the perception of motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America A</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="284" to="299" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><forename type="middle">C</forename><surname>Rust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Odelia</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Anthony</forename><surname>Movshon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spatiotemporal Elements of Macaque V1 Receptive Fields</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="945" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatial structure of complex cell receptive fields measured with natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Touryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gidon</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Dan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="781" to="791" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inferring Nonlinear Neuronal Computation Based on Physiologically Plausible Inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Mcfarland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Butts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1003143</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural Circuit Inference from Function to Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Asari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Gollisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Meister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A convolutional subunit model for neuronal responses in macaque V1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><surname>Vintch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Anthony</forename><surname>Movshon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">44</biblScope>
			<biblScope unit="page" from="14829" to="14841" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cross-orientation suppression in visual area V2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatyana</forename><forename type="middle">O</forename><surname>Rowekamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sharpee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The berkeley wavelet transform: a biologically inspired orthogonal wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Willmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">J</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1537" to="1564" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Performance-optimized hierarchical models predict neural responses in higher visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ha</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">F</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">A</forename><surname>Cadieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darren</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="8619" to="8624" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Modern machine learning far outperforms GLMs at predicting spikes. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><forename type="middle">L</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tucker</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><surname>Ramkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Versteeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><forename type="middle">P</forename><surname>Kording</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">111450</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Explaining the hierarchy of visual representational geometries by remixing of features from many computational vision models. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Seyed-Mahdi Khaligh-Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kendrick</forename><surname>Henriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegeskorte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">9936</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Model Constrained by Visual Hierarchy Improves Prediction of Neural Responses to Natural Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ján</forename><surname>Antolík</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonja</forename><forename type="middle">B</forename><surname>Hofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">A</forename><surname>Bednar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">D</forename><surname>Mrsic-Flogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1004927</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Mcintosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aran</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Nayebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baccus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01825</idno>
		<title level="m">Deep Learning Models of the Retinal Response to Natural Scenes</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>q-bio, stat</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multilayer Recurrent Network Models of Primate Retinal Ganglion Cell Responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleanor</forename><surname>Batty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nora</forename><surname>Brackbill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Heitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Litke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Chichilnisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Paninski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Using deep learning to reveal the neural code for images in primary visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">F</forename><surname>Kindel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elijah</forename><forename type="middle">D</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Zylberberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06208</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs, q-bio</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep convolutional models improve predictions of macaque V1 responses to natural images. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">H</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><forename type="middle">Y</forename><surname>Denfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ecker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">201764</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predicting responses of nonlinear neurons in monkey striate cortex to complex patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Lehky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Desimone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3568" to="3581" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Computational subunits of visual cortical neurons revealed by artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Dan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="8974" to="8979" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonlinear V1 responses to natural scenes revealed by neural network analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">V</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="663" to="679" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The functional diversity of retinal ganglion cells in the mouse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Baden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Berens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miroslav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Rosón</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Euler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7586</biblScope>
			<biblScope unit="page" from="345" to="350" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Inhibition decorrelates visual feature representations in the inner retina</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Berens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timm</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Euler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Baden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">542</biblScope>
			<biblScope unit="issue">7642</biblScope>
			<biblScope unit="page" from="439" to="444" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
