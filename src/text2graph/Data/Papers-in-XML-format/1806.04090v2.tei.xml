<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Atomo: Communication-efficient Learning via Atomic Sparsification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-06-26">June 26, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="institution">UW-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Sievert</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">UW-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengchao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="institution">UW-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Charles</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">UW-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Papailiopoulos</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">UW-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wright</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="institution">UW-Madison</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Atomo: Communication-efficient Learning via Atomic Sparsification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-06-26">June 26, 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Distributed model training suffers from communication overheads due to frequent gradient updates transmitted between compute nodes. To mitigate these overheads, several studies propose the use of sparsified stochastic gradients. We argue that these are facets of a general sparsification method that can operate on any possible atomic decomposition. Notable examples include elementwise, singular value, and Fourier decompositions. We present Atomo, a general framework for atomic sparsification of stochastic gradients. Given a gradient, an atomic decomposition, and a sparsity budget, Atomo gives a random unbiased sparsification of the atoms minimizing variance. We show that methods such as QSGD and TernGrad are special cases of Atomo and show that sparsifiying gradients in their singular value decomposition (SVD), rather than the coordinate-wise one, can lead to significantly faster distributed training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed computing systems have become vital to the success of modern machine learning systems. Work in parallel and distributed optimization has shown that these systems can obtain massive speed up gains in both convex and non-convex settings <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41]</ref>. Several machine learning frameworks such as TensorFlow <ref type="bibr" target="#b0">[1]</ref>, MXNet <ref type="bibr" target="#b9">[10]</ref>, and Caffe2 <ref type="bibr" target="#b26">[27]</ref>, come with distributed implementations of popular training algorithms, such as mini-batch SGD. However, the empirical speed-up gains offered by distributed training, often fall short of the optimal linear scaling one would hope for. It is now widely acknowledged that communication overheads are the main source of this speedup saturation phenomenon <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Communication bottlenecks are largely attributed to frequent gradient updates transmitted between compute nodes. As the number of parameters in state-of-the-art models scales to hundreds of millions <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, the size of gradients scales proportionally. These bottlenecks become even more pronounced in the context of federated learning <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b28">29]</ref>, where edge devices (e.g., mobile phones, sensors, etc) perform decentralized training, but suffer from low-bandwidth during up-link.</p><p>To reduce the cost of of communication during distributed model training, a series of recent studies propose communicating low-precision or sparsified versions of the computed gradients during model updates. Partially initiated by a 1-bit implementation of SGD by Microsoft in <ref type="bibr" target="#b44">[45]</ref>, a large number of recent studies revisited the idea of low-precision training as a means to reduce communication <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b5">6]</ref>. Other approaches for low-communication training focus on sparsification of gradients, either by thresholding small entries or by random sampling <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b48">49]</ref>. Several approaches, including QSGD and TernGrad, implicitly combine quantization and sparsification to maximize performance gains <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b47">48]</ref>, while providing provable guarantees for convergence and performance. We note that quantization methods in the context of gradient based updates have a rich history, dating back to at least as early as the 1970s <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Our Contributions An atomic decomposition represents a vector as a weighted sum of pair-wise orthonormal vectors (so-called atoms), in an inner product space. In this work, we show that stochastic gradient sparsification and quantization are facets of a general approach that sparsifies a gradient on any of its possible atomic decompositions, e.g., its entry-wise or singular value decomposition, its Fourier representation, etc. With this in mind, we develop Atomo, a general framework for atomic sparsification of stochastic gradients. Atomo sets up and optimally solves a meta-optimization that minimizes the variance of the sparsified gradient, subject to the constraints that it is sparse on the atomic basis, and also is an unbiased estimator of the input.</p><p>We show that 1-bit QSGD and TernGrad are in fact special cases of Atomo, and each is optimal (in terms of variance and sparsity), in different parameter regimes. Then, we argue that for some neural network applications, viewing the gradient as a concatenation of matrices (each corresponding to a layer), and applying atomic sparsification to their SVD is meaningful and well-motivated by the fact that these matrices are "nearly" low-rank, e.g., see <ref type="figure" target="#fig_0">Fig. 1</ref>. We show that Atomo on the SVD of each layer's gradient, can lead to less variance, and faster training, for the same communication budget as that of QSGD or TernGrad. We present extensive experiments showing that using Atomo with SVD sparsification, can lead to up to 2× faster training time (including the time to compute the SVD) compared to QSGD, on VGG and ResNet-18, and SVHN and CIFAR-10.</p><p>Relation to Prior Work Atomo is closely related to work on communication-efficient distributed mean estimation in <ref type="bibr" target="#b29">[30]</ref> and <ref type="bibr" target="#b47">[48]</ref>. These works both note, as we do, that variance (or equivalently the mean squared error) controls important quantities such as convergence, and they seek to find a low-communication vector averaging scheme that minimizes it. Our work differs in two key aspects. First, we derive a closed-form solution to the variance minimization problem for all input gradients. Second, Atomo applies to any atomic decomposition, which allows us to compare entry-wise against singular value sparsification for matrices. Using this, we derive explicit conditions for which SVD sparsification leads to lower variance for the same sparsity budget.</p><p>The idea of viewing gradient sparsification through a meta-optimization lens was also used in <ref type="bibr" target="#b49">[50]</ref>. Our work differs in two key ways. First, <ref type="bibr" target="#b49">[50]</ref> consider the problem of minimizing the sparsity of a gradient for a fixed variance, while we consider the reverse problem, that is, minimizing the variance subject to a sparsity budget. The second more important difference is that while <ref type="bibr" target="#b49">[50]</ref> focuses on entry-wise sparsification, we consider a general problem where we sparsify according to any atomic decomposition. For instance, our approach directly applies to sparsifying the singular values of a matrix, which gives rise to faster training algorithms. The gradient of a layer can be seen as a matrix, once we vectorize and appropriately stack the convolutional filters. For all presented data passes, there is a sharp decay in singular values, with the top 3 standing out.</p><p>Finally, low-rank factorizations and sketches of the gradients when viewed as matrices were proposed in <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b28">29]</ref>; arguably most of these methods (with the exception of <ref type="bibr" target="#b28">[29]</ref>) aimed to address the high flops required during inference by using low-rank models. Though they did not directly aim to reduce communication, this arises as a useful side effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Setup</head><p>In machine learning, we often wish to find a model w minimizing the empirical risk</p><formula xml:id="formula_0">f (w) = 1 n n i=1 (w; x i )<label>(1)</label></formula><p>where x i ∈ R d is the i-th data point. One way to approximately minimize f (w) is by using stochastic gradient methods that operate as follows:</p><formula xml:id="formula_1">w k+1 = w k − γ g(w k )</formula><p>where w 0 is some initial model, γ is the step size, and g(w) is a stochastic gradient of f (w), i.e.it is an unbiased estimate of the true gradient g(w) = ∇f (w). Mini-batch SGD, one of the most common algorithms for distributed training, computes g as an average of B gradients, each evaluated on randomly sampled data from the training set. Mini-batch SGD is easily parallelized in the parameter server (PS) setup, where a PS stores the global model, and P compute nodes split the effort of computing the B gradients. Once the PS receives these gradients, it applies them to the model, and sends it back to the compute nodes.</p><p>To prove convergence bounds for stochastic-gradient based methods, we usually require g(w) to be an unbiased estimator of the full-batch gradient, and to have small second moment E g(w) 2 , as this controls the speed of convergence. To see this, suppose w * is a critical point of f , then we have</p><formula xml:id="formula_2">E[ w k+1 − w * 2 2 ] = E[ w k − w * 2 2 ] − 2γ ∇f (w k ), w k − w * − γ 2 E[ g(w k ) 2 2 ]</formula><p>progress at step t .</p><p>Thus, the progress of the algorithm at a single step is, in expectation, controlled by the term E[ g(w k ) ] 2 2 ; the smaller it is, the bigger the progress. This is a well-known fact in optimization, and most convergence bounds for stochastic-gradient based methods, including mini-batch, involve upper bounds on E[ g(w k ) ] 2 2 in convex and nonconvex settings <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b53">54]</ref>. In short, recent results on low-communication variants of SGD design unbiased quantized or sparse gradients, and try to minimize their variance <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b49">50]</ref>. Note that when we require the estimate to be unbiased, minimizing the variance is equivalent to minimizing the second moment.</p><p>Since variance is a proxy for speed of convergence, in the context of communication-efficient stochastic gradient methods, one can ask: What is the smallest possible variance of an unbiased stochastic gradient that can be represented with k bits? Note that under the unbiased assumption, minimizing variance is equivalent to minimizing the second moment of the random vector. This meta-optimization can be cast as the following meta-optimization:</p><formula xml:id="formula_3">min g E g(w) 2 s.t. E[ g(w)] = g(w)</formula><p>g(w) can be expressed with k bits Here, the expectation is taken over the randomness of g. We are interested in designing a stochastic approximation g that "solves" this optimization. However, it seems difficult to design a formal, tractable version of the last constraint. In the next section, we replace this with a simpler constraint that instead requires that g(w) is sparse with respect to a given atomic decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Atomo: Atomic Decomposition and Sparsification</head><p>Suppose that V is an inner product space over R. We will let ·, · and · denote the inner product and induced norm on V .</p><formula xml:id="formula_4">Definition 1. Suppose g = n i=1</formula><p>λ i a i where λ i = 0. We say that this decomposition is atomic if</p><formula xml:id="formula_5">{a i } n i=1</formula><p>is an orthonormal set of vectors.</p><p>In what follows, you may think of g as the stochastic gradient. One example of an atomic decomposition is the entry-wise decomposition of a vector g ∈ R n as g = i g i e i where</p><formula xml:id="formula_6">{e i } n i=1</formula><p>is the standard basis. Similarly, any orthonormal basis {b i } n i=1 of R n gives rise to an atomic decomposition of any g ∈ R n . Another important example is the singular value decomposition of</p><formula xml:id="formula_7">a matrix X = r i=1 σ i u i v T i , as one can verify that {u i v T i } r i=1</formula><p>form an orthonormal set of matrices under the trace inner product. We will focus on sparsification of the SVD in the following sections.</p><p>In general, we are interested in sparsifying g, i.e., finding a sparsified, decomposition that consists of fewer atoms. To relax the optimization stated in the previous section, we would like g to be expressed by s atoms. Our primary motivation is that to communicate g, from say a compute node to a PS, we have to communicate each element of g individually. We can sparsify g in whatever decomposition is most beneficial, and send the weighted atoms of that instead. For instance, if a matrix X is low rank then its singular value decomposition is naturally sparse. Therefore, it may be beneficial to sparsify on the level of singular values instead of sparsifying the entries of X.</p><p>Given some atomic decomposition of g = n i=1 λ i a i , we wish to compute an estimator g that is sparse, unbiased, and such that E[ g 2 ] is small. Note that because g is unbiased, minimizing the second moment E[ g 2 ] is equivalent to minimizing the variance E[ g − g 2 ]. We will use the following sparsification method:</p><formula xml:id="formula_8">g = n i=1 λ i t i p i a i<label>(2)</label></formula><p>where t i ∼ Bernoulli(p i ), for 0 &lt; p i ≤ 1. We refer to this sparsification scheme as atomic sparsification. Note that the t i 's are independent. We have the following lemma about g.</p><formula xml:id="formula_9">Lemma 1. If g = n i=1 λ i a i is an atomic decomposition then E[ g] = g and E[ g 2 ] = n i=1 λ 2 i /p i . Let λ = [λ 1 , . . . , λ n ] T , p = [p 1 , . . . , p n ] T .</formula><p>In order to ensure that this estimator is sparse, we fix some sparsity budget s. That is, we require i p i = s; note that this a sparsity on average constraint. We wish to minimize E[ g 2 ] subject to this constraint. In other words, we wish to solve the following problem:</p><formula xml:id="formula_10">min p n i=1 λ 2 i p i subject to 0 &lt; p i ≤ 1, n i=1 p i = s.<label>(3)</label></formula><p>An equivalent form of this optimization problem was previously presented in <ref type="bibr" target="#b29">[30]</ref> (Section 6.1). The authors considered this problem for entry-wise sparsification and found a closed-form solution for s ≤ λ 1 / λ ∞ . We give a version of their result but extend this to a closed-form solution for all s. A similar optimization problem was given in <ref type="bibr" target="#b49">[50]</ref>, which instead minimizes sparsity subject to a variance constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Atomo probabilities</head><p>Input : λ ∈ R n with |λ 1 | ≥ . . . |λ n |; sparsity budget s such that 0 &lt; s ≤ n.</p><formula xml:id="formula_11">Output : p ∈ R n solving (3). i = 1; while i ≤ n do if |λ i |s ≤ n j=i |λ i | then for k = i, . . . , n do p k = |λ k |s n j=i |λ i | −1 ; end i = n + 1; else p i = 1, s = s − 1; i = i + 1; end end</formula><p>We will show that the Algorithm 1 produces a probability vector p ∈ R n solving (3) for 0 &lt; s ≤ n. While we show in Appendix C that this result can be derived using the KKT conditions, we use an alternative method that focuses on a relaxation of (3) in order to better understand the structure of the problem. This approach has the added benefit of shedding light on what variance is achieved by solving <ref type="bibr" target="#b2">(3)</ref>.</p><p>Note that (3) has a non-empty feasible set only for</p><formula xml:id="formula_12">0 &lt; s ≤ n. Define f (p) := n i=1 λ 2 i /p i .</formula><p>To understand how to solve (3), we first consider the following relaxation:</p><formula xml:id="formula_13">min p n i=1 λ 2 i p i subject to 0 &lt; p i , n i=1 p i = s.<label>(4)</label></formula><p>We have the following lemma about the solutions to (4), first shown in <ref type="bibr" target="#b29">[30]</ref>.</p><formula xml:id="formula_14">Lemma 2 ([30]). Any feasible vector p to (4) satisfies f (p) ≥ 1 s λ 2 1</formula><p>. This is achieved iff</p><formula xml:id="formula_15">p i = |λ i |s λ 1 .<label>(5)</label></formula><p>Lemma 2 implies that if we ignore the constraint that p i ≤ 1, then the optimal p is achieved by setting p i = |λ i |s/ λ 1 . If the quantity in the right-hand side is greater than 1, this does not give us an actual probability. This leads to the following definition.</p><formula xml:id="formula_16">Definition 2. An atomic decomposition g = n i=1 λ i a i is s-unbalanced at entry i if |λ i |s &gt; λ 1 .</formula><p>Fix the atomic decomposition of g. If there are no s-unbalanced entries then we say that the g is s-balanced. We have the following lemma which guarantees that g is s-balanced for s not too large.</p><formula xml:id="formula_17">Lemma 3. An atomic decomposition g = n i=1 λ i a i is s-balanced iff s ≤ λ 1 / λ ∞ .</formula><p>Lemma 2 gives us the optimal way to sparsify s-balanced vectors, since the p that is optimal for (4) is feasible for (3). Moreover, the iff condition in Lemma 2 implies that the optimal assignment of the p i are between 0 and 1 iff v is s-balanced. Suppose now that g is s-unbalanced at entry j. We cannot assign p j as in <ref type="bibr" target="#b4">(5)</ref>. We will show that setting p j = 1 is optimal in this setting. This comes from the following lemma.</p><p>Lemma 4. Suppose that g is s-unbalanced at entry j and that q is feasible in (3). Then ∃p that is feasible in (3) such that f (p) ≤ f (q) and p j = 1.</p><p>We therefore derive the following theorem characterizing solutions to (3).</p><p>Theorem 5. Suppose we sparsify g as in (2) with sparsity budget s.</p><formula xml:id="formula_18">1. If g is s-balanced, then E[ g 2 ] ≥ 1 s λ 2 1</formula><p>with equality if and only if p i = |λ i |s/ λ 1 .</p><p>2. If g is s-unbalanced, then</p><formula xml:id="formula_19">E[ g 2 ] &gt; 1 s λ 2 1</formula><p>and is minimized by p with p j = 1 where j = argmax i=1,...,n |λ i |.</p><p>This theorem implies that Algorithm 1 produces a vector p ∈ R n solving (3). Note that due to the sorting requirement in the input, the algorithm requires O(n log n) operations. As we discuss in Appendix C, we could instead do this in O(sn) operations by, instead of sorting and iterating through the values in order, simply selecting the next unvisited index i maximizing |λ i | and performing the same test/updates. As we show in Appendix C, we need to select at most s indices before the if statement in Algorithm 1 holds. Whether to sort or do selection depends on the size of s relative to log n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Relation to QSGD and TernGrad</head><p>In this section, we will discuss how Atomo is related to two recent quantization schemes, 1-bit QSGD <ref type="bibr" target="#b3">[4]</ref> and TernGrad <ref type="bibr" target="#b50">[51]</ref>. We will show that in certain cases, these schemes are versions of the Atomo for a specific sparsity budget s. Both schemes use the entry-wise atomic decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">1-bit QSGD</head><p>QSGD takes as input g ∈ R n and b ≥ 1. This b governs the number of quantization buckets. When b = 1, this is referred to as 1-bit QSGD. 1-bit QSGD produces a random vector Q(g) defined by</p><formula xml:id="formula_20">Q(g) i = g 2 sign(g i )ζ i .</formula><p>Here, the ζ i ∼ Bernoulli(|g i |/ g 2 ) are independent random variables. A straightforward computation shows that Q(g) can be defined equivalently by</p><formula xml:id="formula_21">Q(g) i = g i t i |g i |/ g 2<label>(6)</label></formula><p>where t i ∼ Bernoulli(|g i |/ g 2 ). Therefore, 1-bit QSGD exactly uses the atomic sparsification framework in (2) with p i = |g i |/ g 2 . The total sparsity budget is therefore given by</p><formula xml:id="formula_22">s = n i=1 p i = g 1 g 2 .</formula><p>By Lemma 3 any g is s-balanced for this s. Therefore, Theorem 5 implies that the optimal way to assign p i with this given s is p i = |g i |/ g 2 . Since this agrees with <ref type="bibr" target="#b5">(6)</ref>, this implies that 1-bit QSGD performs variance-optimal entry-wise sparsification for sparsity budget s = g 1 / g 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">TernGrad</head><p>Similarly, TernGrad takes as input g ∈ R n , and produces a sparsified version T (g) given by</p><formula xml:id="formula_23">T (g) i = g ∞ sign(g i )ζ i where ζ i ∼ Bernoulli(|g i |/ g ∞ ).</formula><p>A straightforward computation shows that T (g) can be defined equivalently by</p><formula xml:id="formula_24">T (g) i = g i t i |g i |/ g ∞<label>(7)</label></formula><p>where</p><formula xml:id="formula_25">t i ∼ Bernoulli(|g i |/ g ∞ ).</formula><p>Therefore, TernGrad exactly uses the atomic sparsification framework in (2) with p i = |g i |/ g ∞ . The total sparsity budget is given by</p><formula xml:id="formula_26">s = n i=1 p i = g 1 g ∞ .</formula><p>By Lemma 3, any g is s-balanced for this s. Therefore, Theorem 5 implies that the optimal way to assign p i with this given s is p i = |g i |/ g ∞ . This agrees with (7). Therefore, TernGrad performs variance-optimal entry-wise sparsification for sparsity budget s = g 1 / g ∞ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">q -quantization</head><p>We can generalize both of these with the following quantization method, which we refer to as</p><formula xml:id="formula_27">q -quantization. Fix q ∈ (0, ∞]. Given g ∈ R n , we define the q -quantization of g, denoted L q (g) by L q (v) i = g q sign(g i )ζ i where ζ i ∼ Bernoulli(|g i |/ g q ).</formula><p>Note that for all i, |g i | ≤ g ∞ ≤ g q , so this does give us a valid probability. We can define L q (v) equivalently by</p><formula xml:id="formula_28">L q (g) i = g i t i |g i |/ g q<label>(8)</label></formula><p>where</p><formula xml:id="formula_29">t i ∼ Bernoulli(|g i |/ g q ).</formula><p>Therefore, q -quantization exactly uses the atomic sparsification framework in (2) with p i = |g i |/ g q . The total sparsity budget is therefore</p><formula xml:id="formula_30">s = n i=1 p i = g 1 g q .</formula><p>By Lemma 3, the optimal way to assign p i with this given s is p i = |g i |/ g q . Since this agrees with (8), Theorem 5 implies the following theorem.</p><p>Theorem 6. q -quantization performs atomic sparsification in the standard basis with p i = |g i |/ g q . This solves (3) for s = g 1 / g q and satisfies E[ L q (g) <ref type="bibr" target="#b1">2</ref> 2 ] = g 1 g q .</p><p>In particular, for q = 2 we get 1-bit QSGD while for q = ∞, we get TernGrad. This shows strong connections between quantization and sparsification methods. Note that as q increases, the sparsity budget for q -quantization increases while the variance decreases. The choice of whether to set q = 2, q = ∞, or q to something else is therefore dependent on the total expected sparsity one is willing to tolerate, and can be tuned for different distributed scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Spectral-Atomo: Sparsifying the Singular Value Decomposition</head><p>In this section we compare different methods for matrix sparsification. The first uses Atomo on the entry-wise decomposition of a matrix, and the second uses Atomo on the singular value decomposition (SVD) of a matrix. We refer to this second approach as Spectral-Atomo. We show that under concrete conditions, Spectral-Atomo incurs less variance than sparsifying entry-wise. We present these conditions and connect them to the equivalence of certain matrix norms.</p><p>Notation For a rank r matrix X, denote its singular value decomposition by</p><formula xml:id="formula_31">X = r i=1 σ i u i v T i . Let σ = [σ 1 , . . . , σ r ] T .</formula><p>Taking the p norm of σ gives a norm on X, referred to as the Schatten p-norm. For p = 1, we get the spectral norm · * , for p = 2 we get the Frobenius norm · F , and for p = ∞, we get the spectral norm · 2 . We define the p,q norm of a matrix X by</p><formula xml:id="formula_32">X p,q =   m j=1 ( n i=1 |X i,j | p ) q/p   1/q .</formula><p>When p = q = ∞, we define this to be X max where X max := max i,j |X i,j |.</p><p>Comparing matrix sparsification methods: Suppose that V is the vector space of real n × m matrices. Given X ∈ V , there are two standard atomic decompositions of X. The first is the entry-wise decomposition</p><formula xml:id="formula_33">X = i,j X i,j e i e T j .</formula><p>The second is the singular value decomposition</p><formula xml:id="formula_34">X = r i=1 σ i u i v T i .</formula><p>If r is small, it may be more efficient to communicate the r(n + m) entries of the SVD, rather than the nm entries of the matrix. Let X and X σ denote the random variables in (2) corresponding to the entry-wise decomposition and singular value decomposition of X, respectively. We wish to compare these two sparsifications.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we compare the communication cost and second moment of these two methods. The communication cost is the expected number of non-zero elements (real numbers) that need to be communicated. For X, a sparsity budget of s corresponds to s non-zero entries we need to communicate. For X σ , a sparsity budget of s gives a communication cost of s(n + m) due to the singular vectors. We compare the optimal second moment from Theorem 5.</p><p>To compare the second moment of these two methods under the same communication cost, we s and suppose X is s-balanced entry-wise. By Theorem 5 and Lemma 3, the second moment in <ref type="table" target="#tab_0">Table  1</ref> is achieved iff</p><formula xml:id="formula_35">s ≤ X 1,1 X max .</formula><p>To achieve the same communication cost with X σ , we take a sparsity budget of s = s/(n + m). By Theorem 5 and Lemma 3, the second moment in <ref type="table" target="#tab_0">Table 1</ref> is achieved iff</p><formula xml:id="formula_36">s = s n + m ≤ X * X 2 .</formula><p>This leads to the following theorem. </p><formula xml:id="formula_37">SVD s(n + m) 1 s X 2 * Theorem 7. Suppose X ∈ R n×m and s ≤ min X 1,1 X max , (n + m) X * X 2 .</formula><p>Then X σ with sparsity s = s/(n + m) incurs the same communication cost as X with sparsity s,</p><formula xml:id="formula_38">and E[ X σ 2 ] ≤ E[ X 2 ] if and only if (n + m) X 2 * ≤ X 2 1,1 .</formula><p>To better understand this condition, we will make use of the following lemma concerning the equivalence of the 1,1 and spectral norms.</p><p>Lemma 8. For any n × m matrix X over R,</p><formula xml:id="formula_39">1 √ nm X 1,1 ≤ X * ≤ X 1,1 .</formula><p>We give a proof of this fact in Appendix B and show that these bounds are the best possible. In other words, there are matrices for which both of the inequalities are equalities. If the first inequality is tight, then</p><formula xml:id="formula_40">E[ X σ 2 ] ≤ E[ X 2 ], while if the second is tight then E[ X σ 2 ] ≥ E[ X 2 ].</formula><p>As we show in the next section, using singular value sparsification can translate in to significantly reduced distributed training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We present an empirical study of Spectral-Atomo and compare it to the recently proposed QSGD <ref type="bibr" target="#b3">[4]</ref>, on a different neural network models and data sets, under real distributed environments. Our main findings are as follows:</p><p>• We observe that spectral-Atomo provides a useful alternative to entry-wise sparsification methods, it reduces communication compared to vanilla mini-batch SGD, and can reduce training time compared to QSGD by up to a factor of 2×. For instance, on VGG11-BN trained on CIFAR-10, spectral-Atomo with sparsity budget 3 achieves 3.96× speedup over vanilla SGD, while 4-bit QSGD achieves 1.68× on a cluster of 16, g2.2xlarge instances.</p><p>• We observe that spectral-Atomo in distributed settings leads to models with negligible accuracy loss when combined with parameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation and setup</head><p>We compare spectral-Atomo with different sparsity budgets to b-bit QSGD across a distributed cluster with a parameter server (PS), implemented in mpi4py <ref type="bibr" target="#b11">[12]</ref> and PyTorch <ref type="bibr" target="#b37">[38]</ref> and deployed on multiple types of instances in Amazon EC2 (e.g., m5.4xlarge, m5.2xlarge, and g2.2xlarge), both PS and compute nodes are of the same type of instance. The PS implementation is standard, with a few important modifications. At the most basic level, it receives gradients from the compute nodes and broadcasts the updated model once a batch has been received.</p><p>In our experiments, we use data augmentation (random crops, and flips), and tuned the step size for every different setup as shown in <ref type="table" target="#tab_3">Table 3</ref>. Momentum and regularization terms are switched off to make the hyperparameter search tractable and the results more legible. Tuning the step sizes for this distributed network for three different datasets and eight different coding schemes can be computationally intensive. As such, we only used small networks so that multiple networks could fit into GPU memory. To emulate the effect of larger networks, we use synchronous message communication, instead of asynchronous.</p><p>Each compute node evaluates gradients sampled from its partition of data. Gradients are then sparsified through QSGD or spectral-Atomo, and then are sent back to the PS. Note that spectralAtomo transmits the weighted singular vectors sampled from the true gradient of a layer. The PS then combines these, and updates the model with the average gradient. Our entire experimental pipeline is implemented in PyTorch <ref type="bibr" target="#b37">[38]</ref> with mpi4py <ref type="bibr" target="#b11">[12]</ref>, and deployed on either g2.2xlarge, m5.2xlarge and m5.4xlarge instances in Amazon AWS EC2. We conducted our experiments on various models, datasets, learning tasks, and neural network models as detailed in <ref type="table" target="#tab_2">Table 2</ref>.    Scalability We study the scalability of these sparsification methods on clusters of different sizes. We used clusters with one PS and n = 2, 4, 8, 16 compute nodes. We ran ResNet-34 on CIFAR-10 using mini-batch SGD with batch size 512 split among compute nodes. The experiment was run on m5.4xlarge instances of AWS EC2 and the results are shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>While increasing the size of the cluster, decreases the computational cost per worker, it causes the communication overhead to grow. We denote as computational cost, the time cost required by each worker for gradient computations, while the communication overhead is represented by the amount time the PS waits to receive the gradients by the slowest worker. This increase in communication cost is non-negligible, even for moderately-sized networks with sparsified gradients. We observed a trade-off in both sparsification approaches between the information retained in the messages after sparsification and the communication overhead.</p><p>Runtime analysis We empirically study runtime costs of spectral-Atomo with sparsity budget set at 1, 2, 3, 6 and made comparisons among b-bit QSGD and TernGrad. We deployed distributed training on ResNet-18 with batch size B = 256 on the CIFAR-10 dataset run with m5.2xlarge instances. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, there is a trade-off between the amount of communication per iteration and the running time for both singular value sparsification and QSGD. In some scenarios, spectral-Atomo attains a higher compression ratio than QSGD and TernGrad. For example, singular value sparsification with sparsity budget 1 may communicate smaller messages than {2, 4}-bit QSGD and TernGrad.</p><p>End-to-end convergence performance We evaluate the end-to-end convergence performance on different datasets and neural networks, training with spectral-Atomo(with sparsity budget s = 1, 2, 3, 4), QSGD (with n = 1, 2, 4, 8 bits), and ordinary mini-batch SGD. The datasets and models are summarized in <ref type="table" target="#tab_2">Table 2</ref>. We use ResNet-18 <ref type="bibr" target="#b22">[23]</ref> and VGG11-BN <ref type="bibr" target="#b45">[46]</ref> for CIFAR-10 <ref type="bibr" target="#b30">[31]</ref> and SVHN <ref type="bibr" target="#b36">[37]</ref>. Again, for each of these methods we tune the step size. The experiments were run on a cluster of 16 compute nodes instantiated on g2.2xlarge instances. The gradients of convolutional layers are 4 dimensional tensors with shape of [x, y, k, k] where x, y are two spatial dimensions and k is the size of the convolutional kernel. However, matrices are required to compute the SVD for spectral-Atomo, and we choose to reshape each layer into a matrix of size [xy/2, 2k 2 ]. This provides more flexibility on the sparsity budget for the SVD sparsification. For QSGD, we use the bucketing and Elias recursive coding methods proposed in <ref type="bibr" target="#b3">[4]</ref>, with bucket size equal to the number of parameters in each layer of the neural network. <ref type="figure" target="#fig_4">Figure 4</ref> and 5 shows how the testing accuracy varies with wall clock time and number of iterations respectively. <ref type="table" target="#tab_5">Tables 4, 5</ref>, and 6 give a detailed account of speedups of singular value sparsification compared to QSGD. In these tables, each method is run until a specified accuracy.  We observe that both QSGD and Atomo speed up model training significantly and achieve similar accuracy to vanilla mini-batch SGD. We also observe that the best performance is not achieve     with the most sparsified, or quantized method, but the optimal method lies somewhere in the middle where enough information is preserved during the sparsification. For instance, 8-bit QSGD converges faster than 4-bit QSGD, and spectral-Atomo with sparsity budget 3, or 4 seems to be the fastest.</p><p>Higher sparsity can lead to a faster running time, but extreme sparsification can adversely affect convergence. For example, for a fixed number of iterations, 1-bit QSGD has the smallest time cost, but may converge much more slowly to an accurate model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we present and analyze Atomo, a general sparsification method for distributed stochastic gradient based methods. Atomo applies to any atomic decomposition, including the entry-wise and the SVD of a matrix, and can be applied to whichever is most beneficial. Atomo generalizes 1-bit QSGD and TernGrad, and provably minimizes the variance of the sparsified gradient subject to a sparsity constraint on the atomic decomposition. We focus on the use Atomo for sparsifying matrices, especially the gradients in neural network training. We show that applying Atomo to the singular values of these matrices can lead to faster training than both vanilla SGD or QSGD, for the same communication budget. We present extensive experiments showing that Atomo can lead to up to a 2× speed-up in training time over QSGD. In the future, we plan to explore the use of Atomo with Fourier decompositions, due to its utility and prevalence in signal processing. We also plan to examine how we can sparsify and compress gradients in a joint fashion to further reduce communication costs. Finally, when sparsifying the SVD of a matrix, we only sparsify the singular values. We also note that it would be interesting to explore jointly sparsification of the SVD and and its singular vectors, which we leave for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Proof of Lemma 2</head><p>Proof. Suppose we have some p satisfying the conditions in (4). We define two auxiliary vectors α, β ∈ R n by</p><formula xml:id="formula_41">α i = |λ i | √ p i . β i = √ p i .</formula><p>Then note that using the fact that i p i = s, we have</p><formula xml:id="formula_42">f (p) = n i=1 λ 2 i p i = n i=1 λ 2 i p i 1 s n i=1 p i = 1 s n i=1 λ 2 i p i n i=1 p i = 1 s α 2 2 β 2 2 .</formula><p>By the Cauchy-Schwarz inequality, this implies</p><formula xml:id="formula_43">f (p) = 1 s α 2 2 β 2 2 ≥ 1 s | α, β | 2 = 1 s n i=1 |λ i | 2 = 1 s λ 2 1 .<label>(9)</label></formula><p>This proves the first part of Lemma 2. In order to have</p><formula xml:id="formula_44">f (p) = 1 s λ 2 1 , (9) implies that we need | α, β | = α 2 β 2 .</formula><p>By the Cauchy-Schwarz inequality, this occurs iff α and β are linearly dependent. Therefore, cα = β for some constant c. Solving, this implies</p><formula xml:id="formula_45">p i = c|λ i |. Since n i=1 p i = s, we have c λ 1 = n i=1 c|λ i | = n i=1 p i = s.</formula><p>Therefore, c = λ 1 /s, which implies the second part of the theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Lemma 4</head><p>Fix q that is feasible in <ref type="bibr" target="#b2">(3)</ref>. To prove Lemma 4 we will require a lemma. Given the atomic decomposition g = n i=1 λ i a i , we say that λ is s-unbalanced at i if |λ i |s &gt; λ 1 , which is equivalent to g being unbalanced in this atomic decomposition at i. For notational simplicity, we will assume that λ is s-unbalanced at i = 1. Let A ⊆ {2, . . . , n}. We define the following notation:</p><formula xml:id="formula_46">s A = i∈A q i . f A (q) = i∈A λ 2 i q i . (λ A ) i = λ i , for i ∈ A, 0, for i / ∈ A.</formula><p>Note that under this notation, Lemma 2 implies that for all p &gt; 0,</p><formula xml:id="formula_47">f A (p) ≥ 1 s A λ A 2 1 . (10) 1. λ A is (s A + q 1 − 1)-balanced. 2. |λ 1 |(s A + q 1 − 1) &gt; λ A 1 .</formula><p>Then there is a vector p that is feasible satisfying f (p) ≤ f (q) and p 1 = 1.</p><p>Proof. Suppose that such a set A exists. Let B = {2, . . . , n}\A. Note that we have</p><formula xml:id="formula_48">f (q) = n i=1 λ 2 i q i = λ 2 1 q 1 + f A (q) + f B (q).</formula><p>By <ref type="bibr" target="#b9">(10)</ref>, this implies</p><formula xml:id="formula_49">f (q) ≥ λ 2 1 q 1 + 1 s A λ A 2 1 + f B (q).<label>(11)</label></formula><p>Define p as follows.</p><formula xml:id="formula_50">p i =          1, for i = 1, |λ i |(s A + q 1 − 1) λ A 1 , for i ∈ A, q i , for i / ∈ A.</formula><p>Note that by Assumption 1 and Lemma 2, we have</p><formula xml:id="formula_51">f A (p) = 1 s A + q 1 − 1 λ A 2 1</formula><p>.</p><formula xml:id="formula_52">Since p i = q i for i ∈ B, we have f B (p) = f B (q). Therefore, f (p) = λ 2 1 + 1 s A + q 1 − 1 λ A 2 1 + f B (q).<label>(12)</label></formula><p>Combining <ref type="formula" target="#formula_0">(11)</ref> and <ref type="formula" target="#formula_0">(12)</ref>, we have</p><formula xml:id="formula_53">f (q) − f (p) = λ 2 1 1 q 1 − 1 + λ A 2 1 1 s A − 1 s A + q 1 − 1 = λ 2 1 1 − q 1 q 1 + λ A 2 1 q 1 − 1 s A (s A + q 1 − 1)</formula><p>.</p><p>Combining this with Assumption 2, we have</p><formula xml:id="formula_54">f (q) − f (p) ≥ λ A 2 1 (s A + q 1 − 1) 2 1 − q 1 q 1 + λ A 2 1 q 1 − 1 s A (r A + q 1 − 1) .<label>(13)</label></formula><p>To show that the RHS of <ref type="formula" target="#formula_0">(13)</ref> is at most 0, it suffices to show</p><formula xml:id="formula_55">s A ≥ q 1 (s A + q 1 − 1).<label>(14)</label></formula><p>However, note that since 0 &lt; q 1 &lt; 1, the RHS of (14) satisfies</p><formula xml:id="formula_56">q 1 (s A + q 1 − 1) = s A q 1 − q 1 (1 − q 1 ) ≤ s A q 1 ≤ s A .</formula><p>Therefore, <ref type="bibr" target="#b13">(14)</ref> holds, completing the proof.</p><p>We can now prove Lemma 4. In the following, we will refer to Conditions 1 and 2, relative to some set A, as the conditions required by Lemma 9.</p><p>Proof. We first show this in the case that n = 2. Here we have the atomic decomposition</p><formula xml:id="formula_57">g = λ 1 a 1 + λ 2 a 2 .</formula><p>The condition that λ is s-unbalanced at i = 1 implies</p><formula xml:id="formula_58">|λ 1 |(s − 1) &gt; |λ 2 |.</formula><p>In particular, this implies s &gt; 1. For A = {2}, Condition 1 is equivalent to</p><formula xml:id="formula_59">|λ 2 |(s A + q 1 − 2) ≤ 0.</formula><p>Note that s A = q 2 and that q 1 + q 2 − 2 = s − 2 by assumption. Since q i ≤ 1, we know that s − 2 ≤ 0 and so Condition 1 holds. Similarly, Condition 2 becomes</p><formula xml:id="formula_60">|λ 1 |(s − 1) &gt; |λ 2 |</formula><p>which holds by assumption. Therefore, Lemma 4 holds for n = 2. Now suppose that n &gt; 2, q is some feasible probability vector, and that λ is s-unbalanced at index 1. We wish to find an A satisfying Conditions 1 and 2. Consider B = {2, . . . , n}. Note that for such B, s B + q 1 − 1 = s − 1. By our unbalanced assumption, we know that Condition 2 holds for B = {2, . . . , n}. If λ B is (s − 1)-balanced, then Lemma 9 implies that we are done.</p><p>Assume that λ B is not (s − 1)-balanced. After relabeling, we can assume it is unbalanced at i = 2. Let C = {3, . . . , n}. Therefore,</p><formula xml:id="formula_61">|λ 2 |(s − 2) &gt; λ C 1 .<label>(15)</label></formula><p>Combining this with the s-unbalanced assumption at i = 1, we find</p><formula xml:id="formula_62">|λ 1 | &gt; λ B 1 s − 1 = |λ 2 | s − 1 + λ C 1 s − 1 &gt; λ C 1 (s − 1)(s − 2) + λ C 1 s − 1 = λ C 1 s − 2 .</formula><p>Therefore,</p><formula xml:id="formula_63">|λ 1 |(s − q 2 − 1) ≥ |λ 1 |(s − 2) &gt; λ C 1 .<label>(16)</label></formula><p>Let D = {1, 3, 4, . . . , n} = {1, . . . , n}\{2}. Then note that <ref type="bibr" target="#b15">(16)</ref> implies that λ D is (s − q 2 )-unbalanced at i = 1. Inductively applying this theorem, this means that we can find a vector p ∈ R |D| such that p 1 = 1 and f D (p ) ≤ f D (q). Moreover, s D (p ) = s − q 2 . Therefore, if we let p be the vector that equals p on D and with p 2 = q 2 , we have</p><formula xml:id="formula_64">f (p 2 ) = f C (p ) + λ 2 2 q 2 ≤ f D (q) + λ 2 2 q 2 = f (q).</formula><p>This proves the desired result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Equivalence of norms</head><p>We are often interested in comparing norms on vectors spaces. This naturally leads to the following definition.</p><p>Definition 3. Let V be a vector space over R or C. Two norms · a , · b are equivalent if there are positive constants C 1 , C 2 such that</p><formula xml:id="formula_65">C 1 x a ≤ x b ≤ C 2 x a</formula><p>for all x ∈ V .</p><p>As it turns out, norms on finite-dimensional vector spaces are always equivalent.</p><p>Theorem 10. Let V be a finite-dimensional vector space over R or C. Then all norms are equivalent.</p><p>In order to compare norms, we often wish to determine the tightest constants which give equivalence between them. In Section 5, we are particularly interested in comparing the X * and X 1,1 on the space of n × m matrices. We have the following lemma.</p><p>Lemma 11. For all n × m real matrices,</p><formula xml:id="formula_66">1 √ nm X 1,1 ≤ X * ≤ X 1,1 .</formula><p>Proof. Suppose that X has the singular value decomposition</p><formula xml:id="formula_67">X = r i=1 σ i u i v T i .</formula><p>We will first show the left inequality. First, note that for any n×m matrix A, A 1,1 ≤ √ nm A F . This follows directly from the fact that for a n-dimensional vector v, v 1 ≤ √ n v 2 . We will also use the fact that for any vectors u ∈ R n , v ∈ R m , uv T F = u 2 v 2 . We then have</p><formula xml:id="formula_68">X 1,1 = r i=1 σ i u i v T i 1,1 ≤ r i=1 σ i u i v T i 1,1 = r i=1 σ i √ nm u i v T i F = r i=1 σ i √ nm u i 2 v i 2 = X * .</formula><p>For the right inequality, note that we have</p><formula xml:id="formula_69">X = i,j X i,j e i e T j</formula><p>This agrees with (21) for n s = n * s . In particular, the minimal value of f occurs at the first value of n s such that the p i in (21) are bounded above by 1.</p><p>Algorithm 1 scans through the sorted λ i and finds the first value of n s for which the probabilities in <ref type="bibr" target="#b20">(21)</ref> are in [0, 1], and therefore finds the optimal p for <ref type="bibr" target="#b16">(17)</ref>. The runtime is dominated by the O(n log n) sorting cost. It is worth noting that we could perform the algorithm in O(sn) time as well. Instead of sorting and then iterating through the λ i in order, at each step we could simply select the next largest |λ i | not yet seen and perform an analogous test and update as in the above algorithm. Since we would have to do the selection step at most s times, this leads to an O(sn) complexity algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The singular values of a convolutional layer's gradient, for ResNet-18 while training on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The timing of the gradient coding methods (QSGD and spectral-Atomo) for different quantization levels, b bits and s sparsity budget respectively for each worker when using a ResNet-34 model on CIFAR10. For brevity, we use SVD to denote spectral-Atomo. The bars represent the total iteration time and are divided into computation time (bottom, solid), encoding time (middle, dotted) and communication time (top, faded).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Runtime analysis of different sparsification methods (singular value sparsification, QSGD, and TernGrad) for ResNet-18 trained on CIFAR-10. The values shown are computation, encoding and communication time as well as the size of the message required to send gradients between workers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Convergence rates for the best performance of QSGD and spectral-Atomo, alongside vanilla SGD. (a) uses ResNet-18 on CIFAR-10, (b) uses ResNet-18 on SVHN, and (c) uses VGG-11-BN on CIFAR-10. For brevity, we use SVD to denote spectral-Atomo.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Convergence rates with respect to number of iterations on: (a) CIFAR-10 on ResNet-18 of best performances from QSGD and SVD (b) SVHN on ResNet-18 of best performances from QSGD and SVD, (c) CIFAR-10 on VGG-11-BN best of performances from QSGD and SVD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Communication cost versus second moment of singular value sparsification and vectorized matrix sparsification of a n × m matrix.</figDesc><table>Communication 
Cost 

Second 
Moment 

Entry-wise 
s 
1 
s 
X 2 

1,1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 :</head><label>2</label><figDesc>The datasets used and their associated learning models and hyperparameters.</figDesc><table>Experiments CIFAR-10 &amp; ResNet-18 SVHN &amp; ResNet-18 CIFAR-10 &amp; VGG-11-BN 

SVD rank 1 
0.0625 
0.1 
0.125 

SVD rank 2 
0.0625 
0.125 
0.125 

SVD rank 3 
0.125 
0.125 
0.0625 

SVD rank 4 
0.0625 
0.125 
0.15 

QSGD 1bit 
0.0078125 
0.0078125 
0.0009766 

QSGD 2bit 
0.0078125 
0.0078125 
0.0009766 

QSGD 4bit 
0.125 
0.046875 
0.015625 

QSGD 8bit 
0.125 
0.125 
0.0625 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Tuned step sizes for experiments</figDesc><table>2 

4 
8 
16 
Number of Workers 

0 

5 

10 

15 

20 

Time per iteration (sec) 

SVD s=1 
SVD s=2 
SVD s=3 
SVD s=4 

QSGD b=1 
QSGD b=2 
QSGD b=4 
QSGD b=8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 :</head><label>4</label><figDesc>Speedups of spectral-Atomo with sparsity budget s and b-bit QSGD using ResNet-18 on CIFAR10</figDesc><table>over vanilla SGD. 

SVD 
s=1 
SVD 
s=2 
SVD 
s=3 
SVD 
s=4 
QSGD 
b=4 
QSGD 
b=8 
Method 

65% 

68% 

71% 

74% 
Test accuracy 

3.66x 
3.02x 
2.78x 
2.71x 
1.25x 
1.23x 

2.71x 
2.48x 
2.53x 
2.38x 
1.13x 
0.99x 

3.16x 
2.96x 
2.55x 
2.5x 
1.57x 
1.29x 

2.89x 
2.82x 
3.96x 
1.96x 
1.68x 
1.43x 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Speedups of spectral-Atomo with sparsity budget s and b-bit QSGD using VGG11-BN on CIFAR-10 over vanilla SGD.</figDesc><table>SVD 
s=1 
SVD 
s=2 
SVD 
s=3 
SVD 
s=4 
QSGD 
b=1 
QSGD 
b=2 
QSGD 
b=4 
QSGD 
b=8 
Method 

80% 

86% 

90% 

92% 
Test accuracy 

1.45x 2.61x 2.63x 1.94x 2.09x 2.89x 2.54x 2.56x 

1.45x 2.61x 1.75x 
1.3x 
2.09x 1.92x 1.69x 1.43x 

1.45x 1.98x 1.98x 1.17x 1.88x 2.16x 1.91x 1.62x 

2.17x 3.96x 3.96x 1.96x 1.91x 2.16x 3.81x 3.24x 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc>Speedups of SVD and QSGD using ResNet-18 on SVHN over vanilla SGD.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by AWS Cloud Credits for Research from Amazon.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>where e i ∈ R n is the i-th standard basis vector, while e j ∈ R m is the j-th standard basis vector. We then have X * ≤ i,j |X i,j | e i e T j * = i,j |X i,j | = X 1,1 .</p><p>In fact, these are the best constants possible. To see this, first consider the matrix X with a 1 in the upper-left entry and 0 elsewhere. Clearly, X * = X 1,1 = 1, so the right-hand inequality is tight. For the left-hand inequality, consider the all-ones matrix X. This has one singular value, √ nm, so X * = √ nm. On the other hand, X 1,1 = nm. Therefore, X 1,1 = √ nm X * in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Analysis of Atomo via the KKT Condtions</head><p>In this section we show how to derive Algorithm 1 using the KKT conditions. Recall that we wish to solve the following optimization problem:</p><p>We first note a few immediate consequences.</p><p>1. If s &gt; n then the problem is infeasible. Note that when s ≥ n, the optimal thing to do is to set all p i = 1, in which case no sparsification takes place.</p><p>2. If λ i = 0, then p i = 0. This follows from the fact that this p i does not change the value of f (p), and the objective could be decreased by allocating more to the p j associated to non-zero λ j . Therefore we can assume that all λ i = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">If |λ</head><p>We therefore assume 0 &lt; s ≤ n and |λ 1 | ≥ |λ 2 | ≥ . . . ≥ |λ n | &gt; 0. As above we define λ := [λ 1 , . . . , λ n ] T . While the formulation of (17) does not allow direct application of the KKT conditions, since we have a strict inequality of 0 &lt; p i , this is fixed with the following lemma.</p><p>Lemma 12. The minimum of (17) is achieved by some p * satisfying</p><p>Proof. Define p by p i = s/n. This vector is clearly feasible in <ref type="bibr" target="#b16">(17)</ref>. Let p be any feasible vector. If f (p) ≤ f (q) then for any i ∈ [n] we have</p><p>Therefore, p i ≥ λ 2 i /f (p). A straightforward computations shows that f (p) = n λ 2 2 /s. Note that this implies that we can restrict to the feasbile set</p><p>This defines a compact region C. Since f is continuous on this set, its maximum value is obtained at some p * .</p><p>The KKT conditions then imply that at any point p solving <ref type="formula">(17)</ref>, we must have</p><p>for some µ ∈ R. Since |λ i | &gt; 0 for all i, we actually must have µ &gt; 0. We therefore have two conditions for all i.</p><p>Note that in either case, to have p 1 feasible we must have µ ≥ λ 2 1 . Combining this with the fact that we can always select p 1 ≥ p 2 ≥ . . . ≥ p n , we obtain the following partial characterization of the solution to <ref type="bibr" target="#b16">(17)</ref>. For some n s ∈ [n], we have p 1 , . . . , p ns = 1 while p i = |λ i |/ √ µ ∈ (0, 1) for i = n s + 1, . . . , n. Combining this with the constraint that</p><p>Rearranging, we obtain</p><p>which then implies that</p><p>Thus, we need to select n s such that the p i in (21) are bounded above by 1. Let n * s denote the first element of [n] for which this holds. Then the condition that p i ≤ 1 for i = n * s + 1, . . . , n is exactly the condition that [λ n * s +1 , . . . , λ n ] is (s − n s )-balanced (see Definition 2. In particular, Lemma 2 implies that, fixing p i = 1 for i = 1, . . . , n * s , the optimal way to assign the remaining p i is by</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Sparse communication for distributed gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alham</forename><surname>Fikri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05021</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Transient weight misadjustment properties for the finite precision LMS algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alexander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1250" to="1258" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Qsgd: Communication-efficient SGD via gradient quantization and encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Alistarh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demjan</forename><surname>Grubic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Vojnovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1707" to="1718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A nonlinear analytical model for the quantized LMS algorithm-the arbitrary step size case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bermudez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">J</forename><surname>Bershad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1175" to="1183" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">signSGD: compressed optimisation for non-convex problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamyar</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04434</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<title level="m">Convex optimization: Algorithms and complexity. Foundations and Trends R in Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="231" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adacomp: Adaptive residual gradient compression for data-parallel distributed training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailash</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02679</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00981</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Revisiting distributed synchronous SGD. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Better mini-batch algorithms via accelerated gradient methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nati</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1647" to="1655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parallel distributed computing using python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">R</forename><surname>Lisandro D Dalcin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">A</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Kler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cosimo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Water Resources</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1124" to="1139" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhay</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05792</idno>
		<title level="m">Big Batch SGD: Automated inference using adaptive batch sizes</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding and optimizing asynchronous low-precision stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Christopher De Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunle</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture</title>
		<meeting>the 44th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="561" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">High-accuracy low-precision training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megan</forename><surname>Christopher De Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Leszczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alana</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marzoev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunle</forename><surname>Christopher R Aberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olukotun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03383</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Global convergence of stochastic gradient descent for some non-convex matrix problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Christopher De Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunle</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2332" to="2341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Taming the wild: A unified analysis of hogwild-style algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Christopher M De Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olukotun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2674" to="2682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1223" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorathan</forename><surname>John C Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Chaturapruek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.00882</idno>
		<title level="m">Asynchronous stochastic convex optimization</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stochastic first-and zeroth-order methods for nonconvex stochastic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2341" to="2368" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the design of gradient algorithms for digitally implemented adaptive filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>R Gitlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuit Theory</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="136" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Synchronous multi-GPU deep learning with low-precision communication: An experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demjan</forename><surname>Grubic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Alistarh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3866</idno>
		<title level="m">Speeding up convolutional neural networks with low rank expansions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Communication-efficient distributed dual coordinate ascent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Takác</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Terhorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3068" to="3076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Linear convergence of gradient and proximalgradient methods under the Polyak-łojasiewicz condition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Nutini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="795" to="811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Federated learning: Strategies for improving communication efficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Konečnỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananda</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Theertha Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bacon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05492</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Randomized distributed mean estimation: Accuracy vs communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Konečnỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07555</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Leblond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04809</idno>
		<title level="m">ASAGA: asynchronous parallel SAGA</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep gradient compression: Reducing the communication bandwidth for distributed training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01887</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An asynchronous parallel stochastic coordinate descent algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Bittorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sridhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="285" to="322" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Perturbed iterate analysis for asynchronous stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horia</forename><surname>Mania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Papailiopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kannan</forename><surname>Ramchandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1507.06970</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Communicationefficient learning of deep networks from decentralized data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eider</forename><surname>H Brendan Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hampson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.05629</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Paleo: A performance model for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">R</forename><surname>Sparks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hogwild: A lock-free approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stochastic variance reduction for nonconvex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Hefny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="314" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">SparCML: high-performance sparse communication for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cèdric</forename><surname>Renggli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Alistarh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08021</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Low-rank matrix factorization for deep neural network training with high-dimensional output targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ebru</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuvana</forename><surname>Arisoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6655" to="6659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasha</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scalable distributed DNN training using commodity gpu cloud computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikko</forename><surname>Strom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Distributed mean estimation with limited communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ananda Theertha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Brendan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcmahan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00429</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Variance-based gradient compression for efficient distributed deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Tsuzuku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroto</forename><surname>Imachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Akiba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06058</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Gradient sparsification for communication-efficient distributed optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiao</forename><surname>Wangni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09854</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Terngrad: Ternary gradients to reduce communication in distributed deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunpeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1508" to="1518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mean-normalized stochastic gradient for large-scale deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wiesler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schluter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="180" to="184" />
		</imprint>
	</monogr>
	<note>2014 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Restructuring of deep neural network acoustic models with singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2365" to="2369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Gradient diversity: a key ingredient for scalable distributed learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Pananjady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Papailiopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kannan</forename><surname>Ramchandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Zipml: Training linear models with end-to-end low precision, and a little bit of deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaan</forename><surname>Kara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Alistarh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4035" to="4043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06160</idno>
		<title level="m">DoReFa-Net: training low bitwidth convolutional neural networks with low bitwidth gradients</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
