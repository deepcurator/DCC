<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RON: Reverse Connection with Objectness Prior Networks for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Tsinghua National Laboratory for Information Science and Technology (TNList)</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Lab. of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
							<email>fcsun@</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Tsinghua National Laboratory for Information Science and Technology (TNList)</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Lab. of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Intel Labs China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
							<email>hpliu@tsinghua.edu.cn2anbang.yao</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Tsinghua National Laboratory for Information Science and Technology (TNList)</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Lab. of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Lu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
							<email>yurong.chen@intel.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Intel Labs China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RON: Reverse Connection with Objectness Prior Networks for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We present RON, an   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We are witnessing significant advances in object detection area, mainly thanks to the deep networks. Current top deep-networks-based object detection frameworks could be grouped into two main streams: the region-based methods <ref type="bibr" target="#b10">[11]</ref> <ref type="bibr" target="#b22">[23]</ref>[10] <ref type="bibr" target="#b15">[16]</ref> and the region-free methods <ref type="bibr" target="#b21">[22]</ref> <ref type="bibr" target="#b18">[19]</ref>.</p><p>The region-based methods divide the object detection  <ref type="figure" target="#fig_5">Figure 1</ref>. Objectness prior generated from a specific image. In this example, sofa is responded at scales (a) and (b), the brown dog is responded at scale (c) and the white spotted dog is responded at scale (d). The network will generate detection results with the guidance of objectness prior .</p><p>task into two sub-problems: At the first stage, a dedicated region proposal generation network is grafted on deep convolutional neural networks (CNNs) which could generate high quality candidate boxes. Then at the second stage, a region-wise subnetwork is designed to classify and refine these candidate boxes. Using very deep CNNs <ref type="bibr" target="#b13">[14]</ref>[27], the Fast R-CNN pipeline <ref type="bibr" target="#b9">[10]</ref>[23] has recently shown high accuracy on mainstream object detection benchmarks <ref type="bibr" target="#b6">[7]</ref>[24] <ref type="bibr" target="#b17">[18]</ref>. The region proposal stage could reject most of the background samples, thus the searching space for object detection is largely reduced <ref type="bibr" target="#b0">[1]</ref> <ref type="bibr" target="#b31">[32]</ref>. Multistage training process is usually developed for joint optimization of region proposal generation and post detection (e.g., <ref type="bibr" target="#b3">[4]</ref>[23] <ref type="bibr" target="#b15">[16]</ref>). In Fast R-CNN <ref type="bibr" target="#b9">[10]</ref>, the region-wise subnetwork repeatedly evaluates thousands of region proposals to generate detection scores. Under Fast R-CNN pipeline, Faster R-CNN shares full-image convolutional features with the detection network to enable nearly costfree region proposals. Recently, R-FCN <ref type="bibr" target="#b3">[4]</ref> tries to make the unshared per-RoI computation of Faster R-CNN to be sharable by adding position-sensitive score maps. Nevertheless, R-FCN still needs region proposals generated from region proposal networks <ref type="bibr" target="#b22">[23]</ref>. To ensure detection accuracy, all methods resize the image to a large enough size (usu-ally with the shortest side of 600 pixels). It is somewhat resource/time consuming when feeding the image into deep networks, both in training and inference time. For example, predicting with Faster R-CNN usually takes 0.2 ms per image using about 5GB GPU memory for VGG-16 networks <ref type="bibr" target="#b26">[27]</ref>. Another solution family is the region-free methods <ref type="bibr" target="#b21">[22]</ref> <ref type="bibr" target="#b18">[19]</ref>. These methods treat object detection as a single shot problem, straight from image pixels to bounding box coordinates by fully convolutional networks (FCNs). The main advantage of these detectors is high efficiency. Originated from YOLO <ref type="bibr" target="#b21">[22]</ref>, SSD <ref type="bibr" target="#b18">[19]</ref> tries to deal object detection problem with multiple layers of deep CNNs. With low resolution input, the SSD detector could get state-of-the-art detection results. However, the detection accuracy of these methods still has room for improvement: (a) Without region proposals, the detectors have to suppress all of the negative candidate boxes only at the detection module. It will increase the difficulties on training the detection module. (b) YOLO detects objects with the top-most CNN layer, without deeply exploring the detection capacities of different layers. SSD tries to improve the detection performance by adding former layers' results. However, SSD still struggles with small instances, mainly because of the limited information of middle layers. These two main bottlenecks affect the detection accuracy of the methods.</p><p>Driven by the success of the two solution families, a critical question arises: is it possible to develop an elegant framework which can smartly associate the best of both methodologies and eliminate their major demerits? We answer this question by trying to bridge the gap between the region-based and region-free methodologies. To achieve this goal, we focus on two fundamental problems: (a) Multi-scale object localization. Objects of various scales could appear at any position of an image, so tens of thousands of regions with different positions/scales/aspect ratios should be considered. Prior works <ref type="bibr" target="#b15">[16]</ref> <ref type="bibr" target="#b2">[3]</ref> show that multi-scale representation will significantly improve object detection of various scales. However, these methods always detect various scales of objects at one layer of a network <ref type="bibr" target="#b15">[16]</ref>[23] <ref type="bibr" target="#b3">[4]</ref>. With the proposed reverse connection, objects are detected on their corresponding network scales, which is more elegant and easier to optimize. (b) Negative space mining. The ratio between object and non-object samples is seriously imbalanced. So an object detector should have effective negative mining strategies <ref type="bibr" target="#b25">[26]</ref>. In order to reduce the searching space of the objects, we create an objectness prior ( <ref type="figure" target="#fig_5">Figure 1</ref>) on convolutional feature maps and jointly optimize it with the detector at training phase.</p><p>As a result, we propose RON (Reverse connection with Objectness prior Networks) object detection framework, which could associate the merits of region-based and region-free approaches. Furthermore, recent complementary advances such as hard example mining <ref type="bibr" target="#b25">[26]</ref>, bounding box regression <ref type="bibr" target="#b10">[11]</ref> and multi-layer representation <ref type="bibr" target="#b15">[16]</ref> <ref type="bibr" target="#b2">[3]</ref> could be naturally employed.</p><p>Contributions. We make the following contributions:</p><p>1. We propose RON, a fully convolutional framework for end-to-end object detection. Firstly, the reverse connection assists the former layers of CNNs with more semantic information. Second, the objectness prior gives an explicit guide to the searching of objects. Finally, the multi-task loss function enables us to optimize the whole network end-to-end on detection performance.</p><p>2. In order to achieve high detection accuracy, effective training strategies like negative example mining and data augmentation have been employed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Object detection is a fundamental and heavilyresearched task in computer vision area. It aims to localize and recognize every object instance with a bounding box <ref type="bibr" target="#b6">[7]</ref> <ref type="bibr" target="#b17">[18]</ref>. Before the success of deep CNNs <ref type="bibr" target="#b16">[17]</ref>, the widely used detection systems are based on the combination of independent components (HOG <ref type="bibr" target="#b29">[30]</ref>, SIFT <ref type="bibr" target="#b20">[21]</ref> et al.). The DPM <ref type="bibr" target="#b7">[8]</ref> and its variants <ref type="bibr" target="#b1">[2]</ref> <ref type="bibr" target="#b4">[5]</ref> have been the dominant methods for years. These methods use object component descriptors as features and sweep through the entire image to find regions with a class-specific maximum response. With the great success of the deep learning on large scale object recognition, several works based on CNNs have been proposed <ref type="bibr" target="#b30">[31]</ref> <ref type="bibr" target="#b24">[25]</ref>. R-CNN <ref type="bibr" target="#b10">[11]</ref> and its variants usually combine region proposals (generated from Selective Search <ref type="bibr" target="#b28">[29]</ref>, Edgeboxes <ref type="bibr" target="#b31">[32]</ref>, MCG <ref type="bibr" target="#b0">[1]</ref>, et al.) and ConvNet based post-classification. These methods have brought dramatic improvement on detection accuracy <ref type="bibr" target="#b16">[17]</ref>. After the original R-CNN, researches are improving it in a variety of ways. The SPP-Net <ref type="bibr" target="#b12">[13]</ref> and Fast R-CNN <ref type="bibr" target="#b9">[10]</ref> speed up the R-CNN approach with RoI-Pooling (Spatial-PyramidPooling) that allows the classification layers to reuse features computed over CNN feature maps. Under Fast R-CNN pipeline, several works try to improve the detection speed and accuracy, with more effective region proposals [23], multi-layer fusion <ref type="bibr" target="#b2">[3]</ref>[16], context information <ref type="bibr" target="#b13">[14]</ref>[9] and more effective training strategy <ref type="bibr" target="#b25">[26]</ref>. R-FCN <ref type="bibr" target="#b3">[4]</ref> tries to reduce the computation time with position-sensitive score maps under ResNet architecture <ref type="bibr" target="#b13">[14]</ref>. Thanks to the FCNs <ref type="bibr" target="#b19">[20]</ref> which could give position-wise information of input images, several works are trying to solve the object detection problem by FCNs. The methods skip the region proposal generation step and predict bounding boxes and detection confidences of multiple categories directly <ref type="bibr" target="#b5">[6]</ref>. YOLO <ref type="bibr" target="#b21">[22]</ref> uses the top most CNN feature maps to predict both confidences and locations for multiple categories. Originated from YOLO, SSD <ref type="bibr" target="#b18">[19]</ref> tries to predict detection results at multiple CNN layers. With carefully designed training strategies, SSD could get competitive detection results. The main advantage of these methods is high time-efficiency. For example, the feed-forward speed of YOLO is 45 FPS, 9× faster than Faster R-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Network Architecture</head><p>This section describes RON object detection framework ( <ref type="figure" target="#fig_1">Figure 2</ref>). We first introduce the reverse connection on traditional CNNs in Section 3.1, such that different network scales have effective detection capabilities. Then in Section 3.2, we explain how to generate candidate boxes on different network scales. Next, we present the objectness prior to guide the search of objects in Section 3.3 and Secction 3.4. Finally, we combine the objectness prior and object detection into a unified network for joint training and testing (Section 3.5).</p><p>Network preparation We use VGG-16 as the test case reference model, which is pre-trained with ImageNet dataset <ref type="bibr" target="#b26">[27]</ref>. Recall that VGG-16 has 13 convolutional lay- ers and 3 fully-connected layers. We convert FC6 (14th layer) and FC7 (15th layer) to convolutional layers <ref type="bibr" target="#b19">[20]</ref>, and use 2×2 convolutional kernels with stride 2 to reduce the resolution of FC7 by half <ref type="bibr" target="#b0">1</ref> . By now, the feature map sizes used for object detection are 1/8 (conv 4 3), 1/16 (conv 5 3), 1/32 (conv 6) and 1/64 (conv 7) of the input size, both in width and height (see <ref type="figure" target="#fig_1">Figure 2</ref> top).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Reverse Connection</head><p>Combining fine-grained details with highly-abstracted information helps object detection with different scales <ref type="bibr" target="#b11">[12]</ref>[20] <ref type="bibr" target="#b13">[14]</ref>. The region-based networks usually fuse multiple CNN layers into a single feature map <ref type="bibr" target="#b2">[3]</ref> <ref type="bibr" target="#b15">[16]</ref>. Then object detection is performed on the fused maps with regionwise subnetworks <ref type="bibr" target="#b9">[10]</ref>. As all objects need to be detected based on the fixed features, the optimization becomes much complex. SSD <ref type="bibr" target="#b18">[19]</ref> detects objects on multiple CNN layers. However, the semantic information of former layers is limited, which affects the detection performance of these layers. This is the reason why SSD has much worse performance on smaller objects than bigger objects <ref type="bibr" target="#b18">[19]</ref>.</p><p>Inspired from the success of residual connection <ref type="bibr" target="#b13">[14]</ref> which eases the training of much deeper networks, we propose the reverse connection on traditional CNN architectures. The reverse connection enables former features to have more semantic information. One reverse connection block is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Firstly, a deconvolutional layer is applied to the reverse fusion map (annotated as rf-map) n + 1, and a convolutional layer is grafted on backbone layer n to guarantee the inputs have the same dimension. Then the two corresponding maps are merged by element-wise addition. The reverse fusion map 7 is the convolutional output (with 512 channels by 3×3 kernels) of the backbone layer 7. After this layer has been generated, each reverse connection block will be generated in the same way, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. In total, there are four reverse fusion maps with different scales.</p><p>Compared with methods using single layer for object detection <ref type="bibr" target="#b15">[16]</ref>[23], multi-scale representation is more effective on locating all scales of objects (as shown in experiments). More importantly, as the reverse connection is learnable, the semantic information of former layers can be significantly enriched. This characteristic makes RON more effective in detecting all scales of objects compared with <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Reference Boxes</head><p>In this section, we describe how to generate bounding boxes on feature maps produced from Section 3.1. Feature maps from different levels within a network are known to have different receptive field sizes <ref type="bibr" target="#b12">[13]</ref>. As the reverse connection can generate multiple feature maps with different scales. We can design the distribution of boxes so that specific feature map locations can be learned to be responsive to particular scales of objects. Denoting the minimum scale with s min , the scales S k of boxes at each feature map k are</p><formula xml:id="formula_0">S k = {(2k − 1) · s min , 2k · s min }, k ∈ {1, 2, 3, 4}. (1)</formula><p>We also impose different aspect ratios { 1 3 , 1 2 , 1, 2, 3} for the default boxes. The width and height of each box are computed with respect to the aspect ratio <ref type="bibr" target="#b22">[23]</ref>. In total, there are 2 scales and 5 aspect ratios at each feature map location. The s min is 1 10 of the input size (e.g., 32 pixels for 320×320 model). By combining predictions for all default boxes with different scales and aspect ratios, we have a diverse set of predictions, covering various object sizes and shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Objectness Prior</head><p>As shown in Section 3.2, we consider default boxes with different scales and aspect ratios from many feature maps. However, only a tiny fraction of boxes covers objects. In other words, the ratio between object and non-object samples is seriously imbalanced. The region-based methods overcome this problem by region proposal networks <ref type="bibr" target="#b15">[16]</ref> <ref type="bibr" target="#b22">[23]</ref>. However, the region proposals will bring translation variance compared with the default boxes. So the Fast R-CNN pipeline usually uses region-wise networks for post detection, which brings repeated computation <ref type="bibr" target="#b3">[4]</ref>. In contrast, we add an objectness prior for guiding the search of objects, without generating new region proposals. Concretely, we add a 3×3×2 convolutional layer followed by a Softmax function to indicate the existence of an object in each box. The channel number of objectness prior maps is 10, as there are 10 default boxes at each location. <ref type="figure" target="#fig_5">Figure 1</ref> shows the multi-scale objectness prior generated from a specific image. For visualization, the objectness prior maps are averaged along the channel dimension. We see that the objectness prior maps could explicitly reflect the existence of an object. Thus the searching space of the objects could be dramatically reduced. Objects of various scales will respond at their corresponding feature maps, and we enable this by appropriate matching and end-to-end training. More results could be seen in the experiment section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Detection and Bounding Box Regression</head><p>Different from objectness prior, the detection module needs to classify regions into K+1 categories (K= 20 for PASCAL VOC dataset and K= 80 for MS COCO dataset, plus 1 for background). We employ the inception module <ref type="bibr" target="#b27">[28]</ref> on the feature maps generated in Section 3.1 to perform detection. Concretely, we add two inception blocks (one block is shown in <ref type="figure" target="#fig_3">Figure 4</ref>) on feature maps, and classify the final inception outputs. There are many inception choices as shown in <ref type="bibr" target="#b27">[28]</ref>. In this paper, we just use the most simple structure.</p><p>With Softmax, the sub-network outputs the per-class score that indicates the presence of a class-specific instance. For bounding box regression, we predict the offsets relative to the default boxes in the cell (see <ref type="figure" target="#fig_4">Figure 5</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Combining Objectness Prior with Detection</head><p>In this section, we explain how RON combines objectness prior with object detection. We assist object detection with objectness prior both in training and testing phases. For training the network, we firstly assign a binary class label to each candidate region generated from Section 3.2. Then if the region covers object, we also assign a classspecific label to it. For each ground truth box, we (i) match it with the candidate region with most jaccard overlap; (ii) match candidate regions to any ground truth with jaccard overlap higher than 0.5. This matching strategy guarantees that each ground truth box has at least one region box assigned to it. We assign negative labels to the boxes with jaccard overlap lower than 0.3.</p><p>By now, each box has its objectness label and classspecific label. The network will update the class-specific labels dynamically for assisting object detection with objectness prior at training phase. For each mini-batch at feed-forward time, the network runs both the objectness prior and class-specific detection. But at the back-propagation phase, the network firstly generates the objectness prior, then for detection, samples whose objectness scores are high than threshold o p are selected ( <ref type="figure">Figure 6</ref>). The extra computation only comes from the selection of samples for backpropagation. With suitable o p (we use o p = 0.03 for all models), only a small number of samples is selected for updating the detection branch, thus the complexity of backward pass has been significantly reduced. <ref type="figure">Figure 6</ref>. Mapping the objectness prior with object detection. We first binarize the objectness prior maps according to op, then project the binary masks to the detection domain of the last convolutional feature maps. The locations within the masks are collected for detecting objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training and Testing</head><p>In this section, we firstly introduce the multi-task loss function for optimizing the networks. Then we explain how to optimize the network jointly and perform inference directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Loss Function</head><p>For each location, our network has three sibling output branches. The first outputs the objectness confidence score p obj = {p obj 0 , p obj 1 }, computed by a Softmax over the 2×A outputs of objectness prior (A = 10 in this paper, as there are 10 types of default boxes). We denote the objectness loss with L obj . The second branch outputs the boundingbox regression loss, denoted by L loc . It targets at minimizing the smoothed L 1 loss <ref type="bibr" target="#b9">[10]</ref> between the predicted location offsets t = (t x , t y , t w , t h ) and the target offsets t * = (t * x , t * y , t * w , t * h ). Different from Fast R-CNN <ref type="bibr" target="#b9">[10]</ref> that regresses the offsets for each of K classes, we just regress the location one time with no class-specific information. The third branch outputs the classification loss L cls|obj for each box, over K+1 categories. Given the objectness confidence score p obj , the branch first excludes regions whose scores are lower than the threshold o p . Then like }. We use a multi-task loss L to jointly train the networks endto-end for objectness prior, classification and bounding-box regression:</p><formula xml:id="formula_1">L obj ,</formula><formula xml:id="formula_2">L = α 1 N obj L obj +β 1 N loc L loc +(1−α−β) 1 N cls|obj L cls|obj .</formula><p>(2) The hyper-parameters α and β in Equation 2 control the balance between the three losses. We normalize each loss term with its input number. Under this normalization, α = β = </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Joint Training and Testing</head><p>We optimize the models described above jointly with end-to-end training. After the matching step, most of the default boxes are negatives, especially when the number of default boxes is large. Here, we introduce a dynamic training strategy to scan the negative space. At training phase, each SGD mini-batch is constructed from N images chosen uniformly from the dataset. At each mini-batch, (a) for objectness prior, all of the positive samples are selected for training. Negative samples are randomly selected from regions with negative labels such that the ratio between positive and negative samples is 1:3; (b) for detection, we first reduce the sample number according to objectness prior scores generated at this mini-batch (as discribed in Section 3.5). Then all of the positive samples are selected. We randomly select negative samples such that the ratio between positive and negative samples is 1:3. We note that recent works such as Faster R-CNN <ref type="bibr" target="#b22">[23]</ref> and R-FCN <ref type="bibr" target="#b3">[4]</ref> usually use multi-stage training for joint optimization. In contrast, the loss function 2 is trained end-to-end in our implementation by back-propagation and SGD, which is more efficient at training phase. At the beginning of training, the objectness prior maps are in chaos. However, along with the training progress, the objectness prior maps are more concentrated on areas covering objects.</p><p>Data augmentation To make the model more robust to various object scales, each training image is randomly sampled by one of the following options: (i) Using the original/flipped input image; (ii) Randomly sampling a patch whose edge length is { 10 } of the original image and making sure that at least one object's center is within this patch. We note that the data augmentation strategy described above will increase the number of large objects, but the optimization benefit for small objects is limited. We overcome this problem by adding a small scale for training. A large object at one scale will be smaller at smaller scales. This training strategy could effectively avoid over-fitting of objects with specific sizes.</p><p>Inference At inference phase, we multiply the classconditional probability and the individual box confidence  predictions. The class-specific confidence score for each box is defined as Equation 3:</p><formula xml:id="formula_3">p cls = p obj · p cls|obj .<label>(3)</label></formula><p>The scores encode both the probability of the class appearing in the box and how well the predicted box fits the object. After the final score of each box is generated, we adjust the boxes according to the bounding box regression outputs. Finally, non-maximum suppression is applied to get the final detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We train and evaluate our models on three major datasets: PASCAL VOC 2007, PASCAL VOC 2012, and MS CO-CO. For fair comparison, all experiments are based on the VGG-16 networks. We train all our models on a single Nvidia TitanX GPU, and demonstrate state-of-the-art results on all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">PASCAL VOC 2007</head><p>On this dataset, we compare RON against seminal Fast R-CNN <ref type="bibr" target="#b9">[10]</ref>, Faster R-CNN <ref type="bibr" target="#b22">[23]</ref>, and the most recently proposed SSD <ref type="bibr" target="#b18">[19]</ref>. All methods are trained on VOC2007 trainval and VOC2012 trainval, and tested on VOC2007 test dataset. During training phase, we initialize the parameters for all the newly added layers by drawing weights from a zero-mean Gaussian distribution with standard deviation 0.01. All other layers are initialized by standard VGG-16 model <ref type="bibr" target="#b9">[10]</ref>. We use the 10 −3 learning rate for the first 90k iterations, then we decay it to 10 −4 and continue training for next 30k iterations. The batch size is 18 for 320×320 model according to the GPU capacity. We use a momentum of 0.9 and a weight decay of 0.0005. <ref type="table">Table 1</ref> shows the result comparisons of the methods 2 . With 320×320 input size, RON is already better than Faster R-CNN. By increasing the input size to 384×384, RON gets 75.4% mAP, outperforming Faster R-CNN by a margin of 2.2%. RON384 is also better than SSD with input size 500×500. Finally, RON could achieve high mAPs of 76.6% (RON320++) and 77.6% (RON384++) with multiscale testing, bounding box voting and flipping <ref type="bibr" target="#b2">[3]</ref>.</p><p>Small objects are challenging for detectors. As shown in <ref type="table">Table 1</ref>, all methods have inferior performance on 'boat' and 'bottle'. However, RON improves performance of these categories by significant margins: 4.0 points improvement for 'boat' and 7.1 points improvement for 'bottle'. In summary, performance of 17 out of 20 categories has been improved by RON.</p><p>To understand the performance of RON in more detail, we use the detection analysis tool from <ref type="bibr" target="#b14">[15]</ref>. <ref type="figure" target="#fig_7">Figure 7</ref> shows that our model can detect various object categories with high quality. The recall is higher than 85%, and is much higher with the 'weak' (0.1 jaccard overlap) criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">PASCAL VOC 2012</head><p>We compare RON against top methods on the comp4 (outside data) track from the public leaderboard on PAS-CAL VOC 2012. The training data is the union set of all VOC 2007, VOC 2012 train and validation datasets, following <ref type="bibr" target="#b22">[23]</ref>[10] <ref type="bibr" target="#b18">[19]</ref>. We see the same performance trend as we observed on VOC 2007 test. The results, as shown in <ref type="table">Table 2</ref>, demonstrate that our model performs the best on this dataset. Compared with Faster R-CNN and other variants <ref type="bibr" target="#b25">[26]</ref> <ref type="bibr" target="#b15">[16]</ref>, the proposed network is significantly better, mainly due to the reverse connection and the use of boxes from multiple feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">MS COCO</head><p>To further validate the proposed framework on a larger and more challenging dataset, we conduct experiments on MS COCO <ref type="bibr" target="#b17">[18]</ref> and report results from test-dev2015 evaluation server. The evaluation metric of MS COCO dataset is different from PASCAL VOC. The average mAP over different IoU thresholds, from 0.5 to 0.95 (written as 0.5:0.95) is the overall performance of methods. This places a significantly larger emphasis on localization compared to the PASCAL VOC metric which only requires IoU of 0.5. We use the 80k training images and 40k validation images <ref type="bibr" target="#b22">[23]</ref> to train our model, and validate the performance on the testdev2015 dataset which contains 20k images. We use the 5×10 −4 learning rate for 400k iterations, then we decay it to 5×10 −5 and continue training for another 150k iterations. As instances in MS COCO dataset are smaller and denser than those in PASCAL VOC dataset, the minimum scale s min of the referenced box size is 24 for 320×320 model, and 32 for 384×384 model. Other settings are the same as PASCAL VOC dataset.</p><p>With the standard COCO evaluation metric, Faster R-CNN scores 21.9% AP, and RON improves it to 27.4% AP. Using the VOC overlap metric of IoU ≥0.5, RON384++ gives a 5.8 points boost compared with SSD500. It is also interesting to note that with 320×320 input size, RON gets  <ref type="bibr" target="#b25">26</ref>.2% AP, improving the SSD with 500×500 input size by 1.8 points on the strict COCO AP evaluation metric.</p><p>We also compare our method against Fast R-CNN with online hard example mining (OHEM) <ref type="bibr" target="#b25">[26]</ref>, which gives a considerable improvement on Fast R-CNN. The OHEM method also adopts recent bells and whistles to further improve the detection performance. The best result of OHEM is 25.5% AP (OHEM++). RON gets 27.4% AP, which demonstrates that the proposed network is more competitive on large dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">From MS COCO to PASCAL VOC</head><p>Large-scale dataset is important for improving deep neural networks. In this experiment, we investigate how the MS COCO dataset can help with the detection performance of PASCAL VOC. As the categories on MS COCO are a superset of these on PASCAL VOC dataset, the fine-tuning process becomes easier compared with the ImageNet pretrained model. Starting from MS COCO pre-trained model, RON leads to 81.3% mAP on PASCAL VOC 2007 and 80.7% mAP on PASCAL VOC 2012.</p><p>The extra data from the MS COCO dataset increases the mAP by 3.7% and 5.3%. <ref type="table">Table 4</ref> shows that the model trained on COCO+VOC has the best mAP on PASCAL VOC 2007 and PASCAL VOC 2012. When submitting, our model with 384×384 input size has been ranked as the top 1 on the VOC 2012 leaderboard among VGG-16 based models. We note that other public methods with better results are all based on much deeper networks <ref type="bibr" target="#b13">[14]</ref>.  <ref type="table">Table 4</ref>. The performance on PASCAL VOC datasets. All models are pre-trained on MS COCO, and fine-tuned on PASCAL VOC.</p><p>6. Ablation Analysis 6.1. Do Multiple Layers Help?</p><p>As described in Section 3, our networks generate detection boxes from multiple layers and combine the results. In this experiment, we compare how layer combinations affect the final performance. For all of the following experiments as shown in <ref type="table">Table 5</ref>, we use exactly the same settings and input size (320×320), except for the layers for object detection. From <ref type="table">Table 5</ref>, we see that it is necessary to use all of the layer 4, 5, 6 and 7 such that the detector could get the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Objectness Prior</head><p>As introduced in Section 3.3, the network generates objectness prior for post detection. The objectness prior maps involve not only the strength of the responses, but also their spatial positions. As shown in <ref type="figure">Figure 8</ref>, objects with various scales will respond at the corresponding maps. The maps can guide the search of different scales of objects, thus significantly reducing the searching space. input map4 map5 map6 map7 <ref type="figure">Figure 8</ref>. Objectness prior maps generated from images.</p><p>We also design an experiment to verify the effect of objectness prior. In this experiment, we remove the objectness prior module and predict the detection results only from the detection module. Other settings are exactly the same as the baseline. Removing objectness prior maps leads to 69.6% mAP on VOC 2007 test dataset, resulting 4.6 points drop from the 74.2% mAP baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Generating Region Proposals</head><p>After removing the detection module, our network could get region proposals. We compare the proposal performance against Faster R-CNN <ref type="bibr" target="#b22">[23]</ref> and evaluate recalls with different numbers of proposals on PASCAL VOC 2007 test set, as shown in <ref type="figure" target="#fig_8">Figure 9</ref>. Both Faster R-CNN and RON achieve promising region proposals when the region number is larger than 100. However, with fewer region proposals, the recall of RON boosts Faster R-CNN by a large margin. Specifically, with top 10 region proposals, our 320 model gets 80.7% recall, outperforming Faster R-CNN by 20 points. This validates that our model is more effective in applications with less region proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have presented RON, an efficient and effective object detection framework. We design the reverse connection to enable the network to detect objects on multi-levels of CNNs. And the objectness prior is also proposed to guide the search of objects. We optimize the whole networks by a multi-task loss function, thus the networks can directly predict final detection results. On standard benchmarks, RON achieves state-of-the-art object detection performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. RON object detection overview. Given an input image, the network firstly computes features of the backbone network. Then at each detection scale: (a) adds reverse connection; (b) generates objectness prior; (c) detects object on its corresponding CN-N scales and locations. Finally, all detection results are fused and selected with non-maximum suppression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. A reverse connection block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. One inception block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Object detection and bounding box regression modules. Top: bounding box regression; Bottom: object classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 3</head><label>1</label><figDesc>works well and is used in all experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Visualization of performance for RON384 on animals, vehicles, and furniture from VOC2007 test. The Figures show the cumulative fraction of detections that are correct (Cor) or false positive due to poor localization (Loc), confusion with similar categories (Sim), with others (Oth), or with background (BG). The solid red line reflects the change of recall with the 'strong' criteria (0.5 jaccard overlap) as the number of detections increases. The dashed red line uses the 'weak' criteria (0.1 jaccard overlap).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Recall versus number of proposals on the PASCAL VOC 2007 test set (with IoU = 0.5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>L cls|obj is computed by a Softmax over K+1 outputs</figDesc><table>for each location p 
cls|obj = {p 

cls|obj 
0 

, p 

cls|obj 
1 

, . . . , p 

cls|obj 
K 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Table 1. Detection results on PASCAL VOC 2007 test set. The entries with the best APs for each object category are bold-faced.</figDesc><table>Method 

mAP aero bike bird boat bottle bus car 
cat 
chair cow table dog horse mbike person plant sheep sofa train tv 
Fast R-CNN[10] 
70.0 
77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 69.9 31.8 70.1 74.8 80.4 70.4 
Faster R-CNN[23] 
73.2 
76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6 
SSD300[19] 
72.1 
75.2 79.8 70.5 62.5 41.3 81.1 80.8 86.4 51.5 74.3 72.3 83.5 84.6 80.6 74.5 46.0 71.4 73.8 83.0 69.1 
SSD500[19] 
75.1 
79.8 79.5 74.5 63.4 51.9 84.9 85.6 87.2 56.6 80.1 70.0 85.4 84.9 80.9 78.2 49.0 78.4 72.4 84.6 75.5 
RON320 
74.2 
75.7 79.4 74.8 66.1 53.2 83.7 83.6 85.8 55.8 79.5 69.5 84.5 81.7 83.1 76.1 49.2 73.8 75.2 80.3 72.5 
RON384 
75.4 
78.0 82.4 76.7 67.1 56.9 85.3 84.3 86.1 55.5 80.6 71.4 84.7 84.8 82.4 76.2 47.9 75.3 74.1 83.8 74.5 
RON320++ 
76.6 
79.4 84.3 75.5 69.5 56.9 83.7 84.0 87.4 57.9 81.3 74.1 84.1 85.3 83.5 77.8 49.2 76.7 77.3 86.7 77.2 
RON384++ 
77.6 
86.0 82.5 76.9 69.1 59.2 86.2 85.5 87.2 59.9 81.4 73.3 85.9 86.8 82.2 79.6 52.4 78.2 76.0 86.2 78.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>Table 3. MS COCO test-dev2015 detection results.</figDesc><table>Method 
Train Data 
Average Precision 
0.5 
0.75 0.5:0.95 
Fast R-CNN[10] 
train 
35.9 
-
19.7 
OHEM[26] 
trainval 
42.5 22.2 
22.6 
OHEM++[26] 
trainval 
45.9 26.1 
25.5 
Faster R-CNN[23] 
trainval 
42.7 
-
21.9 
SSD300[19] 
trainval35k 38.0 20.5 
20.8 
SSD500[19] 
trainval35k 43.7 24.7 
24.4 
RON320 
trainval 
44.7 22.7 
23.6 
RON384 
trainval 
46.5 25.0 
25.4 
RON320++ 
trainval 
47.5 25.9 
26.2 
RON384++ 
trainval 
49.5 27.1 
27.4 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>Table 5. Combining features from different layers.</figDesc><table>detection from layer mAP 
4 
5 
6 
7 
65.6 
68.3 
72.5 
74.2 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The last FC layer (16th layer) is not used in this paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We note that the latest SSD uses new training tricks (color distortion, random expansion and online hard example mining), which makes the results much better. We expect these tricks will also improve our results, which is beyond the focus of this paper.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object detection using stronglysupervised deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Insideoutside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scalable object detection using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object detection via a multiregion and semantic segmentation-aware cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IC-CV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hypernet: Towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Selective search for object recognition. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="154" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An hog-lbp human detector with partial occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving object detection with deep convolutional networks via bayesian optimization and structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
