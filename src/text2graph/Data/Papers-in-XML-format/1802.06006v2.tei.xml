<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Voice Cloning with a Few Samples</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sercanö</forename><surname>Arık</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitong</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kainan</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
						</author>
						<title level="a" type="main">Neural Voice Cloning with a Few Samples</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Voice cloning is a highly desired capability for personalized speech interfaces. Neural network based speech synthesis has been shown to generate high quality speech for a large number of speakers. In this paper, we introduce a neural voice cloning system that takes a few audio samples as input. We study two approaches: speaker adaptation and speaker encoding. Speaker adaptation is based on fine-tuning a multi-speaker generative model with a few cloning samples. Speaker encoding is based on training a separate model to directly infer a new speaker embedding from cloning audios, which is used in a multi-speaker generative model. In terms of naturalness of the speech and its similarity to original speaker, both approaches can achieve good performance, even with very few cloning audios.</p><p>1 While speaker adaptation can achieve better naturalness and similarity, cloning time or required memory for the speaker encoding approach is significantly less, making it favorable for low-resource deployment.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative models based on deep learning have been successfully applied to many domains such as image synthesis <ref type="bibr">(van den Oord et al., 2016;</ref><ref type="bibr" target="#b11">Karras et al., 2017)</ref>, audio synthesis <ref type="bibr" target="#b8">Engel et al., 2017;</ref><ref type="bibr" target="#b2">Arik et al., 2017a)</ref>, and language modeling <ref type="bibr" target="#b10">(Jozefowicz et al., 2016;</ref><ref type="bibr" target="#b16">Merity et al., 2017)</ref>. Deep neural networks are capable of modeling complex data distributions and they scale well with large training data. They can be further conditioned on external inputs to control high-level behaviors, such as dictating the content and style of generated samples.</p><p>For speech synthesis, generative models can be conditioned   and speaker identity <ref type="bibr" target="#b3">(Arik et al., 2017b;</ref><ref type="bibr" target="#b18">Ping et al., 2017)</ref>. While text carries linguistic information and controls the content of the generated speech, speaker representation captures speaker characteristics such as pitch range, speech rate and accent. One approach for multi-speaker speech synthesis is to jointly train a generative model and speaker embeddings on triplets of (text, audio, speaker identity) <ref type="bibr" target="#b3">(Arik et al., 2017b;</ref><ref type="bibr" target="#b18">Ping et al., 2017)</ref>. Embeddings for all speakers are randomly initialized and trained with a generative loss. The idea is to encode the speaker-dependent information with low-dimensional embeddings, while sharing the majority of the model parameters for all speakers. One limitation of such a model is that it can only generate speech for speakers observed during training. A more interesting task is to learn the voice of an unseen speaker from a few speech samples, or voice cloning. Voice cloning can be used in many speech-enabled applications to provide personalized user experience.</p><p>In this work, we focus on voice cloning with limited speech samples from an unseen speaker, which can also be considered in the context of few-shot generative modeling of speech. With a large number of samples, a generative model can be trained from scratch for any target speaker. However, few-shot generative modeling is challenging besides being appealing. The generative model needs to learn the speaker characteristics from limited information provided by a few audio samples and generalize to unseen texts. We explore voice cloning methods with the recently proposed end-to-end neural speech synthesis approaches <ref type="bibr" target="#b18">Ping et al., 2017)</ref>, which apply sequence-to-sequence modeling with attention mechanism. In neural speech synthesis, an encoder converts text to hidden representations, and a decoder estimates the time-frequency representation of speech in an autoregressive way. Compared to traditional unit-selection speech synthesis <ref type="bibr" target="#b23">(Sagisaka et al., 1992)</ref> and statistical parametric speech synthesis <ref type="bibr" target="#b33">(Zen et al., 2009)</ref>, neural speech synthesis has a simpler pipeline and produces more natural speech <ref type="bibr" target="#b25">(Shen et al., 2017b</ref>).</p><p>An end-to-end multi-speaker speech synthesis model is typically parameterized by the weights of generative model and a speaker embedding look-up table, where the latter shall carry the speaker characteristics. In this work, we investigate two questions. First, how well can speaker embeddings capture the differences among speakers? Second, how well can speaker embeddings be learned for an unseen speaker with only a few samples? We compare two voice cloning approaches: (i) speaker adaptation and (ii) speaker encoding, in terms of speech naturalness, speaker similarity, cloning/inference time and model footprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Voice Cloning</head><p>We consider a multi-speaker generative model, f (t i,j , s i ; W, e si ), which takes a text t i,j and a speaker identity s i . The trainable parameters in the model is parameterized by W , and e si denotes the trainable speaker embedding corresponding to s i . Both W and e si are optimized by minimizing a loss function L that penalizes the difference between generated and ground truth audios (e.g. a regression loss for spectrogram):</p><formula xml:id="formula_0">min W,e E si∼S, (ti,j ,ai,j )∼Ts i {L (f (t i,j , s i ; W, e si ), a i,j )} (1)</formula><p>where S is a set of speakers, T si is a training set of textaudio pairs for speaker s i , and a i,j is the ground-truth audio for t i,j of speaker s i . The expectation is estimated over textaudio pairs of all training speakers. In practice, E operator for the loss function is approximated by minibatch. We use W and e to denote the trained parameters and embeddings.</p><p>Speaker embeddings have been shown to effectively capture speaker differences for multi-speaker speech synthesis. They are low-dimension continuous representations of speaker characteristics <ref type="bibr" target="#b3">(Arik et al., 2017b;</ref><ref type="bibr" target="#b18">Ping et al., 2017)</ref>. Despite being trained with a purely generative loss, discriminative properties (e.g. gender or accent) can indeed be observed in embedding space <ref type="bibr" target="#b3">(Arik et al., 2017b)</ref>.</p><p>Voice cloning aims to extract the speaker characteristics for an unseen speaker s k (that is not in S) from a set of cloning audios A s k to generate a different audio conditioned on a given text for that speaker. The two performance metrics for the generated audio are (i) how natural it is and (ii) whether it sounds like it is pronounced by the same speaker.</p><p>The two approaches for neural voice cloning are summarized in <ref type="figure">Fig. 1</ref> and explained in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Speaker adaptation</head><p>The idea of speaker adaptation is to fine-tune a trained multispeaker model for an unseen speaker using a few audios and corresponding texts by applying gradient descent. Finetuning can be applied to either the speaker embedding <ref type="bibr">(Taigman et al., 2017)</ref> or the whole model. For embedding-only adaptation, we have the following objective:</p><formula xml:id="formula_1">min es k E (t k,j ,a k,j )∼Ts k L f (t k,j , s k ; W , e s k ), a k,j</formula><p>(2) where T s k is a set of text-audio pairs for the target speaker s k . For whole model adaptation, we have the following objective:</p><formula xml:id="formula_2">min W,es k E (t k,j ,a k,j )∼Ts k {L (f (t k,j , s k ; W, e s k ), a k,j )} (3)</formula><p>Although the entire model provides more degrees of freedom for speaker adaptation, its optimization is challenging especially for a small number of cloning samples. While running the optimization, careful choice of the number of iterations is crucial for avoiding underfitting or overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Speaker encoding</head><p>We propose a speaker encoding method to directly estimate the speaker embedding from audio samples of an unseen speaker. Such a model does not require any fine-tuning during voice cloning. Thus, the same model can be used for all unseen speakers.</p><p>Specifically, the speaker encoding function, g(A s k ; Θ), takes a set of cloning audio samples A s k and estimates e s k . The function is parametrized by Θ. Ideally, the speaker encoder can be jointly trained with multi-speaker generative model from scratch, with a loss function defined for generated audio quality:</p><formula xml:id="formula_3">min W,Θ E si∼S, (ti,j ,ai,j )∼Ts i {L (f (t i,j , s i ; W, g(A si ; Θ)), a i,j )} .</formula><p>(4) Note that the speaker encoder is trained with the speakers for the multi-speaker generative model. During training, a set of cloning audio samples A si are randomly sampled for training speaker s i . During inference, A s k , audio samples from the target speaker s k , is used to compute g(A s k ; Θ). However, we have observed optimization challenges when training in Eq. 4 is started from scratch. The major problem is fitting an average voice to minimize the overall generative loss, commonly referred as mode collapse in generative modeling literature. One idea to address mode collapse is to introduce discriminative loss functions for intermediate embeddings, <ref type="bibr">2</ref> or generated audios 3 In our case, however, such approaches only slightly improve speaker differences.</p><p>Instead, we propose a separate training procedure for speaker encoder. Speaker embeddings e s i are extracted from a trained multi-speaker generative model f (t i,j , s i ; W, e si ). Then, the speaker encoder model g(A s k ; Θ) is trained to predict the embeddings from sampled cloning audios. There can be several objective functions for the corresponding regression problem. We obtain the best results by simply using an L1 loss between the estimated and target embeddings: Eventually, the entire model can be jointly fine-tuned based on the objective function Eq. 4, using pre-trained W and pretrained Θ as the initial point. Fine-tuning enables generative model to learn how to compensate the errors of embedding estimation, and yields less attention problems. However, generative loss still dominates learning and speaker differences in generated audios may be slightly reduced (see Section 3.3 for details). For speaker encoder g(A s k ; Θ), we propose a neural network architecture comprising three parts (show in <ref type="figure" target="#fig_0">Fig. 2</ref>):</p><formula xml:id="formula_4">min Θ E si∼S {|g(A si ; Θ) − e s i )|} ,<label>(5)</label></formula><p>(i) Spectral processing: We compute mel-spectrograms for cloning audio samples and pass them to PreNet, which contains fully-connected (FC) layers with exponential linear unit (ELU) for feature transformation.</p><p>(ii) Temporal processing: We incorporate temporal contexts using several convolutional layers with gated linear unit and residual connections. Then, average pooling is applied to summarize the whole utterance.</p><p>(iii) Cloning sample attention: Considering that different cloning audios contain different amount of speaker information, we use a multi-head self-attention mechanism <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> to compute the weights for different audios and get aggregated embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Discriminative models for evaluation</head><p>Voice cloning performance metrics can be based on human evaluations through crowdsourcing platforms, but they are slow and expensive during model development. We propose to use two evaluation methods using discriminative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">SPEAKER CLASSIFICATION</head><p>Speaker classifier determines which speaker an audio sample belongs to. For voice cloning evaluation, a speaker classifier can be trained on the set of target speakers used for cloning. High-quality voice cloning would result in high speaker classification accuracy. We use a speaker classifier with similar spectral and temporal processing layers shown in <ref type="figure" target="#fig_4">Fig. 7</ref> and an additional embedding layer before the softmax function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">SPEAKER VERIFICATION</head><p>Speaker verification is the task of authenticating the claimed identity of a speaker, based on a test audio and enrolled audios from the speaker. In particular, it performs binary classification to identify whether the test audio and enrolled audios are from the same speaker (e.g., <ref type="bibr" target="#b26">Snyder et al., 2016)</ref>.</p><p>We consider an end-to-end text-independent speaker verification model <ref type="bibr" target="#b26">(Snyder et al., 2016</ref>) (see Appendix C for more details of model architecture). The speaker verification model can be trained on a multi-speaker dataset, then be directly tested whether the cloned audio and the ground truth audio are from the same speaker. Unlike the speaker classification approach, speaker verification model does not require training with the audios from the target speaker for cloning, hence it can be used for unseen speakers with a few samples. As the quantitative performance metric, the equal error-rate (EER) 4 can be used to measure how close the cloned audios are to the ground truth audios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We compare two approaches for voice cloning. For speaker adaptation approach, we train a multi-speaker generative model and adapt it to a target speaker by fine-tuning the embedding or the whole model. For speaker encoding approach, we also train a speaker encoder, and consider it with and without joint fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>In our first set of experiments (Sections 3.3 and 3.4), multispeaker generative model and speaker encoder model are trained using LibriSpeech dataset <ref type="bibr" target="#b17">(Panayotov et al., 2015)</ref>, which contains audios for 2484 speakers sampled at 16 KHz, totalling 820 hours. LibriSpeech is a dataset for automatic speech recognition, and its audio quality is lower compared to speech synthesis datasets.</p><p>5 Voice cloning is performed using VCTK dataset <ref type="bibr" target="#b29">(Veaux et al., 2017)</ref>. VCTK consists of audios for 108 native speakers of English with various accents sampled at 48 KHz. To be consistent with LibriSpeech dataset, VCTK audio samples are downsampled to 16 KHz. For a chosen speaker, a few cloning audios are sampled randomly for each experiment. The sentences presented in Appendix B are used to generate audios for evaluation.</p><p>In our second set of experiments (Section 3.5), we aim to investigate the impact of the training dataset. We use the VCTK dataset -84 speakers are used for training of the multi-speaker generative model, 8 speakers for validation, and 16 speakers for cloning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model specifications</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">MULTI-SPEAKER GENERATIVE MODEL</head><p>Our multi-speaker generative model is based on the convolutional sequence-to-sequence architecture proposed in <ref type="bibr" target="#b18">(Ping et al., 2017)</ref>, with similar hyperparameters and GriffinLim vocoder. To get better performance, we increase the time-resolution by reducing the hop length and window size parameters to 300 and 1200, and add a quadratic loss term to penalize larger amplitude components superlinearly. For speaker adaptation experiments, we reduce the embedding dimensionality to 128, as it yields less overfitting problems. Overall, the baseline multi-speaker generative model has around 25M trainable parameters when trained for the LibriSpeech dataset. For the second set of experiments, hyperparameters of the VCTK model is used from <ref type="bibr" target="#b18">(Ping et al., 2017)</ref> to train a multi-speaker model for the 84 speakers of VTCK, with Griffin-Lim vocoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">SPEAKER ADAPTATION</head><p>For speaker adaptation approach, either the entire multispeaker generative model parameters or only its speaker embeddings are fine-tuned. For both cases, optimization is separately applied for each of the speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">SPEAKER ENCODER MODEL</head><p>We train speaker encoders for different number of cloning audios separately, to obtain the minimum validation loss. Initially, cloning audios are converted to log-mel spectrograms with 80 frequency bands, with a hop length of 400, a window size of 1600. Log-mel spectrograms are fed to spectral processing layers, which are composed of 2-layer prenet of size 128. Then, temporal processing is applied with two 1-dimensional convolutional layers with a filter width of 12. Finally, multi-head attention is applied with 2 heads and a unit size of 128 for keys, queries and values. The final embedding size is 512. To construct validation set, 25 speakers are held out from training set. A batch size of 64 is used while training, with an initial learning rate of 0.0006 with annealing rate of 0.6 applied every 8000 iterations. Mean absolute error for the validation set is shown in <ref type="figure" target="#fig_0">Fig. 12</ref> in Appendix D. More cloning audios leads to more accurate speaker embedding estimation, especially with the attention mechanism (see Appendix D for more details about the learned attention coefficients).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">SPEAKER CLASSIFICATION MODEL</head><p>We train a speaker classifier on VCTK dataset to classify which of the 108 speakers an audio sample belongs to. Speaker classifier has a fully-connected layer of size 256, 6 convolutional layers with 256 filters of width 4, and a final embedding layer of size 32. The model achieves 100% accuracy for validation set of size 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5.">SPEAKER VERIFICATION MODEL</head><p>We train a speaker verification model on the LibriSpeech dataset to measure the quality of cloned audios compared to ground truth audios from unseen speakers. We hold out 50 speakers from Librispeech as a validation set for unseen speakers. The equal-error-rates (EERs) are estimated by randomly pairing up utterances from the same or different speakers (50% for each case) in test set. We perform 40960 trials for each test set. We describe the details of speaker verification model in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Voice cloning performance</head><p>For speaker adaptation approach, we pick the optimal number of iterations using speaker classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6</head><p>For speaker encoding, we consider voice cloning with and without joint fine-tuning of the speaker encoder and multi-speaker generative model.</p><p>7 <ref type="table">Table 1</ref> summarizes the approaches and lists the requirements for training, data, cloning time and footprint size.   adaptation outperforms the other techniques in both metrics. Speaker encoding approaches yield a lower classification accuracy compared to embedding adaptation, but they achieve a similar speaker verification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">EVALUATIONS BY DISCRIMINATIVE MODELS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">HUMAN EVALUATIONS</head><p>Besides evaluations by discriminative models, we also conduct subject tests on Amazon Mechanical Turk framework. For assessment of the naturalness of the generated audios, we use 5-scale mean opinion score (MOS). For assessment of how similar the generated audios are to the ground truth audios from target speakers, we use 4-scale similarity score with the same question and categories in <ref type="bibr" target="#b31">(Wester et al., 2016)</ref>. We conduct each evaluation independently, so the cloned audios of two different models are not directly compared during rating. Multiple votes on the same sample are aggregated by a majority voting rule. <ref type="table">Tables 2 and 3</ref> show the results of human evaluations. In general, higher number of cloning audios improve both metrics. The improvement is more significant for whole model adaptation as expected, due to the more degrees of freedom provided for an unseen speaker. There is a very slight difference in naturalness for speaker encoding approaches with more cloning audios. Most importantly, speaker encoding does not degrade the naturalness of the baseline multi-speaker generative model. Fine-tuning improves the naturalness of speaker encoding as expected, since it allows the generative model to learn how to compensate the errors of the speaker encoder while training. Similarity scores slightly improve with higher sample counts for speaker encoding, and match the scores for speaker embedding adaptation. The gap of similarity with ground truth is also partially attributed to the limited naturalness of the outputs (as they are trained with LibriSpeech dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Speaker embedding space and manipulation</head><p>As shown in <ref type="figure" target="#fig_3">Fig. 6</ref> and Appendix E, speaker encoder models map speakers into a meaningful latent space. Inspired by word embedding manipulation (e.g. to demonstrate the existence of simple algebraic operations as king -queen = male -female), we consider applying algebraic operations to the inferred embeddings to transform their speech characteristics. To transform gender, we get the averaged speaker embeddings for female and male, and add their difference to a particular speaker. For example, Speaker encoding: with fine-tuning 2.59±0.12 2.67±0.12 2.73±0.13 2.77±0.12 2.77±0.11 <ref type="table">Table 3</ref>. Similarity score evaluations with 95% confidence intervals (when training is done with LibriSpeech dataset and cloning is done with the 108 speakers of the VCTK dataset).</p><formula xml:id="formula_5">BritishMale + AveragedFemale −</formula><p>to obtain an American male speaker. Our results 8 demonstrate high quality audios with specific gender and accent characteristics obtained in this way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Impact of training dataset</head><p>To evaluate the impact of the training dataset, we also consider the voice cloning setting when the training is based on a subset of the VCTK containing 84 speakers, where another 8 speakers are used for validation and 16 for testing. The speaker encoder models generalize poorly for unseen speakers due to limited training speakers. <ref type="table">Tables 4 and 5</ref> present the human evaluation results for the speaker adaptation approach. Speaker verification results are shown in <ref type="figure">Fig. 11</ref> in Appendix C. The significant performance difference between embedding-only and whole-model adaptation underlines the importance of the diversity of training speakers while incorporating speaker-discriminative information into embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Few-shot generative models</head><p>Humans can learn most new generative tasks from only a few examples, and it has motivated research on few-shot generative models. Early studies on few-shot generative modeling mostly focus on Bayesian models. In Lake et al. <ref type="bibr">8</ref> https://audiodemos.github.io/ (2013) and <ref type="bibr" target="#b14">Lake et al. (2015)</ref>, hierarchical Bayesian models are used to exploit compositionality and causality for fewshot generation of characters. In <ref type="bibr" target="#b13">Lake et al. (2014)</ref>, a similar idea is modified to acoustic modeling task, with the goal of generating new words in a different language.</p><p>Recently, deep learning approaches are adapted to few-shot generative modeling, particularly for image generation applications. In <ref type="bibr" target="#b20">Reed et al. (2017)</ref>, few-shot distribution estimation is considered using an attention mechanism and metalearning procedure, for conditional image generation. In <ref type="bibr" target="#b4">Azadi et al. (2017)</ref>, few-shot learning is applied to font style transfer, by modeling the glyph style from a few observed letters, and synthesizing the whole alphabet conditioned on the estimated style. The technique is based on multi-content generative adversarial networks, penalizing the unrealistic synthesized letters compared to the ground truth. In <ref type="bibr" target="#b21">Rezende et al. (2016)</ref>, sequential generative modeling is applied for one-shot generalization in image generation, using a spatial attentional mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Speaker embeddings in speech processing</head><p>Speaker embedding is a well-established approach to encode discriminative information in speakers. It has been used in many speech processing tasks such as speaker recognition/verification <ref type="bibr" target="#b15">(Li et al., 2017)</ref>, speaker diarization <ref type="bibr" target="#b22">(Rouvier et al., 2015)</ref>, automatic speech recognition <ref type="bibr" target="#b7">(Doddipatla, 2016)</ref> and speech synthesis <ref type="bibr" target="#b3">(Arik et al., 2017b)</ref>. In some</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>Sample count 1 5 10 20 100 Speaker adaptation: embedding-only 3.01±0.11 -3.13±0.11 -3.13±0.11 Speaker adaptation: whole-model 2.34±0.13 2.99±0.10 3.07±0.09 3.40±0.10 3.38±0.09 <ref type="table">Table 4</ref>. Mean Opinion Score (MOS) evaluations for naturalness with 95% confidence intervals (when training is done with 84 speakers of the VCTK dataset and cloning is done with 16 speakers of the VCTK dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>Sample count 1 5 10 20 100 Speaker adaptation: embedding-only 2.42±0.13 -2.37±0.13 -2.37±0.12 Speaker adaptation: whole-model 2.55±0.11 2.93±0.11 2.95±0.10 3.01±0.10 3.14±0.10 <ref type="table">Table 5</ref>. Similarity score evaluations with 95% confidence intervals (when training is done with 84 speakers of the VCTK dataset and cloning is done with 16 speakers of the VCTK dataset).</p><p>of these, the model explicitly learns to output embeddings with a discriminative task such as speaker classification. In others, embeddings are randomly initialized and implicitly learned from an objective function that is not directly related to speaker discrimination. For example, in <ref type="bibr" target="#b3">Arik et al. (2017b)</ref>, a multi-speaker generative model is trained to generate audio from text, where speaker embeddings are implicitly learned from a generative loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Voice conversion</head><p>The goal of voice conversion is to modify an utterance from source speaker to make it sound like the target speaker, while keeping the linguistic contents unchanged. One common approach is dynamic frequency warping, to align spectra of different speakers. <ref type="bibr" target="#b0">Agiomyrgiannakis &amp; Roupakia (2016)</ref> proposes a dynamic programming algorithm that simultaneously estimates the optimal frequency warping and weighting transform while matching source and target speakers using a matching-minimization algorithm. <ref type="bibr" target="#b32">Wu et al. (2016)</ref> uses a spectral conversion approach integrated with the locally linear embeddings for manifold learning. There are also approaches to model spectral conversion using neural networks <ref type="bibr" target="#b6">(Desai et al., 2010;</ref><ref type="bibr" target="#b5">Chen et al., 2014;</ref><ref type="bibr" target="#b9">Hwang et al., 2015)</ref>. Those models are typically trained with a large amount of audio pairs of target and source speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we study two approaches for neural voice cloning: speaker adaptation and speaker encoding. We demonstrate that both approaches can achieve good cloning quality even with only a few cloning audios.</p><p>For naturalness, we show that both speaker adaptation and speaker encoding can achieve an MOS for naturalness similar to baseline multi-speaker generative model. Thus, the proposed techniques can potentially be improved with better multi-speaker models in the future (such as replacing GriffinLim with WaveNet vocoder as in <ref type="bibr" target="#b24">Shen et al. (2017a)</ref>).</p><p>For similarity, we demonstrate that both approaches benefit from a larger number of cloning audios. The performance gap between whole-model and embedding-only adaptation indicates that some discriminative speaker information still exists in the generative model besides speaker embeddings. The benefit of compact representation via embeddings is fast cloning and small footprint size per user. Especially for the applications with resource constraints, these practical considerations should clearly favor for the use of speaker encoding approach. Methods to fully embed the speaker information into the embeddings would be an important research direction to improve performance of voice cloning.</p><p>We observe the drawbacks of training the multi-speaker generative model using a speech recognition dataset with low-quality audios and limited diversity in representation of universal set of speakers. Improvements in the quality of dataset will result in higher naturalness and similarity of generated samples. Also, increasing the amount and diversity of speakers should enable a more meaningful speaker embedding space, which can improve the similarity obtained by both approaches. We expect our techniques to benefit significantly from a large-scale and high-quality multi-speaker speech dataset.</p><p>We believe that there are many promising horizons for improvement in voice cloning. Advances in meta-learning, i.e. systematic approaches of learning-to-learn while training, shall be promising to improve voice cloning, e.g. by integrating speaker adaptation or encoding into training. Another approach is inferring the model weights in a more flexible way than the speaker embeddings are being used, to minimize the speaker information loss even with the datasets of limited diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detailed speaker encoder architecture</head><p>Multi-head attention  <ref type="figure">Figure 8</ref>. The sentences used to generate test samples for the voice cloning models. The white space characters / and % follow the same definition as in <ref type="bibr" target="#b18">(Ping et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Voice cloning test sentences</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Speaker verification model</head><p>Given a set of (e.g., 1∼5) enrollment audios 9 and a test audio, speaker verification model performs a binary classification and tells whether the enrollment and test audios are from the same speaker. Although using other speaker verification models (e.g., <ref type="bibr" target="#b26">Snyder et al., 2016)</ref> would also suffice, we choose to create our own speaker verification models using convolutional-recurrent architecture <ref type="bibr" target="#b1">(Amodei et al., 2016)</ref>. We note that our equal-error-rate results on test set of unseen speakers are on par with the state-of-the-art speaker verification models. The architecture of our model is illustrated in <ref type="figure" target="#fig_5">Figure 9</ref>. We compute mel-scaled spectrogram of enrollment audios and test audio after resampling the input to a constant sampling frequency. Then, we apply two-dimensional convolutional layers convolving over both time and frequency bands, with batch normalization and ReLU non-linearity after each convolution layer. The output of last convolution layer is feed into a recurrent layer (GRU). We then mean-pool over time (and enrollment audios if there are many), then apply a fully connected layer to obtain the speaker encodings for both enrollment audios and test audio. We use the probabilistic linear discriminant analysis (PLDA) for scoring the similarity between the two encodings <ref type="bibr" target="#b19">(Prince &amp; Elder, 2007;</ref><ref type="bibr" target="#b26">Snyder et al., 2016)</ref>. The PLDA score <ref type="bibr" target="#b26">(Snyder et al., 2016</ref>) is defined as,</p><formula xml:id="formula_6">s(x, y) = w · x y − x Sx − y Sy + b<label>(6)</label></formula><p>where x and y are speaker encodings of enrollment and test audios respectively after fully-connected layer, w and b are scalar parameters, and S is a symmetric matrix. Then, s(x, y) is feed into a sigmoid unit to obtain the probability that they are from the same speaker. The model is trained using cross-entropy loss. <ref type="table">Table 6</ref> lists hyperparameters of speaker verification model for LibriSpeech dataset.</p><p>In addition to speaker verification test results presented in main text ( <ref type="figure">Figure 5</ref>), we also include the result using 1 enrollment audio when the multi-speaker generative model is trained on LibriSpeech. When multi-speaker generative model is trained on VCTK, the results are in <ref type="figure">Figure 11</ref>. It should be noted that, the EER on cloned audios could be potentially better than on ground truth VCTK, because the speaker verification model is trained on LibriSpeech dataset.  Max gradient norm 100 Gradient clipping max. value 5 <ref type="table">Table 6</ref>. Hyperparameters of speaker verification model for LibriSpeech dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implications of attention</head><p>For a trained speaker encoder model, <ref type="figure" target="#fig_1">Fig. 13</ref> exemplifies attention distributions for different audio lengths. The attention mechanism can yield highly non-uniformly distributed coefficients while combining the information in different cloning samples, and especially assigns higher coefficients to longer audios, as intuitively expected due to the potential more information content in them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Speaker embedding space learned by the encoder</head><p>To analyze the speaker embedding space learned by the trained speaker encoders, we apply principal component analysis to the space of inferred embeddings and consider their ground truth labels for gender and region of accent from the VCTK dataset. <ref type="figure">Fig. 14</ref> shows visualization of the first two principal components. We observe that speaker encoder maps the cloning audios to a latent space with highly meaningful discriminative patterns. In particular for gender, a one dimensional linear transformation from the learned speaker embeddings can achieve a very high discriminative accuracy -although the models never see the ground truth gender label while training.  <ref type="figure">Figure 14</ref>. First two principal components of the inferred embeddings, with the ground truth labels for gender and region of accent for the VCTK speakers as in <ref type="bibr" target="#b3">(Arik et al., 2017b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Similarity scores</head><p>For the result in <ref type="table">Table 3</ref>, <ref type="figure">Fig. 15</ref> shows the distribution of the scores given by MTurk users as in <ref type="bibr" target="#b31">(Wester et al., 2016)</ref>. For 10 sample count, the ratio of evaluations with the 'same speaker' rating exceeds 70 % for all models. <ref type="figure">Figure 15</ref>. Distribution of similarity scores for 1 and 10 sample counts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Speaker encoder architecture. See Appendix A for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Performance of whole model adaptation and speaker embedding adaptation for voice cloning in terms of speaker classification accuracy for 108 VCTK speakers. Different numbers of cloning samples and fine-tuning iterations are evaluated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. Comparison of speaker adaptation and speaker encoding approaches in term of speaker classification accuracy with different numbers of cloning samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Visualization of estimated speaker embeddings by speaker encoder. The first two principal components of the average speaker embeddings for the speaker encoder with 5 sample count. Only British and North American regional accents are shown as they constitute the majority of the labeled speakers in the VCTK dataset. Please see Appendix E for more detailed analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FC[Figure 7 .</head><label>7</label><figDesc>Figure 7. Speaker encoder architecture with intermediate state dimensions. (batch: batch size, N samples : number of cloning audio samples |As k |, T : number of mel spectrograms timeframes, F mel : number of mel frequency channels, F mapped : number of frequency channels after prenet, d embedding : speaker embedding dimension). Multiplication operation at the last layer represents inner product along the dimension of cloning samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Architecture of speaker verification model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .Figure 11 .</head><label>1011</label><figDesc>Figure 10. Speaker verification EER (using 1 enrollment audio) vs. number of cloning audio samples. Multi-speaker generative model and speaker verification model are trained using LibriSpeech dataset. Voice cloing is performed using VCTK dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 .Figure 13 .</head><label>1213</label><figDesc>Figure 12. Mean absolute error in embedding estimation vs. the number of cloning audios for a validation set of 25 speakers, shown with the attention mechanism and without attention mechanism (by simply averaging).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Cloned audio samples can be found in https:// audiodemos.github.io</figDesc><table>* Equal contribution 
1 Baidu Research. 
Correspon-
dence to: 
SercanÖ. Arık &lt;sercanarik@baidu.com&gt;, 
Jitong 
Chen 
&lt;chenjitong01@baidu.com&gt;, 
Kainan 
Peng 
&lt;pengkainan@baidu.com&gt;, 
Wei 
Ping 
&lt;pingwei01@baidu.com&gt;. 

1 on text </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Figure 1. Speaker adaptation and speaker encoding approaches for training, cloning and audio generation.</figDesc><table>Multi-speaker 
generative model 
Text 
Audio 

Speaker 
embedding 

Multi-speaker 
generative model 

Speaker embedding 

Text 
Audio 

Speaker identity 

Speaker encoder 
model 

Speaker embedding 

Cloning audios 

Multi-speaker 
generative model 

Speaker embedding 

Text 
Audio 

Speaker encoder 
model 

Cloning audios 

Speaker embedding 

Speaker encoder 
model 

Cloning audios 

Training 

Multi-speaker 
generative model 

Speaker embedding 

Text 
Audio 

Speaker identity 

Multi-speaker 
generative model 

Speaker embedding 

Multi-speaker 
generative model 

Speaker embedding 

Cloning 
texts 

Cloning 
texts 

Cloning 
audios 

Cloning 
audios 

Cloning 

or 

Multi-speaker 
generative model 
Text 
Audio 

Speaker embedding 

Audio generation 

Speaker adaptation 
Speaker encoding 

Trainable 

Fixed 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Table 1. Comparison of requirements for speaker adaptation and speaker encoding. Cloning time interval assumes 1-10 cloning audios. Inference time is for an average sentence. All assume implementation on a TitanX GPU.</figDesc><table>Speaker adaptation 
Speaker encoding 
Approaches 
Embedding-only 
Whole-model 
Without fine-tuning With fine-tuning 
Pre-training 
Multi-speaker generative model 
Data 
Text and audio 
Audio 
Cloning time 
∼ 8 hours 
∼ 0.5 − 5 mins 
∼ 1.5 − 3.5 secs 
∼ 1.5 − 3.5 secs 
Inference time 
∼ 0.4 − 0.6 secs 
Parameters per speaker 
128 
∼ 25 million 
512 
512 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>Parameter Audio resampling freq. 16 KHz Bands of Mel-spectrogram 80 Hop length 400 Convolution layers, channels, filter, strides 1, 64, 20 × 5, 8 × 2</figDesc><table>Recurrent layer size 
128 
Fully connected size 
128 
Dropout probability 
0.9 
Learning Rate 
10 

−3 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We have experimented classification loss by mapping the embeddings to speaker class labels via a softmax layer. 3 We have experimented integrating a pre-trained speaker classifier to promote speaker difference of generated audios.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The decision threshold can be changed to trade-off between false acceptance rate and false rejection rate. The equal error-rate refers to rhe point when the two rates are equal. 5 We designed a segmentation and denoising pipeline to process LibriSpeech, as described in Ping et al. (2017).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">For whole model adaptation, we pick the number of iterations as 100 for 1, 2 and 3 cloning audio samples, 1000 for 5 and 10 cloning audio samples. For speaker embedding adaptation, we fix the number of iterations as 100K for all cases. 7 The learning rate and annealing parameters are optimized for joint fine-tuning. both adaptation approaches, the classification accuracy significantly increases with more samples, up to ten samples. In the low sample count regime, adapting the speaker embedding is less likely to overfit the samples than adapting the whole model. The two methods also require different numbers of iterations to converge. Compared to whole model adaptation, which converges around 1000 iterations for even 100 cloning audio samples, embedding adaptation takes significantly more iterations to converge.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Enrollment audios are from the same speaker.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Voice morphing that improves tts quality using an optimal dynamic frequency warping-and-weighting transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Agiomyrgiannakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoi</forename><surname>Roupakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5650" to="5654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rishita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jingliang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guoliang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep voice: Real-time neural text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sercanömer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Greg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yongguo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubho</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<idno>abs/1702.07825</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep voice 2: Multi-speaker neural text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sercanömer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">F</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kainan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1705.08947</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multicontent gan for few-shot font style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samaneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhaowen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1708.02182</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Voice conversion using deep neural networks with layer-wise generative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Dai</surname></persName>
		</author>
		<idno type="doi">10.1109/TASLP.2014.2353991</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1859" to="1872" />
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spectral mapping using artificial neural networks for voice conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yegnanarayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Prahallad</surname></persName>
		</author>
		<idno type="doi">10.1109/TASL.2010.2047683</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="954" to="964" />
			<date type="published" when="2010-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speaker adaptive training in deep neural networks using speaker dependent bottleneck features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Doddipatla</surname></persName>
		</author>
		<idno type="doi">10.1109/ICASSP.2016.7472687</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016-03" />
			<biblScope unit="page" from="5290" to="5294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cinjon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01279</idno>
		<title level="m">Neural audio synthesis of musical notes with wavenet autoencoders</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A probabilistic interpretation for artificial neural network-based voice conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename></persName>
		</author>
		<idno type="doi">10.1109/APSIPA.2015.7415330</idno>
	</analytic>
	<monogr>
		<title level="m">2015 AsiaPacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="552" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename><surname>Timo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno>abs/1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">One-shot learning by inverting a compositional causal process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">One-shot learning of generative speech concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ying Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<editor>CogSci</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="doi">10.1126/science.aab3050</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep speaker: an end-to-end neural speaker embedding system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaokong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xuewei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyao</forename><surname>Zhu</surname></persName>
		</author>
		<idno>abs/1705.02304</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Regularizing and optimizing LSTM language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shirish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1708.02182</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Librispeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vassil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guoguo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanjeev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
	<note>2015 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep voice 3: 2000-speaker neural text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kainan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sercanömer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<idno>abs/1710.07654</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic linear discriminant analysis for inferences about identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><forename type="middle">Jd</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Few-shot autoregressive density estimation: Towards learning to learn distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yutian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aäron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
		<idno>abs/1710.10304</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">One-shot generalization in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shakir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ivo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<editor>Balcan, Maria Florina and Weinberger, Kilian Q.</editor>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="20" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Speaker diarization through speaker embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rouvier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Favre</surname></persName>
		</author>
		<idno type="doi">10.1109/EUSIPCO.2015.7362751</idno>
	</analytic>
	<monogr>
		<title level="m">23rd European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<date type="published" when="2015-08" />
			<biblScope unit="page" from="2082" to="2086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Atr µ-talk speech synthesis system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshinori</forename><surname>Sagisaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nobuyoshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoto</forename><surname>Iwahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Mimura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second International Conference on Spoken Language Processing</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Natural tts synthesis by conditioning wavenet on mel spectrogram predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruoming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Navdeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zongheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhifeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Agiomyrgiannakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ruoming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Navdeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zongheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhifeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05884</idno>
		<title level="m">Natural tts synthesis by conditioning wavenet on mel spectrogram predictions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep neural network-based speaker embeddings for end-to-end speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pegah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garciaromero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Carmiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanjeev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="165" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Voice synthesis for in-the-wild speakers via a phonological loop. CoRR, abs/1707.06588, 2017. van den Oord</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eliya ; Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
	<note>Conditional image generation with pixelcnn decoders</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Llion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Illia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
	<note>Guyon, I</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirsten</forename><surname>Macdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Tacotron: A fully end-to-end text-to-speech synthesis model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daisy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yonghui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Navdeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zongheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhifeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agiomyrgiannakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rif</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>CoRR, abs/1703.10135</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Analysis of the voice conversion challenge 2016 evaluation results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirjam</forename><surname>Wester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">09</biblScope>
			<biblScope unit="page" from="1637" to="1641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hsin-min. Locally linear embedding for exemplar-based spectral conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsin-Te</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chin-Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2016-09" />
			<biblScope unit="page" from="1652" to="1656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Statistical parametric speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiichi</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1039" to="1064" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
