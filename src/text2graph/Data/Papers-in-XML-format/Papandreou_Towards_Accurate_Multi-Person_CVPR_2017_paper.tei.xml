<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Accurate Multi-person Pose Estimation in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
							<email>tylerzhu@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nori</forename><surname>Kanazawa</surname></persName>
							<email>kanazawa@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
							<email>toshev@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
							<email>tompson@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Bregler</surname></persName>
							<email>bregler@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
							<email>kpmurphy]@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google, Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Accurate Multi-person Pose Estimation in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual interpretation of people plays a central role in the quest for comprehensive image understanding. We want to localize people, understand the activities they are involved in, understand how people move for the purpose of Virtual/Augmented Reality, and learn from them to teach autonomous systems. A major cornerstone in achieving these goals is the problem of human pose estimation, defined as 2-D localization of human joints on the arms, legs, and keypoints on torso and the face.</p><p>Recently, there has been significant progress on this problem, mostly by leveraging deep Convolutional Neural Networks (CNNs) trained on large labeled datasets <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b7">8]</ref>. However, most prior work has focused on the simpler setting of predicting the pose of a single person assuming the location and scale of the person is provided in the form of a ground truth bounding box or torso keypoint position, as in the popular MPII <ref type="bibr" target="#b1">[2]</ref> and FLIC <ref type="bibr" target="#b39">[40]</ref> datasets.</p><p>In this paper, we tackle the more challenging setting of pose detection 'in the wild', in which we are not provided with the ground truth location or scale of the person instances. This is harder because it combines the problem of person detection with the problem of pose estimation. In crowded scenes, where people are close to each other, it can be quite difficult to solve the association problem of determining which body part belongs to which person.</p><p>The recently released COCO person keypoints detection dataset and associated challenge <ref type="bibr" target="#b30">[31]</ref> provide an excellent vehicle to encourage research, establish metrics, and measure progress on this task. It extends the COCO dataset <ref type="bibr" target="#b31">[32]</ref> with additional annotations of 17 keypoints (12 body joints and 5 face landmarks) for every medium and large sized person in each image. A large number of persons in the dataset are only partially visible. The degree of match between ground truth and predicted poses in the COCO keypoints task is measured in terms of object keypoint similarity (OKS), which ranges from 0 (poor match) to 1 (perfect match). The overall quality of the combined person detection and pose estimation system in the benchmark is measured in terms of an OKS-induced average precision (AP) metric. In this paper, we describe a system that achieves state-of-the-art results on this challenging task.</p><p>There are two broad approaches for tackling the multiperson pose estimation problem: bottom-up, in which keypoint proposals are grouped together into person instances, and top-down, in which a pose estimator is applied to the output of a bounding-box person detector. Recent work <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24]</ref> has advocated the bottom-up approach; in their experiments, their proposed bottom-up methods outperformed the top-down baselines they compared with.</p><p>In contrast, in this work we revisit the top-down approach and show that it can be surprisingly effective. The proposed system is a two stage pipeline with state-of-art constituent components carefully adapted to our task. In the first stage, we predict the location and scale of boxes which are likely to contain people. For this we use the Faster-RCNN method <ref type="bibr" target="#b36">[37]</ref> on top of a ResNet-101 CNN <ref type="bibr" target="#b21">[22]</ref>, as implemented by <ref type="bibr" target="#b22">[23]</ref>. In the second stage, we predict the locations of each keypoint for each of the proposed person boxes. For this we use a ResNet <ref type="bibr" target="#b21">[22]</ref> applied in a fully convolutional fashion to predict activation heatmaps and offsets for each keypoint, similar to the works of Pishchulin et al. <ref type="bibr" target="#b34">[35]</ref> and Insafutdinov et al. <ref type="bibr" target="#b24">[25]</ref>, followed by combining their predictions using a novel form of heatmap-offset aggregation. We avoid duplicate pose detections by means of a novel keypoint-based Non-Maximum-Suppression (NMS) mechanism building directly on the OKS metric (which we call OKS-NMS), instead of the cruder box-level IOU NMS. We also propose a novel keypoint-based confidence score estimator, which we show leads to greatly improved AP compared to using the Faster-RCNN box scores for ranking our final pose proposals. The system described in this paper is an improved version of our G-RMI entry to the COCO 2016 keypoints detection challenge.</p><p>Using only publicly available data for training, our final system achieves average precision of 0.649 on the COCO test-dev set and 0.643 on the COCO test-standard set, outperforming the winner of the 2016 COCO keypoints challenge <ref type="bibr" target="#b7">[8]</ref>, which gets 0.618 on test-dev and 0.611 on teststandard, as well as the very recent Mask-RCNN <ref type="bibr" target="#b20">[21]</ref> methods which gets 0.631 on test-dev. Using additional in-house labeled data we obtain an even higher average precision of 0.685 on the test-dev set and 0.673 on the test-standard set, more than 5% absolute performance improvement over the best previous methods. These results have been attained with single-scale evaluation and using a single CNN for box detection and a single CNN for pose estimation. Multiscale evaluation and CNN model ensembling might give additional gains.</p><p>In the rest of the paper, we discuss related work and then describe our method in more detail. We then perform an experimental study, comparing our system to recent stateof-the-art, and we measure the effects of the different parts of our system on the AP metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>For most of its history, the research in human pose estimation has been heavily based on the idea of part-based models, as pioneered by the Pictorial Structures (PS) model of Fischler and Elschlager <ref type="bibr" target="#b15">[16]</ref>. One of the first practical and well performing methods based on this idea is Deformable Part Model (DPM) by Felzenswalb et al. <ref type="bibr" target="#b14">[15]</ref>, which spurred a large body of work on probabilistic graphical models for 2-D human pose inference <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b17">18]</ref>. The majority of these methods focus on developing tractable inference procedures for highly articulated models, while at the same time capturing rich dependencies among body parts and properties.</p><p>Single-Person Pose With the development of Deep Convolutional Neural Networks (CNN) for vision tasks, stateof-art performance on pose estimation is achieved using CNNs <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b7">8]</ref>. The problem can be formulated as a regression task, as done by Toshev and Szegedy <ref type="bibr" target="#b44">[45]</ref>, using a cascade of detectors for top-down pose refinement from cropped input patches. Alternatively, Jain et al. <ref type="bibr" target="#b26">[27]</ref> trained a CNN on image patches, which was applied convolutionally at inference time to infer heatmaps (or activity-maps) for each keypoint independently. In addition, they used a "DPM-like" graphical-model post processing step to filter heatmap potentials and to impose interjoint consistency. Following this work, Tompson et al. <ref type="bibr" target="#b43">[44]</ref> used a multi-scale fully-convolutional architecture trained on whole images (rather than image crops) to infer the heatmap potentials, and they reformulated the graphical model from <ref type="bibr" target="#b26">[27]</ref> -simplifying the tree structure to a stargraph and re-writing the belief propagation messages -so that the entire system could be trained end-to-end.</p><p>Chen et al. <ref type="bibr" target="#b9">[10]</ref> added image-dependent priors to improve CNN performance. By learning a lower-dimensional image representation, they clustered the input image into a mixture of configurations of each pair of consecutive joints. Depending on which mixture is active for a given input image, a separate pairwise displacement prior was used for graphical model inference, resulting in stronger pairwise priors and improved overall performance.</p><p>Bulat et al. <ref type="bibr" target="#b6">[7]</ref> use a cascaded network to explicitly infer part relationships to improve inter-joint consistency, which the authors claim effectively encodes part constraints and inter-joint context. Similarly, Belagiannis &amp; Zisserman <ref type="bibr" target="#b5">[6]</ref> also propose a cascaded architecture to infer pairwise joint (or part) locations, which is then used to iteratively refine unary joint predictions, where unlike <ref type="bibr" target="#b6">[7]</ref>, they propose iterative refinement using a recursive neural network.</p><p>Inspired by recent work in sequence-to-sequence modeling, Gkioxari et al. <ref type="bibr" target="#b19">[20]</ref> propose a novel network structure where body part locations are predicted sequentially rather than independently, as per traditional feed-forward networks. Body part locations are conditioned on the input image and all other predicted parts, yielding a model which promotes sequential reasoning and learns complex inter-joint relationships.</p><p>The state-of-the-art approach for single-person pose on the MPII human pose <ref type="bibr" target="#b1">[2]</ref> and FLIC <ref type="bibr" target="#b39">[40]</ref> datasets is the CNN model of Newell et al. <ref type="bibr" target="#b32">[33]</ref>. They propose a novel CNN architecture that uses skip-connections to promote multi-scale feature learning, as well as a repeated pooling-upsampling ("hourglass") structure that results in improved iterative pose refinement. They claim that their network is able to more efficiently learn various spatial relationship associated with the body, even over large pixel displacements, and with a small number of total network parameters.</p><p>Top-Down Multi-Person Pose The problem of multiperson pose estimation presents different challenges, unadressed by the above work. Most of the approaches for multi-person pose aim at associating person part detections with person instances. The top down way to establish these associations, which is closest to our approach, is to first perform person detection followed by pose estimation. For example, Pishchulin et al. <ref type="bibr" target="#b35">[36]</ref> follow this paradigm by using PS-based pose estimation. A more robust to occlusions person detector, modeled after poselets, is used by Gkioxari et al. <ref type="bibr" target="#b18">[19]</ref>. Further, Yang and Ramanan <ref type="bibr" target="#b46">[47]</ref> fuse detection and pose in one model by using a PS model. The inference procedure allows for pose estimation of multiple person instances per image analogous to PS-based object detection. A similar multi-person PS with additional explicit occlusion modeling is proposed by Eichner and Ferrari <ref type="bibr" target="#b12">[13]</ref>. The very recent Mask-RCNN method <ref type="bibr" target="#b20">[21]</ref> extends Faster-RCNN <ref type="bibr" target="#b36">[37]</ref> to also support keypoint estimation, obtaining very competitive results. On a related note, 2-D person detection is used as a first step in several 3D pose estimation works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Bottom-Up Multi-Person Pose A different line of work is to detect body parts instead of full persons, and to subsequently associate these parts to human instances, thus performing pose estimation in a bottom up fashion. Such approaches employ part detectors and differ in how associations among parts are expressed, and the inference procedure used to obtain full part groupings into person instances. Pishchulin et al. <ref type="bibr" target="#b34">[35]</ref> and later Insafutdinov et al. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24]</ref> formulate the problem of pose estimation as part grouping and labeling via a Linear Program. A similar formulation is proposed by Iqbal et al. <ref type="bibr" target="#b25">[26]</ref>. A probabilistic approach to part grouping and labeling is also proposed by Ladicky et al. <ref type="bibr" target="#b28">[29]</ref>, leveraging a HOG-based system for part detections.</p><p>Cao et al. <ref type="bibr" target="#b7">[8]</ref> winning entry to the 2016 COCO person keypoints challenge <ref type="bibr" target="#b31">[32]</ref> combines a variation of the unary joint detector architecture from <ref type="bibr" target="#b45">[46]</ref> with a part affinity field regression to enforce inter-joint consistency. They employ a greedy algorithm to generate person instance proposals in a bottom-up fashion. Their best results are obtained in an additional top-down refinement process in which they run a standard single-person pose estimator <ref type="bibr" target="#b45">[46]</ref> on the person instance box proposals generated by the bottom-up stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>Our multi-person pose estimation system is a two step cascade, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. In the second stage, we apply a pose estimator to the image crop extracted around each candidate person instance in order to localize its keypoints and re-score the corresponding proposal.</p><p>Our approach is inspired by recent state-of-art object detection systems such as <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b42">43]</ref>, which propose objects in a class agnostic fashion as a first stage and refine their label and location in a second stage. We can think of the first stage of our method as a proposal mechanism, however of only one type of object -person. Our second stage serves as a refinement where we (i) go beyond bounding boxes and predict keypoints and (ii) rescore the detection based on the estimated keypoints. For computational efficiency, we only forward to the second stage person box detection proposals with score higher than 0.3, resulting in only 3.5 proposals per image on average. In the following, we describe in more detail the two stages of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Person Box Detection</head><p>Our person detector is a Faster-RCNN system <ref type="bibr" target="#b36">[37]</ref>. In all experiments reported in this paper we use a ResNet-101 network backbone <ref type="bibr" target="#b21">[22]</ref>, modified by atrous convolution <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30]</ref> to generate denser feature maps with output stride equal to 8 pixels instead of the default 32 pixels. We have also experimented with an Inception-ResNet CNN backbone <ref type="bibr" target="#b41">[42]</ref>, which is an architecture integrating Inception layers <ref type="bibr" target="#b42">[43]</ref> with residual connections <ref type="bibr" target="#b21">[22]</ref>, which performs slightly better at the cost of increased computation.</p><p>The CNN backbone has been pre-trained for image classification on Imagenet. In all reported experiments, both the region proposal and box classifier components of the Faster-RCNN detector have been trained using only the person category in the COCO dataset and the box annotations for the remaining 79 COCO categories have been ignored. We use the Faster-RCNN implementation of <ref type="bibr" target="#b22">[23]</ref> written in Tensorflow <ref type="bibr" target="#b0">[1]</ref>. For simplicity and to facilitate reproducibility we do not utilize multi-scale evaluation or model ensembling in the Faster-RCNN person box detection stage. Using such enhancements can further improve our results at the cost of significantly increased computation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Person Pose Estimation</head><p>The pose estimation component of our system predicts the location of all K = 17 person keypoints, given each person bounding box proposal delivered by the first stage.</p><p>One approach would be to use a single regressor per keypoint, as in <ref type="bibr" target="#b44">[45]</ref>, but this is problematic when there is more than one person in the image patch (in which case a keypoint can occur in multiple places). A different approach addressing this issue would be to predict activation maps, as in <ref type="bibr" target="#b26">[27]</ref>, which allow for multiple predictions of the same keypoint. However, the size of the activation maps, and thus the localization precision, is limited by the size of the net's output feature maps, which is a fraction of the input image size, due to the use of max-pooling with decimation.</p><p>In order to address the above limitations, we adopt a combined classification and regression approach. For each spatial position, we first classify whether it is in the vicinity of each of the K keypoints or not (which we call a "heatmap"), then predict a 2-D local offset vector to get a more precise estimate of the corresponding keypoint location. Note that this approach is inspired by work on object detection, where a similar setup is used to predict bounding boxes, e.g. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b36">37]</ref>. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates these three output channels per keypoint. Image Cropping We first make all boxes have the same fixed aspect ratio by extending either the height or the width of the boxes returned by the person detector without distorting the image aspect ratio. After that, we further enlarge the boxes to include additional image context: we use a rescaling factor equal to 1.25 during evaluation and a random rescaling factor between 1.0 and 1.5 during training (for data augmentation). We then crop from the resulting box the image and resize to a fixed crop of height 353 and width 257 pixels. We set the aspect ratio value to 353/257 = 1.37.</p><p>Heatmap and Offset Prediction with CNN We apply a ResNet with 101 layers <ref type="bibr" target="#b21">[22]</ref> on the cropped image in a fully convolutional fashion to produce heatmaps (one channel per keypoint) and offsets (two channels per keypoint for the xand y-directions) for a total of 3 · K output channels, where K = 17 is the number of keypoints. We initialize our model from the publicly available Imagenet pretrained ResNet-101 model of <ref type="bibr" target="#b21">[22]</ref>, replacing its last layer with 1x1 convolution with 3 · K outputs. We follow the approach of <ref type="bibr" target="#b8">[9]</ref>: we employ atrous convolution to generate the 3 · K predictions with an output stride of 8 pixels and bilinearly upsample them to the 353x257 crop size.</p><p>In more detail, given the image crop, let f k (x i ) = 1 if the k-th keypoint is located at position x i and 0 otherwise. Here k ∈ {1, . . . , K} is indexing the keypoint type and i ∈ {1, . . . , N } is indexing the pixel locations on the 353x257 image crop grid. Training a CNN to produce directly the highly localized activations f k (ideally delta functions) on a fine resolution spatial grid is hard.</p><p>Instead, we decompose the problem into two stages. First, for each position x i and each keypoint k, we compute the probability h k (x i ) = 1 if ||x i − l k || ≤ R that the point x i is within a disk of radius R from the location l k of the k-th keypoint. We generate K such heatmaps, solving a binary classification problem for each position and keypoint independently.</p><p>In addition to the heatmaps, we also predict at each position i and each keypoint k the 2-D offset vector F k (x i ) = l k − x i from the pixel to the corresponding keypoint. We generate K such vector fields, solving a 2-D regression problem for each position and keypoint independently.</p><p>After generating the heatmaps and offsets, we aggregate them to produce highly localized activation maps f k (x i ) as follows:</p><formula xml:id="formula_0">f k (x i ) = j 1 πR 2 G(x j + F k (x j ) − x i )h k (x j ) , (1)</formula><p>where G(·) is the bilinear interpolation kernel. This is a form of Hough voting: each point j in the image crop grid casts a vote with its estimate for the position of every keypoint, with the vote being weighted by the probability that it is in the disk of influence of the corresponding keypoint. The normalizing factor equals the area of the disk and ensures that if the heatmaps and offsets were perfect, then f k (x i ) would be a unit-mass delta function centered at the position of the k-th keypoint.</p><p>The process is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. We see that predicting separate heatmap and offset channels and fusing them by the proposed voting process results into highly localized activation maps which precisely pinpoint the position of the keypoints. Model Training We use a single ResNet model with two convolutional output heads. The output of the first head passes through a sigmoid function to yield the heatmap probabilities h k (x i ) for each position x i and each keypoint k. The training targeth k (x i ) is a map of zeros and ones, withh k (x i ) = 1 if ||x i − l k || ≤ R and 0 otherwise. The corresponding loss function L h (θ) is the sum of logistic losses for each position and keypoint separately. To accelerate training, we follow <ref type="bibr" target="#b24">[25]</ref> and add an extra heatmap prediction layer at intermediate layer 50 of ResNet, which contributes a corresponding auxiliary loss term.</p><p>For training the offset regression head, we penalize the difference between the predicted and ground truth offsets. The corresponding loss is</p><formula xml:id="formula_1">L o (θ) = k=1:K i:||l k −xi||≤R H(||F k (x i )−(l k −x i )||) , (2)</formula><p>where H(u) is the Huber robust loss, l k is the position of the k-th keypoint, and we only compute the loss for positions x i within a disk of radius R from each keypoint <ref type="bibr" target="#b36">[37]</ref>.</p><p>The final loss function has the form</p><formula xml:id="formula_2">L(θ) = λ h L h (θ) + λ o L o (θ) ,<label>(3)</label></formula><p>where λ h = 4 and λ o = 1 is a scalar factor to balance the loss function terms. We sum this loss over all the images in a minibatch, and then apply stochastic gradient descent.</p><p>An important consideration in model training is how to treat cases where multiple people exist in the image crop in the computation of heatmap loss. When computing the heatmap loss at the intermediate layer, we exclude contributions from within the disks around the keypoints of background people. When computing the heatmap loss at the final layer, we treat as positives only the disks around the keypoints of the foreground person and as negatives everything else, forcing the model to predict correctly the keypoints of the person in the center of the box.</p><p>Pose Rescoring At test time, we apply the model to each image crop. Rather than just relying on the confidence from the person detector, we compute a refined confidence estimate, which takes into account the confidence of each keypoint. In particular, we maximize over locations and average over keypoints, yielding our final instance-level pose detection score:</p><formula xml:id="formula_3">score(I) = 1 K K k=1 max xi f k (x i )<label>(4)</label></formula><p>We have found that ranking our system's pose estimation proposals using 4 significantly improves AP compared to using the score delivered by the Faster-RCNN box detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OKS-Based Non Maximum Suppression</head><p>Following standard practice, we use non maximal suppression (NMS) to eliminate multiple detections in the person-detector stage. The standard approach measures overlap using intersection over union (IoU) of the boxes. We propose a more refined variant which takes the keypoints into account. In particular, we measure overlap using the object keypoint similarity (OKS) for two candidate pose detections. Typically, we use a relatively high IOU-NMS threshold (0.6 in our experiments) at the output of the person box detector to filter highly overlapping boxes. The subtler OKS-NMS at the output of the pose estimator is better suited to determine if two candidate detections correspond to false positives (double detection of the same person) or are true positives (two people in close proximity to each other).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>We have implemented out system in Tensorflow <ref type="bibr" target="#b0">[1]</ref>. We use distributed training across several machines equipped with Tesla K40 GPUs.</p><p>For person detector training we use 9 GPUs. We optimize with asynchronous SGD with momentum set to 0.9. The learning rate starts at 0.0003 and is decreased by a factor of 10 at 800k steps. We train for 1M steps. <ref type="figure">Figure 4</ref>: Detection and pose estimation results using our system on a random selection from the COCO test-dev set. For each detected person, we display the detected bounding box together with the estimated keypoints. All detections for one person are colored the same way. It is worth noting that our system works in heavily cluttered scenes (third row, rightmost and last row, right); it deals well with occlusions (last row, left) and hallucinates occluded joints. Last but not least, some of the false positive detections are in reality correct as they represent pictures of people (first row, middle) or toys (fourth row, middle).   For pose estimator training we use two machines equipped with 8 GPUs each and batch size equal to 24 (3 crops per GPU times 8 GPUs). We use a fixed learning rate of 0.005 and Polyak-Ruppert parameter averaging, which amounts to using during evaluation a running average of the parameters during training. We train for 800k steps.</p><p>All our networks are pre-trained on the Imagenet classification dataset <ref type="bibr" target="#b37">[38]</ref>. To train our system we use two dataset variants; one that uses only COCO data (COCOonly), and one that appends to this dataset samples from an internal dataset (COCO+int). For the COCO-only dataset we use the COCO keypoint annotations <ref type="bibr" target="#b31">[32]</ref>: From the 66,808 images (273,469 person instances) in the COCO train+val splits, we use 62,174 images (105,698 person instances) in COCO-only model training and use the remaining 4,301 annotated images as mini-val evaluation set. Our COCO+int training set is the union of COCO-only with an additional 73,024 images randomly selected from Flickr. This in-house dataset contains an additional 227,029 person instances annotated with keypoints following a procedure similar to that described by Lin et al. <ref type="bibr" target="#b30">[31]</ref>. The additional training images have been verified to have no overlap with the COCO training, validation or test sets.</p><p>We have trained our Faster-RCNN person box detection module exclusively on the COCO-only dataset. We have experimented training our ResNet-based pose estimation module either on the COCO-only or on the augmented COCO+int datasets and present results for both. For COCO+int pose training we use mini-batches that contain COCO and in-house annotation instances in 1:1 ratio. <ref type="table" target="#tab_0">Table 1</ref> shows the COCO keypoint test-dev split performance of our system trained on COCO-only or trained on COCO+int datasets. A random selection of test-dev inference samples are shown in <ref type="figure">Figure 4</ref>. <ref type="table" target="#tab_1">Table 2</ref> shows the COCO keypoint test-standard split results of our model with the pose estimator trained on either COCO-only or COCO+int training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">COCO Keypoints Detection State-of-the-Art</head><p>Even with COCO-only training, we achieve state-of-theart results on the COCO test-dev and test-standard splits, outperforming the COCO 2016 challenge winning CMUPose team <ref type="bibr" target="#b7">[8]</ref> and the very recent Mask-RCNN method <ref type="bibr" target="#b20">[21]</ref>. Our best results are achieved with the pose estimator trained on COCO+int data, yielding an AP score of 0.673 on test-standard, an absolute 6.2% improvement over the 0.611 test-standard score of CMU-Pose <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study: Box Detection Module</head><p>An important question for our two-stage system is its sensitivity to the quality of its box detection and pose estimator constituent modules. We examine two variants of the ResNet-101 based Faster-RCNN person box detector, (a) a fast 600x900 variant that uses input images with small side 600 pixels and large side 900 pixels and (b) an accurate 800x1200 variant that uses input images with small side 800 pixels and large side 1200 pixels. Their box detection AP on our COCO person mini-val is 0.466 and 0.500, respectively. Their box detection AP on COCO test-dev is 0.456 and 0.487, respectively. For reference, the person box detection AP on COCO test-dev of the top-performing multicrop/ensemble entry of <ref type="bibr" target="#b22">[23]</ref> is 0.539. We have also tried feeding our pose estimator module with the ground truth person boxes to examine its oracle performance limit in isolation from the box detection module. We report our COCO mini-val results in <ref type="table" target="#tab_2">Table 3</ref> for pose estimators trained on either COCO-only or COCO+int. We use the accurate Faster-RCNN (800x1200) box detector for all results in the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study: Pose Estimation Module</head><p>We have experimented with alternative CNN setups for our pose estimation module. We have explored CNN network backbones based on either the faster ResNet-50 or the more accurate ResNet-101, while keeping ResNet-101 as CNN backbone for the Faster-RCNN box detection module. We have also experimented with two sizes for the image crops that are fed as input to the pose estimator:    Smaller (257x185) for faster inference or larger (353x257) for higher accuracy. We report in <ref type="table" target="#tab_3">Table 4</ref> COCO test-dev results for the four CNN backbone/ crop size combinations, using COCO+int for pose estimator training. We see that ResNet-101 performs about 2% better but in computationconstrained environments ResNet-50 remains a competitive alternative. We use the accurate ResNet-101 (353x257) pose estimator with disk radius R = 25 pixels in the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">OKS-Based Non Maximum Suppression</head><p>We examine the effect of the proposed OKS-based nonmaximum suppression method at the output of the pose estimator for different values of the OKS-NMS threshold. In all experiments the value of the IOU-NMS threshold at the output of the person box detector remains fixed at 0.6. We report in <ref type="table" target="#tab_4">Table 5</ref> COCO mini-val results using either COCOonly or COCO+int for pose estimator training. We fix the OKS-NMS threshold to 0.5 in the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we address the problem of person detection and pose estimation in cluttered images 'in the wild'. We present a simple two stage system, consisting of a person detection stage followed by a keypoint estimation stage for each person. Despite its simplicity it achieves state-of-art results as measured on the challenging COCO benchmark.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>photo credit: Moreseth Figure 1: Overview of our two stage cascade model. In the first stage, we employ a Faster-RCNN person detector to produce a bounding box around each candidate person instance. In the second stage, we apply a pose estimator to the image crop extracted around each candidate person instance in order to localize its keypoints and re-score the corresponding proposal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Network target outputs. Left &amp; Middle: Heatmap target for the left-elbow keypoint (red indicates heatmap of 1). Right: Offset field L2 magnitude (shown in grayscale) and 2-D offset vector shown in red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Our fully convolutional network predicts two targets: (1) Disk-shaped heatmaps around each keypoint and (2) magnitude of the offset fields towards the exact keypoint position within the disk. Aggregating them in a weighted voting process results in highly localized activation maps. The figure shows the heatmaps and the pointwise magnitude of the offset field on a validation image. Note that in this illustration we super-impose the channels from the different keypoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 4: Detection and pose estimation results using our system on a random selection from the COCO test-dev set. For each detected person, we display the detected bounding box together with the estimated keypoints. All detections for one person are colored the same way. It is worth noting that our system works in heavily cluttered scenes (third row, rightmost and last row, right); it deals well with occlusions (last row, left) and hallucinates occluded joints. Last but not least, some of the false positive detections are in reality correct as they represent pictures of people (first row, middle) or toys (fourth row, middle). Figure best viewed zoomed in on a monitor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Performance on COCO keypoint test-dev split.</figDesc><table>AP 
AP .5 AP .75 AP (M) AP (L) 
AR 
AR .5 AR .75 AR (M) AR (L) 
CMU-Pose [8] 
0.618 0.849 0.675 
0.571 
0.682 0.665 0.872 
0.718 
0.606 
0.746 
Mask-RCNN [21] 
0.631 0.873 0.687 
0.578 
0.714 
G-RMI (ours): COCO-only 0.649 0.855 0.713 
0.623 
0.700 0.697 0.887 
0.755 
0.644 
0.771 
G-RMI (ours): COCO+int 
0.685 0.871 0.755 
0.658 
0.733 0.733 0.901 
0.795 
0.681 
0.804 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Performance on COCO keypoint test-standard split.</figDesc><table>AP 
AP .5 AP .75 AP (M) AP (L) 
AR 
AR .5 AR .75 AR (M) AR (L) 
CMU-Pose[8] 
0.611 0.844 0.667 
0.558 
0.684 0.665 0.872 
0.718 
0.602 
0.749 
G-RMI (ours): COCO-only 0.643 0.846 0.704 
0.614 
0.696 0.698 0.885 
0.755 
0.644 
0.771 
G-RMI (ours): COCO+int 
0.673 0.854 0.735 
0.642 
0.726 0.730 0.898 
0.789 
0.675 
0.805 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Ablation on the box detection module: Performance on COCO keypoint mini-val when using alternative box detec- tion modules trained on COCO-only or ground truth boxes. We use the default ResNet-101 pose estimation module trained on either COCO-only or COCO+int. We mark with an asterisk our default box detection module used in all other experiments.</figDesc><table>Box Module 
Poser Train 
AP 
AP .5 AP .75 AP (M) AP (L) 
AR 
AR .5 AR .75 AR (M) AR (L) 
Faster-RCNN (600x900) 
COCO-only 0.657 0.831 0.721 
0.617 
0.725 0.699 0.856 
0.754 
0.634 
0.788 
Faster-RCNN (800x1200) 

 *  

COCO-only 0.667 0.851 0.730 
0.633 
0.726 0.708 0.874 
0.763 
0.652 
0.786 
Ground-truth boxes 
COCO-only 0.704 0.904 0.771 
0.684 
0.746 0.736 0.911 
0.794 
0.693 
0.796 
Faster-RCNN (600x900) 
COCO+int 0.693 0.854 0.757 
0.650 
0.762 0.730 0.871 
0.786 
0.665 
0.819 
Faster-RCNN (800x1200) 

 *  

COCO+int 0.700 0.860 0.764 
0.665 
0.760 0.742 0.888 
0.800 
0.686 
0.820 
Ground-truth boxes 
COCO+int 0.745 0.925 0.815 
0.725 
0.783 0.774 0.930 
0.835 
0.735 
0.831 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 :</head><label>4</label><figDesc>Ablation on the pose estimation module: Performance on COCO keypoint test-dev when using alternative pose estimation modules trained on COCO+int. We use the default ResNet-101 box detection module trained on COCO-only. We mark with an asterisk our default pose estimation module used in all other experiments.</figDesc><table>Pose Module 
Poser Train 
AP 
AP .5 AP .75 AP (M) AP (L) 
AR 
AR .5 AR .75 AR (M) AR (L) 
ResNet-50 (257x185) 
COCO+int 0.649 0.853 0.722 
0.627 
0.693 0.699 0.890 
0.763 
0.650 
0.766 
ResNet-50 (353x257) 
COCO+int 0.666 0.862 0.734 
0.638 
0.717 0.714 0.894 
0.774 
0.661 
0.787 
ResNet-101 (257x185) 
COCO+int 0.661 0.862 0.734 
0.641 
0.708 0.712 0.895 
0.777 
0.662 
0.782 
ResNet-101 (353x257) 

 *  

COCO+int 0.685 0.871 0.755 
0.658 
0.733 0.733 0.901 
0.795 
0.681 
0.804 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Performance (AP) on COCO keypoint mini-val with varying values for the OKS-NMS threshold. The pose estimator has been trained with either COCO-only or COCO+int data.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to the authors of <ref type="bibr" target="#b22">[23]</ref> for making their excellent Faster-RCNN implementation available to us. We would like to thank Hartwig Adam for encouraging and supporting this project and Akshay Gogia and Gursheesh Kour for managing our internal annotation effort.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1669" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d pictorial structures revisited: Multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Recurrent human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arxiv</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08050v1</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human pose estimation using body parts dependent joint regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Better appearance models for pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">We are family: Joint pose estimation of multiple persons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="228" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scalable object detection using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2147" to="2154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The representation and matching of pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Elschlager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE TOC</title>
		<imprint>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Articulated pose estimation using discriminative armlet classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using k-poselets for detecting people and localizing their keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3582" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Chained predictions using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870v2</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">Mask r-cnn</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10012</idno>
		<title level="m">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Andres, and B. Schiele. Articulated multi-person tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01465</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation with local joint-to-person associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="627" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning human pose estimation features with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning Effective Human Pose Estimation from Inaccurate Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Human pose estimation using a joint pixel-wise and part-wise formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3578" to="3585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<title level="m">Coco 2016 keypoint challenge</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Articulated people detection and pose estimation: Reshaping the future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thormählen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3178" to="3185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adaptive pose priors for pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Modec: Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Articulated part-based model for joint object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="723" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Join training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">Convolutional pose machines</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
