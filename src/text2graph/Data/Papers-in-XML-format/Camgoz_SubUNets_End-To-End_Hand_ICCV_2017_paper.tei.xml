<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SubUNets: End-to-end Hand Shape and Continuous Sign Language Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Necati</forename><surname>Cihan Camgoz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Surrey Guildford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hadfield</surname></persName>
							<email>s.hadfield@surrey.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Surrey Guildford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Koller</surname></persName>
							<email>koller@cs.rwth-aachen.de</email>
							<affiliation key="aff2">
								<orgName type="institution">RWTH Aachen Univ-ersity</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bowden</surname></persName>
							<email>r.bowden@surrey.ac.uk</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Surrey Guildford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SubUNets: End-to-end Hand Shape and Continuous Sign Language Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We propose a novel deep learning approach to solve simultaneous alignment and recognition problems (referred  to as "Sequence-to-sequence" learning </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Perception is a hierarchical process; our understanding of the world as a whole, is based on recognising different parts of the world and understanding their spatio-temporal interactions. As an example, for recognising human actions we not only recognise where the different body parts are located, but how they move relative to each other and in relation to surrounding objects. More generally, most spatio-temporal learning problems can be broken down into meaningful "subunit" problems. However, the subunits often have complex, unsynchronised, causal relationships, making it very challenging to model them jointly. <ref type="figure">Figure 1</ref>. Overview of a SubUNet and its building blocks. In this example our input sequences are hand patch videos and target sequences are hand shape classes. Hand-Icons from <ref type="bibr" target="#b30">[31]</ref>.</p><p>Until recent years, most spatio-temporal computer vision techniques have extracted hand-crafted intermediate representations and then used classical temporal modelling approaches such as Hidden Markov Models and Conditional Random Fields <ref type="bibr" target="#b34">[35]</ref>. The emergence of modern deep learning methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b33">34]</ref> has removed the need for such tailored representations and enabled systems to implicitly learn both the spatial and the temporal features. However, the disadvantage of deep learning is that it can be difficult to encode expert knowledge (such as suitable subunits or intermediate representations). This is especially true when dealing with sequence-to-sequence modelling problems, where the different subunits may not be synchronised with each other and can exhibit complex causal relationships.</p><p>In this paper, we present SubUNets 1 , a novel deep learning architecture for sequence-to-sequence learning tasks, where the systems are expected to produce a sequence of outputs from a given video. Contrary to other video to text approaches, our method explicitly models the contextual subunits of the task while training the network for the main task. This allows us not only to encode expert knowledge about the properties of the task, but also to exploit a much wider range of annotation sources, and to exploit implicit transfer learning between tasks. We demonstrate this approach for the problem of Continuous Sign Language recognition, where the recognition systems are expected to detect and recognise the individual signs in a given video and produce a text translation. This problem is particularly well suited to our SubUNets approach as unlike spoken languages, sign is famously multi-channel. Information is carried in the hand shape, motions, body pose and even facial gestures. Additionally, there is a wealth of expert linguistic knowledge relating to sign language and the interactions between it's different modalities.</p><p>The contributions of this paper can be listed as:</p><p>• An end-to-end framework for explicitly modelling the subunits during sequence-to-sequence learning.</p><p>• The first end-to-end system for continuous sign language recognition alignment and recognition, based on explicit subunit modelling.</p><p>• A thorough comparison of different decoding schemes for networks using CTC loss. The rest of the paper is organized as follows: In Section 2 we go over the related work on sequence-to-sequence modelling, and continuous sign language recognition. In Section 3 we depict SubUNets and go further into detail of its components. First we apply SubUNets to the problem of hand shape recognition in Section 4, achieving state-of-the-art performance without needing to realign the data. Then we describe our application of SubUNets to the challenge of Continuous Sign Language recognition in Section 5. Here we demonstrate how SubUNets can be combined to model the asynchronous relationship between different channels of information and that combining different loss layers allows expert knowledge to be incorporated which increases recognition performance. Finally, we conclude the paper in Section 6 by discussing our findings and the possible future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Sequence-to-sequence learning methods can be grouped into two categories: Encoder-Decoder Networks <ref type="bibr" target="#b37">[38]</ref> and approaches based on Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b15">[16]</ref>.</p><p>Encoder-Decoder networks first emerged from the field of Neural Machine Translation (NMT) <ref type="bibr" target="#b31">[32]</ref>. Kalchbrenner and Blunsom <ref type="bibr" target="#b23">[24]</ref> proposed the first encoder-decoder network <ref type="bibr" target="#b0">1</ref> Not to be confused with U-Nets <ref type="bibr" target="#b35">[36]</ref> that uses a single Recurrent Neural Network for both encoding and decoding sequences. Following this Sutzkever et al. <ref type="bibr" target="#b37">[38]</ref> and Cho et al. <ref type="bibr" target="#b7">[8]</ref> proposed separating the encoding and decoding jobs into two separate RNNs. Although this approach improved their machine translation performance, there were still issues with modelling the long term dependencies between the input and output sequences. To overcome this problem Bahdanau et al. <ref type="bibr" target="#b3">[4]</ref> proposed attention mechanisms that were able to learn where to focus on the input sequence depending on the output. These successes in NMT encouraged computer vision researchers to adopt encoderdecoder networks for applications such as image captioning <ref type="bibr" target="#b42">[43]</ref>, activity recognition <ref type="bibr" target="#b12">[13]</ref> and lip-reading <ref type="bibr" target="#b8">[9]</ref>.</p><p>The second group of sequence-to-sequence learning approaches are based on CTC, proposed by Graves et al. <ref type="bibr" target="#b15">[16]</ref>. This approach has been widely used in the fields of Speech Recognition <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b1">2]</ref> and Hand Writing Recognition <ref type="bibr" target="#b16">[17]</ref>. As CTC is an ideal method for tasks where the data is weakly labelled, computer vision researchers have also applied this sequence-to-sequence learning method to sentence-level lip reading <ref type="bibr" target="#b2">[3]</ref> and action recognition <ref type="bibr" target="#b20">[21]</ref>.</p><p>In this paper, we demonstrate our proposed sequence-tosequence learning techniques in the domain of continuous sign language recognition. This is due to its multi-channel nature <ref type="bibr" target="#b10">[11]</ref>, and the large amounts of expert linguistic knowledge available.</p><p>Until recently, most sign language recognition research was conducted on isolated sign samples <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b4">5]</ref>. However, with the availability of large datasets, such as RWTH-PHOENIX-Weather-2014 <ref type="bibr" target="#b13">[14]</ref>, research interest has started to shift towards continuous sign language recognition. As frame level annotations are hard to come by in continuous datasets, most of the work to date required an alignment step to localize individual signs in videos <ref type="bibr" target="#b9">[10]</ref>. The work that is most relevant to this paper is by Koller et al. <ref type="bibr" target="#b26">[27]</ref> which combines deep-representations with traditional HMM based temporal modelling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SubUNets</head><p>In this section we present a novel deep learning architecture for generic video to sequence learning problems, employing smaller specialized sub-networks. This approach forces the network to explicitly model domain specific expert knowledge, better constraining the overarching recognition problem. We refer to these smaller specialized networks as SubUNets, as they are trained to model subunits of a given task.</p><p>Each SubUNet consists of three tiers of neural network. Firstly, Convolutional Neural Networks (CNNs) take images as inputs and extract spatial features. Secondly, Bidirectional Long Short Term Memory Layers (BLSTM) temporally model the spatial features extracted by the CNNs. Finally a Connectionist Temporal Classification (CTC) Loss Layer allows the networks to be trained with different length videos and label sequences. We depict a sample SubUNet architecture that learns hand shapes from cropped hand images in <ref type="figure">Figure 1</ref>. In the remainder of this section, we will provide further details on each tier of the SubUNets, and describe how to train them in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spatial Feature Extraction: Convolutional Neural Networks</head><p>In SubUNets we employ 2D CNNs for learning the spatial feature representations. Given an input image I with c channels, the 2D convolution layers extract the feature map F by convolving the image with the weights w as in F (x, y) = c δx,δy I(x+δx, y+δy, c)×w(δx, δy)+b <ref type="formula">(1)</ref> where x and y represent the pixel coordinates of the image I and b is the bias term. The spatial neighbourhood that δ x and δ y are drawn from is defined by the kernel size of the convolution layer.</p><p>Although the SubUNets approach can exploit any CNN architecture for spatial modelling, our experiments use CaffeNet due to its low memory consumption (see Section 3.4 for further details). CaffeNet is a variant of AlexNet <ref type="bibr" target="#b29">[30]</ref> that has five convolutional and three fully connected layers. We discard the last fully connected layer and use the weights that were pre-trained on ImageNet <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temporal Modelling: Bidirectional LSTMs</head><p>Two dimensional convolutional neural networks have achieved state-of-the-art performance for many spatial recognition tasks <ref type="bibr" target="#b38">[39]</ref>. However they do not have the ability to model temporal transitions of a video sequence. The spatiotemporal convolutional networks <ref type="bibr" target="#b40">[41]</ref> can theoretically model temporal change in the spatial domain but their ability to represent state transitions is limited. Instead, we model the temporal aspects of our input sequences using Recurrent Neural Networks (RNNs).</p><p>One of the main difficulties when training RNNs is the vanishing gradient problem. The error generated from each time step (and it's associated gradients) diminishes during the course of the sequence <ref type="bibr" target="#b32">[33]</ref>. In order to preserve the long term dependencies from the effects of vanishing gradients Hochreiter et al. <ref type="bibr" target="#b19">[20]</ref> proposed Long Short Term Memory (LSTM) units.</p><p>LSTMs try to overcome the vanishing gradient problem by proposing a cell state in addition to the hidden state that classic RNNs use. Furthermore, it has specialized, input, forget and update gates that minimize the diminishing effects of long term dependencies.</p><p>An LSTM unit takes as input, the cell state, C t−1 and hidden state, h t−1 from the previous time step along side the spatial data F t at the current time step. It then computes the input gate i t , forget gate f t and the update gateC t as:</p><formula xml:id="formula_0">f t = σ(W f · [h t−1 , F t ] + b f ) (2) i t = σ(W i · [h t−1 , F t ] + b i ) (3) C t = tanh(W c · [h t−1 , F t ] + b c )<label>(4)</label></formula><p>Using the calculated gate values, the LSTM unit calculates the output o t , cell state C t and the hidden state h t values to pass to the next time step as:</p><formula xml:id="formula_1">C t = f t * C t−1 + i t * C t (5) o t = σ(W o · [h t−1 , F t ] + b o ) (6) h t = o t * tanh(C t )<label>(7)</label></formula><p>From these equations, it is obvious that an LSTM produces the output at a time step t using the current spatial information F t and the information leading up to this point, encoded in the hidden state h t−1 . Thus, any time step following t has no effect on the output of the LSTM at time step t. Although, this gives LSTM the ability to operate in real-time, there is useful information in the following frames that is not being used to constrain the current frame's prediction.</p><p>Therefore, we deploy BLSTMs as our temporal modelling layer. A BLSTM contains two LSTM layers operating in opposite directions along the time domain (See <ref type="figure">Figure 1)</ref>. The outputs of the two LSTMs are then concatenated before being fed deeper into the network. The main idea of the BLSTM is to provide knowledge about the full sequence during prediction. The output of the BLSTM at time t is based on both of the hidden states encoding F 1:t−1 and F T :t+1 . In our SubUNets each BLSTM layer has 2048 units, 1024 units in each direction. Although, the use of BLSTM layers limits the real-time capabilities, on-line prediction is still achievable with a sliding window approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Sequence-to-Sequence Learning: Connectionist Temporal Classification</head><p>When trained with Cross Entropy Loss <ref type="bibr" target="#b14">[15]</ref>, both the classic feed-forward and recurrent architectures require a label for each sample or time step. However, nearly all sequence-to-sequence problems have different length input and target sequences. One way to overcome this problem might be to segment the input sequences and assign a corresponding label to each time step. However, this level of annotation for every sub-unit on large datasets would be impractical. Furthermore, segmenting an input sequence in this manner often introduces label ambiguity, as the system is forced to predict the same class across the start, middle and end of a segment. Therefore, an additional structure is required to effectively train sequence-to-sequence models using feed-forward and recurrent neural networks.</p><p>Connectionist Temporal Classification (CTC), a loss layer proposed by Graves et al. <ref type="bibr" target="#b15">[16]</ref>, is one of the most popular approaches to training sequence-to-sequence models. When using generic loss functions to train a network with L target labels, (vocabulary), we structure our architecture to have |L| outputs, each one corresponding to one of the labels. This allows our network to produce posteriors over each label for every time step. CTC introduces a blank label and creates an extended vocabulary L ′ , where L ′ = L ∪ { }, and restructures the network by adding another output unit corresponding to the blank label. The blank label accounts for silence and transitions that may exist between target labels in the sequence, removing the need for per frame annotation.</p><p>Although the blank label solves some of the problems, the network still has to learn which parts of the input sequence s T , with T time steps, corresponds to silence and transitions. To solve this, the CTC defines a mapping func-</p><formula xml:id="formula_2">tion B : L ′T → L U (where U ≤ T ) between extended vocabulary sequences π ∈ L ′T and label sequences l ∈ L U</formula><p>by collapsing repetitions and removing the blank labels in π. Given an input sequence s, the probability of observing a label sequence l is computed by marginalising over all extended vocabulary sequences that would give rise to l. In other words, if we define an inverse mapping function B −1 which produces every possible extended vocabulary sequence π corresponding to label sequence l, then the probability of l given an input sequence s is:</p><formula xml:id="formula_3">p(l|s) = π∈B −1 (l) p(π|s)<label>(8)</label></formula><p>However, as the length of label sequence increases, the number of corresponding extended vocabulary sequences π expands drastically. To overcome this, CTC uses dynamic programming to efficiently calculate the loss and its gradient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details and Training</head><p>The proposed architecture is implemented using the BLVC Caffe <ref type="bibr" target="#b22">[23]</ref> framework and the CTC implementation of ChWick 2 . Training was done on a single Titan X GPU with Maxwell chip architecture and 12 GB VRAM. The code of our paper is publicly available <ref type="bibr" target="#b2">3</ref> . While choosing our SubUNet layer architectures, memory usage was of particular importance, so that the combined SubUNets would fit into a single GPU. This is exacerbated by the need for CTC to simultaneously have the posteriors from all frames of a sequence in order to calculate the loss, meaning entire sequences must be processed as a whole. Therefore, a set of preliminary experiments was conducted using a dummy SubUNet (One layer of BLSTM with 100 units in each direction) with all well known CNN architectures, to check the practical limitations of memory use on a single Titan X GPU. In these experiments images were resized to the input size of each network, i.e. 224 × 224 or 227 × 227.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN Architecture</head><p>#frames Memory (MB) ResNet-50 <ref type="bibr" target="#b18">[19]</ref> 35 12201 GoogLeNet <ref type="bibr" target="#b39">[40]</ref> 160 12081 VGG-16 <ref type="bibr" target="#b36">[37]</ref> 175 12025 SqueezeNet v1.1 <ref type="bibr" target="#b21">[22]</ref> 320 12109 AlexNet <ref type="bibr" target="#b29">[30]</ref> 1080 12111 VGG-F <ref type="bibr" target="#b6">[7]</ref> 1340 12131 CaffeNet <ref type="bibr" target="#b22">[23]</ref> 1450 12104 <ref type="table">Table 1</ref>. Most Common Architectures and the maximum number of frames we can load to them on a single GPU.</p><p>As can be seen in <ref type="table">Table 1</ref>, the dummy SubUNet using CaffeNet was able to support batches containing significantly longer sequences. Therefore, CaffeNet is used as the spatial encoding layer for all remaining experiments. To be able to train variable length input and output sequences in a single batch and to avoid the memory allocation overhead we resized all frames to 227 × 227 and padded all input sequences to 300 frames.</p><p>All of our networks were trained using the Adam Optimization Algorithm <ref type="bibr" target="#b24">[25]</ref> with a learning rate of 10 −4 and the default parameters: β 1 = 0.9, β 2 = 0.999, ǫ = 10 −8 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Hand SubUNet: End-to-End Hand Shape Recognition and Alignment</head><p>To demonstrate the power of the proposed SubUNet approach we focus on the challenging task of Continuous Sign Language Recognition. One of the primary information carrying modalities in sign language is the hand shape. Therefore, as our first SubUNet, we train a network that learns to simultaneously recognize and time-align hand shape sequences from videos of cropped hand patches. Hand shape recognition is a challenging task in its own right, due to a hands' highly articulated nature. For instance, the same hand shapes (with the same linguistic meaning) but viewed from different directions, results in drastically different appearances in the image due to self occlusion. To be able to generalize across this variation, without over-fitting, requires vast amounts of training data.</p><p>We use the One-Million Hands <ref type="bibr" target="#b26">[27]</ref> dataset for training the Hand SubUNet. The dataset consists of cropped hand images collated from publicly available datasets, including Danish <ref type="bibr" target="#b28">[29]</ref>, New Zealand <ref type="bibr" target="#b30">[31]</ref> and German (RWTH-PHOENIXWeather-2014 <ref type="bibr" target="#b13">[14]</ref> sign languages. It has over 1.2 million hand images, from which 1 million images were labelled with one of 60 hand shape classes. The dataset contains 23 different signers, which helps our network to generalize over different users as well as language. The statistics of the dataset can be seen in <ref type="table" target="#tab_1">Table 2</ref>. The majority of the dataset comes from the Training set of RWTH-PHOENIX-Weather-2014, a continuous sign language dataset which will be used in our continuous sign language recognition experiments in Section 5.  The One-Million Hands dataset provides frame-level annotation for these sequences. However, as we are focussing on the more challenging sequence-to-sequence problem, we remove repetitions of the frame-level annotations to form our target sequence of hand shapes.</p><p>For our network architecture, we used the first 7 layers (5 Convolution, 2 Fully Connected Layers) of the CaffeNet, followed by a single layer of BLSTM with 1024 units in each direction. As the size of our vocabulary for this SubUNet is 61 (60 hand shapes and the blank CTC label) we follow the BLSTM layer with an inner product layer of 61 units. At the end, a CTC Loss Layer is deployed to be able to learn both alignment and recognition in a sequence-to-sequence manner. A simplified visualization of the network can be seen in <ref type="figure" target="#fig_0">Figure 2</ref>, while <ref type="figure">Figure 1</ref> illustrates the network after being unrolled in time.</p><p>The network was trained for 5000 iterations with a minibatch size of 90 sequences, using the Adam Optimizer as described in Section 3.4. Optimization is terminated when the training loss has converged.</p><p>To evaluate the performance of our network we used the 3361 manually annotated hand images provided by <ref type="bibr" target="#b26">[27]</ref>, which are from the Development set of the RWTH-PHOENIX-Weather-2014 dataset. Again, because we are interested in the more challenging alignment &amp; recognition problem, we run the system on the full (unseen) test sequences from which these images were taken. We then extract and evaluate the estimated hand shapes for the subset of frames which have ground truth.</p><p>As shown in <ref type="table">Table 3</ref>, our Hand SubUNet surpasses the hand shape recognition performance of the state-of-the-art CNN-based method proposed by Koller et al. <ref type="bibr" target="#b26">[27]</ref>, by a margin of 18% Top-1 accuracy, which is a relative improvement of 30%. Koller et al. <ref type="bibr" target="#b26">[27]</ref> iteratively realigned and retrained his network whereas the SubUNet architecture automatically overcomes the frame alignment issue. These experiments show us that SubUNets are able to learn both the alignment and the recognition jointly from sequences in an end-to-end fashion, without requiring any other alignment procedure.</p><p>We will now demonstrate the power of SubUNet to the sequence-to-sequence learning problem by applying it to endto-end, multi-channel, continuous sign language recognition.</p><p>Top-1 Top-3 Top-5 Top-10 Koller et al. <ref type="bibr" target="#b26">[27]</ref> 62.  <ref type="table">Table 3</ref>. Hand SubUNet's hand shape recognition results on the One-Million Hands dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Sign SubUNets: End-to-End Continuous Sign Language Recognition</head><p>Compared to their spoken counter parts, Sign Languages are multi-channel languages. Its users convey information using a combination of hand and face gestures, hand movements, upper body pose and facial expressions. The nature of sign languages, makes it an ideal target application for the SubUNets-based approach.</p><p>Due to the difficulty in collecting annotations, most of the sign language recognition datasets that have been developed, consist of isolated sign videos <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Although these datasets are suitable for isolated recognition <ref type="bibr" target="#b4">[5]</ref>, they do not support the ultimate aim of sign language recognition research: the translation of sign language utterances to their spoken languages equivalents. Indeed, training a sign language recognition system using these isolated datasets is equivalent to training a machine translation systems using a dictionary alone. Such a system would be unable to learn the higher order sentence-level relationship between words or signs <ref type="bibr" target="#b3">4</ref> . To be able to train a sentence-level sign language recognition system, we used the RWTH-PHOENIX-Weather-2014 dataset, a DGS (German Sign Language) dataset that consists of Weather Forecast Footage. The dataset contains both the full frames and the cropped hands of the signers. This multichannel data is ideal to test our SubUNet network. For both information channels there are 6841 sequences containing a total of 77,321 words. The statistics of the dataset can be seen in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>To be able to assess the benefits of SubUNets for Continuous Sign Language Recognition, we conducted experiments using a variety architectures.  All of the sign language recognition networks were trained for 6000 iterations using the Adam Optimizer as described in Section 3.4 with a mini-batch size of 60 sequences. Its performance on the development set was evaluated at every epoch, which is 96 iterations. If the training loss has not converged after 60 epochs, we restart the training using the best performing iteration with a lower learning rate and train until the training loss convergence.</p><p>To evaluate the performance of a model, we fed the development and test set sequences through the network and extracted posterior probabilities for each word and the blank CTC label. These posteriors are then passed through TensorFlow's implementation of CTC beam decoder <ref type="bibr" target="#b0">[1]</ref> with a beam width of 100 to obtain the final sequence predictions. To facilitate comparison with previous publications we measure word error rate (WER) as: WER = #deletions + #insertions + #substitutions #number of reference observations <ref type="formula">(9)</ref> 5.1. Word SubUNet: End-to-End Continuous Sign Language Recognition Single Channel As our first sentence-level architecture we train SubUNets that learn the mapping between the given input sequence and the word sequences. As depicted in <ref type="figure" target="#fig_1">Figure 3</ref>, this network is similar to the proposed Hand SubUNet. However, as words have a more abstract relationship to the images than the observable hand shapes, we employ a deeper BLSTM structure (adding BLSTM-2) to help the network model the temporal relationships within the input sequence.</p><p>As can be seen in <ref type="table" target="#tab_6">Table 5</ref>, having two layers in our Word SubUNets improves our sentence-level recognition performance. However, in order to combine multiple SubUNets for different information channels (in subsequent experiments), adding further layers is infeasible due to GPU memory limitations.</p><p>In theory, full frame sequences should provide all necessary channels of information for a sign. In other words hand shape are by definition contained in the full body frame and  the network should be able to find and use this information. However, the problem is under-constrained, and it is unclear what information the network will actually use to predict word sequences. Due to the network's resolution, the most likely candidates are hand shape, arm motions and upper body pose. To see how much the network benefits from having the additional information in the full frame we train another SubUNet using the same network architecture and parameters but this time using only the cropped hands as the input sequences.  <ref type="table">Table 6</ref>. Evaluation of training Word SubUNets on different information channels.</p><p>As can be seen in <ref type="table">Table 6</ref>, training a Word SubUNet with the hand patches worsens our performance by 2% WER. This means the network trained on the full frames does make use of the additional information contained in the full frames. However, it is still unclear how redundant the information is. Do the full frames contain all the information from the hand patches plus a small amount of novel information? Or is the additional context of motion in the full frame experiment compensating for the loss of hand shape information? To answer these questions, we propose combining networks that model hand shape (Hand Patches Word-SubUNets and Hand-SubUNets) with the Full Frame Word-SubUNet to see if the sentence-level recognition performance benefits from both sources of information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Combining SubUNets: End-to-End Continuous Sign Language Recognition from Multiple Channels</head><p>So far we have trained three SubUNets: A hand SubUNet that predicts hand shape sequences from hand patches and two word SubUNets which perform sentence-level sign language recognition from either full frame or hand patch sequences. Although our experiments have demonstrated that a word level SubUNet is able to make use of the additional information from the Full Frames, it is unknown how novel this information is compared to the hand shape network. Therefore, we combine pre-trained networks, trained at the word level, for both Hand SubUNets and and Full Frame SubUNets to create a larger network that takes advantage of both sources of information.</p><p>Due to the asynchronous nature of the sign language modalities, we put the combined information of the Full Frame Word SubUNet and Hand SubUNet through an additional BLSTM layer. This models the temporal relationship between the modalities. As most datasets don't have annotations for both the hand shape and signs (making it impossible to jointly train all streams), we investigate the effects of fixing the weights for pre-trained SubUNets. By doing so we hope to determine how much the SubUNets benefits from tuning themselves to the new compound architecture. Therefore we train two networks. In the first experiment ("Fixed"), we pre-train the two SubUNets depicted in <ref type="figure" target="#fig_2">Figure 4</ref>. Combining the two networks at their final BLSTM layers into a 3rd BLSTM, IP and CTC layer and therefore maintaining 3 loss layers. In the figure, blue blocks are pre-trained and fixed, green blocks are removed, while white block are trained for the task. In the second variant ("Not Fixed"), all weights are trained using the gradients produced by all three loss layers.  This experiment provides two very important insights into combining SubUNets. Firstly, as shown in <ref type="table" target="#tab_9">Table 7</ref>, allowing the SubUNets to tune themselves to the new network structure by training end-to-end yields significantly improved results. Secondly, and more interestingly, the combination of the different SubUNet modalities outperforms all previous experiments using isolated SubUNets. This reinforces the idea that guided subunit learning is extremely valuable in sequence-to-sequence recognition.</p><p>For our final experiment, we evaluate how much the expert knowledge embedded within the SubUNets is contributing to the system. The inspiration behind this expert knowledge, comes from how humans teach and learn similar representations. For example, both linguists, and students learning sign, would classify the hand shape related to a sign as being a distinct but related entity to the motion of that that sign.</p><p>We investigate this using the best performing network from the previous section (the "Not Fixed" combination of Hand SubUNet and Full Frame Word SubUNet from <ref type="figure" target="#fig_2">Figure 4</ref>). As we trained this network, the additional supervisory information of the Hand SubUNet forces the hand patches stream to learn hand shapes explicitly, mimicking its human counterpart, which we named Expert SubUNets.</p><p>For comparison we instead leave the network free to train but replace the hand shape CTC with another Word CTC (as in <ref type="figure" target="#fig_3">Figure 5</ref>), which we named Generic SubUNets. In this case the network receives the same level of supervision, and one could argue that the supervision is more specific to the task at hand. The network is given the freedom to learn any intermediate representation it wishes in order to solve the overarching problem. However, as <ref type="table">Table 8</ref> shows, forcing networks to learn expert knowledge representations actually results in better performance on sentence-level sign language recognition. Although, both of the networks have a similar WER on the development set, the architecture that explicitly learns the intermediate hand shape representations performs better on the test set. Furthermore, the number of deletion and insertions is much more balanced for the network that mimics human learning. This implies that it is also performing better at the alignment task. Therefore, in light of these experiments we can conclude that training deep neural networks using SubUNets that explicitly model expert knowledge results in better constrained and more general solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Decoding of networks trained with CTC loss</head><p>In this final section we explore the effects of different decoding and post-processing techniques during sequenceto-sequence prediction. Previously, the CTC outputs were decoded for prediction by performing a beam search on the sum of the probabilities over all possible alignment paths (as proposed by <ref type="bibr" target="#b15">[16]</ref>). In other words it attempts to choose the best label for each frame, marginalised over all previous and future labellings. We refer to this approach as 'Full Sum' decoding. We contrast this against a greedy 'Viterbi' decoding which only considers the maximum path. <ref type="table" target="#tab_12">Table 9</ref> compares the two decoding strategies, showing that Full Sum decoding outperforms its counterpart by 0.5% points on dev and 0.6% points on the test set. However, this gain comes at a price of much higher computational complexity.  The significant impact of this change in post-processing raises an interesting question: Are there more advanced post-processing techniques that could further improve the performance of the system? We therefore apply an additional pre-learnt language model during decoding, similar to that proposed by <ref type="bibr" target="#b27">[28]</ref>. <ref type="figure" target="#fig_4">Figure 6</ref> shows the difference between the three tested decoding schemes. First the CTC topology, which binds a class posterior state to a tied blank state. Second, an intermediate topology referred to as LM, where the CTC style segments are joined with optional intermediate silence states that do not belong to the classes, but share the same distribution as the blank states. Finally, a HMM inspired topology is depicted, where two class states are bound together (sharing the same probability distribution) with optional tied silence states in between. <ref type="table">Table 10</ref> summarises the decoding results employing the different topologies. We see that the HMM topology with a language model and the intermediate silence state outperforms the standard CTC topology by nearly 3% on development set. The table also shows that our proposed technique performs comparably to previous state-of-the-art research on this dataset, with the significant advantages that there is no need for a separate alignment step, the system can be trained end-to-end, and it extends easily to additional SubUNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Discussion</head><p>In this paper we have proposed SubUNets, a novel deep learning architecture that learns intermediate representations to guide the learning procedure. We have applied the proposed method to the challenging task of Continuous Sign Language Recognition.  As hands are one of the most informative channels of a sign we have trained a hand shape recognition network using the SubUNet architecture, that learns to predict hand shape sequences from a video. We trained and evaluated our hand shape recognizing SubUNet on the One Million Hands dataset <ref type="bibr" target="#b26">[27]</ref> and reported state-of-the-art frame level accuracy (Top 1: 80.3%, Top 5: 93.9%), improving on previous research by around 30%.</p><p>Our experiments on Continuous Sign Language recognition show that having SubUNets that learn intermediate representations helps the network generalize better. Moreover we have thoroughly evaluated the effects of different decoding schemes and have seen the benefits of extra post processing, reporting competitive results to the state-of-the-art, without the need for an explicit segmentation of the signs.</p><p>As future work, it would be interesting to investigate hierarchical SubUNets, where each expert system is comprised of lower level expert systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Hand SubUNet: End-to-end Hand Shape Recognition network from sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Word SubUNet: End-to-end Sentence-level Continuous Sign Language Recognition Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Combination of Hand SubUNets and Full Frame Word SubUNets. Blue and Green Blocks represent the weights that are going to be fixed and the weights that are going to omitted in the fixed setup, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Combining Word SubUNets that model Full Frame and Hand Patches to sentence-level sign language.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Showing three different model topologies used for decoding. Skip paths have only been illustrated on the first model segments. Round circles refer to single (class) states, whereas squares mean tied states (blank or silence). The top row shows the standard CTC topology. The middle row shows the CTC topology with optional silence insertions in between class symbols. The last row shows a HMM topology, where the same class distribution is shared across two states in a segment and optional silence can be inserted in between class symbols.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Statistics of the One Million Hands dataset which con-
tains cropped hand patches from existing Danish, New Zealand 
('NZ') and German('DGS' -RWTH-PHOENIX-Weather-2014) 
sign language datasets. See [27] for more details. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Summary of RWTH-PHOENIX-Weather-2014 dataset.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 .</head><label>5</label><figDesc>Evaluation of having a deeper network.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 7 .</head><label>7</label><figDesc>Evaluation of fixing SubUNets weights or allowing them to train end-to-end.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head></head><label></label><figDesc>Dev Test del/ins WER del/ins WER Generic SubUNets 27.1/1.6 43.0 26.8/1.5 42.6 Expert SubUNets 19.6/2.7 43.1 18.7/2.9 42.1 Table 8. Comparison of Generic and Expert SubUNet systems with other approaches.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head></head><label></label><figDesc>Decoding Dev Test del/ins WER del/ins WER Viterbi 20.4/2.9 43.6 19.4/2.9 42.7 Full Sum 19.6/2.7 43.1 18.7/2.9 42.1</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 9 .</head><label>9</label><figDesc>Impact of the Full Sum and Viterbi decoding variants.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head></head><label></label><figDesc>HMM-LM 14.6/4.0 40.8 14.3/4.0 40.7 [26] 23.6/4.0 57.3 23.1/4.4 55.6 [27] 16.3/4.6 47.1 15.2/4.6 45.1 Table 10. Evaluation of different decoding schemes and comparison with previous research.</figDesc><table>Model Structure 
Dev 
Test 
del/ins WER del/ins WER 
CTC 
19.6/2.7 43.1 18.7/2.9 42.1 
LM 
12.3/6.2 42.5 15.2/4.5 42.2 
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/BVLC/caffe/pull/4681/ 3 https://github.com/neccam/SubUNets</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">It is important to note that sign languages do not contain a direct equivalent to sentences, the term is used here to clarify the concept to the reader and refers to a meaningful phrase which consists of a sequence of continuous signs.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was funded by the SNSF Sinergia project Scalable Multimodal Sign Language Technology for Sign Language Learning and Assessment (SMILE)" grant agreement number CRSII2 160811. We would also like to thank NVIDIA for their GPU grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Large-Scale Machine Learning on Heterogeneous Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01599</idno>
		<title level="m">Lipnet: Sentence-level lipreading</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sign Language Recognition for Assisting the Deaf in Hospitals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgöz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kındıroglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akarun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Human Behavior Understanding</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BosphorusSign: A Turkish Sign Language Recognition Corpus in Health and Finance Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kindiroglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karabuklu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kelepir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ozsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akarun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Language Resources and Evaluation (LREC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Return of the Devil in the Details: Delving Deep into Convolutional Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">On the Properties of Neural Machine Translation: EncoderDecoder Approaches. Syntax, Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lip Reading Sentences in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning Signs from Subtitles: A Weakly Supervised Approach to Sign Language Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sign Language Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Analysis of Humans</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long-Term Recurrent Convolutional Networks for Visual Recognition and Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Extensions of the Sign Language Recognition and Translation Corpus RWTH-PHOENIX-Weather</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellgardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Language Resources and Evaluation (LREC)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Novel Connectionist System for Unconstrained Handwriting Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bertolami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Speech Recognition with Deep Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Connectionist Temporal Modeling for Weakly Supervised Action Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">SqueezeNet: AlexNet-level Accuracy with 50x Fewer Parameters and &lt;1MB Model Size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional Architecture for Fast Feature Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent Continuous Translation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<title level="m">Continuous Sign Language Recognition: Towards Large Vocabulary Statistical Recognition Systems Handling Multiple Signers. Computer Vision and Image Understanding (CVIU)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep Hand: How to Train a CNN on 1 Million Hand Images When Your Data Is Continuous and Weakly Labelled</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Sign: Hybrid CNN-HMM for Continuous Sign Language Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zargaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kristoffersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Troelsgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Hardell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hardell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Niemela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sandholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toft</surname></persName>
		</author>
		<title level="m">Ordbog over Dansk Tegnsprog</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The Online Dictionary of New Zealand Sign Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mckee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mckee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pivac</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Neural Machine Translation and Sequence-tosequence Models: A Tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01619</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the difficulty of training Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Beyond Temporal Pooling: Recurrence and Temporal Convolutions for Gesture Recognition in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pigou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Herreweghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dambre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A Survey on Vision-based Human Action Recognition. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning Spatiotemporal Features with 3D Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Isolated Sign Language Recognition with Grassmann Covariance Matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Accessible Computing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Show</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
