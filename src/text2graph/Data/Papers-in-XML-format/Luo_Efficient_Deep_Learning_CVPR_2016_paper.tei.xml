<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Deep Learning for Stereo Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
							<email>wenjie@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
							<email>aschwing@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
							<email>urtasun@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Deep Learning for Stereo Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reconstructing the scene in 3D is key in many applications such as robotics and self-driving cars. To ease this process, 3D sensors such as LIDAR are commonly employed. Utilizing cameras is an attractive alternative, as it is typically a more cost-effective solution. However, despite decades of research, estimating depth from a stereo pair is still an open problem. Dealing with cclusions, large saturated areas and repetitive patterns are some of the remaining challenges.</p><p>Many approaches have been developed that try to aggregate information from local matches. Cost aggregation, for example, averages disparity estimates in a local neighborhood. Similarly, semi-global block matching and Markov random field based methods combine pixelwise predictions and local smoothness into an energy function. However all these approaches employ cost functions that are hand crafted, or where only a linear combination of features is learned from data.</p><p>In the past few years we have witnessed a revolution in high-level vision, where deep representations are learned directly from pixels to solve many scene understanding tasks with unprecedented performance. These approaches currently are the state-of-the-art in tasks such as detection, segmentation and classification.</p><p>Very recently, convolutional networks have also been exploited to learn how to match for the task of stereo estimation <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b28">28]</ref>. Current approaches learn the parameters of the matching network by treating the problem as binary classification; Given a patch in the left image, the task is to predict if a patch in the right image is the correct match. While <ref type="bibr" target="#b29">[29]</ref> showed great performance in challenging benchmarks such as KITTI <ref type="bibr" target="#b11">[11]</ref>, it is computationally very expensive, requiring a minute of computation in the GPU. This is due to the fact that they exploited a siamese architecture followed by concatenation and further processing via a few more layers to compute the final score.</p><p>In contrast, in this paper we propose a matching network which is able to produce very accurate results in less than a second of GPU computation. Towards this goal, we exploit a product layer which simply computes the inner product between the two representations of a siamese architecture. We train our network by treating the problem as multi-class classification, where the classes are all possible disparities. This allows us to get calibrated scores, which result in much better matching performance when compared to <ref type="bibr" target="#b29">[29]</ref>. We refer the reader to <ref type="figure" target="#fig_0">Fig. 1</ref> for an illustration of our approach. We demonstrate the effectiveness of our approach on the challenging KITTI benchmark and show competitive results when exploiting smoothing techniques. Our code and data can be fond online at: http://www.cs.toronto.edu/ deepLowLevelVision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Over the past decades many stereo algorithms have been developed. Since a discussion of all existing approaches would exceed the scope of this paper, we restrict ourselves mostly to a subset of recent methods that exploit learning and can mostly be formulated as energy minimization.</p><p>Early learning based approaches focused on correcting an initially computed matching cost <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17]</ref>. Learning has been also utilized to tune the hyper-parameters of the energy-minimization task. Among the first to train these hyper-parameters were <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b19">19]</ref>, which investigated different forms of probabilistic graphical models.</p><p>Slanted plane models model groups of pixels with slanted 3D planes. They are very competitive in autonomous driving scenarios, where robustness is key. They have a long history, dating back to <ref type="bibr" target="#b1">[2]</ref> and were shown to be very successful on the Middleburry benchmark <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b24">24]</ref> as well as on KITTI <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27]</ref>.</p><p>Holistic models which solve jointly many tasks have also been explored. The advantage being that many tasks in low-level and high level-vision are related, and thus one can benefit from solving them together. For example <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b13">13]</ref> jointly solved for stereo and semantic segmentation. Guney and Geiger <ref type="bibr" target="#b12">[12]</ref> investigated the utility of high-level vision tasks such as object recognition and semantic segmentation for stereo matching.</p><p>Estimating the confidence of each match is key when employing stereo estimates as a part of a pipeline. Learning methods were successfully applied to this task, e.g., by combining several confidence measures via a random forest classifier <ref type="bibr" target="#b14">[14]</ref>, or by incorporating random forest predictions into a Markov random field <ref type="bibr" target="#b23">[23]</ref>.</p><p>Convolutional neural networks(CNN) have been shown to perform very well on high-level vision tasks such as image classification, object detection and semantic segmentation. More recently, CNNs have been applied to low-level vision tasks such as optical flow prediction <ref type="bibr" target="#b10">[10]</ref>. In the context of stereo estimation, <ref type="bibr" target="#b29">[29]</ref> utilize CNN to compute the matching cost between two image patches. In particular, they used a siamese network which takes the same sized left and right image patches with a few fully-connected layers on top to predict the matching cost. They trained the model to minimize a binary cross-entropy loss. In similar spirit to <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b28">[28]</ref>  catenating left and right image patches as different channels works best, at the cost of being very slow. Our work is most similar to <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b28">28]</ref> with two main differences. First, we propose to learn a probability distribution over all disparity values using a smooth target distribution. As a consequence we are able to capture correlations between the different disparities implicitly. This contrasts a <ref type="bibr" target="#b29">[29]</ref> which performs independent binary predictions on image patches. Second, on top of the convolution layers we use a simple dot-product layer to join the two branches of the network. This allows us to do a orders of magnitude faster computation. We note that in concurrent work unpublished at the time of submission of our paper <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b7">7]</ref> also introduced a dot-product layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Learning for Stereo Matching</head><p>We are interested in computing a disparity image given a stereo pair. Throughout this paper we assume that the image pairs are rectified, thus the epipolar lines are aligned with the horizontal image axis. Let y i ∈ Y i represent the disparity associated with the i-th pixel, and let |Y i | be the cardinality of the set (typically 128 or 256). Stereo algorithms estimate a 3-dimensional cost volume by computing for each pixel in the left image a score for each possible disparity value. This is typically done by exploiting a small patch around the given pixel and a simple hand-crafted representation of each patch. In contrast, in this paper we exploit convolutional neural networks to learn how to match.</p><p>Towards this goal, we utilize a siamese architecture, where each branch processes the left or right image respectively. In particular, each branch takes an image as input, and passes it through a set of layers, each consisting of a spatial convolution with a small filter-size (e.g., 5 × 5 or 3 × 3), followed by a spatial batch normalization and a rectified linear unit (ReLU). Note that we remove the ReLU from the last layer in order to not loose the information encoded in the negative values. In our experiments we exploit different number of filters per layer, either 32 or 64 and share the parameters between the two branches.</p><p>In contrast to existing approaches which exploit concatenation followed by further processing, we use a product layer which simply computes the inner product between the two representations to compute the matching score. This simple operation speeds up the computation significantly. We refer the reader to <ref type="figure" target="#fig_1">Fig. 2</ref> which depicts an example of a 4-layer network with filter-size 3 × 3, which results in a receptive field of size 9 × 9.</p><p>Training: We use small left image patches extracted at random from the set of pixels for which ground truth is available to train the network. This strategy provides us with a diverse set of examples and is memory efficient. In particular, each left image patch is of size equivalent to the size of our network's receptive field. Let (x i , y i ) be the image coordinates of the center of the patch extracted at random from the left image, and let d xi,yi be the corresponding ground truth disparity. We use a larger patch for the right image which expands both the size of the receptive field as well as all possible disparities (i.e., displacements). The output of the two branches of the siamese network is hence a single 64-dimensional representation for the left branch, and |Y i | × 64 for the right branch. These two vectors are then passed as input to an inner-product layer which computes a score for each of the |Y i | disparities. This allow us to compute a softmax for each pixel over all possible disparities.</p><p>During training we minimize cross-entropy loss with respect to the weights w that parameterize the network</p><formula xml:id="formula_0">min w i,yi p gt (y i ) log p i (y i , w).</formula><p>Since we are interested in a 3-pixel error metric we use a smooth target distribution p gt (y i ), centered around the ground-truth y</p><formula xml:id="formula_1">GT i , i.e., p gt (y i ) =        λ 1 if y i = y GT i λ 2 if |y i − y GT i | = 1 λ 3 if |y i − y GT i | = 2 0 otherwise .</formula><p>For this paper we set λ 1 = 0.5, λ 2 = 0.2 and λ 3 = 0.05. Note that this contrasts cross entropy for classification, where p gt (y i ) is a delta function placing all its mass on the annotated groundtruth configuration.</p><p>We train our network using stochastic gradient descent back propagation with AdaGrad <ref type="bibr" target="#b8">[8]</ref>. Similar to momentbased stochastic gradient descent, AdaGrad adapts the gradient based on historical information. Contrasting moment based methods it emphasizes rare but informative features. We adapt the learning rates every few thousand iterations as detailed in the experimental section.</p><p>Testing: Contrasting the training procedure where we compose a mini-batch by randomly sampling locations from different training images, we can improve the speed performance during testing. Our siamese network computes a 64-dimensional feature representation for every pixel i.</p><p>To efficiently obtain the cost volume, we compute the 64-dimensional representation only once for every pixel i, and during computation of the cost volume we re-use its values for all disparities that involve this location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Smoothing Deep Net Outputs</head><p>Given the unaries obtained with a CNN, we compute predictions for all disparities at each image location. Note that simply outputting the most likely configuration for every pixel is not competitive with modern stereo algorithms, which exploit different forms of cost aggregation, post processing and smoothing. This is particularly important to deal with complex regions with occlusions, saturation or repetitive patterns.</p><p>Over the past decade many different MRFs have been proposed to solve the stereo estimation problem. Most approaches define each random variable to be the disparity of a pixel, and encode smoothness between consecutive or nearby pixels. An alternative approach is to segment the image into regions and estimate a slanted 3D plane for each region. In this paper we investigate the effect of different smoothing techniques. Towards this goal, we formulate stereo matching as inference in several different Markov random fields (MRFs), with the goal of smoothing the matching results produced by our convolutional neural network. In particular, we look into cost aggregation, semiglobal block matching as well as the slanted plane approach of <ref type="bibr" target="#b27">[27]</ref> as means of smoothing. In the following we briefly review these techniques.</p><p>Cost aggregation: We exploited a very simple cost aggregation approach, which simply performs average pooling over a window of size 5 × 5.</p><p>Semi global block matching: Semi-global block matching augments the unary energy term obtained from convolutional neural nets by introducing additional pairwise potentials which encourage smooth disparities. Specifically,</p><formula xml:id="formula_2">E(y) = N i=1 E i (y i ) + (i,j)∈E E i,j (y i , y j ),</formula><p>where E refers to 4-connected grid and the unary energy E i (y i ) is the output of the neural net.  We define the pairwise energy as</p><note type="other">&gt; 2 pixel &gt; 3 pixel &gt; 4 pixel &gt; 5 pixel End-Point Runtime(s) Non</note><formula xml:id="formula_3">E i,j (y i , y j ) =    0 if y i = y j c 1 if |y i − y j | = 1 c 2 otherwise</formula><p>, with variable constants c 1 &lt; c 2 . We follow the approach of <ref type="bibr" target="#b29">[29]</ref>, where c 1 and c 2 is decreased if there is strong evidence for edges at the corresponding locations in either the left or the right image. We refer the reader to their paper for more details.</p><p>Slanted plane: To construct a depth-map, this approach performs block-coordinate descent on an energy involving appearance, location, disparity, smoothness and boundary energies. More specifically, we first over-segment the image using an extension of the SLIC energy <ref type="bibr" target="#b0">[1]</ref>. For each superpixel we then compute slanted plane estimates <ref type="bibr" target="#b27">[27]</ref> which should adhere to the depth-map evidence obtained from the convolutional neural network. We then iterate these two steps to minimize an energy function which takes into account appearance, location, disparity, smoothness and boundary energies. We refer the interested reader to <ref type="bibr" target="#b27">[27]</ref> for details.</p><p>Sophisticated post-processing: In [30] a three-step postprocessing is designed to perform interpolation, subpixel enhancement and a refinement. The interpolation step resolves conflicts between the disparity maps computed for the left and right images by performing a left-right consistency check. Subpixel enhancement fits a quadratic function to neighboring points to obtain an enhanced depth-map. To smooth the disparity map without blurring edges, the final refinement step applies a median filter and a bilateral filter. We only use the interpolation step as we found the other two don't always further improve performance in our case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Evaluation</head><p>We evaluate the performance of different convolutional neural network structures and different smoothing techniques on the KITTI 2012 <ref type="bibr" target="#b11">[11]</ref> and 2015 <ref type="bibr" target="#b20">[20]</ref> datasets. Before training we normalize each image to have zero mean and standard deviation of one. We initialize the parameters of our networks using a uniform distribution. We employ the AdaGrad algorithm <ref type="bibr" target="#b8">[8]</ref> and use a learning rate of 1e −2 . The learning rate is decreased by a factor of 5 after 24k iterations and then further decreased by a factor of 5 every 8k iterations. We use a batch size of 128. We trained the network for 40k iterations which takes around 6.5 hours on an NVIDIA Titan-X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">KITTI 2012 Results</head><p>The KITTI 2012 dataset contains 194 training and 195 test images. To compare the different network architectures described below, we use as training set 160 image pairs randomly selected, and the remaining 34 image pairs as our validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Matching Networks:</head><p>We first show our network's matching ability and compare it to existing matching networks <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b30">30]</ref>. In this experiment we do not employ smoothing or post processing, but just utilize the raw output of the network. Following KITTI, we employ the percentage of pixels with disparity errors larger than a fixed threshold as well as end-point error as metrics. We refer to our architecture as 'Ours <ref type="bibr" target="#b19">(19)</ref>.' It consists of 9 layers of 3 × 3 convolutions resulting in a receptive field size of 19 × 19 pixels. As shown in <ref type="table">Table 1</ref>, our 9-layer network achieves a 3-pixel non-occluded stereo error of 8.61% after only 0.14 seconds of computation. In contrast, <ref type="bibr" target="#b29">[29]</ref> obtains 12.99% after a significantly longer time of 20.13 seconds.   Their faster version <ref type="bibr" target="#b30">[30]</ref> requires 0.20 second which results in a much lower performance of 15.53%. As shown in the table, our network outperforms previously designed convolutional neural networks by a large margin on all criteria.</p><p>Smoothing Comparison: Next, we evaluate different algorithms for smoothing and post-processing when employing different network sizes. In particular, we evaluate cost aggregation, semi-global block matching and slanted plane smoothing, which are described in the previous section. We also experiment with different receptive field sizes for our network, which corresponds to changing the depth of our architecture. As before, we use 'Ours(n)' to refer to our architecture with a receptive field size of n × n pixel. We investigated n = 9, 19, 29, 37. We use kernels of size 3 × 3 for n = 9 and n = 19, while the kernels were of size 5 × 5</p><p>for n = 39. To achieve a receptive field of 29 we use 5 layers of 5 × 5 and 4 layers of 3 × 3. This keeps the number of layers bounded to 9.</p><p>As shown in <ref type="table" target="#tab_1">Table 2</ref>, networks with different receptive field sizes result in errors ranging from 6.61% (for n = 37 to 16.69% for n = 9. The corresponding error for <ref type="bibr" target="#b30">[30]</ref> is 12.99% for their slow and more accurate model, and 15.53% for their fast model. After smoothing, the differences in stereo error achieved by the networks are no longer significant. All of them achieve an error slightly less than 4%. Since depth-maps tend to be very smooth we think that an aggressive smoothing helps to flatten the noisy unary potentials. In addition we observe that utilizing simple cost aggregation to encourage local smoothness further helps to slightly improve the results. This is due to the fact that  such techniques eliminate small isolated noisy areas. While the post-processing proposed in <ref type="bibr" target="#b30">[30]</ref> focuses on occlusions and sub-pixel enhancement, <ref type="bibr" target="#b27">[27]</ref> adds extra robustness to non-textured areas by fitting slanted planes. Both methods improve the semi-global block matching output slightly. Our best performing model combination achieves a 3 pixel stereo error of 3.64%.</p><note type="other">&gt; 2 pixel &gt; 3 pixel &gt; 4 pixel &gt; 5 pixel End-Point Runtime(s) Non</note><p>Comparison to State-of-the-art: To evaluate the test set performance we trained our model having a receptive field of 19 pixels, i.e., "Ours <ref type="bibr" target="#b19">(19)</ref>," on the entire training set. The obtained test set performance is shown in <ref type="table" target="#tab_3">Table 3</ref>. Since we did not particularly focus on finding a good combination of smoothness and unaries, our performance is slightly below the current state-of-the-art.</p><p>Qualitative Analysis: Examples of stereo estimates by our approach are depicted in <ref type="figure" target="#fig_2">Fig. 3</ref>. We observe that our approach suffers from texture-less regions as well as regions with repetitive patterns such as fences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">KITTI 2015 Results</head><p>The KITTI 2015 dataset consists of 200 training and 200 test images. Instead of the gray-scale images used for the KITTI 2012 dataset we directly process the RGB data. To compare the different network architectures, we randomly selected 160 image pairs as training set and use the remaining 40 image pairs for validation purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Matching Networks:</head><p>We first show our network's matching ability and compare it to existing matching networks <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b30">30]</ref>. In this experiment we do not employ smoothing or post processing, but just utilize the raw output of the network. We refer to our architecture via 'Ours(37).' It consists of 9 layers of 5 × 5 convolutions resulting in a receptive field size of 37 × 37 pixels. As shown in <ref type="table">Table 4</ref>, our 9-layer network achieves a 3-pixel stereo error of 7.23% after only 0.34 seconds of processing time, whereas <ref type="bibr" target="#b29">[29]</ref> obtains 12.45% after a significantly longer processing time of 22.76 seconds. Their faster version <ref type="bibr" target="#b30">[30]</ref> requires 0.21 seconds but results in a much lower performance of 14.96% when compared to our approach. Again, our network outperforms previously designed convolutional neural networks by a large margin on all criteria.</p><p>Smoothing Comparison: <ref type="table" target="#tab_5">Table 5</ref> shows results of applying different post processing techniques to different network architectures. As when processing KITTI 2012 images, we observe that the difference in network performance vanishes after applying smoothing techniques. Our best performing combination achieves a 3-pixel error of 4.14% on the validation set.</p><p>Influence of Depth and Filter Size Next, we evaluate the influence of the depth and receptive field size of our CNNs in terms of matching performance and running time. <ref type="figure" target="#fig_3">Fig. 4a</ref> shows matching performance as a function of the networks' receptive field size. We observe that an increasing receptive field size achieves better performance. However, when the receptive field is very large, the improvement is subtle, since the network starts to overlook the details of small objects and depth discontinuities. Our findings are consistent for both non-occluded and for all pixel. As shown in <ref type="figure" target="#fig_3">Fig. 4b</ref>, the running time and number of parameters are highly correlated. Note that models with larger receptive field do not necessarily have more parameters since the amount of trainable weights also depends on the number of filters and the channel size of each convolutional layer.</p><p>Comparison to State-of-the-art: To evaluate the test set performance, we choose the best model with current smoothing techniques, which has a receptive field of 37 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All/All</head><p>All/Est Noc/All Noc/Est Runtime D1-bg D1-fg D1-all D1-bg D1-fg D1-all D1-bg D1-fg D1-all D1-bg D1-fg D1-all (s) MBM <ref type="bibr" target="#b9">[9]</ref> 4 pixel, i.e., "Ours(37)." The obtained test set performance is shown in <ref type="table">Table 6</ref>. We achieve on-par results in significantly less time.</p><p>Qualitative Results: We provide results from the test set in <ref type="figure">Fig. 5</ref>. Again, we observe that our algorithm suffers from texture-less regions as well as regions with repetitive patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Convolutional neural networks have been recently shown to perform extremely well for stereo estimation. Current architectures rely on siamese networks which exploit concatenation follow by further processing layers, requiring a minute on the GPU to process a stereo pair. In contrast, in this paper we have proposed a matching network which is able to produce very accurate results in less than a second of GPU computation. Our key contribution is to replace the concatenation layer and subsequent processing layers by a single product layer, which computes the score. We trained the networks using cross-entropy over all possible disparities. This allows us to get calibrated scores, which result in much better matching performance when compared to existing approaches. We have also investigated the effect of different smoothing techniques to further improve performance. In the future we plan to utilize our approach for other low-level vision tasks such as optical flow. We also plan to build smoothing techniques that are tailored to our approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: To learn informative image patch representations we employ a siamese network which extracts marginal distributions over all possible disparities for each pixel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our four-layer siamese network architecture which has a receptive field size of 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: KITTI 2012 test set: (left) original image, (center) stereo estimates, (right) stereo errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Evaluation of stereo error (a), runtime and number of parameters (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>-Occ All Non-Occ All Non-Occ All Non-Occ All Non-Occ All MC-CNN-acrt [29] 15.02 16.92 12.99 14.93 12.04 13.98 11.38 13.32 4.39 px 5.21 px 20.13 MC-CNN-fast [29] 17.72 19.56 15.53 17.41 14.41 16.31 13.60 15.51 4.77 px 5.Comparison of the output of the matching network across different error metrics on the KITTI 2012 validation set.</figDesc><table>63 px 
0.20 
Ours(19) 
10.87 12.86 
8.61 
10.64 
7.62 
9.65 
7.00 
9.03 3.31 px 4.2 px 
0.14 
Table 1: Unary CA SGM[30] Post[30] Slanted[27] Ours(9) Ours(19) Ours(29) Ours(37) MC-CNN-acrt[29] MC-CNN-fast[29] 

16.69 
8.61 
7.64 
6.61 
12.99 
15.53 

12.14 
7.48 
6.86 
6.09 
6.32 
-

4.57 
3.99 
4.12 
3.96 
3.34 
4.53 

4.11 
3.73 
3.99 
3.88 
3.22 
3.73 

3.96 
3.64 
3.81 
3.83 
3.36 
3.83 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different smoothing methods. The table illustrates non-occluded 3 pixel error on the KITTI 2012 validation set.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Comparison to stereo state-of-the-art on the test set of the KITTI 2012 benchmark.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>-Occ All Non-Occ All Non-Occ All Non-Occ All Non-Occ AllComparison of the output of the matching network across different error metrics on the KITTI 2015 validation set.</figDesc><table>MC-CNN-acrt [29] 

15.20 16.83 12.45 14.12 11.04 12.72 10.13 11.80 4.01 px 4.66 px 
22.76 

MC-CNN-fast [29] 

18.47 20.04 14.96 16.59 13.18 14.83 12.02 13.67 4.27 px 4.93 px 
0.21 
Ours(37) 
9.96 
11.67 
7.23 
8.97 
5.89 
7.62 
5.04 
6.78 1.84 px 2.56 px 
0.34 
Table 4: Unary CA SGM[30] Post[30] Slanted[27] Ours(9) Ours(19) Ours(29) Ours(37) MC-CNN-acrt[29] MC-CNN-fast[29] 

15.25 
8.95 
7.23 
7.13 
12.45 
14.96 

11.43 
8.00 
6.60 
6.58 
7.78 
-

5.18 
4.74 
4.62 
4.73 
3.48 
5.05 

4.41 
4.23 
4.31 
4.38 
3.10 
4.74 

4.25 
4.20 
4.14 
4.19 
3.11 
4.79 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison of smoothing methods using different CNN output. The table illustrates the non-occluded 3 pixel error 
on the KITTI 2015 validation set. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>: Comparison to stereo state-of-the-art on the test set of KITTI 2015 benchmark.</figDesc><table>.69 13.05 6.08 
4.69 13.05 6.08 
4.33 12.12 5.61 
4.33 12.12 5.61 
0.13 
SPS-St [27] 
3.84 12.67 5.31 
3.84 12.67 5.31 
3.50 11.61 4.84 
3.50 11.61 4.84 
2 
MC-CNN [30] 2.89 8.88 3.89 
2.89 8.88 3.88 
2.48 7.64 3.33 
2.48 7.64 3.33 
67 
Displets v2 [12] 3.00 5.56 3.43 
3.00 5.56 3.43 
2.73 4.95 3.09 
2.73 4.95 3.09 
265 
Ours(37) 
3.73 8.58 4.54 
3.73 8.58 4.54 
3.32 7.44 4.00 
3.32 7.44 4.00 
1 
Table 6</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This work was partially supported by ONR-N00014-14-1-0232 and NSERC. We would like to thank NVIDIA for supporting our research by donating GPUs and Shenlong Wang for help with the figures.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiway cut for stereo and motion with slanted surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A layered stereo matching algorithm using image segmentation and global visibility constraints. ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gelautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extracting 3D sceneconsistent object proposals and depth from stereo images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Surface stereo with soft segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object stereo -joint stereo matching and object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Figure 5: KITTI 2015 test set: (left) original image, (center) stereo estimates, (right) stereo errors</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A deep visual correspondence embedding model for stereo matching costs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A multi-block-matching approach for stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Einecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eggert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning Optical Flow with Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbaş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flownet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Displets: Resolving stereo ambiguities using object knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Direction Matters: Depth Estimation with a Surface Normal Classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Haene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladický</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ensemble learning for confidence measures in stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haeusler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kondermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Segment-based stereo matching using belief propagation and a self-adapting dissimilarity measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sormann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A method for learning matching errors for stereo computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stereo matching via learning multiple experts behaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pulling Things out of Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladický</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning for stereo vision using the structured support vector machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object Scene Flow for Autonomous Vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning conditional random fields for stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Middlebury stereo vision page</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<ptr target="http://www.middlebury.edu/stereo" />
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to detect ground control points for improving the accuracy of stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spyropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mordohai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A region based stereo matching algorithm using cooperative optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-G</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Continuous markov random fields for robust stereo estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust monocular epipolar flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient joint segmentation, occlusion labeling, stereo and flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Computing the stereo matching cost with a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Žbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.05970</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Estimating optimal parameters for MRF stereo from a single image pair. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
