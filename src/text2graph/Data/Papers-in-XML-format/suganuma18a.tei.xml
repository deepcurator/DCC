<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting the Potential of Standard Convolutional Autoencoders for Image Restoration by Evolutionary Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Suganuma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mete</forename><surname>Ozay</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Okatani</surname></persName>
						</author>
						<title level="a" type="main">Exploiting the Potential of Standard Convolutional Autoencoders for Image Restoration by Evolutionary Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Researchers have applied deep neural networks to image restoration tasks, in which they proposed various network architectures, loss functions, and training methods. In particular, adversarial training, which is employed in recent studies, seems to be a key ingredient to success. In this paper, we show that simple convolutional autoencoders (CAEs) built upon only standard network components, i.e., convolutional layers and skip connections, can outperform the state-of-the-art methods which employ adversarial training and sophisticated loss functions. The secret is to search for good architectures using an evolutionary algorithm. All we did was to train the optimized CAEs by minimizing the 2 loss between reconstructed images and their ground truths using the ADAM optimizer. Our experimental results show that this approach achieves 27.8 dB peak signal to noise ratio (PSNR) on the CelebA dataset and 33.3 dB on the SVHN dataset, compared to 22.8 dB and 19.0 dB provided by the former state-of-the-art methods, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of image restoration, which is to recover a clean image from its corrupted version, is usually an ill-posed inverse problem. In order to resolve or mitigate its ill-posedness, researchers have incorporated image priors such as edge statistics <ref type="bibr" target="#b7">(Fattal, 2007)</ref>, total variation <ref type="bibr" target="#b33">(Perrone &amp; Favaro, 2014)</ref>, and sparse representation <ref type="bibr" target="#b0">(Aharon et al., 2006;</ref><ref type="bibr" target="#b45">Yang et al., 2010)</ref>, which are built on intuition or statistics of natural images. Recently, learning-based methods which use convolutional neural networks (CNNs) <ref type="bibr" target="#b17">(LeCun et al., 1998;</ref><ref type="bibr" target="#b15">Krizhevsky et al., 2012)</ref> were introduced to overcome 1 RIKEN, Tokyo, Japan 2 Tohoku University, Sendai, Japan.</p><p>Correspondence to: Masanori Suganuma &lt;sug-anuma@vision.is.tohoku.ac.jp&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the 35</head><p>th International Conference on Machine Learning, <ref type="bibr">Stockholm, Sweden, PMLR 80, 2018</ref><ref type="bibr">. Copyright 2018</ref> by the author(s).</p><p>the limitation of these hand-designed or simple priors, and have significantly improved the state-of-the-art.</p><p>In these studies, researchers have approached the problem mainly from two directions. One is to design new network architectures and/or new loss functions. The other is to develop new training methods, such as the employment of adversarial training <ref type="bibr" target="#b8">(Goodfellow et al., 2014)</ref>. Later studies naturally proposed more complicated architectures to improve the performance of earlier architectures. <ref type="bibr" target="#b23">Mao et al. (2016)</ref> proposed an architecture consisting of a chain of symmetric convolutional and deconvolutional layers, between which they added skip connections <ref type="bibr" target="#b36">(Srivastava et al., 2015;</ref><ref type="bibr" target="#b9">He et al., 2016)</ref>. <ref type="bibr" target="#b39">Tai et al. (2017)</ref> proposed an 80-layer memory network which contains a recursive unit and a gate unit. <ref type="bibr" target="#b44">Yang et al. (2017)</ref> proposed an image inpainting framework that uses two networks: one for capturing the global structure of an image, and one for reducing the discrepancy of texture appearance inside and outside missing image regions. While many studies employ the 2 distance between the clean and recovered images, some propose to use new loss functions such as the perceptual loss to obtain perceptually better results <ref type="bibr" target="#b12">(Johnson et al., 2016;</ref><ref type="bibr" target="#b18">Ledig et al., 2017)</ref>.</p><p>There are also studies on the development of new training methods. A recent trend is to use adversarial training, where two networks are trained in an adversarial setting; a generator network is trained to perform image restoration, and a discriminator network is trained to distinguish whether an input is a true image or a restored one. The first work employing this framework for image inpainting is the context encoder of <ref type="bibr" target="#b32">Pathak et al. (2016)</ref>. They minimize the sum of a reconstruction loss over an encoder-decoder network for restoring intensities of missing pixels and additionally an adversarial loss over a discriminator network. <ref type="bibr" target="#b10">Iizuka et al. (2017)</ref> proposed an improved framework in which global and local context discriminators are used to generate realistic images. While the above studies require the shapes of missing regions (i.e., masks) for training, <ref type="bibr" target="#b46">Yeh et al. (2017a)</ref> proposed a method which does not need masks for training. Their method first learns a latent manifold of clean images by GANs and search for the closest encoding of a corrupted image to infer missing regions. Despite its success in various application domains, GANs have several issues, such as difficulty of training (e.g., mode collapse), difficulty with evaluation of generated samples <ref type="bibr" target="#b22">(Lucic et al., 2017)</ref>, and theoretical limitations <ref type="bibr" target="#b1">(Arora et al., 2017)</ref>.</p><p>A question arises from these recent developments: what is (the most) important of these ingredients, i.e., the design of network architectures, loss functions, and adversarial training? In this study, we report that convolutional autoencoders (CAEs) built only on standard components can outperform the existing methods on standard benchmark tests of image restoration. We achieve this by employing an evolutionary algorithm <ref type="bibr" target="#b38">(Suganuma et al., 2017)</ref> to exploit the potential of standard CAEs, which optimizes the number and size of filters and connections of each layer along with the total number of layers. We did not use adversarial training or any sophisticated loss; all we did was to train the discovered architecture with the standard 2 loss using the ADAM optimizer <ref type="bibr" target="#b13">(Kingma &amp; Ba, 2015)</ref>. The contribution of this study is summarized as follows:</p><p>• We show that simple CAEs built upon standard components such as convolutional layers and skip connections can achieve the state-of-the-art performance in image restoration tasks. Their training is performed by minimization of a standard 2 loss; no adversarial training or novel hand-designed loss is used.</p><p>• We propose to use an evolutionary algorithm to search for good architectures of the CAEs, where the hyperparameters of each layer and connections of the layers are optimized.</p><p>• To the best of our knowledge, this is the first study of automatic architecture search for image restoration tasks. Previous studies proposed methods for image classification and tested them on the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deep Learning for Image Restoration</head><p>Deep networks have shown good performance on various image restoration tasks, such as image denoising <ref type="bibr" target="#b40">(Xie et al., 2012;</ref><ref type="bibr" target="#b48">Zhang. et al., 2017)</ref>, single image super-resolution (SISR) <ref type="bibr" target="#b4">(Dong et al., 2014;</ref><ref type="bibr" target="#b18">Ledig et al., 2017)</ref>, deblurring and compressive sensing <ref type="bibr" target="#b16">Kulkarni et al., 2016;</ref><ref type="bibr" target="#b28">Mousavi &amp; Baraniuk, 2017)</ref>, in addition to those mentioned in Section 1. In particular, recent studies tend to rely on the framework of GANs <ref type="bibr" target="#b8">(Goodfellow et al., 2014)</ref> for training to improve accuracy or perceptual quality of restored images, e.g., <ref type="bibr" target="#b32">(Pathak et al., 2016;</ref><ref type="bibr" target="#b46">Yeh et al., 2017a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Automatic Design of Network Architectures</head><p>Neural networks have been and are being designed manually, sometimes with a lot of trial and error. Recently, increasing attention is being paid to automatic design of network architectures and hyperparameters <ref type="bibr" target="#b25">(Miikkulainen et al., 2017;</ref><ref type="bibr" target="#b41">Xie &amp; Yuille, 2017;</ref><ref type="bibr" target="#b19">Liu et al., 2017;</ref><ref type="bibr" target="#b3">Brock et al., 2018;</ref><ref type="bibr" target="#b20">Liu et al., 2018)</ref>. The recent studies are roughly divided into two categories; those based on evolutionary algorithms and on reinforcement learning.</p><p>The employment of evolutionary algorithms for neural architecture search has a long history <ref type="bibr" target="#b35">(Schaffer et al., 1992;</ref><ref type="bibr" target="#b37">Stanley &amp; Miikkulainen, 2002)</ref>. In the past, the weights and connections of neural networks are attempted to be jointly optimized, whereas in recent studies, only architectures are optimized by evolutionary algorithms, and their weights are left to optimization by SGD and its variants. <ref type="bibr" target="#b34">Real et al. (2017)</ref> showed that evolutionary algorithms can explore the space of large-scale neural networks, and achieve competitive performance in standard object classification datasets, although their method relies on large computational resources (e.g., a few hundred GPUs and ten days). <ref type="bibr" target="#b38">Suganuma et al. (2017)</ref> proposed a designing method based on cartesian genetic programming <ref type="bibr" target="#b27">(Miller &amp; Thomson, 2000)</ref>, showing that architectural search can be performed using two GPUs in ten days.</p><p>Another approach to neural architecture search is to use reinforcement learning. There are studies that employ the REINFORCE algorithm, policy gradient, and Q-learning to learn network topology <ref type="bibr" target="#b2">Baker et al., 2017;</ref><ref type="bibr" target="#b50">Zhong et al., 2017;</ref>. These reinforcement learning-based approaches tend to be computational resource hungry, e.g., requiring 10-800 GPUs.</p><p>In this study, we employ the method of <ref type="bibr" target="#b38">Suganuma et al. (2017)</ref> due to its computational efficiency, although we think that other recent light-weight methods could also be employed. As their method was tested only on classification tasks as in other similar studies, we tailor it to designing CAEs for image restoration tasks. As will be described, we confine the search space to symmetric CAEs, by which we make it possible to design competitive architectures with a limited amount of computational resource (using 1 to 4 GPUs in a few days).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Evaluation Methods for Image Restoration</head><p>There is a growing tendency that perceptual quality rather than signal accuracy is employed for evaluation of image restoration methods <ref type="bibr" target="#b18">(Ledig et al., 2017;</ref><ref type="bibr" target="#b46">Yeh et al., 2017a)</ref>. The shared view seems to be that employment of adversarial training and/or sophisticated loss such as the perceptual loss tends to deliver sharper and more realistic images, while their pixel-to-pixel differences (e.g., PSNR) from their ground truths tend not to be smaller (or sometimes even larger). In this study, however, we stick to the pixel-topixel difference due to the following reasons. First, which evaluation measure should be used depends on for which purpose we use these "image restoration" methods. For a photo-editing software, looking more photo-realistic will be more important. For the purpose of 'pure' image restoration in which the goal is to predict intensities of missing pixels, it will be more important that each pixel has a value closer to its ground truth (see examples of inpainting images of numbers in <ref type="figure" target="#fig_0">Figure 2</ref>). Second, popular quality measures, such as mean opinion score (MOS), need human raters, and their values are not easy to reproduce particularly when there are only small differences between images under comparison. Finally, we also note that our method does sometimes provide sharper images compared to existing methods (see examples of inpainting images with random pixel masks in <ref type="figure" target="#fig_0">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evolutionary Convolutional Autoencoders</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Search Space of Network Architectures</head><p>We consider convolutional autoencoders (CAEs) which are built only on standard building blocks of ConvNets, i.e., convolutional layers with optional downsampling and skip connections. We further limit our attention to symmetric CAEs such that their first half (encoder part) is symmetric to the second half (decoder part). We add a final layer to obtain images of fixed channels (i.e., single-channel grayscale or three-channel color images) on top of the decoder part, for which either one or three filters of 3 × 3 size are used. Therefore, specification of the encoder part of a CAE solely determines its entire architecture. The encoder part can have an arbitrary number of convolutional layers up to a specified maximum. Each convolutional layer can have an arbitrary number and size of (single-size) filters, and is followed by ReLU <ref type="bibr" target="#b29">(Nair &amp; Hinton, 2010)</ref>. Additionally, it can have an optional skip connection <ref type="bibr" target="#b36">(Srivastava et al., 2015;</ref><ref type="bibr" target="#b9">He et al., 2016;</ref><ref type="bibr" target="#b23">Mao et al., 2016)</ref>, which connects the layer to its mirrored counterpart in the decoder part. To be specific, the output feature maps (obtained after ReLU) of the layer are passed to and are element-wise added to the output feature maps (obtained before ReLU) of the counterpart layer. We can use additional downsampling after each convolutional layer depending on tasks; whether to use downsampling is determined in advance, and thus is not selected by architectural search, as will be explained later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Representation of CAE Architectures</head><p>Following <ref type="bibr" target="#b38">(Suganuma et al., 2017)</ref>, we represent architectures of CAEs by directed acyclic graphs defined on a twodimensional grid. This graph is optimized by the evolutionary algorithm explained below, where the graph is called <ref type="figure">Figure 1</ref>. An example of a genotype and a phenotype. A phenotype is a graph representation of a network architecture, and a genotype encodes a phenotype. They encode only the encoder part of a CAE, and its decoder part is automatically created so as to be symmetrical to the encoder part. In this example, the phenotype is defined on the grid of three rows and three columns.</p><p>phenotype, and is encoded by a data structure called genotype <ref type="bibr" target="#b5">(Eiben &amp; Smith, 2003)</ref>.</p><p>Phenotype A phenotype is a directed acyclic graph defined on a two-dimensional grid of M rows and N columns; see <ref type="figure">Figure.</ref>1. Each node of the graph, which is identified by a unique id node in the range [1, M N ] in a column-major order of the grid, represents a convolutional layer followed by a ReLU in a CAE. An edge connecting two nodes represents the connectivity of the two corresponding layers. The graph has two additional special nodes called input and output nodes; the former represents the input layer of the CAE, and the latter represents the output of the encoder part, or equivalently the input of the decoder part of the CAE. As the input of each node is connected to at most one node, there is a single unique path starting from the input node and ending at the output node. This unique path identifies the architecture of the CAE, as shown in the middle row of <ref type="figure">Figure 1</ref>. Note that nodes depicted in the neighboring two columns are not necessarily connected. Thus, the CAE can have different number of layers depending on how their nodes are connected. Since the maximum number of layers (of the encoder part) of the CAE is N , the total number of layers is 2N + 1 including the output layer. In order to control how the number of layers will be chosen, we introduce a hyper-parameter called level-back L, such that nodes given in the n-th column are allowed to be connected from nodes given in the columns ranging from n − L to n − 1. If we use smaller L, then the resulting CAEs will tend to be deeper.</p><p>Genotype A genotype encodes a phenotype, and is manipulated by the evolutionary algorithm. The genotype encoding a phenotype with M rows and N columns has M N + 1 genes, each of which represents attributes of a node with two integers (i.e., type T and connection C). The type T specifies the number F and size k of filters of the node, and whether the layer has skip connections or not, by an integer encoding their combination. The connection C specifies the node by id node that is connected to the input of this node. The last (M N + 1)-st gene represents the output node, which stores only connection C determining the node connected to the output node. An example of a genotype is given at the top row of <ref type="figure">Figure 1</ref>, where F ∈ {64, 128, 256} and k ∈ {1 × 1, 3 × 3, 5 × 5}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evolutionary Strategy</head><p>We use a simple form of the (1 + λ) evolutionary strategy <ref type="bibr" target="#b27">(Miller &amp; Thomson, 2000)</ref> to perform search in the architecture space. In this strategy, λ children are generated from a single parent at each generation, and the best performing child compared to its parent becomes the new parent at the next generation. The performance of each individual (i.e., a generated CAE), called fitness, is measured by peak signal to noise ratio (PSNR) between the restored and ground truth images evaluated on the validation dataset. The genotype is updated to maximize the fitness as generation proceeds.</p><p>The details are given in Algorithm 1. The algorithm starts with an initial parent, which is chosen to be a minimal CAE having a single convolution layer and a single deconvolution layer.</p><p>At each generation, λ children are generated by applying mutations to the parent (line 5). We use a point mutation as the genetic operator, where integer values of the type T and connection C of each gene are randomly changed with a mutation probability r. If a gene is decided to be changed, the mutation operator chooses a value at random for each T and C from their predefined sets.</p><p>The generated λ children are individually trained using the training set. We train each child for I iterations using the ADAM optimizer <ref type="bibr" target="#b13">(Kingma &amp; Ba, 2015)</ref> with learning rate lr, and a mini-batch size of b (line 6). For the training loss, we use the mean squared error (MSE) between the restored images and their ground truths. After the training phase is completed, the performance of each child is evaluated on the validation set and is assigned to its fitness value (line 7). Finally, the best individual is selected from the set of parent and the children, and replaced the parent in the next generation (line 9 − 12). This procedure is repeated for G generations.</p><p>We can obtain a single unique path starting from the input node and ending at the output node using our representation. The computed unique path represents the architecture of the CAE. We call nodes on this path functioning nodes. As some (in fact, most) of nodes in a phenotype are not functioning nodes and do not express the resulting CAE, the mutation has the possibility of affecting only non-functioning nodes, i.e., the CAE architecture does not change by the mutation.</p><p>In that case, we skip the evaluation of the CAE and apply the mutation operator repeatedly until the resulting CAE architecture does change. Moreover, if the fitness values of the children do not improve, then we modify a parent <ref type="bibr" target="#b27">(Miller &amp; Thomson, 2000;</ref><ref type="bibr" target="#b26">Miller &amp; Smith, 2006)</ref>; in this case, we change only the non-functioning nodes so that the realized CAE (i.e., functioning nodes) will not change (line 14).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We conducted experiments to test the effectiveness of our approach. We chose two tasks, image inpainting and denoising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Details of Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">INPAINTING</head><p>We followed the procedures suggested in <ref type="bibr" target="#b46">(Yeh et al., 2017a)</ref> for experimental design. We used three benchmark datasets; the CelebFaces Attributes Dataset (CelebA) <ref type="bibr" target="#b21">(Liu et al., 2015)</ref>, the Stanford Cars Dataset (Cars) <ref type="bibr" target="#b14">(Krause et al., 2013)</ref>, and the Street View House Numbers (SVHN) <ref type="bibr" target="#b30">(Netzer et al., 2011)</ref>. The CelebA contains 202, 599 images, from which we randomly selected 100, 000, 1,000, and 2,000 images for training, validation, and test, respectively. All images were cropped in order to properly contain the entire face, and resized to 64 × 64 pixels. For Cars and SVHN, we used the provided training and testing split. The images of Cars were cropped according to the provided bounding boxes, and resized to 64 × 64 pixels. The images of SVHN were resized to 64 × 64 pixels.</p><p>We generated images with missing regions of the following three types: a central square block mask (Center), random pixel masks such that 80% of all the pixels were randomly masked (Pixel), and half image masks such that a randomly chosen vertical or horizontal half of the image was masked (Half). For the latter two, a mask was randomly generated for each training minibatch and for each test image.</p><p>Considering the nature of this task, we consider CAEs endowed with downsampling. To be specific, the same counts of downsampling and upsampling with stride = 2 were employed such that the entire network has a symmetric hourglass shape. For simplicity, we used a skip connection and downsampling in an exclusive manner; in other words, every layer (in the encoder part) employed either a skip connection or downsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">DENOISING</head><p>We followed the experimental procedures described in <ref type="bibr" target="#b23">(Mao et al., 2016;</ref><ref type="bibr" target="#b39">Tai et al., 2017)</ref>. We used grayscale 300 and 200 images belonging to the BSD500 dataset <ref type="bibr" target="#b24">(Martin et al., 2001)</ref> to generate training and test images, respectively. For each image, we randomly extracted 64 × 64 patches, to each of which Gaussian noise with different σ = 30, 50 and 70 are added. As utilized in the previous studies, we trained a single model for all different noise levels.</p><p>For this task, we used CAE models without downsampling following the previous studies <ref type="bibr" target="#b23">(Mao et al., 2016;</ref><ref type="bibr" target="#b39">Tai et al., 2017)</ref>. We zero-padded the input feature maps computed in each convolution layer not to change the size of input and output feature space of the layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Configurations of Architectural Search</head><p>For the proposed evolutionary algorithm, we chose the mutation probability as r = 0.1, the number of children as λ = 4, and the number of generations as G = 250. For the phenotype, we used the graph with M = 3, N = 20 and level-back L = 5. For the number F and size k of filters at each layer, we chose them from {64, 128, 256} and {1 × 1, 3 × 3, 5 × 5}, respectively. During an evolution process, we trained each CAE for I = 20k iterations with a mini-batch of size b = 16. We set the learning rate lr of the ADAM optimizer to be 0.001. Following completion of the evolution process, we fine-tuned the best CAE using the training set of images for additional 500k iterations, in which the learning rate is reduced by a factor of 10 at the 200k and 400k iterations. We then calculated its performance using the test set of images. We implemented our method using PyTorch <ref type="bibr" target="#b31">(Paszke et al., 2017)</ref>, and performed the experiments using four P100 GPUs. Execution of the evolutionary algorithm and the fine-tuning of the best model took about three days for the inpainting tasks and four days for the denoising tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with Existing Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">INPAINTING</head><p>As mentioned above, we follow the experimental procedure employed in <ref type="bibr" target="#b46">(Yeh et al., 2017a)</ref>. In the paper, the authors reported the performances of their proposed method, Semantic Image Inpainting (SII), and Context Autoencoder (CE) <ref type="bibr" target="#b32">(Pathak et al., 2016)</ref>. However, we found that CE can provide considerably better results than those reported in <ref type="bibr" target="#b46">(Yeh et al., 2017a)</ref> in terms of both PSNR and visual quality. Thus, we report here PSNR and SSIM values of CE that we obtained by running the authors' code 1 . In order to calculate SSIM values of SII, which were not reported in <ref type="bibr" target="#b46">(Yeh et al., 2017a)</ref>, we run the authors' code 2 for SII.</p><p>In order to further validate effectiveness of the evolutionary search, we evaluate two baseline architectures; one is the architecture generated by a random search (RANDOM), and the other is the architecture having the same depth as the best performing architecture found by our method but having the constant number (64) of fixed size (3 × 3) filters in each layer with a skip connection (BASE). In the random search, we generate ten architectures at random in the same search space, and report the average PSNR and SSIM values of them. All other experimental setups are the same. <ref type="table" target="#tab_1">Table 1</ref> shows the PSNR and SSIM values obtained using five methods on three datasets and three masking patterns. Our method (i.e., the CAE optimized by the evolutionary algorithm) is referred as E-CAE. We run the evolutionary algorithm three times, and report the average accuracy values of the three optimized CAEs. As we can see, our method outperforms the other four methods for each of the datasetmask combinations. It should also be noted that CE and SII use mask patterns for inference; to be specific, their networks estimate only pixel intensities of the missing regions specified by the provided masks, and then they are merged with the unmasked regions of clean pixels. Thus, the pixel intensities of unmasked regions are identical to their ground truths. On the other hand, our method does not use masks; it outputs a complete image such that the missing regions are hopefully inpainted correctly. We then calculate the   <ref type="bibr" target="#b32">(Pathak et al., 2016)</ref>, Semantic Image Inpainting (SII) <ref type="bibr" target="#b46">(Yeh et al., 2017a)</ref>, and CAEs designed by our evolutionary algorithm (E-CAE) using three datasets and three masking patterns.  PSNR of the output image against the ground truth without identifying missing regions. This difference should favor CE and SII, and nevertheless our method performs better.</p><p>Sample inpainted images obtained by E-CAE along with the masked inputs, and the ground truths are shown in <ref type="figure" target="#fig_0">Figure 2</ref>. As we choose the same images as those used in <ref type="bibr" target="#b46">(Yeh et al., 2017a)</ref>, the readers can easily check differences in visual quality from CE and SII. It is observed overall that E-CAE performs stably; the output images do not have large errors for all types of masks. It performs particularly well for random pixel masks (the middle column of <ref type="figure" target="#fig_0">Figure 2)</ref>; the images are realistic and sharp. It is also observed that E-CAE tends to yield less sharp images for images with a filled region of missing pixels. However, E-CAE can infer their contents accurately, as shown in the examples of inpainting images of numbers (the rightmost column of <ref type="figure" target="#fig_0">Figure 2</ref>); CE and SII provide either obscure images of numbers which are difficult to recognize, or sharp images of wrong numbers; see <ref type="figure">Figure 18</ref> and 21 of <ref type="bibr" target="#b47">(Yeh et al., 2017b)</ref>. <ref type="figure" target="#fig_1">Figure 3</ref> shows several examples of difficult cases for E-CAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">DENOISING</head><p>We compare our method with two baseline architectures (i.e., RANDOM and BASE described in Section 4.3.1) and two state-of-the-art methods; RED <ref type="bibr" target="#b23">(Mao et al., 2016)</ref> and MemNet <ref type="bibr" target="#b39">(Tai et al., 2017)</ref>. <ref type="table" target="#tab_3">Table 2</ref> shows PSNR and SSIM values for three versions of the BSD200 test set with different noise levels σ = 30, 50, and 70, where the performance values of RED and MemNet are obtained from <ref type="bibr" target="#b39">(Tai et al., 2017)</ref>. Our method again achieves the best performance for all cases except a single case (MemNet for σ = 30). It is worth noting that the networks of RED and MemNet have 30 and 80 layers, respectively, whereas our best CAE has only 15 layers (including the decoder part and the output layer), showing that our evolutionary method was able to find simpler architectures that can provide more accurate results.</p><p>An example of an image recovered by our method is shown  <ref type="bibr" target="#b23">(Mao et al., 2016)</ref>, MemNet <ref type="bibr" target="#b39">(Tai et al., 2017)</ref>, and E-CAE.  <ref type="figure">Figure 4</ref>. Examples of images reconstructed by E-CAE for the denoising task. The first column shows the input image with noise level σ = 50.</p><p>in <ref type="figure">Figure 4</ref>. As we can see, E-CAE correctly removes the noise, and produces an image as sharp as the ground truth. <ref type="table" target="#tab_5">Table 3</ref> shows the top five best performing architectures designed by our method for the image inpainting task using center masks on the CelebA dataset and the denoising task, along with their performances measured on their test datasets. One of the architectures best performing for each task is shown in <ref type="figure">Figure 5</ref>. It is observed that although their overall structures do not look very unique, mostly due to the limited search space of CAEs, the number and size of filters are quite different across layers, which are hard to manually determine. Although it is difficult to give a general interpretation of why the parameters of each layer are chosen, we can make the following observations: i) regardless of the tasks, almost all networks have a skip connection at the first layer, implying that the input images contain essential information to yield accurate outputs; ii) 1 × 1 convolution seems to be important ingredients for both tasks; 1 × 1 conv. layers dominate the denoising networks, and all the inpainting networks employ two 1 × 1 conv. layers; iii) when comparing the inpainting networks with the denoising networks, we observe the following differences: the largest filters of size 5 × 5 tend to be employed by the former more often than the latter (2.8 vs 0.8 layers in average), and 1 × 1 filters tend to be employed by the former less often than the latter (2.0 vs. 3.2 layers in average).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analysis of Optimized Architectures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Effects of Parameters of Evolutionary Search</head><p>The evolutionary algorithm has several parameters, two of which, i.e., the mutation probability (r) and the number of children (λ), tend to have particularly large impact on the performance of the optimized E-CAEs. Using the center mask inpainting task on the CelebA dataset, we analyze their impact in detail in this subsection.</p><p>Effect of mutation probability Employment of a larger mutation probability (r) will change the structures of CAEs more drastically at each generation, and make the process of architecture search less stable. On the other hand, a large mutation probability will contribute to reduce the possibility of being trapped in local optima. <ref type="figure">Figure 6</ref> (a) shows the relation between different mutation probabilities and the performances of CAEs obtained by using them; their performances are calculated on the validation set. It is observed from the plots that smaller mutation probabilities tend to deliver lower accuracy at initial generations, but eventually provide higher accuracy after a sufficient number of generations are generated. The best result was obtained for r = 0.1.</p><p>Effect of number of children Employment of a larger number (λ) of children will enable us to perform search in a wider subspace of the architecture space at each generation, but at the expense of larger computational cost per generation. <ref type="figure">Figure 6</ref> (b) shows the relation between different λ values (λ = 1, 2, 4, 8, and 16) and the performances of the optimized CAEs. The best performance is obtained for λ = 4 using a sufficient number of generations, but there is not much difference in the final PSNR results obtained by different number of children. Interestingly, even the evolution performed using λ = 1, which uses the minimum computational cost per generation, yields a competitive result. Specifically, it took 1.68 days on one P100 GPU for training, and achieved PSNR = 29.80 on the test set after fine-tuning of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have first introduced an evolutionary algorithm that searches for good architectures of convolutional autoencoders (CAEs) for image restoration tasks. We have then shown that the CAEs designed by our algorithm  26.18 0.7037 <ref type="figure">Figure 5</ref>. One of the best performing architectures given in <ref type="table" target="#tab_5">Table 3</ref> for inpainting (upper) and denoising (lower) tasks. <ref type="figure">Figure 6</ref>. Improvement of PSNR of E-CAE by increasing number of generations obtained using the evolutionary algorithm for (a) different mutation probabilities, and (b) different number of children. The center mask inpainting task on the CelebA dataset is used. PSNR is calculated using the validation set.</p><p>outperform the state-of-the-art networks for image inpainting and denoising, despite the fact that these networks are built on combination of complicated architectures with very deep layers, (multiple) hand-designed losses, and adversarial training; our CAEs consist only of standard convolutional layers with optional skip connections, and they are simply trained by the ADAM optimizer to minimize standard 2 loss. Although our CAEs have simple architectures, their space is still very high-dimensional; CAEs can have an arbitrary number of layers, each of which has an arbitrary number and size of filters as well as whether to use a skip connection. Our evolutionary algorithm can find good architectures in this high-dimensional space. This implies that there is still much room for exploration of search spaces of architectures of classical convolutional networks, which may apply to other tasks such as single image colorization , depth estimation <ref type="bibr" target="#b6">(Eigen et al., 2014;</ref><ref type="bibr" target="#b42">Xu et al., 2017)</ref>, and optical flow estimation <ref type="bibr" target="#b11">(Ilg et al., 2017)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Examples of inpainting results obtained by E-CAE (CAEs designed by the evolutionary algorithm).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Examples of blurry reconstructions generated by E-CAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>3) − CS(64, 1) − C(128, 3) − CS(256, 5) − CS(64, 1) − C(64, 3) − CS(128, 5) 29.91 0.9343 CS(128, 5) − CS(256, 3) − C(64, 1) − CS(128, 3) − CS(64, 5) − CS(64, 1) − C(128, 5) − C(256, 5) 29.89 0.9334 CS(128, 3) − CS(64, 3) − C(64, 5) − CS(256, 3) − C(128, 3) − CS(128, 5) − CS(64, 1) − CS(64, 1) 29.88 0.9346 CS(64, 1) − C(128, 5) − CS(64, 3) − C(64, 1) − CS(256, 5) − C(128,CS(64, 3) − C(64, 1) − C(128, 3) − CS(64, 1) − CS(128, 5) − C(128, 3) − C(64, 1) 26.67 0.7313 CS(64, 5) − CS(256, 1) − C(256, 1) − C(64, 3) − CS(128, 1) − C(64, 3) − CS(128, 1) − C(128, 3) 26.28 0.7113 CS(64, 3) − C(64, 1) − C(128, 3) − CS(64, 1) − CS(128, 5) − C(128, 3) − C(64, 1) 26.28 0.7107 CS(128, 3) − CS(64, 1) − C(64, 3) − C(64, 3) − CS(64, 1) − C(64, 3) 26.20 0.7047 CS(64, 5) − CS(128, 1) − CS(256, 3) − CS(128, 1) − CS(128, 1) − C(64, 1) − CS(64, 3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>best ← argmax i=1,2,...,λ {f itness i }if f itness best &gt; F p thengeneration = generation + 1 17: end while 18: Output: parent (the best architecture of CAEs found by the evolutionary search).</figDesc><table>Algorithm 1 Evolutionary strategy for a CAE. 
1: Input: G (number of generations), r (mutation proba-
bility), λ (children size), S (Training set), V (Validation 
set). 
2: Initialization: (i) Generate a parent, (ii) train the 
model on the S, and (iii) assign the fitness F p using 
the set V . 
3: while generation &lt; G do 

4: 

for i = 1 to λ do 

5: 

children i ← Mutation(parent, r) 

6: 

model i ← Train(children i , S) 

7: 

f itness i ← Evaluate(model i , V ) 

8: 

end for 

9: 

10: 

11: 

parent ← children best 

12: 

F p ← f itness best 

13: 

else 

14: 

parent ← Modify(parent, r) 

15: 

end if 

16: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Inpainting results. Comparison of two baseline architectures (RANDOM and BASE), Context Autoencoder (CE)</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Denoising results on BSD200. Comparison of results of two baseline architectures (RANDOM and BASE), RED</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Best performing five architectures of E-CAE. C(F, k) indicates that the layer has F filters of size k × k without a skip connection. CS indicates that the layer has a skip connection. This table shows only the encoder part of CAEs. For the denoising, the average values of PSNR and SSIM of three noise levels are shown.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/pathak22/context-encoder 2 https://github.com/moodoki/semantic image inpainting</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partly supported by CREST, JST Grant Number JPMJCR14D1, and the ImPACT Program "Tough Robotics Challenge" of the Council for Science, Technology, and Innovation (Cabinet Office, Government of Japan).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generalization and equilibrium in generative adversarial nets (gans)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="224" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Smash: one-shot model architecture search through hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Introduction to Evolutionary Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Eiben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>SpringerVerlag</publisher>
			<biblScope unit="volume">53</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image upsampling via imposed edge statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="1" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reconnet: Non-iterative reconstruction of images from compressively sensed measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kerviche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashok</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murphy</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00559</idno>
	</analytic>
	<monogr>
		<title level="j">Progressive neural architecture search</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Are gans created equal? a large-scale study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10337</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2802" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malik</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evolving deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Francon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shahrzad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Navruzyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duffy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hodjat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GECCO</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Redundancy and computational efficiency in cartesian genetic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="174" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cartesian genetic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Thomson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroGP</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="121" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to invert: Signal recovery via deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2272" to="2276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Total variation blind deconvolution: The devil is in the details</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Perrone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2909" to="2916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Combinations of genetic algorithms and neural networks: A survey of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Schaffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Whitley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Eshelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COGANN</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="1" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2377" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Evolving neural networks through augmenting topologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="127" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A genetic programming approach to designing convolutional neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suganuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nagao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GECCO</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cnn</forename><surname>Genetic</surname></persName>
		</author>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1379" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-scale continuous crfs as sequential deep networks for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5354" to="5362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1790" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">High-resolution image inpainting using multi-scale neural patch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6721" to="6729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image superresolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semantic image inpainting with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawajohnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5485" to="5493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawajohnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Do</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07539v3</idno>
		<title level="m">Semantic image inpainting with deep generative models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Practical network blocks design with Q-Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07012</idno>
		<title level="m">Learning transferable architectures for scalable image recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
