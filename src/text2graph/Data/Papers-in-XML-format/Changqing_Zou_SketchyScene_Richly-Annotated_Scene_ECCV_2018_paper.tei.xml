<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:17+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SketchyScene: Richly-Annotated Scene Sketches</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Zou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">⋆1</forename><surname>Qian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>⋆2</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofei</forename><surname>Du</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Mo</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Shandong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengying</forename><surname>Gao</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Shandong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>⋆⋆4</postCode>
									<settlement>College Park</settlement>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SketchyScene: Richly-Annotated Scene Sketches</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>2 C. Zou et al.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>Sketch dataset· Scene sketch · Sketch segmentation ⋆ equal contribution</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It is a sunny day.</head><p>It is a family picnic. There are four people, a basket, two apples, one cup,two bananas, and on a picnic rug. There is two trees in the distance. <ref type="figure">Fig. 1</ref>. A scene sketch from our dataset SketchyScene that is user-generated based on the reference image shown, a segmentation result (middle) obtained by a method trained on SketchyScene, and a typical application: sketch captioning.</p><p>Abstract. We contribute the first large-scale dataset of scene sketches, SketchyScene, with the goal of advancing research on sketch understanding at both the object and scene level. The dataset is created through a novel and carefully designed crowdsourcing pipeline, enabling users to efficiently generate large quantities of realistic and diverse scene sketches. SketchyScene contains more than 29,000 scene-level sketches, 7,000+ pairs of scene templates and photos, and 11,000+ object sketches. All objects in the scene sketches have ground-truth semantic and instance masks. The dataset is also highly scalable and extensible, easily allowing augmenting and/or changing scene composition. We demonstrate the potential impact of SketchyScene by training new computational models for semantic segmentation of scene sketches and showing how the new dataset enables several applications including image retrieval, sketch colorization, editing, and captioning, etc. The dataset and code can be found at https://github.com/SketchyScene/SketchyScene.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the age of data-driven computing, large-scale datasets have become a driving force for improving and differentiating the performance, robustness, and generality of machine learning algorithms. In recent years, the computer vision community have embraced a number of large and richly annotated datasets for images (e.g., ImageNET <ref type="bibr" target="#b7">[8]</ref> and Microsoft COCO <ref type="bibr" target="#b14">[15]</ref>), 3D objects (e.g., ShapeNET <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31]</ref> and PointNET <ref type="bibr" target="#b16">[17]</ref>), and scene environments (e.g., SUN <ref type="bibr" target="#b31">[32]</ref> and the NYU database <ref type="bibr" target="#b21">[22]</ref>). Among the various representations of visual forms, hand-drawn sketches occupy a special place since, unlike most others, they come from human creation. Humans are intimately familiar with sketches as an art form and sketching is arguably the most compact, intuitive, and frequently adopted mechanism to visually express and communicate our impression and ideas.</p><p>Significant progress has been made on sketch understanding and sketch-based modeling in computer vision and graphics recently <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25]</ref>. Several large-scale sketch datasets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b9">10]</ref> have also been constructed and utilized along the way. Nevertheless, these datasets have all been formed by object sketches and the sketch analysis and processing tasks have mostly been at the stroke or object level. Extending both to the scene level is a natural progression towards a deeper and richer reasoning about sketched visual forms. The ensuing analysis and data synthesis problems become more challenging since a sketched scene may contain numerous objects interacting in a complex manner. While scene understanding is one of the hallmark tasks of computer vision, the problem of understanding scene sketches have not been well studied.</p><p>In this paper, we introduce the first large-scale dataset of scene sketches, which we refer to as SketchyScene, to facilitate sketch understanding and processing at both the object and scene level. Obviously, converting images to edge maps <ref type="bibr" target="#b32">[33]</ref> does not work since the results are characteristically different from hand-drawn sketches. Automatically composing existing object sketches based on predefined layout templates and fitting the object sketches into stock photos are both challenging problems that are unlikely to yield a large quantity of realistic outcomes (see <ref type="figure">Fig. 2(b)</ref>). In our work, we resort to crowdsourcing and design a novel and intuitive interface to reduce burden placed on the users and improve their productivity. Instead of asking the users to draw entire scene sketches from scratch, which can be tedious and intimidating, we provide object sketches so that the scene sketches can be created via simple interactive operations such as drag-n-dropping and scaling the object sketches. To ensure diversity and realism of the scene sketches, we provide reference images to guide/inspire the users during their sketch generation. With a user-friendly interface, participants can create high-quality scene sketches efficiently. On the other hand, the scene sketches synthesized this way are by and large sketchy sketches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19]</ref> and they do not quite resemble ones produced by professional artists.</p><p>SketchyScene contains both object-and scene-level data, accompanied with rich annotations. In total, the dataset has more than 29,000 scene sketches and more than 11,000 object sketches belonging to 45 common categories. In <ref type="figure">Fig. 2</ref>. (a) reference image; (b) response of edge detector; (c) synthesized scene using object sketches from Sketchy and TU-Berlin (using the same pipeline as (f)); (d) nonartist's drawing with the hint of a short description; (e) artist's drawing with the hint of reference image; (f) synthesized scene using our system. Processes of (c)-(f) take 6, 8, 18, and 5 minutes, respectively.</p><formula xml:id="formula_0">(a) (b) (c) (d) (e) (f)</formula><p>addition, more than 7,000 pairs of scene sketch templates and reference photos and more than 200,000 labeled instances are provided. Note that, all objects in the scene sketches have ground-truth semantic and instance masks. More importantly, SketchyScene is flexible and extensible due to its object-oriented synthesis mechanism. Object sketches in a sketch scene template can be switched in/out using available instances in SketchyScene to enrich the dataset. We demonstrate the potential impact of SketchyScene through experiments. Foremost, the dataset provides a springboard to investigate an assortment of problems related to scene sketches (a quick Google Image Search on "scene sketches" returns millions of results). In our work, we investigate semantic segmentation of scene sketches for the first time. To this end, we evaluated the advanced natural image segmentation model, DeepLab-v2 <ref type="bibr" target="#b2">[3]</ref>, exploring the effect of different factors and providing informative insights. We also demonstrate several applications enabled by the new dataset, including sketch-based scene image retrieval, sketch colorization, editing, and captioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Large-scale sketch datasets</head><p>There has been a surge of large-scale sketch datasets in recent years, mainly driven by applications such as sketch recognition/synthesis <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> and SBIR <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b18">19]</ref>. Yet the field remains relatively under-developed with existing datasets mainly facilitating object-level analysis of sketches. This is a direct result of the non-ubiquitous nature of human sketches data -they have to be carefully crowd-sourced other than automatically crawled for free (as for photos).</p><p>TU-Berlin <ref type="bibr" target="#b8">[9]</ref> is the first such large-scale crowd-sourced sketch dataset which was primarily designed for sketch recognition. It consist of 20,000 sketches spanning over 250 categories. The more recent QuickDraw <ref type="bibr" target="#b9">[10]</ref> dataset is much larger, with 50 million sketches across 345 categories. Albeit being large enough to facilitate stroke-level analysis <ref type="bibr" target="#b5">[6]</ref>, sketches sourced in these datasets were produced by sketching towards a semantic concept (e.g., "cat", "house"), without a reference photo or mental recollection of natural scene/objects. This greatly limits the level of visual detail and variations depicted, therefore making them unfitting for fine-grained matching and scene-level parsing. For example, faces are almost all in their frontal view, and depicted as a smiley in QuickDraw.</p><p>The concurrent work of <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b18">[19]</ref> progressed the field further by collecting object instance sketches for FG-SBIR. QMUL database <ref type="bibr" target="#b35">[36]</ref> consists of 716 sketch-photo pairs across two object categories (shoe and chair), with reference photos crawled from on-line shopping websites. Sketchy <ref type="bibr" target="#b18">[19]</ref> contains 75,471 sketches and 12,500 corresponding photos across a much wider selection of categories (125 in total). Object instance sketches are produced by asking crowdsourcers to depict their mental recollection of a reference photo. In comparison with concept sketches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, they by and large exhibit more object details and have matching poses with the reference photos. However, a common drawback for both, for the purpose this project, lies with their limited pose selection and object configurations. QMUL sketches exhibit only one object pose (side view) under a single object configuration. Scene sketches albeit exhibits more object poses and configurations, are still restricted since their reference photos mainly consists of single objects centered on relatively plain backgrounds (thus depicts no object interactions). This drawback essentially renders them both unsuitable for our task of scene sketch parsing, where complex mutual object interactions dictate high degree of object pose and configuration variations, as well as subtle details. For example, within a picnic scene depicted in <ref type="figure">Figure 1</ref>, people appear in different poses and configurations with subtle eye contacts among each other. <ref type="figure">Fig. 2(c)</ref> shows a composition result using sketches from Sketchy and TU-Berlin.</p><p>SketchyScene is the first large-scale dataset specifically designed for scenelevel sketch understanding. It differs from all aforementioned datasets in that it goes beyond single object sketch understanding to tackle scene sketch, and purposefully includes an assorted selection of object sketches with diverse poses, configurations and object details to accommodate the complex scene-level object interactions. Although the existing dataset Abstract Scenes <ref type="bibr" target="#b37">[38]</ref> serves a similar motivation for understanding high-level semantic information in visual data, they focus on abstract scenes composed using clip arts, which include much more visual cues such as color and texture. In addition, their scenes are restricted in describing interactions between two characters and a handful of objects, while the scene contents and mutual object interactions in SketchyScene are a lot more diverse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sketch understanding</head><p>Sketch recognition is perhaps the most studied problem in sketch understanding. Since the release of TU-Berlin dataset <ref type="bibr" target="#b8">[9]</ref>, many works have been proposed and recognition performance had long passed human-level <ref type="bibr" target="#b36">[37]</ref>. Existing algorithms can be roughly classified into two categories: 1) those using hand-crafted features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>, and 2) those learning deep feature representation <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b9">10]</ref>, where the latter generally outperforms the former by a clear margin. Other stream of work had delved into parsing object-level sketches into their semantic parts. <ref type="bibr" target="#b25">[26]</ref> proposes an entropy descent stroke merging algorithm for both part-level and object-level sketch segmentation. Huang et al. <ref type="bibr" target="#b12">[13]</ref> leverages a repository of 3D template models composed of semantically labeled components to derive partlevel structures. Schneider and Tuytelaars <ref type="bibr" target="#b20">[21]</ref> performs sketch segmentation by looking at salient geometrical features (such as T junctions and X junctions) under a CRF framework. Instead of studying single object recognition or partlevel sketch segmentation, this work conducts exploratory study for scene-level parsing of sketches, by proposing the first large-scale scene sketch dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Scene sketch based applications</head><p>While no prior work aimed at parsing sketches at scene-level, some interesting applications had been proposed that utilize scene sketches as input. Sketch2Photo <ref type="bibr" target="#b4">[5]</ref> is a system which combines sketching and photo montage for realistic image synthesis, where Sketch2Cartoon <ref type="bibr" target="#b28">[29]</ref> is a similar system that works on cartoon images. Similarly, assuming objects have been segmented in a sketchy scene, Xu et al. <ref type="bibr" target="#b33">[34]</ref> proposed a system named sketch2scene which automatically generates 3D scenes by aligning retrieved 3D shapes to segmented objects in 2D sketch scenes. Sketch2Tag <ref type="bibr" target="#b26">[27]</ref> is a SBIR system where scene items are automatically recognized and used as a text query to improve retrieval performance. Without exception, all aforementioned applications involve manual tagging and/or segmentation of sense sketches. In this work, we provide means of automatic segmentation of scene sketches, and demonstrate the potential of the proposed dataset by proposing a few novel applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SketchyScene Dataset</head><p>A scene sketch dataset should reflect the scenes with sufficient diversity, in terms of their configurations, object interactions and subtle appearance details, where sketch should also contain multiple objects of different categories. Besides, the volume of a dataset is important, especially in the context of deep learning. However, as previously discussed, building such dataset based on existing datasets is infeasible, while collecting data from humans can be expensive and timeconsuming, therefore an efficient and effective data collection pipeline is required.</p><p>The easiest solution is to ask people to draw a scene directly with provided objects or scene labels as hints (i.e., the strategy used in <ref type="bibr" target="#b8">[9]</ref>). Unfortunately, this method has proven to be infeasible in our case: (1) Most people are not trained artists. As a result, they struggled to draw complex objects present in a scene, especially when they are in different poses and object configurations (see <ref type="figure">Fig. 2</ref>(d)); (2) although different people have different drawing styles, people tend to draw a specific scene layout. For example, given the hint "several people are playing on the ground, sun, tree, cloud, balloon and dog", people always draw the objects along a horizontal line. That makes the collected scene sketches monotonous in layout and sparse in visual feature. (3) Importantly, this solution is unscalable -it takes average person around 8 minutes to finish a scene sketch of reasonable quality, where costing 18 minutes for a professional (see <ref type="figure">Fig. 2(e)</ref>). This will prohibit us from collecting a large-scale dataset.</p><p>A new data collection strategy is thus devised which is to synthesize a sketchy scene by composing provided object components under the guidance of a reference image. The whole process includes three steps. Step1: Data Preparation. We selected 45 categories for our dataset, including objects and stuff classes. Specifically, we first considered several common scenes (e.g., garden, farm, dinning room, and park) and extracted 100 objects/stuff classes from them as raw candidates. Then we defined three super-classes, i.e. Weather, Object, and Field (Environment), and assigned the candidates into each super-class. Finally, we selected 45 from them by considering their combinations and commonness in real life.</p><p>Instead of asking workers to draw each object, we provided them with plenty of object sketches (each object candidate is also refer to a "component") as candidates. In order to have enough variations in the object appearance in terms of pose and appearance, we searched and downloaded around 1,500 components for each category. Then we employed 5 experienced workers to manually sort out the sketches containing single component or cutout individual component from sketches having multiple components. For some categories with few searched components (&lt;20) like "umbrella", components were augmented through manual drawing. We totally collected 11,316 components for all 44 categories (excluding "road", which are all hand-drawing, and "others"). These components of each category are split into three sets: training (5468), validation (2362), and test (3486). Representative components of the 45 categories are shown in <ref type="figure" target="#fig_0">Figure 3</ref>.</p><p>In order to guarantee the diversity of the scene layout in our dataset, we additionally collected a set of cartoon photos as reference images. Through sampling the class label(s) from each of our predefined super-classes, e.g., sun (Weather), rabbit (Object), mountain (Environment), we generated 1,800 query items 1 . And around 300 cartoon photos were retrieved for each query items. After removing the repeated ones manually, there are 7,264 reference images (4730 images are unique). These reference images are also split into three sets for training <ref type="bibr" target="#b4">(5,</ref><ref type="bibr">616)</ref>, validation (535), and test (1,113). Step2: Scene Sketch Synthesis. To boost the efficiency of human creators, we devised a customary, web-based application for sketch scene synthesis. About</p><p>Step 1: select a subject appeared in the reference image, then the program will randomly pick 12 candidates images</p><p>Step 2A: if no candidate images are available (e.g. road), sketch with the pen and eraser tool.</p><p>Step 2B: click a candidate image to add it into the scene; the user can move, scale, and rotate the candidate image to align with the reference image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Repeat</head><p>Step 1 and 2 to complete the usersynthetized scene. 80 workers were employed to create scene sketches. <ref type="figure" target="#fig_1">Figure 4</ref> shows the interface of the application (named "USketch").</p><p>As explained before, we facilitated the creation of the sketchy scene images by allowing the worker to drag, rotate, scale, and deform the component sketch with the guidance of the reference image. The process is detailed in <ref type="figure" target="#fig_1">Fig. 4</ref>. It's worth noting that (1) we provided different sets of component sketches (even the same category) to different workers, to implicitly control the diversity of object sketches. Otherwise, workers tend to select the first several samples from the candidate pool; (2) We required the workers to produce as various occlusions as possible during the scene synthesis. This is to simulate the real scenarios and facilitate the research in segmentation. Our server recorded the transformation and semantic labels of each scene item of resulting sketchy scenes.</p><p>At this step, we collected one scene sketch based on each reference image, using the components from the corresponding component repository. We therefore obtained 7,264 unique scene sketches. These unique scene sketches are further used as scene templates to generate more scene sketches.</p><p>Step3: Annotation and Data Augmentation. The reference image is designed to help the worker to compose the scene and enrich the layouts of the scene sketches. However, the objects in a reference image is not necessarily included in our dataset, i.e., 45 categories. In order to facilitate the future research by providing more accurate annotations, we required workers to annotate the alignment status of each object instance.</p><p>Given there are plenty of components in our dataset, an efficient data augmentation strategy is to replace the object sketch with the rest components from the same category. Specifically, we automatically generated another 20 scene sketches for each worker-generated scene, and asked the worker to select the 4 most reasonable scenes for each scene template of Step2. Finally, we got 29K+ sketchy scene images after data augmentation. Dataset Statistics and Analysis. To summarize, we totally obtain: </p><formula xml:id="formula_1">frequency for each category. L L L L L L L L L Fig. 6</formula><p>. From left to right: reference image, synthesized sketchy scene ("L" is used to mark the category alignment), ground-truth of semantic and instance segmentation.</p><p>1. 7,264 unique scene templates created by human. Each scene template contains at least 3 object instances, where the maximum number of object instances is 94. On average there are 16 instances, 6 object classes, and 7 occluded instances per template. The maximum number of occluded instances is 66. <ref type="figure" target="#fig_2">Figure. 5</ref> shows the distribution of object frequencies. 2. 29,056 scene sketches after data augmentation (Step 3); 3. 11,316 object sketches belonging to 44 categories. These components can be used for object-level sketch research tasks; 4. 4730 unique reference cartoon style images which have pairwise object correspondence to the scene sketches; 5. All sketches have 100% accurate semantic-level and instance-level segmentation annotation (as shown in <ref type="figure">Fig. 6</ref>).</p><p>Extensibility. With the scene templates and sketch components provided in the dataset, SketchyScene can be further augmented. (1) People can segment each sketch component to get the part-level or stroke-level information; (2) The sketch components can be replaced by sketches from other resources to generate scene sketches with more varied styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Sketch Scene Segmentation</head><p>SketchyScene can be used to study various computer vision problems. In this section, we focus on semantic segmentation of scene sketches by modifying an existing image segmentation model. Performance is evaluated on SketchyScene to help us identify future research directions.</p><p>Problem Definition. In semantic segmentation, each pixel needs to be classified into one of the candidate classes. Specifically, there is a label space L = {l 1 , l 2 , ..., l K }, K refers to the number of object of stuff classes. Each sketch scene image s = {p 1 , p 2 , ..., p N } ∈ R H×W contains N = W × H pixels. A model trained for semantic segmentation is required to assign a label to each pixel 2 . So far the definition of sketch scene segmentation is identical to that of photo image segmentation. However, different from photos, a sketch only consists of black lines (pixel intensity value equals to 0) and white background (pixel value equals to 255). Given the fact that only black pixels convey semantic information, we define the semantic segmentation in sketchy scenes as predicting a class label for each pixel whose value is 0. Taking the second image of <ref type="figure">Fig. 6</ref> as an example, when segmenting trees, house, sun and cloud, all black pixels on the line segments (including contours and the lines within the contours) should be classified while the rest white pixels are treated as background. Challenges. Segmenting a sketchy scene is challenging due to the sparsity of visual feature. First of all, a sketch scene image is dominated by white pixels. In SketchyScene, the background ratio is 87.83%. The rest pixels belong to K foreground classes. The classes are thus very imbalanced. Second, segmenting occluded objects becomes much harder. In photos, an object instance often contain uniform color or texture. Such cues do not exist in a sketch scene image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Formulation</head><p>We employ a state-of-the-art semantic segmentation model developed for photo images, DeepLab-v2 <ref type="bibr" target="#b2">[3]</ref>, which is customized for segmenting scene sketches. DeepLab-v2 has three key features, including atrous convolution, spatial pyramid spatial pooling (ASPP), and utilizing fully-connected CRF as post-processing. It is a FCN-based <ref type="bibr" target="#b15">[16]</ref> model, i.e., adapting a classification model for segmentation by replacing the final fully connected layer(s) with fully convolutional layer(s). For each input sketch, the output is a K ×h×w tensor, K represents the number of class while h×w are the output segmentation dimension. A common per-pixel softmax cross-entropy is used during training.</p><p>Among the three features, fully-connected CRF, or denseCRF, is widely used in segmentation as a post-process. However, there are large blank areas in scene sketches which should be treated differently. We show that directly applying DeepLab-v2 to model the sketches results in inferior performance, and denseCRF further degrades the coarse segmentation results (see Sec.4.2).</p><p>Based on the characteristics of sketchy scenes, we propose to ignore the background class during modeling. This is because (1) the ratio of background pixels is much higher than the non-background pixels, which may introduce bias into the model; (2) the background information is provided in the input image and we can filter them out easily by treating the input as a mask after segmentation. Specifically, in our implementation, the background pixels do not contribute to the loss during training. During the inference, these background pixels are assigned a non-background class label, followed by a denseCRF for refinement. Finally, the background pixels are filtered out by the input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments</head><p>We conducted all the experiments on SketchyScene, using the set of 7,264 unique scene sketch templates which are split into training (5,616), validation(535), and test <ref type="bibr" target="#b0">(1,</ref><ref type="bibr">113)</ref>. Microsoft COCO is employed to verify the effectiveness of pre-training. Implementation details. We use Tensorflow and ResNet101 as the base network. The initial learning rate is set to 0.0001 and mini-batch size to 1. We set the maximum training iterations as 100K and the optimiser is Adam. We keep the data as their original size (750 * 750), without applying any data augmentation on the input as we are not targeting optimal performance. We use deconvolution to scale the prediction to the same size as the ground truth mask. For denseCRF, we set the hyper parameters σ α , σ β , σ γ to 7, 3, and 3, respectively. Competitors. We compare four existing models for segmenting natural photos: FCN-8s <ref type="bibr" target="#b3">[4]</ref>, SegNet <ref type="bibr" target="#b0">[1]</ref>, DeepLab-v2 <ref type="bibr" target="#b2">[3]</ref> and DeepLab-v3 <ref type="bibr" target="#b15">[16]</ref>. FCN-8s is the first deep segmentation model adapted from deep classification. It further combines coarse and fine features from different layers to boost performance. SegNet employs an encoder-decoder architecture which modifies the upsampling process to generate a more accurate segmentation result. DeepLab-v2 employs atrous convolution and denseCRF for segmentation, as explained in Sec.4. Compared with DeepLab-v2, DeepLab-v3 incorporates global information and batch normalization, achieving comparable performance as DeepLab-v2 without employing denseCRF for refinement. In our experiment, FCN-8s and SegNet use VGG-16 while both DeepLab-v2 and v3 use ResNet101 as the base network. For fair comparison, we apply the same data processing strategy in all four models. Evaluation Metrics. Four metrics are used to evaluate each model: Overall accuracy (OVAcc) indicates the ratio of correctly classified pixels; Mean accuracy (MeanAcc) computes the ratio of the correctly classified pixels over all classes; Mean Intersection over Union (MIoU), a commonly used metric for segmentation, computes the ratio between the intersection and union of two sets, averaged over all classes; FWIoU improves MIoU slightly by adding a class weight. Comparison. <ref type="table" target="#tab_0">Table 1</ref> compares the performance of different baseline models on the new task. Clearly, both DeepLab-v2 and DeepLab-v3 perform much better than FCN and SegNet. However, DeepLab-v3 yielded similar performance as DeepLab-v2, indicating that contextual information does not have much effect for the task. This can be explained by the sparsity of sketchy scenes, and the fact that the structures in a sketch are more diverse than those in natural photos. Thus contextual information is less important than that in natural images. Qualitative Results. <ref type="figure" target="#fig_3">Figure 7</ref> shows several segmentation results generated by DeepLab-v2 (each class is highlighted by a different colour). Although the results are encouraging, there is still large space to improve. In particular, the failure are mainly caused by two reasons: (1) The intra-class variation is large  while sketch itself is significantly deformed. For example, in the bottom image of the fourth column of <ref type="figure" target="#fig_3">Fig. 7</ref>, the "sheep" (highlighted in purple) is classified as a "dog" (highlighted in green); (2) occlusions between different object instances or instances being spatially too close. As shown in the top image of the fourth column, "cat", "human" and "sofa" are clustered together, making the pixels in the junction part classified wrongly. Since sketches are sparse in visual cues and we only utilize pixel-level information, it would thus be helpful to integrate object-level information. Note that the second problem is more challenging and sketch-specific. In photo images, pixels on the contours are typically ignored. However, they are the only pixels of interest in the new task. Therefore, some sketch-specific model design need to be introduced. For examples, some perceptual grouping principles <ref type="bibr" target="#b17">[18]</ref> can be introduced to remedy this problem. See more segmentation results in the Supplementary Material.</p><p>Effect of Background. As discussed earlier, the large area of background is the key problem to be solved for sketchy scene segmentation. We propose to ignore the background class during model training. When considering the background class, it mainly affects two processes, modeling by deep network and refinement by denseCRF. So we decoupled them and conducted the following experiments: (1) withBG (train&amp;test): considering the background during training the deep model and applying denseCRF for refinement, and (2) withBG (train): only consider the background during training but ignore this class for refinement, i.e., when generating the coarse segmentation, the model assigns the background class pixels a non-background class label and then feed it to denseCRF. <ref type="table">Table 2</ref> compares the performance. We can make the following observations: (1) The performance measured in both Mean Accuracy and MIoU have a significant improvement when excluding background as a class. The Overall Accuracy and FWIoU drop since the accuracy on "background" class is much higher than other classes; (2) The processing of background mainly affects the performance of denseCRF. This is as expected because it infers the label for each pixel by considering the neighbor pixels, thus the classes which have large ratio in the images tend to spread out. Some qualitative results are shown in <ref type="figure" target="#fig_4">Fig. 8</ref>. From the images shown in the second column, we can see with the refinement of denseCRF, lots of pixels are merged into "background". The last image shows the result following our proposed data processing.</p><p>Effect of Pre-training. Our final model is pre-trained on ImageNet and finetuned on SketchyScene. In this experiment, we implemented three pre-training variants: (1) Variant-1: Based on the ImageNet pre-trained model, we further pre-trained on the large-scale natural image segmentation dataset, i.e., Microsoft COCO, then fine-tuned on SketchyScene. (2) Variant-2: Instead of pre-training on the natural images, we trained the model on edge maps extracted from the COCO dataset. In this variant, the mask of each object is region-based, i.e., with inner region pixels. (3)Variant-3: To simulate the target task, we further remove the inner region pixels of the mask used in Variant-2. That is, the mask covers the edges only, which is more similar to our final task. <ref type="table">Table 3</ref> shows: (1) pre-training on COCO does not help. This is probably due to the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Other Applications using SketchyScene</head><p>In this section, we propose several interesting applications which are enabled by our SketchyScene dataset. Image retrieval. Here we demonstrate an application of scene-level SBIR, which complements conventional SBIR <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b11">12]</ref> by using scene-level sketches to retrieve images. Considering the objects presented in the sketches of SketchyScene are not 100% aligned with the reference images (as explained in Sec.3), we selected sketch-photo pairs whose semantic-IoU are higher than 0.5 (2,472 pairs for training and validation while 252 for testing). Here semantic-IoU refers to the category-level overlap between the scene sketch and reference image. We develop a triplet ranking network similar to <ref type="bibr" target="#b35">[36]</ref> by changing the base network to InceptionV3 <ref type="bibr" target="#b27">[28]</ref>, and adding a sigmoid cross-entropy loss as the auxiliary loss (this is to utilize the object category information to learn a more domaininvarint feature). We report accuracy at rank1 (acc.@1) and rank10 (acc.@10) inline with other SBIR papers. Overall, we obtain 32.13% on acc.@1 and 69.48% on acc.@10. <ref type="figure" target="#fig_5">Fig. 9</ref> offers an example qualitative retrieval results. Sketch captioning and editing. Here we demonstrate two simple applications, namely sketch captioning and sketch editing (illustrated in <ref type="figure" target="#fig_6">Fig. 10(b)</ref> and (c), respectively). The assumption is, based on the segmentation results, with extra annotations like image description, an image captioning model can be developed based on SketchyScene. Furthermore, people can edit a specific object using the instance-level annotations. As shown in <ref type="figure" target="#fig_6">Fig. 10(c)</ref>, the "chicken" can be  changed to a "duck" while the other objects are kept the same. Both of these two applications could be useful for children education. Sketch colorization. Here we show the potential of using our dataset to achieve automatic sketch colorization, when combined with the recognition and segmentation engine developed in Sec. 4.2. In <ref type="figure" target="#fig_6">Figure 10</ref>(d), we show a colorization result by assigning different colors to different segmented objects, taking into account of their semantic labels (e.g., the sun is red). Dynamic scene synthesis. Finally, we demonstrate a more advanced application, dynamic sketch scene synthesis. We achieve this by manipulating our scene templates to construct a series of frames which are then colorized coherently across all frames. <ref type="figure" target="#fig_6">Fig. 10</ref>(d)-(f) depicts an example, "chicken road crossing".</p><p>6 Conclusion, Discussion, and Future Work</p><p>In this paper, we introduce the first large-scale dataset of scene sketches, termed SketchyScene. It consists of a total of 29,056 scene sketches, generated using 7,264 scene templates and 11,316 object sketches. Each object in the scene is further augmented with semantic labels and instance-level masks. The dataset was collected following a modular data collection process, which makes it highly extensible and scalable. We have shown the main challenges and informative insights of adapting multiple image-based segmentation models to scene sketch data. There are several promising future directions to further enhance our scene sketch dataset, including adding scene-level annotations and text captions to enable applications such as text-based scene generation. Acknowledgment. This work was partially supported by the China National 973 Program (2015CB352501), NSFC-ISF(61561146397), NSERC 611370, and the China Scholarship Council (CSC).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Representative object sketches of SketchyScene.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Interface and work flow of USketch for crowdsourcing the dataset. See areas of function buttons (upper left), component display (lower left), and canvas (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Object instance frequency for each category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Visualizations of our segmentation results. Left: 6 examples with good segmentation results; right: two failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Comparing segmentation results when including/excluding background (BG).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Retrieval results. The corresponding reference image is highlighted by red box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Applications: captioning (b), editing (c), colorization (d), and dynamic scene synthesis (d-f).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Comparison of DeepLab-v2 and other baselines (%)</figDesc><table>Model 
OVAcc 
MeanAcc 
MIoU 
FWIoU 

val 
test 
val 
test 
val 
test 
val 
test 

FCN-8s 
83.38 73.78 62.82 57.80 45.26 39.16 73.63 60.16 
SegNet 
84.61 78.61 58.29 54.05 42.56 38.32 76.28 67.91 
DeepLab-v3 
92.71 88.07 82.83 76.40 73.03 63.69 86.71 79.19 
DeepLab-v2(final) 92.94 88.38 84.95 75.92 73.49 63.10 87.10 79.76 

Table 2. Comparison of including/excluding background (%) 

Model 
OVAcc 
MeanAcc 
MIoU 
FWIoU 

val 
test 
val 
test 
val 
test 
val 
test 

with BG (train&amp;test) 95.38 94.22 42.48 34.56 38.34 30.05 91.29 89.34 
with BG (train) 
90.21 86.41 73.54 66.49 61.50 52.58 82.67 77.09 
w/o BG (final) 
92.94 88.38 84.95 75.92 73.49 63.10 87.10 79.76 

Table 3. Comparison of pre-training strategy (%) 

Model 
OVAcc 
MeanAcc 
MIoU 
FWIoU 

val 
test 
val 
test 
val 
test 
val 
test 

Variant-1 
93.07 88.67 82.23 74.97 71.41 62.12 87.42 80.19 
Variant-2 
91.22 87.08 76.91 71.70 65.41 57.81 84.36 78.01 
Variant-3 
91.47 86.44 79.17 72.24 67.91 58.54 84.80 77.18 
Pre-ImageNet (final) 92.94 88.38 84.95 75.92 73.49 63.10 87.10 79.76 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We add another label "cartoon" to each query in order to retrieve cartoon photos</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In this study, we consider a sketch s as a bitmap image.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">ShapeNet: An information-rich 3D model repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1512.03012</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sketch2photo: internet image montage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="124" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Sketch-pix2seq: a model to generate sketches of multiple categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04121</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.08232</idno>
		<title level="m">Smart, sparse contours to represent and edit images</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">How do humans sketch objects?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03477</idno>
		<title level="m">A neural representation of sketch drawings</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sketch-a-classifier: Sketchbased photo classifier generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A performance evaluation of gradient field hog descriptor for sketch based image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="790" to="806" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data-driven segmentation and labeling of freehand sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="175" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeldriven sketch reconstruction with structure-oriented retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH ASIA</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Technical Briefs</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Making better use of edges via perceptual grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Burnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<title level="m">The sketchy database: Learning to retrieve badly drawn bunnies. SIGGRAPH</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sketch classification and classification-driven analysis using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="174" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Example-based sketch segmentation and labeling using crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="151" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning to sketch with shortcut cycle consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep spatial-semantic attention for fine-grained sketch-based image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Live sketch: Video-driven dynamic deformation of static drawings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CHI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Free hand-drawn sketch segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sketch2tag: automatic hand-drawn sketch recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1255" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sketch2cartoon: Composing cartoon images by sketching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="789" to="790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Sketch-based 3d shape retrieval using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">SUN database: Largescale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sketch2scene: sketch-based co-retrieval and co-placement of 3d models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Sketchmate: Deep hashing for million-scale human sketch retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Sketch me that shoe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sketch-a-net: A deep neural network that beats humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="411" to="425" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Bringing semantics into focus using visual abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Separation of line drawings based on split faces for 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
