<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy-Kien</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tohoku University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Okatani</surname></persName>
							<email>okatani@vision.is.tohoku.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tohoku University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">RIKEN Center for AIP</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>A key solution to visual question answering (VQA)   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>There has been a significant progress in the study of visual question answering (VQA) over a short period of time since its introduction, showing rapid boost of performance for common benchmark datasets. This progress has been mainly brought about by two lines of research, the development of better attention mechanisms and the improvement in fusion of features extracted from an input image and question.</p><p>Since introduced by Bahdanau et al. <ref type="bibr" target="#b2">[3]</ref>, attention has been playing an important role in solutions of various problems of artificial intelligence ranging from tasks using single modality (e.g., language, speech, and vision) to multimodal tasks. For VQA, attention on image regions generated from the input question was first introduced <ref type="bibr" target="#b29">[28]</ref> and then several extensions have been proposed <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b3">4]</ref>. Meanwhile, researchers have proposed several methods for feature fusion <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b33">32]</ref>, where the aim is to obtain better fused representation of image and question pairs. These studies updated the state-of-the-art for common benchmark datasets at the time of each publication.</p><p>We observe that these two lines of research have been independently conducted so far. This is particularly the case with the studies of feature fusion methods, where attention is considered to be optional, even though the best performance is achieved with it. However, we think that they are rather two different approaches towards the same goal. In particular, we argue that a better attention mechanism leads to a better fused representation of image-question pairs.</p><p>Motivated by this, we propose a novel co-attention mechanism for improved fusion of visual and language representations. Given representations of an image and a question, it first generates an attention map on image regions for each question word and an attention map on question words for each image region. It then performs computation of attended features, concatenation of multimodal representations, and their transformation by a single layer network with ReLU and a residual connection. These computations are encapsulated into a composite network that we call dense co-attention layer, since it considers every interaction between any image region and any question word. The layer has fully symmetric architecture between the two modalities, and can be stacked to form a hierarchy that enables multi-step interactions between the image-question pair.</p><p>Starting from initial representations of an input image and question, each dense co-attention layer in the layer stack updates the representations, which are inputted to the next layer. Its final output are then fed to a layer for answer prediction. We use additional attention mechanisms in the initial feature extraction as well as the answer prediction layer. We call the entire network including all these components the dense coattention network (DCN). We show the effectiveness of DCNs by several experimental results; they achieve the new state-of-the-art for VQA 1.0 and 2.0 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we briefly review previous studies of VQA with a special focus on the developments of attention mechanisms and fusion methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Attention Mechanisms</head><p>Attention has proved its effectiveness on many tasks and VQA is no exception. A number of methods have been developed so far, in which question-guided attention on image regions is commonly used. They are categorized into two classes according to the type of employed image features. One is the class of methods that use visual features from some region proposals, which are generated by Edge Boxes <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b10">11]</ref> or Region Proposal Network <ref type="bibr" target="#b25">[24]</ref>. The other is the class of methods that use convolutional features (i.e., activations of convolutional layers) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b29">28,</ref><ref type="bibr" target="#b33">32]</ref>.</p><p>There are several approaches to creation and use of attention maps. Yang et al. <ref type="bibr" target="#b29">[28]</ref> developed stacked attention network that produces multiple attention maps on the image in a sequential manner, aiming at performing multiple steps of reasoning. Kim et al. <ref type="bibr" target="#b12">[13]</ref> extended this idea by incorporating it into a residual architecture to produce better attention information. Chen et al. <ref type="bibr" target="#b3">[4]</ref> proposed a structured attention model that can encode cross-region relation, aiming at properly answering questions that involve complex inter-region relations.</p><p>Earlier studies mainly considered question-guided attention on image regions. In later studies, the opposite orientation of attention, i.e., image-guided attention on question words, is considered additionally. Lu et al. <ref type="bibr" target="#b18">[17]</ref> introduced the co-attention mechanism that generates and uses attention on image regions and on question words. To reduce the gap of image and question features, Yu et al. <ref type="bibr" target="#b32">[31]</ref> utilized attention to extract not only spatial information but also language concept of the image. Yu et al. <ref type="bibr" target="#b33">[32]</ref> combined the mechanism with a novel multi-modal feature fusion of image and question.</p><p>We point out that the existing attention mechanisms only consider a limited amount of possible interactions between image regions and question words. Some consider only attention on image regions from a whole question. Coattention additionally considers attention on question words but it is created from a whole image. We argue that this can be a significant limitation of the existing approaches. The proposed mechanism can deal with every interaction between any image region and any question word, which possibly enables to model unknown complex image-question relations that are necessary for correctly answering questions. </p><formula xml:id="formula_0">Q 1 Q L V 1 V L Figure 1:</formula><p>The global structure of the dense co-attention network (DCN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multimodal Feature Fusion</head><p>The common framework of existing methods is that visual and language features are independently extracted from the image and question at the initial step, and they are fused at a later step to compute the final prediction. In early studies, researchers employed simple fusion methods such as the concatenation, summation, and element-wise product of the visual and language features, which are fed to fully connected layers to predict answers.</p><p>It was then shown by Fukui et al. <ref type="bibr" target="#b4">[5]</ref> that a more complicated fusion method does improve prediction accuracy; they introduced the bilinear (pooling) method that uses an outer product of two vectors of visual and language features for their fusion. As the outer product gives a very high-dimensional feature, they adopt the idea of Gao et al. <ref type="bibr" target="#b5">[6]</ref> to compress the fused feature and name it the Multimodal Compact Bilinear (MCB) pooling method. However, the compacted feature of the MCB method still tends to be high-dimensional to guarantee robust performance, Kim et al. <ref type="bibr" target="#b14">[14]</ref> proposed low-rank bilinear pooling using Hadamard product of two feature vectors, which is called the Multimodal Low-rank Bilinear (MLB) pooling. Pointing out that MLB suffers from slow convergence rate, Yu et al. <ref type="bibr" target="#b33">[32]</ref> proposed the Multi-modal Factorized Bilinear (MFB) pooling, which computes a fused feature with a matrix factorization trick to reduce the number of parameters and improve convergence rate.</p><p>The attention mechanisms can also be considered feature fusion methods, regardless of whether it is explicitly mentioned, since they are designed to obtain a better representation of image-question pairs based on their interactions. This is particularly the case with co-attention mechanisms in which the two features are treated symmetrically. Our dense co-attention network is based on this observation. It fuses the two features by multiple applications of the attention mechanism that can use more fine-grained interactions between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dense Co-Attention Network (DCN)</head><p>In this section, we describe the architecture of DCNs; see <ref type="figure">Fig.1</ref> for its overview. It consists of a stack of dense co-attention layers that fuses language and visual features repeatedly, on top of which an answer prediction layer that predict answers in a multi-label classification setting <ref type="bibr" target="#b25">[24]</ref>. We first explain the initial feature extraction from the input question and image (Sec.3.1) and then describe the dense co-attention layer (Sec.3.2) and the answer prediction layer (Sec.3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Extraction</head><p>We employ pretrained networks that are commonly used in previous studies <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b3">4]</ref> for encoding or extracting features from images, questions, and answers, such as pretrained ResNet <ref type="bibr" target="#b8">[9]</ref> with some differences from earlier studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Question and Answer Representation</head><p>We use bi-directional LSTM for encoding questions and answers. Specifically, a question consisting of N words is first converted into a sequence {e Q 1 , ..., e Q N } of GloVe vectors <ref type="bibr" target="#b22">[21]</ref>, which are then inputted into a one-layer bi-directional LSTM (Bi-LSTM) with a residual connection as</p><formula xml:id="formula_1">− → q n = Bi-LSTM( −−→ q n−1 , e Q n ),<label>(1)</label></formula><formula xml:id="formula_2">← − q n = Bi-LSTM( ←−− q n+1 , e Q n ).<label>(2)</label></formula><p>We then create a matrix</p><formula xml:id="formula_3">Q = [q 1 , ..., q N ] ∈ R d×N where q n = [ − → q n ⊤ , ← − q n ⊤ ] ⊤ (n = 1, . . . , N ).</formula><p>We will also use</p><formula xml:id="formula_4">s Q = [ − → q N ⊤ , ← − q 1 ⊤ ]</formula><p>⊤ , concatenation of the last hidden states in the two paths, for obtaining representation of an input image (Sec.3.1.2). We randomly initialized the Bi-LSTM. It is worth noting that we initially used a pretrained twolayer Bi-LSTM that yields Context Vectors (CoVe) in <ref type="bibr" target="#b19">[18]</ref>, which we found does not contribute to performance.</p><p>We follow a similar procedure to encode answers. An answer of M words is converted into {e </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. , M ). We will use s</head><formula xml:id="formula_5">A = [ − → a M ⊤ , ← − a 1 ⊤ ]</formula><p>⊤ for answer representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Image Representation</head><p>As in many previous studies, we use a pretrained CNN (i.e., a ResNet <ref type="bibr" target="#b8">[9]</ref> with 152 layers pretrained on ImageNet) to extract visual features of multiple image regions, but our extraction method is slightly different. We extract features from four conv. layers and then use a question-guided attention on these layers to fuse their features. We do this to exploit the maximum potential of the subsequent dense co-attention layers. We conjecture that features at different levels in the hierarchy of visual representation <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b31">30]</ref> will be necessary to correctly answer a wide range of questions.</p><p>To be specific, we extract outputs from the four conv. layers (after ReLU) before the last four pooling layers. These are tensors of different sizes (i.e., 256 × 112 × 112, by applying max pooling with a different pooling size and one-by-one convolution to each. We also apply l 2 normalization on the depth dimension of each tensor as in <ref type="bibr" target="#b1">[2]</ref>. We reshape the normalized tensors into four d × T matrices, where T = 14 × 14.</p><formula xml:id="formula_6">Concat FC Attention Comput./weighting Concat FC V l Q l V l+1 Q l+1 V l Q l</formula><p>Next, attention on the four layers is created from s Q , the representation of the whole question defined above. We use a two-layer neural network having 724 hidden units with ReLU non-linearity to project s Q to the scores of the four layers as</p><formula xml:id="formula_7">[s 1 , s 2 , s 3 , s 4 ] = MLP(s Q ),<label>(3)</label></formula><p>which are then normalized by softmax to obtain four attention weights α 1 , . . . , α 4 . The weighted sum of the above four matrices is computed, yielding a d</p><formula xml:id="formula_8">× T matrix V = [v 1 , ..., v T ]</formula><p>, which is our representation of the input image. It stores the image feature at the t-th image region in its t-th column vector of size d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dense Co-Attention Layer 3.2.1 Overview of the Architecture</head><p>We now describe the proposed dense co-attention layer; see <ref type="figure" target="#fig_2">Fig.2</ref>. It takes the question and image representations Q and V as inputs and then outputs their updated versions. We denote the inputs to the (l + 1)-st layer by</p><formula xml:id="formula_9">Q l = [q l1 , ..., q lN ] ∈ R d×N and V l = [v l1 , ..., v lT ] ∈ R d×T . For the first layer inputs, we set Q 0 = Q = [q 1 , ..., q N ] and V 0 = V = [v 1 , ..., v T ].</formula><p>The proposed architecture has the following properties. First, it is a co-attention mechanism <ref type="bibr" target="#b18">[17]</ref>. Second, the coattention is dense in the sense that it considers every interaction between any word and any region. To be specific, our mechanism creates one attention map on regions per each word and creates one attention map on words per each region (see <ref type="figure" target="#fig_3">Fig.3</ref>). Third, it can be stacked as shown in <ref type="figure">Fig.1.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Dense Co-attention Mechanism</head><p>Basic method for attention creation For the sake of explanation, we first explain the basic method for creation of attention maps, which we will extend later. Given Q l and V l , two attention maps are created as shown in <ref type="figure" target="#fig_3">Fig.3</ref>. Their computation starts with the affinity matrix</p><formula xml:id="formula_10">V l Q lV l Q l T x Softmax Softmax x x T A l A &gt; Vl A &gt; Ql</formula><formula xml:id="formula_11">A l = V ⊤ l W l Q l ,<label>(4)</label></formula><p>where W l is a learnable weight matrix. We normalize A l in row-wise to derive attention maps on question words conditioned by each image region as</p><formula xml:id="formula_12">A Q l = softmax(A l ),<label>(5)</label></formula><p>and also normalize A l in column-wise to derive attention maps on image regions conditioned by each question word as</p><formula xml:id="formula_13">A V l = softmax(A ⊤ l ).<label>(6)</label></formula><p>Note that each row of A Q l and A V l contains a single attention map.</p><p>Nowhere-to-attend and memory It often occurs at the creation and application of each attention map that there is no particular region or word that the model should attend.</p><p>To deal with such cases, we add K elements to N question words as well as to T image regions, as in <ref type="bibr" target="#b27">[26]</ref>. In <ref type="bibr" target="#b27">[26]</ref>, the authors only use K = 1, but we found it effective to use K &gt; 1, which is expected to additionally serve as a memory for storing useful information <ref type="bibr" target="#b7">[8]</ref>. More specifically, incorporating two matrices</p><formula xml:id="formula_14">M Q l ≡ [q l⊘1 , ..., q l⊘ K ] ∈ R d×K and M V l ≡ [v l⊘1 , ..., v l⊘ K ] ∈ R d×K</formula><p>, which are learnable parameters, we augment the matrix Q l and V l in the row di-</p><formula xml:id="formula_15">rection asQ l = [q l1 , ..., q lN , q l⊘1 , ..., q l⊘ K ] ∈ R d×(N +K) andṼ l = [v l1 , ..., v lT , v l⊘1 , ..., v l⊘ K ] ∈ R d×(T +K)</formula><p>. This augmentation of Q l and V l provides A l of size (T + K) × (N + K); A Q l and A V l are of size (T + K) × (N + K) and (N + K) × (T + K), respectively.</p><p>Parallel attention In several studies <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">25]</ref>, multiple attention maps are created and applied to target features in a parallel manner, which provides multiple attended features, and then they are fused by concatenation. In <ref type="bibr" target="#b26">[25]</ref>, features are first linearly projected to multiple lower-dimensional spaces, for each of which the above attention function is performed. We adopt a similar approach that uses multiple attention maps here, but we use average instead of concatenation for fusion of the multiple attended features, because we found it works better in our case.</p><p>To be specific, we linearly project the d-dimensional features (stored in</p><note type="other">the columns) ofṼ l andQ l to multiple lower dimensional spaces. Let h be the number of lower dimensional spaces and d h (≡ d/h) be their dimension. We denote the linear projections by W</note><formula xml:id="formula_16">(i) V l ∈ R d h ×d and W (i) Q l ∈ R d h ×d (i = 1, .</formula><p>. . , h). Then the affinity matrix between the projected features in the i-th space is given as</p><formula xml:id="formula_17">A (i) l = (W (i) V lṼ l ) ⊤ (W (i) Q lQ l ).<label>(7)</label></formula><p>Attention maps are created from each affinity matrix by column-wise and row-wise normalization as</p><formula xml:id="formula_18">A (i) Q l = softmax A (i) l √ d h ,<label>(8)</label></formula><formula xml:id="formula_19">A (i) V l = softmax A (i)⊤ l √ d h .<label>(9)</label></formula><p>As we employ multiplicative (or dot-product) attention as explained below, average fusion of multiple attended features is equivalent to averaging our attention maps as</p><formula xml:id="formula_20">A Q l = 1 h h i=1 A (i) Q l ,<label>(10)</label></formula><formula xml:id="formula_21">A V l = 1 h h i=1 A (i) V l .<label>(11)</label></formula><p>Attended feature representations We employ multiplicative attention to derive attended feature representationŝ Q l andV l of the question and image, as shown in <ref type="figure" target="#fig_3">Fig.3</ref>. As A Q l and A V l store attention maps in their rows and their last K rows correspond to "nowhere-to-attend" or memory, we discard them when applying them toQ l andṼ l aŝ</p><formula xml:id="formula_22">Q l =Q l A Q l [1 : T, : ] ⊤ ,<label>(12)</label></formula><formula xml:id="formula_23">andV l =Ṽ l A V l [1 : N, : ] ⊤ ,<label>(13)</label></formula><p>respectively <ref type="bibr" target="#b0">1</ref> . Note thatQ l is the same size as V l (i.e. d×T ) andV l is the same size as Q l (i.e. d × N ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Fusing Image and Question Representations</head><p>After computing the attended feature representationsQ l andV l , we fuse the image and question representations, as shown in the right half of <ref type="figure" target="#fig_2">Fig.2</ref>. The matrixV l stores in its n-th column the attended representation of the entire image conditioned on the n-th question word. Then, the nth column vectorv ln is fused with the representation q ln of n-th question word by concatenation to form 2d-vector [q</p><formula xml:id="formula_24">⊤ ln ,v ⊤ ln ]</formula><p>⊤ . This concatenated vector is projected back to a d-dimensional space by a single layer network followed by the ReLU activation and residual connection as</p><formula xml:id="formula_25">q (l+1)n = ReLU W Q l q ln v ln + b Q l + q ln ,<label>(14)</label></formula><p>where W Q l ∈ R d×2d and b Q l ∈ R d are learnable weights and biases. An identical network (with the same weights and biases) is applied to each question word (n = 1, . . . , N ) independently, yielding</p><formula xml:id="formula_26">Q l+1 = [q (l+1)1 , . . . , q (l+1)N ] ∈ R d×N .</formula><p>Similarly, the representation v lt of t-th image region is concatenated with the representationq lt of the whole question words conditioned on the t-th image region, and then projected back to a d-dimensional space as</p><formula xml:id="formula_27">v (l+1)t = ReLU W V l v lt q lt + b V l + v lt ,<label>(15)</label></formula><p>where W V l ∈ R d×2d and b V l ∈ R d are weights and biases. The application of an identical network to each image region (t = 1, . . . , T ) yields</p><formula xml:id="formula_28">V l+1 = [v (l+1)1 , . . . , v (l+1)T ] ∈ R d×T .</formula><p>It should be noted that the above two fully-connected networks have different parameters (i.e., W Q l , W V l etc.) for each layer l.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Answer Prediction</head><p>Given the final outputs Q L and V L of the last dense coattention layer, we predict answers. As they contain the representation of N question words and T image regions, we first perform self-attention function on each of them to obtain aggregated representations of the whole question and image. This is done for Q L as follows: i) compute 'scores' s q L1 , . . . , s q LN of q L1 , . . . , q LN by applying an identical two-layer MLP with ReLU nonlinearity in its hidden layer; ii) then apply softmax to them to derive attention weights α Using s Q L and s V L thus computed, we predict answers. We consider three methods to do this here. The first one is to compute inner product between the sum of s Q L and s V L and s A , the answer representation defined in Sec.3.1.1, as (score of the answer encoded as where σ is the logistic function and W is a learnable weight matrix. The second and third ones are to use a MLP for computing scores for a set of predefined answers, which is a widely used approach in recent studies. The two differ in how to fuse s Q L and s V L , i.e., summation</p><formula xml:id="formula_29">s A ) = σ s ⊤ A W s Q L + s V L , (16)</formula><formula xml:id="formula_30">(score of answers) = σ MLP s Q L + s V L ,<label>(17)</label></formula><p>or concatenation</p><formula xml:id="formula_31">(score of answers) = σ MLP s Q L s V L ,<label>(18)</label></formula><p>where MLP is a two layer MLP having 1024 hidden units with ReLU non-linearity. The first one is the most flexible, as it allows us to deal with any answers that are not considered at the time of training the entire network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we present results of the experiments conducted to evaluate the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We used two most popular datasets, VQA <ref type="bibr" target="#b1">[2]</ref> and VQA 2.0 <ref type="bibr" target="#b6">[7]</ref>, for our experiments. VQA (also known as VQA 1.0) contains human-annotated question-answer pairs on 204,721 images from Microsoft COCO dataset <ref type="bibr" target="#b16">[16]</ref>. There are three predefined splits of questions, train, val and test or test-standard, which consist of 248,349, 121,512, and  , and is more balanced in term of language bias. We evaluate our models on the challenging Open-Ended task of both datasets.</p><p>As in <ref type="bibr" target="#b25">[24]</ref>, we choose correct answers appearing more than 5 times for VQA and 8 times for VQA 2.0 to form the set of candidate answers. Following previous studies, we train our network on train + val splits and report the test-dev and test-standard results from the VQA evaluation server (except for the ablation test shown below). We use the evaluation protocol of <ref type="bibr" target="#b1">[2]</ref> in all the experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Setup</head><p>For both of the datasets, we use the Adam optimizer with the parameters α = 0.001, β 1 = 0.9, and β 2 = 0.99. During the training procedure, we make the learning rate (α) decay at every 4 epochs for VQA and 7 epochs for VQA 2.0 with an exponential rate of 0.5. All models are trained up to 16 and 21 epochs on VQA and VQA 2.0, respectively. To prevent overfitting, dropouts <ref type="bibr" target="#b24">[23]</ref> are used after each fully connected layers with a dropout ratio p = 0.3 and after the LSTM with a dropout ratio p = 0.1. The batch size is set to 160 and 320 for VQA and VQA 2.0. We set the dimension d of the feature space in the dense co-attention layers (equivalently, the size of its hidden layers) to be 1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>The architecture of the proposed DCN is composed of multiple modules. To evaluate the contribution of each module to final prediction accuracy, we conducted ablation tests. Using VQA 2.0, we evaluated several versions of DCNs with different parameters and settings by training them on the train split and calculating its performance on the val split. The results are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>The first block of the table shows the effects of imagequestion co-attention. The numbers are performances obtained by a DCN with only question-guided attention on images (I ← Q), with only image-guided attention on question words (I → Q), and the standard DCN with coattention (I ↔ Q). The single-direction variants generates only attention in either side of the two paths in the dense co-attention layer; the rest of the computations remain the same. The network with co-attention performs the best, verifying the effectiveness of our co-attention implementation.</p><p>The second block of the table shows the impacts of K, which is the row size of M Q l and M V l that are used for augmenting Q l and V l . This augmentation is originally introduced to be able to deal with "nowhere to attend", which can be implemented by K = 1 <ref type="bibr" target="#b27">[26]</ref>. However, we found that the use of K &gt; 1 improves performance to a certain extent, which we think is because M Q l and M V l work as external memory that can be used through attention mechanism <ref type="bibr" target="#b7">[8]</ref>. As shown in the table, K = 3 yields the best performance.</p><p>The third and fourth blocks of the table show choices of the number h of parallel attention maps and L of stacked layers. The best result was obtained for h = 4 and L = 3.</p><p>The last two blocks of the table show effects of the use of attention in the answer prediction layer and the image extraction layer; the use of attention improves accuracy by about 1.3% and 0.5%, respectively. <ref type="table" target="#tab_1">Table 2</ref> shows the performance of our method on VQA 1.0 along with published results of others. The entries 'DCN (n)' indicate which method for score computation is employed from (16)- <ref type="bibr" target="#b19">(18)</ref>. It is seen from the table that our method outperforms the best published result (MF-SIG-T3) by a large margin of 0.9% ∼ 1.1% on both test-dev and teststandard sets. Furthermore, the improvements can be seen in all of the entries (Other with 1.1%, Number with 3.4%, Yes/No with 0.6% on test-standard set) implying the capacity of DCNs to model multiple types of complex relations between question-image pairs. Notably, we achieve significant improvements of 3.0% and 3.4% for the question type Number on test-dev and test-standard sets, respectively. <ref type="table" target="#tab_1">Table 2</ref> also shows the performances of DCNs with a different answer prediction layer that uses <ref type="bibr" target="#b16">(16)</ref>, <ref type="bibr" target="#b18">(17)</ref>, and (18) for score computation. It is seen that <ref type="formula" target="#formula_1">(17)</ref> shows at least comparable performance to the others and even attains the best performance of 67.02% in test-standard set. <ref type="table" target="#tab_2">Table 3</ref> shows comparisons of our method to previous published results on VQA 2.0 and also that of the winner of VQA 2.0 Challenge 2017 in both test-dev and test-standard sets. It is observed in <ref type="table" target="#tab_2">Table 3</ref> that our approach outperforms the state-of-the-art published method (MF-SIG-T3) by a large margin of 2.1% on test-dev set, even though the MF-SIG-T3 model was trained with VQA 2.0 and an augmented dataset (Visual Genome <ref type="bibr" target="#b15">[15]</ref>). It is noted that the improvements are seen in all the question types (Other with 1.71%, Number with 3.66%, and Yes/No with 2.41%). Comparing our DCN with the winner of VQA 2.0 Challenge 2017, Adelaide model. Our best DCN (17) delivers 1.5% and 1.37% improvements in every question types over the Adelaide+Detector on test-dev and test-standard, respectively. It is worth to point out that the winner method uses a detector (Region Proposal Network) trained on annotated regions of Visual Genome dataset <ref type="bibr" target="#b15">[15]</ref> to extract visual features; and that the model is trained using also an external dataset, i.e., the Visual Genome question answering dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with Existing Methods</head><p>It should also be noted that while achieving the best performance in VQA dataset, the size of the DCNs (i.e., the number of parameters) is comparable or even smaller than the former state-of-the-art methods, as shown in <ref type="table" target="#tab_3">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Evaluation</head><p>Complementary image-question pairs are available in VQA 2.0 <ref type="bibr" target="#b6">[7]</ref>, which are pairs of the same question and different images with different answers. To understand the behaviour of the trained DCN, we visualize attention maps that the DCN generates for some of the complementary image-question pairs. Specifically, we show multiplication of an input image and question with their attention maps α  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>No. params MCB <ref type="bibr" target="#b4">[5]</ref> 63M MLB <ref type="bibr" target="#b14">[14]</ref> 25M MFB <ref type="bibr" target="#b33">[32]</ref> 46M DCN <ref type="bibr" target="#b19">(18)</ref> 32M DCN <ref type="bibr" target="#b18">(17)</ref> 31M DCN <ref type="formula" target="#formula_1">(16)  28M</ref> in <ref type="figure" target="#fig_5">Fig.4</ref>. Each row shows the results for two pairs of the same question and different images, from which we can observe that the DCN is able to look at right regions to find the correct answers. Then, the first column shows the results for two pairs of the same image and different questions. It is observed that the DCN focuses on relevant image regions and question words to produce answers correctly. More visualization results including failure cases are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a novel network architecture for VQA named the dense co-attention network. The core of the network is the dense co-attention layer, which is designed to enable improved fusion of visual and language representations by considering dense symmetric interactions between the input image and question. The layer can be stacked to perform multi-step image-question interactions. The layer stack combined with the initial feature extraction step and the final answer prediction layer, both of which have their own attention mechanisms, form the dense co-attention network. The experimental results on two datasets, VQA and VQA 2.0, confirm the effectiveness of the proposed architecture.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and then inputted to the same Bi-LSTM, yielding the hidden states − → a m and ← − a m (m = 1, . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The internal structure of a single dense coattention layer of layer index l + 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Computation of dense co-attention maps and attended representations of the image and question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Ln . Following the same procedure with an MLP with different weights, we derive attention weights α V 1 , . . . , α V T and then compute an aggregated rep- resentation s V L from v L1 , . . . , v LT .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Typical examples of attended image regions and question words for complementary image-question pairs from VQA 2.0 dataset. Each row contains visualization for two pairs of the same question but different images and answers. The original image and question are shown along with their attention maps generated in the answer prediction layer. The brightness of image pixels and redness of words indicate the attention weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>. . . , α Q N (defined in Sec.3.3) generated in the answer prediction layer. A typical example is shown</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Ablation study on each module of DCNs using the validation set of the Open-Ended task (VQA 2.0). * indicates modules employed in the final model.</figDesc><table>Category 
Detail 
Accuracy 
Attention direction 
I ← Q 
60.95 
I → Q 
62.63 
I ↔ Q* 
62.94 
Memory size (K) 
1 
62.53 
3* 
62.94 
5 
62.83 
Number (h) of 
2 
62.82 
parallel attention 
4* 
62.94 
maps 
8 
62.81 
Number (L) of 
1 
62.43 
stacked layers 
2 
62.82 
3* 
62.94 
4 
62.67 
Attention in answer 
Attention used* 
62.94 
prediction layer 
Avg of features 
61.63 
Attention in image 
Attention used* 
62.94 
extraction layer 
Only last conv layer 
62.39 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Results of the proposed method along with published results of others on VQA 1.0 in similar conditions (i.e., a single model; trained without an external dataset).</figDesc><table>Model 
Test-dev 
Test-standard 
Overall Other Number Yes/No Overall Other Number Yes/No 
VQA team [2] 
57.75 
43.08 
36.77 
80.50 
58.16 
43.73 
36.53 
80.569 
SMem [27] 
57.99 
43.12 
37.32 
80.87 
58.24 
43.48 
37.53 
80.80 
SAN [28] 
58.70 
46.10 
36.60 
79.30 
58.90 
-
-
-
FDA [11] 
59.24 
45.77 
36.16 
81.14 
59.54 
-
-
-
DNMN [1] 
59.40 
45.50 
38.60 
81.10 
59.40 
-
-
-
HieCoAtt [17] 
61.00 
51.70 
38.70 
79.70 
62.10 
-
-
-
RAU [20] 
63.30 
53.00 
39.00 
81.90 
63.20 
52.80 
38.20 
81.70 
DAN [19] 
64.30 
53.90 
39.10 
83.00 
64.20 
54.00 
38.10 
82.80 
Strong Baseline [12] 
64.50 
55.20 
39.10 
82.20 
64.60 
55.20 
39.10 
82.00 
MCB [5] 
64.70 
55.60 
37.60 
82.50 
-
-
-
-
N2NMNs [10] 
64.90 
-
-
-
-
-
-
-
MLAN [31] 
64.60 
53.70 
40.20 
83.80 
64.80 
53.70 
40.90 
83.70 
MLB [14] 
65.08 
54.87 
38.21 
84.14 
65.07 
54.77 
37.90 
84.02 
MFB [32] 
65.90 
56.20 
39.80 
84.00 
65.80 
56.30 
38.90 
83.80 
MF-SIG-T3 [4] 
66.00 
56.37 
39.34 
84.33 
65.88 
55.89 
38.94 
84.42 
DCN (16) 
66.43 
56.23 
42.37 
84.75 
66.39 
56.23 
41.81 
84.53 
DCN (17) 
66.89 
57.31 
42.35 
84.61 
67.02 
56.98 
42.34 
85.04 
DCN (18) 
66.83 
57.44 
41.66 
84.48 
66.66 
56.83 
41.27 
84.61 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Results of the proposed method along with published results of others on VQA 2.</figDesc><table>0 in similar conditions (i.e., a 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Model sizes of DCNs and several bilinear fusion methods. The numbers include the parameters of LSTM networks and exclude those of ResNets.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The notation (1 : T, : ) indicates the submatrix in the first T rows, as in Python.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was partly supported by JSPS KAKENHI Grant Number JP15H05919, JST CREST Grant Number JPMJCR14D1, Council for Science, Technology and Innovation (CSTI), Cross-ministerial Strategic Innovation Promotion Program (Infrastructure Maintenance, Renovation and Management ), and the ImPACT Program Tough Robotics Challenge of the Council for Science, Technology, and Innovation (Cabinet Office, Government of Japan).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computational Linguistics (HLT-NAACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Structured attentions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yanpeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shuaiyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kewei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Compact bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A focused dynamic attention model for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01485</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Show, ask, attend, and answer: A strong baseline for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elqursh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03162</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-O</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multimodal Residual Learning for Visual QA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hadamard product for low-rank bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00107</idno>
		<title level="m">Learned in translation: Contextualized word vectors</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00471</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Training recurrent answering units with joint loss minimization for VQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03647</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Where to look: Focus regions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tips and tricks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02711</idno>
	</analytic>
	<monogr>
		<title level="m">Learnings from the 2017 challenge</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Understanding neural networks through deep visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-level attention networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-modal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
