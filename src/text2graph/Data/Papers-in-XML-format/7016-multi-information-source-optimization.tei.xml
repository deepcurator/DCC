<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Information Source Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Poloczek</surname></persName>
							<email>poloczek@email.arizona.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Systems and Industrial Engineering</orgName>
								<orgName type="department" key="dep2">Chief Analytics Office IBM Armonk</orgName>
								<orgName type="department" key="dep3">School of Operations Research and Information Engineering</orgName>
								<orgName type="institution" key="instit1">University of Arizona Tucson</orgName>
								<orgName type="institution" key="instit2">Cornell University Ithaca</orgName>
								<address>
									<postCode>85721, 10504, 14853</postCode>
									<region>AZ, NY, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Systems and Industrial Engineering</orgName>
								<orgName type="department" key="dep2">Chief Analytics Office IBM Armonk</orgName>
								<orgName type="department" key="dep3">School of Operations Research and Information Engineering</orgName>
								<orgName type="institution" key="instit1">University of Arizona Tucson</orgName>
								<orgName type="institution" key="instit2">Cornell University Ithaca</orgName>
								<address>
									<postCode>85721, 10504, 14853</postCode>
									<region>AZ, NY, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">I</forename><surname>Frazier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Systems and Industrial Engineering</orgName>
								<orgName type="department" key="dep2">Chief Analytics Office IBM Armonk</orgName>
								<orgName type="department" key="dep3">School of Operations Research and Information Engineering</orgName>
								<orgName type="institution" key="instit1">University of Arizona Tucson</orgName>
								<orgName type="institution" key="instit2">Cornell University Ithaca</orgName>
								<address>
									<postCode>85721, 10504, 14853</postCode>
									<region>AZ, NY, NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Information Source Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We consider Bayesian methods for multi-information source optimization (MISO), in which we seek to optimize an expensive-to-evaluate black-box objective function while also accessing cheaper but biased and noisy approximations ("information sources"). We present a novel algorithm that outperforms the state of the art for this problem by using a Gaussian process covariance kernel better suited to MISO than those used by previous approaches, and an acquisition function based on a one-step optimality analysis supported by efficient parallelization. We also provide a novel technique to guarantee the asymptotic quality of the solution provided by this algorithm. Experimental evaluations demonstrate that this algorithm consistently finds designs of higher value at less cost than previous approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We consider Bayesian multi-information source optimization (MISO), in which we optimize an expensive-to-evaluate black-box objective function while optionally accessing cheaper biased noisy approximations, often referred to as "information sources (IS)". This arises when tuning machine learning algorithms: instead of using the whole dataset for the hyperparameter optimization, one may use a small subset or even a smaller related dataset <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>. We also face this problem in robotics: we can evaluate a parameterized robot control policy in simulation, in a laboratory, or in a field test <ref type="bibr" target="#b14">[15]</ref>. Cheap approximations promise a route to tractability, but bias and noise complicate their use. An unknown bias arises whenever a computational model incompletely models a real-world phenomenon, and is pervasive in applications.</p><p>We present a novel algorithm for this problem, misoKG, that is tolerant to both noise and bias and improves substantially over the state of the art. Specifically, our contributions are:</p><p>• The algorithm uses a novel acquisition function that maximizes the incremental gain per unit cost. This acquisition function generalizes and parallelizes previously proposed knowledgegradient methods for single-IS Bayesian optimization <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37]</ref> to MISO.</p><p>• We prove that this algorithm provides an asymptotically near-optimal solution. If the search domain is finite, this result establishes the consistency of misoKG.</p><p>We present a novel proof technique that yields an elegant, short argument and is thus of independent interest.</p><p>Related Work: To our knowledge, MISO was first considered by Swersky, Snoek, and Adams <ref type="bibr" target="#b33">[34]</ref>, under the the name multi-task Bayesian optimization. This name was used to suggest problems in which the auxiliary tasks could meaningfully be solved on their own, while we use the term MISO to indicate that the IS may be useful only in support of the primary task. Swersky et al. <ref type="bibr" target="#b33">[34]</ref> showed that hyperparameter tuning in classification can be accelerated through evaluation on subsets of the validation data. They proposed a GP model to jointly model such "auxiliary tasks" and the primary task, building on previous work on GP regression for multiple tasks in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b34">35]</ref>. They choose points to sample via cost-sensitive entropy search <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">39]</ref>, sampling in each iteration a point that maximally reduces uncertainty in the optimum's location, normalized by the query cost.</p><p>We demonstrate in experiments that our approach improves over the method of Swersky et al. <ref type="bibr" target="#b33">[34]</ref>, and we believe this improvement results from two factors: first, our statistical model is more flexible in its ability to model bias that varies across the domain; second, our acquisition function directly and maximally reduces simple regret in one step, unlike predictive entropy search which maximally reduces the maximizer's entropy in one step and hence only indirectly reduces regret.</p><p>Lam, Allaire, and Willcox <ref type="bibr" target="#b17">[18]</ref> also consider MISO, under the name non-hierarchical multi-fidelity optimization. They propose a statistical model that maintains a separate GP for each IS, and fuse them via the method of Winkler <ref type="bibr" target="#b39">[40]</ref>. They apply a modified expected improvement acquisition function on these surrogates to first decide what design x * to evaluate and then select the IS to query; the latter is decided by a heuristic that aims to balance information gain and query cost. We demonstrate in experiments that our approach improves over the method of Lam et al. <ref type="bibr" target="#b17">[18]</ref>, and we believe this improvement results from two factors: first, their statistical approach assumes an independent prior on each IS, despite their being linked through modeling a common objective; and second their acquisition function selects the point to sample and the IS to query separately via a heuristic rather than jointly using an optimality analysis.</p><p>Beyond these two works, the most closely related work is in the related problem of multi-fidelity optimization. In this problem, IS are supposed to form a strict hierarchy <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b14">15]</ref>. In addition, most of these models limit the information that can be obtained from sources of lower fidelity <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19]</ref>: Given the observation of x at some IS , one cannot learn more about the value of x at IS with higher fidelity by querying IS anywhere else (see Sect. C for details and a proof). Picheny et al. <ref type="bibr" target="#b23">[24]</ref> propose a quantile-based criterion for optimization of stochastic simulators, supposing that all simulators provide unbiased approximations of the true objective. From this body of work, we compare against Kandasamy et al. <ref type="bibr" target="#b14">[15]</ref>, who present an approach for minimizing both simple and cumulative regret, under the assumption that the maximum bias of an information source strictly decreases with its fidelity.</p><p>An interesting special case of MISO is warm-starting Bayesian optimization. Here information sources correspond to samples that were taken previously on objective functions related to the current objective. For example, this scenario occurs when we are to re-optimize whenever parameters of the objective change or whenever new data becomes available. Poloczek et al. <ref type="bibr" target="#b24">[25]</ref> demonstrated that a variant of the algorithm proposed in this article can reduce the optimization costs significantly by warm-starting Bayesian optimization, as does the algorithm of Swersky et al. <ref type="bibr" target="#b33">[34]</ref>.</p><p>Outside of both the MISO and multi-fidelity settings, Klein et al. <ref type="bibr" target="#b16">[17]</ref> considered hyperparameter optimization of machine learning algorithms over a large dataset D. Supposing access to subsets of D of arbitrary sizes, they show how to exploit regularity of performance across dataset sizes to significantly speed up the optimization process for support vector machines and neural networks.</p><p>Our acquisition function is a generalization of the knowledge-gradient policy of Frazier, Powell, and Dayanik <ref type="bibr" target="#b7">[8]</ref> to the MISO setting. This generalization requires extending the one-step optimality analysis used to derive the KG policy in the single-IS setting to account for the impact of sampling a cheap approximation on the marginal GP posterior on the primary task. From this literature, we leverage an idea for computing the expectation of the maximum of a collection of linear functions of a normal random variable, and propose a parallel algorithm to identify and compute the required components.</p><p>The class of GP covariance kernels we propose are a subset of the class of linear models of coregionalization kernels <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b1">2]</ref>, with a restricted form derived from a generative model particular to MISO. Focusing on a restricted class of kernels designed for our application supports accurate inference with less data, which is important when optimizing expensive-to-evaluate functions.</p><p>Our work also extends the knowledge-gradient acquisition function to the variable cost setting. Similar extensions of expected improvement to the variable cost setting can be found in Snoek et al. <ref type="bibr" target="#b31">[32]</ref> (the EI per second criterion) and in Le Gratiet and Cannamela <ref type="bibr" target="#b18">[19]</ref>.</p><p>We now formalize the problem we consider in Sect. 2, describe our statistical analysis in Sect. 3.1, specify our acquisition function and parallel computation method in Sects. 3.2 and 3.3, provide a theoretical guarantee in Sect. 3.4, present numerical experiments in Sect. 4, and conclude in Sect. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Formulation</head><p>Given a continuous objective function g : D → R on a compact set D ⊂ R d of feasible designs, our goal is to find a design with objective value close to max x∈D g(x). We have access to M possibly biased and/or noisy IS indexed by ∈ [M ] 0 . (Here, for any a ∈ Z + we use [a] as a shorthand for the set {1, 2, . . . , a}, and further define [a] 0 = {0, 1, 2, . . . , a}.) Observing IS at design x provides independent, conditional on f ( , x), and normally distributed observations with mean f ( , x) and finite variance λ (x). In <ref type="bibr" target="#b33">[34]</ref>, IS ∈ [M ] 0 are called "auxiliary tasks" and g the primary task. These sources are thought of as approximating g, with variable bias. We suppose that g can be observed directly without bias (but possibly with noise) and set f (0, x) = g(x). The bias f ( , x) − g(x) is also referred to as "model discrepancy" in the engineering and simulation literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>. Each IS is also associated with a query cost function c (x) : D → R + . We assume that the cost function c (x) and the variance function λ (x) are both known and continuously differentiable (over D). In practice, these functions may either be provided by domain experts or may be estimated along with other model parameters from data (see Sect. 4 and B.2, and <ref type="bibr" target="#b26">[27]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The misoKG Algorithm</head><p>We now present the misoKG algorithm and describe its two components: a MISO-focused statistical model in Sect. 3.1; and its acquisition function and parallel computation in Sect. 3.2. Sect. 3.3 summarizes the algorithm and Sect. 3.4 provides a theoretical performance guarantee. Extensions of the algorithm are discussed in Sect. D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Statistical Model</head><p>We now describe a generative model for f that results in a Gaussian process prior on f with a parameterized class of mean functions µ : [M ]×D → R and covariance kernels Σ :</p><formula xml:id="formula_0">([M ]×D)</formula><p>2 → R. This allows us to use standard tools for Gaussian process inference -first finding the MLE estimate of the parameters indexing this class, and then performing Gaussian process regression using the selected mean function and covariance kernel -while also providing better estimates for MISO than would a generic multi-output GP regression kernel that does not consider the MISO application.</p><p>We construct our generative model as follows. For each &gt; 0 suppose that a function δ : D → R for each &gt; 0 was drawn from a separate independent GP, δ ∼ GP (µ , Σ ), and let δ 0 be identically 0. In our generative model δ will be the bias f ( , x) − g(x) for IS . We additionally set µ (x) = 0 to encode a lack of a strong belief on the direction of the bias. (If one had a strong belief that an IS is consistently biased in one direction, one may instead set µ to a constant estimated using maximum a posteriori estimation.) Next, within our generative model, we suppose that g : D → R was drawn from its own independent GP, g ∼ GP (µ 0 , Σ 0 ), for some given µ 0 and Σ 0 , and suppose f ( , x) = f (0, x) + δ (x) for each . We assume that µ 0 and Σ with ≥ 0 belong to one of the standard parameterized classes of mean functions and covariance kernels, e.g., constant µ 0 and Matérn Σ .</p><p>With this construction, f is a GP: given any finite collection of</p><formula xml:id="formula_1">points i ∈ [M ], x i ∈ D with i = 1, . . . , I, (f ( i , x i ) : i = 1, . . . , I</formula><p>) is a sum of independent multivariate normal random vectors, and thus is itself multivariate normal. Moreover, we compute the mean function and covariance kernel of f :</p><formula xml:id="formula_2">for , m ∈ [M ] 0 and x, x ∈ D, µ( , x) = E [f ( , x)] = E [g(x)] + E [δ (x)] = µ 0 (x) Σ (( , x), (m, x )) = Cov(g(x) + δ (x), g(x ) + δ m (x )) = Σ 0 (x, x ) + 1 ,m · Σ (x, x )</formula><p>, where 1 ,m denotes Kronecker's delta, and where we have used independence of δ , δ m , and g. We refer the reader to https://github.com/misoKG/ for an illustration of the model. This generative model draws model discrepancies δ independently across IS. This is appropriate when IS are different in kind and share no relationship except that they model a common objective. In the supplement (Sect. B) we generalize this generative model to model correlation between model discrepancies, which is appropriate when IS can be partitioned into groups, such that IS within one group tend to agree more amongst themselves than they do with IS in other groups. Sect. B also discusses the estimation of the hyperparameters in µ 0 and Σ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Acquisition Function</head><p>Our optimization algorithm proceeds in rounds, selecting a design x ∈ D and an information source ∈ [M ] 0 in each. The value of the information obtained by sampling IS at x is the expected gain in the quality of the best design that can be selected using the available information. That is, this value is the difference in the expected quality of the estimated optimum before and after the sample. We then normalize this expected gain by the cost c (x) associated with the respective query, and sample the IS and design with the largest normalized gain. Without normalization we would always query the true objective, since no other IS provides more information about g than g itself.</p><p>We formalize this idea. Suppose that we have already sampled n points X n and made the observations Y n . Denote by E n the expected value according to the posterior distribution given X n , Y n , and let</p><formula xml:id="formula_3">µ (n) ( , x) := E n [f ( , x)].</formula><p>The best expected objective value across the designs, as estimated by our statistical model, is max x ∈D µ (n) (0, x ). Similarly, if we take an additional sample of IS</p><formula xml:id="formula_4">(n+1)</formula><p>at design x (n+1) and compute our new posterior mean, the new best expected objective value across the designs is max x ∈D µ (n+1) (0, x ), whose distribution depends on what IS we sample, and where sample it. Thus, the expected value of sampling at ( , x) normalized by the cost is</p><formula xml:id="formula_5">MKG n ( , x) = E n max x ∈D µ (n+1) (0, x ) − max x ∈D µ (n) (0, x ) c (x) (n+1) = , x (n+1) = x ,</formula><p>(1) which we refer to as the misoKG factor of the pair ( , x). The misoKG policy then samples at the pair ( , x) that maximizes MKG n ( , x), i.e., (</p><formula xml:id="formula_6">(n+1) , x (n+1) ) ∈ argmax ∈[M ]0,x∈D MKG n ( , x)</formula><p>, which is a nested optimization problem.</p><p>To make this nested optimization problem tractable, we first replace the search domain D in Eq. (1) by a discrete set A ⊂ D of points, for example selected by a Latin Hypercube design. We may then compute MKG n ( , x) exactly. Toward that end, note that</p><formula xml:id="formula_7">E n max x ∈A µ (n+1) (0, x ) (n+1) = , x (n+1) = x = E n max x ∈A {µ (n) (0, x ) +σ n x ( , x) · Z} (n+1) = , x (n+1) = x ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_8">Z ∼ N (0, 1) andσ n x ( , x) = Σ n ((0, x ), ( , x))/ [λ (x) + Σ n (( , x), ( , x))] 1 2</formula><p>. Here Σ n is the posterior covariance matrix of f given X n , Y n .</p><p>We parallelize the computation of MKG n ( , x) for fixed , x, enabling it to utilize multiple cores. Then</p><formula xml:id="formula_9">( (n+1) , x (n+1) ) is obtained by computing MKG n ( , x) for all ( , x) ∈ [M ]</formula><p>0 × A, a task that can be parallelized over multiple machines in a cluster. We begin by sorting the points in A in parallel by increasing value ofσ n x ( , x) (for fixed , x). Thereby we remove some points easily identified as dominated. A point x j is dominated if max i µ (n) (0, x i ) +σ n xi ( , x)Z is unchanged for all Z if the maximum is taken excluding x j . Note that a point x j is dominated by</p><formula xml:id="formula_10">x k ifσ n xj ( , x) =σ n x k ( , x) and µ (n) (0, x j ) ≤ µ (n) (0, x k )</formula><p>, since x k has a higher expected value than x j for any realization of Z. Let S be the sorted sequence without such dominated points. We will remove more dominated points later.</p><p>Since c (x) is a constant for fixed , x, we may express the conditional expectation in Eq. (1) as</p><formula xml:id="formula_11">E n maxi{ai+biZ}−maxi ai c (x) = En[maxi{ai+biZ}−maxi ai] c (x)</formula><p>, where a i = µ (n) (0, x i ) and b i =σ n xi ( , x) for x i ∈ S. We split S into consecutive sequences S 1 , S 2 , . . . , S C , where C is the number of cores used for computing MKG n ( , x) and S i , S i+1 overlap in one element: that is, for S j = {x j1 , . . . , x j k }, x (j−1) k = x j1 and x j k = x (j+1) 1 hold. Each x ji ∈ S j specifies a linear function a ji +b ji Z (ordered by increasing slopes in S). We are interested in the realizations of Z for which a ji +b ji Z ≥ a i +b i Z for any i and hence compute the intersections of these functions. The functions for x ji and</p><formula xml:id="formula_12">x ji+1 intersect in d ji = (a ji −a ji+1 )/(b ji+1 −b ji ). Observe if d ji ≤ d ji−1</formula><p>, then a ji +b ji Z ≤ max{a ji−1 +b ji−1 Z, a ji+1 +b ji+1 Z} for all Z: x ji is dominated and hence dropped from S j . In this case we compute the intersection of the affine functions associated with x j−1 and x j+1 and iterate the process.</p><p>Points in S j may be dominated by the rightmost (non-dominated) point in S j−1 . Thus, we compute the intersection of the rightmost point of S j−1 and the leftmost point of S j , iteratively dropping all dominated points of S j . If all points of S j are dominated, we continue the scan with S j+1 and so on. Observe that we may stop this scan once there is a point that is not dominated, since the points in any sequence S j have non-decreasing d-values. If some of the remaining points in S j are dominated by a point in S j with j &lt; j − 1, then this will be determined when the scan initiated by S j reaches S j . Subsequently, we check the other direction, i.e. whether x j1 dominates elements of S j−1 , starting with the rightmost element of S j−1 . These checks for dominance are performed in parallel for neighboring sequences.</p><p>[8] showed how to compute sequentially the expected maximum of a collection of affine functions. In particular, their Eq.</p><formula xml:id="formula_13">(14) [8, p. 605] gives E n [max i {a i +b i Z} − max i a i ] = C j=1 k−1 h=1 (b j h+1 −b j h )u(−|d j h |)</formula><p>, where u is defined as u(z) = zΦ(z) + φ(z) for the CDF and PDF of the normal distribution. We compute the inner sums simultaneously; the computation of the outer sum could be parallelized by recursively adding pairs of inner sums, although we do not do so to avoid communication overhead. We summarize the parallel algorithm below.</p><p>The Parallel Algorithm to compute ( (n+1) , x (n+1) ):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Scatter the pairs ( , x) ∈ [M ]</head><p>0 × A among the machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Each computes MKG</head><p>n ( , x) for its pairs. To compute MKG n ( , x) in parallel:</p><p>a. Sort the points in A by ascendingσ n x ( , x) in parallel, thereby removing dominated points. Let S be the sorted sequence.</p><p>b. Split S into sequences S 1 , . . . , S C , where C is the number of cores used to compute MKG n ( , x). Each core computes xi∈S C (b i+1 − b i )u(−|d i |) in parallel, then the partial sums are added to obtain E n [max</p><formula xml:id="formula_14">i {a i + b i Z} − max i a i ]. 3. Determine ( (n+1) , x (n+1) ) ∈ argmax ∈[M ]0,x∈D MKG n ( , x) in parallel.</formula><p>3.3 Summary of the misoKG Algorithm.</p><p>1. Using samples from all information sources, estimate hyperparameters of the Gaussian process prior as described in Sect. B.2. Then calculate the posterior on f based on the prior and samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Until the budget for samples is exhausted do:</head><p>Determine the information source ∈[M ] 0 and the design x∈D that maximize the misoKG factor proposed in Eq. <ref type="formula">(1)</ref> and observe IS (x). Update the posterior distribution with the new observation.</p><p>3. Return argmax x ∈A µ (n) (0, x ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Provable Performance Guarantees.</head><p>The misoKG chooses an IS and an x such that the expected gain normalized by the query cost is maximized. Thus, misoKG is one-step Bayes optimal in this respect, by construction.</p><p>We establish an additive bound on the difference between misoKG's solution and the unknown optimum, as the number of queries N → ∞. For this argument we suppose that µ( , x)=0 ∀ , x and Σ 0 is either the squared exponential kernel or a four times differentiable Matérn kernel. Moreover, let x OPT ∈ argmax x ∈D f (0, x ), and d = max x ∈D min x ∈A dist(x , x ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1. Let x *</head><p>N ∈ A be the point that misoKG recommends in iteration N . For each p ∈ [0, 1) there is a constant K p such that with probability p</p><formula xml:id="formula_15">lim N →∞ f (0, x * N ) ≥ f (0, x OPT ) − K p · d.</formula><p>We point out that Frazier, Powell, and Dayanik <ref type="bibr" target="#b7">[8]</ref> showed in their seminal work an analogous result for the case of a single information source with uniform query cost (Theorem 4 in <ref type="bibr" target="#b7">[8]</ref>).</p><p>In Sect. A we prove the statement for the MISO setting that allows multiple information sources that each have query costs c (x) varying over the search domain D. This proof is simple and short. Also note that Theorem 3 establishes consistency of misoKG for the special case that D is finite, since then d = 0. Interestingly, we can compute K p given Σ and p. Therefore, we can control the additive error K p · d by increasing the density of A, leveraging the scalability of our parallel algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Numerical Experiments</head><p>We now compare misoKG to other state-of-the-art MISO algorithms. We implemented misoKG's statistical model and acquisition function in Python 2.7 and C++ leveraging functionality from the Metrics Optimization Engine <ref type="bibr" target="#b22">[23]</ref>. We used a gradient-based optimizer <ref type="bibr" target="#b27">[28]</ref> that first finds an optimizer via multiple restarts for each IS separately and then picks ( (n+1) , x (n+1) ) with maximum misoKG factor among these. An implementation of our method is available at https://github.com/misoKG/.</p><p>We compare to misoEI of Lam et al. <ref type="bibr" target="#b17">[18]</ref> and to MTBO+, an improved version of Multi-Task Bayesian Optimization proposed by Swersky et al. <ref type="bibr" target="#b33">[34]</ref>. Following a recommendation of Snoek 2016, our implementation of MTBO+ uses an improved formulation of the acquisition function given by Hernández-Lobato et al. <ref type="bibr" target="#b11">[12]</ref>, Snoek and et al. <ref type="bibr" target="#b30">[31]</ref>, but otherwise is identical to MTBO; in particular, it uses the statistical model of <ref type="bibr" target="#b33">[34]</ref>. Sect. E provides detailed descriptions of these algorithms.</p><p>Experimental Setup. We conduct experiments on the following test problems: (1) the 2-dimensional Rosenbrock function modified to fit the MISO setting by Lam et al. <ref type="bibr" target="#b17">[18]</ref>; (2) a MISO benchmark proposed by Swersky et al. <ref type="bibr" target="#b33">[34]</ref> in which we optimize the 4 hyperparameters of a machine learning algorithm, using a small, related set of smaller images as cheap IS; (3) an assemble-to-order problem from Hong and Nelson <ref type="bibr" target="#b12">[13]</ref> in which we optimize an 8-dimensional target stock vector to maximize the expected daily profit of a company as estimated by a simulator.</p><p>In MISO settings the amount of initial data that one can use to inform the methods about each information source is typically dictated by the application, in particular by resource constraints and the availability of the respective source. In our experiments all methods were given identical initial datasets for each information source in every replication; these sets were drawn randomly via Latin Hypercube designs. For the sake of simplicity, we provided the same number of points for each IS, set to 2.5 points per dimension of the design space D. Regarding the kernel and mean function, MTBO+ uses the settings provided in <ref type="bibr" target="#b30">[31]</ref>. The other algorithms used the squared exponential kernel and a constant mean function set to the average of a random sample.</p><p>We report the "gain" over the best initial solution, that is the true objective value of the respective design that a method would return at each iteration minus the best value in the initial data set. If the true objective value is not known for a given design, we report the value obtained from the information source of highest fidelity. This gain is plotted as a function of the total cost, that is the cumulative cost for invoking the information sources plus the fixed cost for the initial data; this metric naturally generalizes the number of function evaluations prevalent in Bayesian optimization. Note that the computational overhead of choosing the next information source and sample is omitted, as it is negligible compared to invoking an information source in real-world applications. Error bars are shown at the mean ± 2 standard errors averaged over at least 100 runs of each algorithm. For deterministic sources a jitter of 10 −6 is added to avoid numerical issues during matrix inversion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Rosenbrock Benchmarks</head><p>We consider the design space D = [−2, 2] 2 , and M = 2 information sources. IS 0 is the Rosenbrock function g(x) = (1 − x 1 ) 2 + 100 · (x 2 − x g(x)+v ·sin(10·x 1 +5·x 2 ), where the additional oscillatory component serves as model discrepancy.</p><p>We assume a cost of 1000 for each query to IS 0 and a cost of 1 for IS 1.</p><p>Since all methods converged to good solutions within few queries, we investigate the ratio of gain to cost: <ref type="figure" target="#fig_0">Fig. 1</ref> (l) displays the gain of each method over the best initial solution as a function of the total cost inflicted by querying information sources. The new method misoKG offers a significantly better gain per unit cost and finds an almost optimal solution typically within 5 − 10 samples. Interestingly, misoKG relies only on cheap samples, proving its ability to successfully handle uncertainty. MTBO+, on the other hand, struggles initially but then eventually obtains a near-optimal solution, too. To this end, it makes usually one or two queries of the expensive truth source after about 40 steps. misoEI shows a odd behavior: it takes several queries, one of them to IS 0, before it improves over the best initial design for the first time. Then it jumps to a very good solution and subsequently samples only the cheap IS.</p><p>For the second setup, we set u = 1, v = 2, and suppose for IS 0 uniform noise of λ 0 (x) = 1 and query cost c 0 (x) = 50. Now the difference in costs is much smaller, while the variance is considerably bigger. The results are displayed in <ref type="figure" target="#fig_0">Fig. 1 (r)</ref>: as for the first configuration, misoKG outperforms the other methods from the start. Interestingly, misoEI's performance is drastically decreased compared to the first setup, since it only queries the expensive truth. Looking closer, we see that misoKG initially queries only the cheap information source IS 1 until it comes close to an optimal value after about five samples. It starts to query IS 0 occasionally later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Image Classification Benchmark</head><p>This classification problem was introduced by Swersky et al. <ref type="bibr" target="#b33">[34]</ref> to demonstrate that MTBO can reduce the cost of hyperparameter optimization by leveraging a small dataset as information source. The goal is to optimize four hyperparameters of the logistic regression algorithm <ref type="bibr" target="#b35">[36]</ref> using a stochastic gradient method with mini-batches (the learning rate, the L2-regularization parameter, the batch size, and the number of epochs) to minimize the classification error on the MNIST dataset <ref type="bibr" target="#b20">[21]</ref>. This dataset contains 70,000 images of handwritten digits: each image has 784 pixels. IS 1 uses the USPS dataset <ref type="bibr" target="#b37">[38]</ref> of about 9000 images with 256 pixels each. The query costs are 4.5 for IS 1 and 43.69 for IS 0. A closer examination shows that IS 1 is subject to considerable bias with respect to IS 0, making it a challenge for MISO algorithms. <ref type="figure">Fig.2</ref> (l) summarizes performance: initially, misoKG and MTBO+ are on par. Both clearly outperform misoEI that therefore was stopped after 50 iterations. misoKG and MTBO+ continued for 150 steps (with a lower number of replications). misoKG usually achieves an optimal test error of about 7.1% on the MNIST testset after about 80 queries, matching the classification performance of the best setting reported by Swersky et al. <ref type="bibr" target="#b33">[34]</ref>. Moreover, misoKG achieves better solutions than MTBO+ at the same costs. Note that the results in <ref type="bibr" target="#b33">[34]</ref> show that MTBO+ will also converge to the optimum eventually.  <ref type="figure">Figure 2</ref>: (l) The performance on the image classification benchmark of <ref type="bibr" target="#b33">[34]</ref>. misoKG achieves better test errors after about 80 steps and converges to the global optimum. (r) misoKG outperforms the other algorithms on the assemble-to-order benchmark that has significant model discrepancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Assemble-To-Order Benchmark</head><p>The assemble-to-order (ATO) benchmark is a reinforcement learning problem from a business application where the goal is to optimize an 8-dimensional target level vector over <ref type="bibr">[0,</ref><ref type="bibr" target="#b19">20]</ref> 8 (see Sect. G for details). We set up three information sources: IS 0 and 2 use the discrete event simulator of Xie et al. <ref type="bibr" target="#b41">[42]</ref>, whereas the cheapest source IS 1 invokes the implementation of Hong and Nelson. IS 0 models the truth.</p><p>The two simulators differ subtly in the model of the inventory system. However, the effect in estimated objective value is significant: on average the outputs of both simulators at the same target vector differ by about 5% of the score of the global optimum, which is about 120, whereas the largest observed bias out of 1000 random samples was 31.8. Thus, we are witnessing a significant model discrepancy. <ref type="figure">Fig. 2 (r)</ref> summarizes the performances. misoKG outperforms the other algorithms from the start: misoKG averages at a gain of 26.1, but inflicts only an average query cost of 54.6 to the information sources. This is only 6.3% of the query cost that misoEI requires to achieve a comparable score. Interestingly, misoKG and MTBO+ utilize mostly the cheap biased IS, and therefore are able to obtain significantly better gain to cost ratios than misoEI. misoKG's typically first calls IS 2 after about 60 − 80 steps. In total, misoKG queries IS 2 about ten times within the first 150 steps; in some replications misoKG makes one late call to IS 0 when it has already converged. Our interpretation is that misoKG exploits the cheap, biased IS 1 to zoom in on the global optimum and switches to the unbiased but noisy IS 2 to identify the optimal solution exactly. This is the expected (and desired) behavior for misoKG when the uncertainty of f (0, x * ) is not expected to be reduced sufficiently by queries to IS 1. MTBO+ trades off the gain versus cost differently: it queries IS 0 once or twice after 100 steps and directs all other queries to IS 1, which might explain the observed lower performance. misoEI, which employs a two-step heuristic for trading off predicted gain and query cost, almost always chose to evaluate the most expensive IS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a novel algorithm for MISO that uses a mean function and covariance matrix motivated by a MISO-specific generative model. We have proposed a novel acquisition function that extends the knowledge gradient to the MISO setting and comes with a fast parallel method for computing it. Moreover, we have provided a theoretical guarantee on the solution quality delivered by this algorithm, and demonstrated through numerical experiments that it improves significantly over the state of the art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (l) The Rosenbrock benchmark with the parameter setting of [18]: misoKG offers an excellent gain-to-cost ratio and outperforms its competitors substantially. (r) The Rosenbrock benchmark with the alternative setup.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by NSF CAREER CMMI-1254298, NSF CMMI-1536895, NSF IIS-1247696, AFOSR FA9550-12-1-0200, AFOSR FA9550-15-1-0038, and AFOSR FA9550-16-1-0046.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A mathematical and computational framework for multifidelity design and analysis with computer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Allaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Willcox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal for Uncertainty Quantification</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Kernels for vector-valued functions: A review. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="195" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-task gaussian process prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">V</forename><surname>Bonilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning about physical parameters: the importance of model discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brynjarsdottir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>O&amp;apos;hagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Probability and Stochastics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Çınlar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graduate texts in Mathematics</title>
		<imprint>
			<biblScope unit="volume">261</biblScope>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-fidelity optimization via surrogate modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Forrester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sóbester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Keane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">463</biblScope>
			<biblScope unit="page" from="3251" to="3269" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A knowledge-gradient policy for sequential information collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I</forename><surname>Frazier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dayanik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2410" to="2439" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Knowledge Gradient Policy for Correlated Normal Beliefs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I</forename><surname>Frazier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dayanik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INFORMS Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="599" to="613" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Posterior consistency of Gaussian process prior for nonparametric binary regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2413" to="2429" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Geostatistics for Natural Resources Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goovaerts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
		<respStmt>
			<orgName>Oxford University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Entropy search for information-efficient global optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1809" to="1837" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predictive entropy search for efficient global optimization of black-box functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="918" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discrete optimization via simulation using compass</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="129" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Sequential kriging optimization using multiplefidelity evaluations. Structural and Multidisciplinary Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Notz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="369" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gaussian process bandit optimisation with multi-fidelity evaluations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dasarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<ptr target="https://github.com/kirthevasank/mf-gp-ucb.LastAccessedon04/22/2017" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predicting the output from a complex computer code when fast approximations are available</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>O&amp;apos;hagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fast bayesian optimization of machine learning hyperparameters on large datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1605.07079</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multifidelity optimization using statistical surrogate modeling for non-hierarchical information sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Allaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Willcox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">56th AIAA/ASCE/AHS/ASC Structures, Structural Dynamics, and Materials Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cokriging-based sequential design strategies using fast cross-validation techniques for multi-fidelity computer codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Gratiet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cannamela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="418" to="427" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recursive co-kriging model for design of computer experiments with multiple levels of fidelity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Gratiet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garnier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal for Uncertainty Quantification</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/.LastAccessedon05/15/2017" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Envelope theorems for arbitrary choice sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milgrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Segal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="583" to="601" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moe</surname></persName>
		</author>
		<ptr target="http://yelp.github.io/MOE/,2016.LastAccessedon05/15/2017" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Quantile-based optimization of noisy computer experiments with tunable precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Picheny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ginsbourger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Richet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Caplin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="13" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Warm starting bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poloczek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I</forename><surname>Frazier</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1608.03585" />
	</analytic>
	<monogr>
		<title level="m">Winter Simulation Conference (WSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequential selection with unknown correlation structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">O</forename><surname>Ryzhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="931" to="948" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Gaussian Processes for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<idno>ISBN 0-262-18253-X</idno>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The correlated knowledge gradient for simulation optimization of continuous parameters using gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I</forename><surname>Frazier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="996" to="1026" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Parallel predictive entropy search for batch global optimization of expensive objective functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3330" to="3338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Personal communication</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<ptr target="http://github.com/HIPS/Spearmint,2017.LastAccessedon05/15/2017" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0912.3995</idno>
		<title level="m">Gaussian process optimization in the bandit setting: No regret and experimental design</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-task bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semiparametric latent factor models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Logistic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Theano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Theano</surname></persName>
		</author>
		<ptr target="http://deeplearning.net/tutorial/code/logistic_sgd.py.LastAccessedon05/16/2017" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stratified bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Toscano-Palmerin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I</forename><surname>Frazier</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1602.02338" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Monte Carlo and Quasi-Monte Carlo Methods in Scientific Computing, 2016. Accepted for Publication</title>
		<meeting>the 12th International Conference on Monte Carlo and Quasi-Monte Carlo Methods in Scientific Computing, 2016. Accepted for Publication</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usps</forename><surname>Dataset</surname></persName>
		</author>
		<ptr target="http://mldata.org/repository/data/viewslug/usps/.LastAccessedon05/16/2017" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An informational approach to the global optimization of expensive-to-evaluate functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Villemonteix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="509" to="534" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Combining probability distributions from dependent information sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="479" to="488" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bayesian optimization with gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poloczek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I</forename><surname>Frazier</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1703.04389" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, 2017. Accepted for Publication</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Assemble to order simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I</forename><surname>Frazier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chick</surname></persName>
		</author>
		<ptr target="http://simopt.org/wiki/index.php?title=Assemble_to_Order&amp;oldid=447" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
