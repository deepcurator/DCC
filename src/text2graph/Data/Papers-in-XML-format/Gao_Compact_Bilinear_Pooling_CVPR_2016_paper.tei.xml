<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader>
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:24+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compact Bilinear Pooling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
							<email>obeijbom@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
							<email>ning.zhang@snapchat.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Snapchat Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
							<email>trevor@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">UC Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Compact Bilinear Pooling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract xml:lang="en">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text>
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Encoding and pooling of visual features is an integral part of semantic image analysis methods. Before the influential 2012 paper of Krizhevsky et al. <ref type="bibr" target="#b16">[17]</ref> rediscovering the models pioneered by <ref type="bibr" target="#b18">[19]</ref> and related efforts, such methods typically involved a series of independent steps: feature extraction, encoding, pooling and classification; each thoroughly investigated in numerous publications as the bag of visual words (BoVW) framework. Notable contributions include HOG <ref type="bibr" target="#b8">[9]</ref>, and SIFT <ref type="bibr" target="#b23">[24]</ref> descriptors, fisher encoding <ref type="bibr" target="#b25">[26]</ref>, bilinear pooling <ref type="bibr" target="#b2">[3]</ref> and spatial pyramids <ref type="bibr" target="#b17">[18]</ref>, each significantly improving the recognition accuracy.</p><p>Recent results have showed that end-to-end backpropagation of gradients in a convolutional neural network We propose a compact bilinear pooling method for image classification. Our pooling method is learned through end-to-end back-propagation and enables a lowdimensional but highly discriminative image representation. Top pipeline shows the Tensor Sketch projection applied to the activation at a single spatial location, with * denoting circular convolution. Bottom pipeline shows how to obtain a global compact descriptor by sum pooling.</p><p>(CNN) enables joint optimization of the whole pipeline, resulting in significantly higher recognition accuracy. While the distinction of the steps is less clear in a CNN than in a BoVW pipeline, one can view the first several convolutional layers as a feature extractor and the later fully connected layers as a pooling and encoding mechanism. This has been explored recently in methods combining the feature extraction architecture of the CNN paradigm, with the pooling &amp; encoding steps from the BoVW paradigm <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b7">8]</ref>. Notably, Lin et al. recently replaced the fully connected layers with bilinear pooling achieving remarkable improvements for fine-grained visual recognition <ref type="bibr" target="#b22">[23]</ref>. However, their final representation is very high-dimensional; in their paper the encoded feature dimension, d, is more than 250, 000. Such representation is impractical for several reasons: (1) if used with a standard one-vs-rest linear classifier for k classes, the number of model parameters becomes kd, which for e.g. k = 1000 means &gt; 250 million model parameters, <ref type="bibr" target="#b1">(2)</ref> for retrieval or deployment scenarios which require features to be stored in a database, the storage becomes expensive; storing a millions samples requires 2TB of storage at dou-ble precision, (3) further processing such as spatial pyramid matching <ref type="bibr" target="#b17">[18]</ref>, or domain adaptation <ref type="bibr" target="#b10">[11]</ref> often requires feature concatenation; again, straining memory and storage capacities, and (4) classifier regularization, in particular under few-shot learning scenarios becomes challenging <ref type="bibr" target="#b11">[12]</ref>. The main contribution of this work is a pair of bilinear pooling methods, each able to reduce the feature dimensionality three orders of magnitude with little-to-no loss in performance compared to a full bilinear pooling. The proposed methods are motivated by a novel kernelized viewpoint of bilinear pooling, and, critically, allow back-propagation for end-to-end learning.</p><p>Our proposed compact bilinear methods rely on the existence of low dimensional feature maps for kernel functions. Rahimi <ref type="bibr" target="#b28">[29]</ref> first proposed a method to find explicit feature maps for Gaussian and Laplacian kernels. This was later extended for the intersection kernel, χ 2 kernel and the exponential χ 2 kernel <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36]</ref>. We show that bilinear features are closely related to polynomial kernels and propose new methods for compact bilinear features based on algorithms for the polynomial kernel first proposed by Kar <ref type="bibr" target="#b14">[15]</ref> and Pham <ref type="bibr" target="#b26">[27]</ref>; a key aspect of our contribution is that we show how to back-propagate through such representations.</p><p>Contributions: The contribution of this work is threefold. First, we propose two compact bilinear pooling methods, which can reduce the feature dimensionality two orders of magnitude with little-to-no loss in performance compared to a full bilinear pooling. Second, we show that the back-propagation through the compact bilinear pooling can be efficiently computed, allowing end-to-end optimization of the recognition network. Third, we provide a novel kernelized viewpoint of bilinear pooling which not only motivates the proposed compact methods, but also provides theoretical insights into bilinear pooling. Implementations of the proposed methods, in Caffe and MatConvNet, are publicly available: https://github.com/ gy20073/compact_bilinear_pooling</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Bilinear models were first introduced by Tenenbaum and Freeman <ref type="bibr" target="#b31">[32]</ref> to separate style and content. Second order pooling have since been considered for both semantic segmentation and fine grained recognition, using both handtuned <ref type="bibr" target="#b2">[3]</ref>, and learned features <ref type="bibr" target="#b22">[23]</ref>. Although repeatedly shown to produce state-of-the art results, it has not been widely adopted; we believe this is partly due to the prohibitively large dimensionality of the extracted features.</p><p>Several other clustering methods have been considered for visual recognition. Leung and Malik used vector quantization in the Bag of Visual Words (BoVW) framework <ref type="bibr" target="#b19">[20]</ref> initially used for texture classification, but later adopted for other visual tasks. VLAD <ref type="bibr" target="#b13">[14]</ref> and Improved Fisher Vector <ref type="bibr" target="#b25">[26]</ref>   <ref type="table">Table 1</ref>: Pooling methods property overview: Fully connected pooling <ref type="bibr" target="#b16">[17]</ref>, is compact and can be learned end-toend by back propagation, but it requires a fixed input image size and is less discriminative than other methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23]</ref>. Fisher encoding is more discriminative but high dimensional and can not be learned end-to-end <ref type="bibr" target="#b7">[8]</ref>. Bilinear pooling is discriminative and tune-able but very high dimensional <ref type="bibr" target="#b22">[23]</ref>. Our proposed compact bilinear pooling is as effective as bilinear pooling, but much more compact.</p><p>by including second order information in the descriptors. Fisher vector has been recently been used to achieved startof-art performances on many data-sets <ref type="bibr" target="#b7">[8]</ref>.</p><p>Reducing the number of parameters in CNN is important for training large networks and for deployment (e.g. on embedded systems). Deep Fried Convnets <ref type="bibr" target="#b39">[40]</ref> aims to reduce the number of parameters in the fully connected layer, which usually accounts for 90% of parameters. Several other papers pursue similar goals, such as the Fast Circulant Projection which uses a circular structure to reduce memory and speed up computation <ref type="bibr" target="#b5">[6]</ref>. Furthermore, Network in Network <ref type="bibr" target="#b21">[22]</ref> uses a micro network as the convolution filter and achieves good performance when using only global average pooling. We take an alternative approach and focus on improving the efficiency of bilinear features, which outperform fully connected layers in many studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Compact bilinear models</head><p>Bilinear pooling <ref type="bibr" target="#b22">[23]</ref> or second order pooling <ref type="bibr" target="#b2">[3]</ref> forms a global image descriptor by calculating:</p><formula xml:id="formula_0">B(X ) = s∈S x s x T s (1)</formula><p>where X = (x 1 , . . . , x |S| , x s ∈ R c ) is a set of local descriptors, and S is the set of spatial locations (combinations of rows &amp; columns). Local descriptors, x s are typically extracted using SIFT <ref type="bibr" target="#b23">[24]</ref>, HOG <ref type="bibr" target="#b8">[9]</ref> or by a forward pass through a CNN <ref type="bibr" target="#b16">[17]</ref>. As defined in (1), B(X ) is a c × c matrix, but for the purpose of our analysis, we will view it as a length c 2 vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">A kernelized view of bilinear pooling</head><p>Image classification using bilinear descriptors is typically achieved using linear Support Vector Machines (SVM) or logistic regression. These can both be viewed as linear kernel machines, and we provide an analysis below <ref type="bibr" target="#b0">1</ref> . Given two sets of local descriptors: X and Y, a linear kernel machine compares these as:</p><formula xml:id="formula_1">B(X ), B(Y) = s∈S x s x T s , u∈U y u y T u = s∈S u∈U x s x T s , y u y T u = s∈S u∈U x s , y u 2 (2)</formula><p>From the last line in (2), it is clear that the bilinear descriptor compares each local descriptor in the first image with that in the second image and that the comparison operator is a second order polynomial kernel. Bilinear pooling thus gives a linear classifier the discriminative power of a second order kernel-machine, which may help explain the strong empirical performance observed in previous work <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Compact bilinear pooling</head><p>In this section we define the proposed compact bilinear pooling methods. Let k(x, y) denote the comparison kernel, i.e. the second order polynomial kernel. If we could find some low dimensional projection function φ(x) ∈ R d , where d &lt;&lt; c 2 , that satisfy φ(x), φ(y) ≈ k(x, y), then we could approximate the inner product of (2) by:</p><formula xml:id="formula_2">B(X ), B(Y) = s∈S u∈U x s , y u 2 ≈ s∈S u∈U φ(x), φ(y) ≡ C(X ), C(Y) ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">C(X ) := s∈S φ(x s )<label>(4)</label></formula><p>is the compact bilinear feature. It is clear from this analysis that any low-dimensional approximation of the polynomial kernel can be used to towards our goal of creating a compact bilinear pooling method. We investigate two such approximations: Random Maclaurin (RM) <ref type="bibr" target="#b14">[15]</ref> and Tensor Sketch (TS) <ref type="bibr" target="#b26">[27]</ref>, detailed in Alg. 1 and Alg. 2 respectively. RM is an early approach developed to serve as a low dimensional explicit feature map to approximate the polynomial kernel <ref type="bibr" target="#b14">[15]</ref>. The intuition is straight forward. <ref type="bibr" target="#b0">1</ref> We ignore the normalization (signed square root and ℓ 2 normalization) which is typically applied before classification If w 1 , w 2 ∈ R c are two random −1, +1 vectors and</p><formula xml:id="formula_4">φ(x) = w 1 , x w 2 , x , then for non-random x, y ∈ R c , E[φ(x)φ(y)] = E[ w 1 , x w 1 , y ] 2 = x, y 2 .</formula><p>Thus each projected entry in RM has an expectation of the quantity to be approximated. By using d entries in the output, the estimator variance could be brought down by a factor of 1/d. TS uses sketching functions to improve the computational complexity during projection and tend to provide better approximations in practice <ref type="bibr" target="#b26">[27]</ref>. Similar to the RM approach, Count Sketch <ref type="bibr" target="#b3">[4]</ref>, defined by Ψ(x, h, s) in Algorithm 2, has the favorable property that:</p><formula xml:id="formula_5">E[ Ψ(x, h, s), Ψ(y, h, s) ] = x, y [4]. Moreover, one can show that Ψ(x ⊗ y, h, s) = Ψ(x, h, s) * Ψ(y, h, s), i.</formula><p>e. the count sketch of two vectors' outer product is the convolution of individual's count sketch <ref type="bibr" target="#b26">[27]</ref>. Then the same approximation in expectation follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Back propagation of compact bilinear pooling</head><p>In this section we derive back-propagation for the two compact bilinear pooling methods and show they're efficient both in computation and storage.</p><p>For RM, let L denote the loss function, s the spatial index, d the projected dimension, n the index of the training sample and y n d ∈ R the output of the RM layer at dimension d for instance n. Back propagation of RM pooling can then be written as:</p><formula xml:id="formula_6">∂L ∂x n s = d ∂L ∂y n d k W k (d), x n s Wk(d) ∂L ∂W k (d) = n ∂L ∂y n d s Wk(d), x n s x n s<label>(5)</label></formula><p>where k = 1, 2,k = 2, 1, and</p><formula xml:id="formula_7">W k (d) is row d of matrix W k .</formula><p>For TS, using the same notation,</p><formula xml:id="formula_8">∂L ∂x n s = d ∂L ∂y n d k T k d (x n s ) • s k ∂L ∂s k = n,d ∂L ∂y n d s T k d (x n s ) • x n s<label>(6)</label></formula><p>where</p><formula xml:id="formula_9">T k d (x) ∈ R c and T k d (x) c = Ψ(x, hk, sk) d−h k (c) . When d − h k (c) is negative, it denotes the circular index (d − h k (c)) + D,</formula><p>where D is the projected dimensionality. Note that in TS, we could only get a gradient for s k . h k is combinatorial, and thus fixed during back-prop.</p><p>The back-prop equation for RM can be conveniently written as a few matrix multiplications. It has the same computational and storage complexity as its forward pass, and can be calculated efficiently. Similarly, Equation 6 can also be expressed as a few FFT, IFFT and matrix multiplication operations. The computational and storage complexity of TS are also similar to its forward pass. <ref type="table">Table 2</ref>: Dimension, memory and computation comparison among bilinear and the proposed compact bilinear features. Parameters c, d, h, w, k represent the number of channels before the pooling layer, the projected dimension of compact bilinear layer, the height and width of the previous layer and the number of classes respectively. Numbers in brackets indicate typical value when bilinear pooling is applied after the last convolutional layer of VGG-VD <ref type="bibr" target="#b30">[31]</ref> model on a 1000-class classification task, i.e. c = 512, d = 10, 000, h = w = 13, k = 1000. All data are stored in single precision.</p><formula xml:id="formula_10">Full Bilinear Random Maclaurin (RM) Tensor Sketch (TS) Dimension c 2 [262K] d [10K] d [10K] Parameters Memory 0 2cd [40MB] 2c [4KB] Computation O(hwc 2 ) O(hwcd) O(hw(c + d log d)) Classifier Parameter Memory kc 2 [1000MB] kd [40MB] kd [40MB]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Random Maclaurin Projection</head><p>Input:</p><formula xml:id="formula_11">x ∈ R c Output: feature map φ RM (x) ∈ R d , such that φ RM (x), φ RM (y) ≈ x, y 2 1. Generate random but fixed W 1 , W 2 ∈ R</formula><p>d×c , where each entry is either +1 or −1 with equal probability.</p><formula xml:id="formula_12">2. Let φ RM (x) ≡ 1 √ d (W 1 x) • (W 2 x)</formula><p>, where • denotes element-wise multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Tensor Sketch Projection</head><p>Input:</p><formula xml:id="formula_13">x ∈ R c Output: feature map φ T S (x) ∈ R d , such that φ T S (x), φ T S (y) ≈ x, y 2 1.</formula><p>Generate random but fixed h k ∈ N c and s k ∈ {+1, −1} c where h k (i) is uniformly drawn from {1, 2, . . . , d}, s k (i) is uniformly drawn from {+1, −1}, and k = 1, 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Next, define sketch function</p><formula xml:id="formula_14">Ψ(x, h, s) = {(Qx) 1 , . . . , (Qx) d }, where (Qx) j = t:h(t)=j s(t)x t 3. Finally, define φ T S (x) ≡ FFT −1 (FFT(Ψ(x, h 1 , s 1 )) • FFT(Ψ(x, h 2 , s 2 )))</formula><p>, where the • denotes element-wise multiplication. <ref type="table">Table 2</ref> shows the comparison among bilinear and compact bilinear feature using RM and TS projections. Numbers indicated in brackets are the typical values when applying VGG-VD <ref type="bibr" target="#b30">[31]</ref> with the selected pooling method on a 1000-class classification task. The output dimension of our compact bilinear feature is 2 orders of magnitude smaller than the bilinear feature dimension. In practice, the proposed compact representations achieve similar performance to the fully bilinear representation using only 2% of the bilinear feature dimension, suggesting a remarkable 98% redundancy in the bilinear representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Some properties of compact bilinear pooling</head><p>The RM projection requires moderate amounts of parameter memory (i.e. the random generated but fixed matrix), while TS require almost no parameter memory. If a linear classifier is used after the pooling layer, i.e, a fully connected layer followed by a softmax loss, the number of classifier parameters increases linearly with the pooling output dimension and the number of classes. In the case mentioned above, classification parameters for bilinear pooling would require 1000MB of storage. Our compact bilinear method, on the other hand, requires far fewer parameters in the classification layer, potentially reducing the risk of over-fitting, and performing better in few shot learning scenarios <ref type="bibr" target="#b11">[12]</ref>, or domain adaptation <ref type="bibr" target="#b10">[11]</ref> scenarios.</p><p>Computationally, Tensor Sketch is linear in d log d + c, whereas bilinear is quadratic in c, and Random Maclaurin is linear in cd <ref type="table">(Table 2</ref>). In practice, the computation time of the pooling layers is dominated by that of the convolution layers. With the Caffe implementation and K40c GPU, the forward backward time of the 16-layer VGG <ref type="bibr" target="#b30">[31]</ref> on a 448× 448 image is 312ms. Bilinear pooling requires 0.77ms and TS (with d = 4096) requires 5.03ms . TS is slower because FFT has a larger constant factor than matrix multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Alternative dimension reduction methods</head><p>PCA, which is a commonly used dimensionality reduction method, is not a viable alternative in this scenario due to the high dimensionality of the bilinear feature. Solving a PCA usually involves operations on the order of O(d 3 ), where d is the feature dimension. This is impractical for the high dimensionality, d = 262K used in bilinear pooling.</p><p>Lin et al. <ref type="bibr" target="#b22">[23]</ref> circumvented these limitations by using PCA before forming the bilinear feature, reducing the bilinear feature dimension on CUB200 <ref type="bibr" target="#b38">[39]</ref> from 262,000 to 33,000. While this is a substantial improvement, it still accounts for 12.6% of the original dimensionality. Moreover, the PCA reduction technique requires an expensive initial sweep over the whole dataset to get the principle components. In contrast, our proposed compact bilinear methods do not require any pre-training and can be as small as 4096 dimensions. For completeness, we compare our method to this baseline in Section 4.3.</p><p>Another alternative is to use a random projections. However, this requires forming the whole bilinear feature and projecting it to lower dimensional using some random linear operator. Due to the Johnson-Lindenstrauss lemma <ref type="bibr" target="#b9">[10]</ref>, the random projection largely preserves pairwise distances between the feature vectors. However, deploying this method requires constructing and storing both the bilinear feature and the fixed random projection matrix. For example, for VGG-VD, the projection matrix will have a shape <ref type="figure" target="#fig_0">N (0, 1)</ref>, would occupy 10.5GB of memory, which is too much for a high-end GPU such as K40. A sparse random projection matrix would improve the memory consumption to around 40MB <ref type="bibr" target="#b20">[21]</ref>, but would still requires forming bilinear feature first. Furthermore, it requires sparse matrix operations on GPU, which are inevitably slower than dense matrix operations, such as the one used in RM (Alg. 1).</p><note type="other">of c 2 × d, where c and d are the number of channels in the previous layer and the projected dimension, as above. With d = 10, 000 and c = 512, the projection matrix has 2.6 billion entries, making it impractical to store and work with. A classical dense random Gaussian matrix, with entries being i.i.d.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we detail four sets of experiments. First, in Sec. 4.2, we investigate some design-choices of the proposed pooling methods: appropriate dimensionality, d and whether to tune the projection parameters, W . Second, in Sec. 4.3, we conduct a baseline comparison against a PCA based compact pooling method. Third, in Sec. 4.4, we look at how bilinear pooling in general, and the proposed compact methods in particular, perform in comparison to state-of-the-art on three common computer vision benchmark data-sets. Fourth, in Sec. 4.5, we investigate a situation where a low-dimensional representation is particularly useful: few-shot learning. We begin by providing the experimental details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental details</head><p>We evaluate our design on two network structures: the M-net in <ref type="bibr" target="#b4">[5]</ref> (VGG-M) and the D-net in <ref type="bibr" target="#b30">[31]</ref> (VGG-D). We use the convolution layers of the each network as the local descriptor extractor. More precisely, in the notation of Sec. 3, x s is the activation at each spatial location of the convolution layer output. Specifically, we retain the first 14 layers of VGG-M (conv 5 + ReLU) and the first 30 layers in VGG-D (conv 5 3 + ReLU), as used in <ref type="bibr" target="#b22">[23]</ref>. In addition to bilinear pooling, we also compare to fully connected layer and improved fisher vector encoding <ref type="bibr" target="#b25">[26]</ref>. The latter one is known to outperform other clustering based coding methods <ref type="bibr" target="#b7">[8]</ref>, such as hard or soft vector quantization <ref type="bibr" target="#b19">[20]</ref> and VLAD <ref type="bibr" target="#b13">[14]</ref>. All experiments are performed using MatConvNet <ref type="bibr" target="#b33">[34]</ref>, and we use 448 × 448 input image size, except fully connected pooling as mentioned below. . Before the final classification layer, we add an element-wise signed square root layer (y = sign(x) |x|) and an instance-wise ℓ 2 normalization.</p><p>Compact Bilinear Pooling: Our two proposed compact bilinear pooling methods are evaluated in the same exact experimental setup as the bilinear pooling, including the signed square root layer and the ℓ 2 normalization layer. Both compact methods are parameterized by a used-defined projection dimension d and a set of random generated projection parameters. For notational convenience, we use W to refer to the projection parameters, although they are generated and used differently (Algs. 1, 2). When integer constraints are relaxed, W can be learned as part of the endto-end back-propagation. The appropriate setting of d, and of whether or not to tune W , depends on the amount of training data, memory budget, and the difficulty of the classification task. We discuss these design choices in Sec. 4.2; in practice we found that d = 8000 is sufficient for reaching close-to maximum accuracy, and that tuning the projection parameters has a positive, but small, boost.</p><p>Fully Connected Pooling: The fully connected baseline refer to a classical fine tuning scenario, where one starts from a network trained on a large amount of images, such as VGG-M, and replace the last classification layer with a random initialized k-way classification layer before finetuning. We refer to this as the "fully connected" because this method has two fully connected layers between the last convolution layer and the classification layer. This method requires a fixed input image sizes, dictated by the network structure. For the VGG nets used in this work, the input size is 224 × 224, and we thus re-size all images to this size for this method.</p><p>Improved Fisher Encoding: Similarly to bilinear pooling, fisher encoding <ref type="bibr" target="#b25">[26]</ref> has recently been used as an encoding &amp; pooling alternative to the fully connected layers <ref type="bibr" target="#b7">[8]</ref>. Following <ref type="bibr" target="#b7">[8]</ref>, the activations of last convolutional layer (excluding ReLU) are used as input the encoding step, and the encoding uses 64 GMM components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Learning Configuration</head><p>During fine-tuning, we initialized the last layer using the weights of the trained logistic regression and attach a corresponding logistic loss. We then fine tune the whole network until convergence using a constant small learning rate of 10 −3 , a weight decay of 5 × 10 −4 , a batch size of 32 for VGG-M and 8 for VGG-D. In practice, convergence occur in &lt; 100 epochs. Note that for RM and TS, backpropagation can be used simply as a way to tune the deeper layers of the network (as it is used in full bilinear pooling), or to also tune the projection parameters, W . We investigate both options in Sec. 4.2. Fisher vector has an unsupervised dictionary learning phase, and it is unclear how to perform fine-tuning <ref type="bibr" target="#b7">[8]</ref>. We therefore do not evaluate Fisher Vector under fine-tuning.</p><p>In Sec. 4.2 we also evaluate each method as a feature extractor. Using the forward-pass through the network, we train a linear classifier on the activations. We use ℓ 2 regularized logistic regression: λ||w|| 2 2 + i l( x i , w , y i ) with λ = 0.001 as we found that it slightly outperforms SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Configurations of compact pooling</head><p>Both RM and TS pooling have a user defined projection dimension d, and a set of projection parameters, W . To investigate the parameters of the proposed compact bilinear methods, we conduced extensive experiments on the CUB-200 <ref type="bibr" target="#b36">[37]</ref> dataset which contains 11,788 images of 200 bird species, with a fixed training and testing set split. We evaluate in the mode where part annotations are not provided at neither training nor testing time, and use VGG-M for all experiments in this section. <ref type="figure" target="#fig_2">Fig. 2</ref> summarizes our results. As the projection dimension d increases, the two compact bilinear methods reach the performance of the full bilinear pooling. When not finetuned, the error of TS with d = 16K is 1.7% less than that of bilinear feature, while only using 6.1% of the original number of dimensions. When fine tuned, the performance gap disappears: TS with d = 16K has an error rate of 22.66%, compared to 22.44% of bilinear pooling.</p><p>In lower dimension, RM outperforms TS, especially when tuning W . This may be because RM pooling has more parameters, which provides additional learning capacity despite the low-dimensional output <ref type="table">(Table 2)</ref>. Conversely, TS outperforms RM when d &gt; 2000. This is consistent with the results of Pham &amp; Pagm, who evaluated these projections methods on several smaller data-sets <ref type="bibr" target="#b26">[27]</ref>. Note that these previous studies did not use pooling nor fine-tuning as part of their experimentation. <ref type="figure" target="#fig_2">Fig. 2</ref> also shows performances using extremely low dimensional representation, d = 32, 128 and 512. While the performance decreased significantly for the fixed representation, fine-tuning brought back much of the discriminative capability. For example, d = 32 achieved less than 50% error on the challenging 200-class fine grained classification task. Going up slightly, to 512 dimensions, it yields 25.54% error rate. This is only 3.1% drop in performance compared to the 250,000 dimensional bilinear feature. Such extremely compact but highly discriminative image feature represen- tations are useful, for example, in image retrieval systems. For comparison, Wang et al. used a 4096 dimensional feature embedding in their recent retrieval system <ref type="bibr" target="#b37">[38]</ref>.</p><p>In conclusion, our experiments suggest that between 2000 and 8000 features dimension is appropriate. They also suggest that the projection parameters, W should only be tuned when one using extremely low dimensional representations (the 32 dimensional results is an exception). Our experiments also confirmed the importance of fine-tuning, emphasizing the critical importance of using projection methods which allow fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to the PCA-Bilinear baseline</head><p>As mentioned in Section 3.3, a simple alternative dimensionality reduction method would be to use PCA before bilinear pooling <ref type="bibr" target="#b22">[23]</ref>. We compare this approach with our compact Tensor Sketch method on the CUB <ref type="bibr" target="#b36">[37]</ref> dataset with VGG-M <ref type="bibr" target="#b4">[5]</ref> network. The PCA-Bilinear baseline is implemented by inserting an 1 × 1 convolution before the bilinear layer with weights initialized by PCA. The number of outputs, k of this convolutional layer will determine the feature dimension (k 2 ).</p><p>Results with various k 2 are shown in <ref type="table" target="#tab_2">Table 3</ref>. The gap between the PCA-reduced bilinear feature and TS feature is large especially when the feature dimension is small and network not fine tuned. When fine tuned, the gap shrinks but the PCA-Bilinear approach is not good at utilizing larger dimensions. For example, the PCA approach reaches a 23.8% error rate at 16K dimensions, which is larger than the 23.2% error rate of TS at 4K dimensions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation across multiple data-sets</head><p>Bilinear pooling has been studied extensively. Carreira et al. used second order pooling to facilitate semantic segmentation <ref type="bibr" target="#b2">[3]</ref>. Lin et al. used bilinear pooling for finegrained visual classification <ref type="bibr" target="#b22">[23]</ref>, and Rowchowdhury used bilinear pooling for face verification <ref type="bibr" target="#b29">[30]</ref>. These methods all achieved state-of-art on the respective tasks indicating the wide utility of bilinear pooling. In this section we show that the compact representations perform on par with bilinear pooling on three very different image classification tasks. Since the compact representation requires orders of magnitude less memory, this suggests that it is the preferable method for a wide array of visual recognition tasks.</p><p>Fully connected pooling, fisher vector encoding, bilinear pooling and the two compact bilinear pooling methods are compared on three visual recognition tasks: fine-grained visual categorization represented by CUB-200-2011 <ref type="bibr" target="#b36">[37]</ref>, scene recognition represented by the MIT indoor scene recognition dataset <ref type="bibr" target="#b27">[28]</ref>, and texture classification represented by the Describable Texture Dataset <ref type="bibr" target="#b6">[7]</ref>. Sample figures are provided in <ref type="figure" target="#fig_3">Fig. 3</ref>, and dataset details in <ref type="table">Table 5</ref>. Guided by our results in Sec. 4.2 we use d = 8192 dimensions and fix the projection parameters W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Bird species recognition</head><p>CUB is a fine-grained visual categorization dataset. Good performance on this dataset requires identification of overall bird shape, texture and colors, but also capacity to focus on subtle differences, such as the beak-shapes. The only supervision we use is the image level class labels, without referring to either part or bounding box annotations.</p><p>Our results indicate that bilinear and compact bilinear pooling outperforms fully connected and fisher vector by a large margin, both with and without fine-tuning <ref type="table">(Table 4)</ref>. Among the compact bilinear methods, TS consistently outperformed RS. For the larger VGG-D network, bilinear pooling achieved 19.90% error rate before fine tuning, while RM and TS achieved 21.83% and 20.50% respectively. This is a modest 1.93% and 0.6% performance loss considering the huge reduction in feature dimension (from 250k to 8192). Notably, this difference disappeared after fine-tuning when the bilinear pooling methods all reached an error rate of 16.0%. This is, to the best of our knowledge, the state of the art performance on this dataset without part annotation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref>. The story is similar for the smaller VGG-M network: TS is more favorable than RM and the performance gap between compact full bilinear shrinks to 0.5% after fine tuning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Indoor scene recognition</head><p>Scene recognition is quite different from fine-grained visual categorization, requiring localization and classification of discriminative and non-salient objects. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, the intra-class variation can be quite large.</p><p>As expected, and previously observed <ref type="bibr" target="#b7">[8]</ref>, improved Fisher vector encoding outperformed fully connected pooling by 6.87% on the MIT scene data-set <ref type="table">(Table 4</ref>). More surprising, bilinear pooling outperformed Fisher vector by 3.03%. Even though bilinear pooling was proposed for object-centric tasks, such as fine grained visual recognition, this experiment thus suggests that is it appropriate also for scene recognition. Compact TS performs slightly worse (0.94%) than full bilinear pooling, but 2.09% better than Fisher vector. This is notable, since fisher vector is used in the current state-of-art method for this dataset <ref type="bibr" target="#b7">[8]</ref>.</p><p>Surprisingly, fine-tuning negatively impacts the errorrates of the full and compact bilinear methods, about 2%. We believe this is due to the small training-set size and large number of convolutional weights in VGG-D, but it deserves further attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Texture classification</head><p>Texture classification is similar to scene recognition in that it requires attention to small features which can occur anywhere in the image plane. Our results confirm this, and  <ref type="table">Table 4</ref>: Classification error of fully connected (FC), fisher vector, full bilinear (FB) and compact bilinear pooling methods, Random Maclaurin (RM) and Tensor Sketch (TS). For RM and TS we set the projection dimension, d = 8192 and we fix the projection parameters, W . The number before and after the slash represents the error without and with fine tuning respectively. Some fine tuning experiments diverged, when VGG-D is fine-tuned on MIT dataset. These are marked with an asterisk and we report the error rate at the 20th epoch.</p><p>Data-set # train img # test img # classes CUB <ref type="bibr" target="#b36">[37]</ref> 5994 5794 200 MIT <ref type="bibr" target="#b27">[28]</ref> 4017 1339 67 DTD <ref type="bibr" target="#b6">[7]</ref> 1880 3760 47 <ref type="table">Table 5</ref>: Summary statistics of data-sets in Sec. 4.4</p><p>we see similar trends as on the MIT data-set <ref type="table">(Table 4)</ref>. Again, Fisher encoding outperformed fully connected pooling by a large margin and RM pooling performed on par with Fisher encoding, achieving ∼ 34.5% error-rate using VGG-D. Both are out-performed by ∼ 2% using full bilinear pooling which achieves 32.50%. The compact TS pooling method achieves the strongest results at 32.29% errorrate using the VGG-D network. This is 2.18% better than the fisher vector and the lowest reported single-scale error rate on this data-set 2 . Again, fine-tuning did not improve the results for full bilinear pooling or TS, but it did for RM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">An application to few-shot learning</head><p>Few-shot learning is the task of generalizing from a very small number of labeled training samples <ref type="bibr" target="#b11">[12]</ref>. It is important in many deployment scenarios where labels are expensive or time-consuming to acquire <ref type="bibr" target="#b0">[1]</ref>.</p><p>Fundamental results in learning-theory show a relationship between the number of required training samples and the size of the hypothesis space (VC-dimension) of the classifier that is being trained <ref type="bibr" target="#b32">[33]</ref>. For linear classifiers, the hypothesis space grows with the feature dimensions, and we therefore expect a lower-dimensional representation to be better suited for few-shot learning scenarios. We investigate this by comparing the full bilinear pooling method (d = 250, 000) to TS pooling (d = 8192). For these experiments we do not use fine-tuning and use VGG-M as the local feature extractor.</p><p>When only one example is provided for each class, TS <ref type="bibr" target="#b1">2</ref> Cimpoi et al. extract descriptors at several scales to achieve their stateof-the-art results <ref type="bibr" target="#b7">[8]</ref> achieves a score of 15.5%, which is a 22.8% relative improvement over full bilinear pooling, or 2.9% in absolute value, confirming the utility of a low-dimensional descriptor for few-shot learning. The gap remains at 2.5% with 3 samples per class or 600 training images. As the number of shots increases, the scores of TS and the bilinear pooling increase rapidly, converging around 15 images per class, which is roughly half the dataset. <ref type="table">(Table 5</ref>  <ref type="table">Table 6</ref>: Few shot learning comparison on the CUB dataset. Results given as mean average precision for k training images from each of the 200 categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have modeled bilinear pooling in a kernelized framework and suggested two compact representations, both of which allow back-propagation of gradients for end-to-end optimization of the classification pipeline. Our key experimental results is that an 8K dimensional TS feature has the same performance as a 262K bilinear feature, enabling a remarkable 96.5% compression. TS is also more compact than fisher encoding, and achieves stronger results. We believe TS could be useful for image retrieval, where storage and indexing are central issues or in situations which require further processing: e.g. part-based models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13]</ref>, conditional random fields, multi-scale analysis, spatial pyramid pooling or hidden Markov models; however these studies are left to future work. Further, TS reduces network and classification parameters memory significantly which can be critical e.g. for deployment on embedded systems. Finally, after having shown how bilinear pooling uses a pairwise polynomial kernel to compare local descriptors, it would be interesting to explore how alternative kernels can be incorporated in deep visual recognition systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: We propose a compact bilinear pooling method for image classification. Our pooling method is learned through end-to-end back-propagation and enables a lowdimensional but highly discriminative image representation. Top pipeline shows the Tensor Sketch projection applied to the activation at a single spatial location, with * denoting circular convolution. Bottom pipeline shows how to obtain a global compact descriptor by sum pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Pooling: Both VGG-M and VGG-D have 512 channels in the final convolutional layer, meaning that the bilinear feature dimension is 512×512 ≈ 250K. We use a symmetric underlying network structure, corresponding to the B-CNN[M,M] and B-CNN[D,D] configurations in [23]. We did not experiment with the asymmetric structure such as B-CNN[M, D] because it is shown to have similar perfor- mance as the B-CNN[D,D] [23]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Classification error on the CUB dataset. Comparison of Random Maclaurin (RM) and Tensor Sketch (TS) for various combinations of projection dimensions and finetuning options. The two horizontal lines shows the performance of fine-tuned and non fine-tuned Fully Bilinear (FB).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Samples images from the three datasets examined in Sec. 4.4. Each row contains samples from indigo bunting in CUB, jewelery shop in MIT, and honey comb in DTD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>encoding improved over hard vector quantization</figDesc><table>Com-
pact 

Highly dis-
criminative 

Flexible 
input size 

End-to-end 
learnable 

Fully con-
nected 

✓ 
✗ 
✗ 
✓ 

Fisher en-
coding 

✗ 
✓ 
✓ 
✗ 

Bilinear 
pooling 

✗ 
✓ 
✓ 
✓ 

Compact 
bilinear 

✓ 
✓ 
✓ 
✓ 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Comparison between PCA reduced feature and TS.</figDesc><table>Numbers refer to Top 1 error rates without and with fine 
tuning respectively. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>).</figDesc><table># images 
1 
2 
3 
7 
14 

Bilinear 
12.7 21.5 27.4 41.7 53.9 
TS 
15.6 24.1 29.9 43.1 54.3 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated annotation of coral reef survey images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Edmunds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kline</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1170" to="1177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient largescale structured learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1806" to="1813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic segmentation with second-order pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="430" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Finding frequent items in data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farach-Colton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automata, languages and programming</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="693" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3531</idno>
		<title level="m">Return of the devil in the details: Delving deep into convolutional nets</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03436</idno>
		<title level="m">Fast neural networks with circulant projections</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep filter banks for texture recognition, description, and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02620</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An elementary proof of the Johnson-Lindenstrauss lemma. International Computer Science Institute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="99" to="105" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0907.1815</idno>
		<title level="m">Frustratingly easy domain adaptation</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Random feature maps for dot product kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Karnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fine-grained recognition without part annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5546" to="5555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Representing and recognizing the visual appearance of materials using three-dimensional textons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Very sparse random projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Bilinear CNN models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.07889</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features. In Computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The proceedings of the seventh IEEE international conference on</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Max-margin additive classifiers for detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="40" to="47" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast and scalable polynomial kernels via explicit feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recognizing indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="413" to="420" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learnedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01342</idno>
		<title level="m">Face identification with bilinear CNNs</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Separating style and content with bilinear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1247" to="1283" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The nature of statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">MatConvNet -convolutional neural networks for MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient additive kernels via explicit feature maps. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="480" to="492" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generalized RBF feature maps for efficient detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vempati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1386" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>Caltech-UCSD birds 200. 2010. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moczulski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7149</idno>
		<title level="m">Deep fried convnets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
