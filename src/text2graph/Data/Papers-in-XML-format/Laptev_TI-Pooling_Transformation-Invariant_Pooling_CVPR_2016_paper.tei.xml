<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TI-POOLING: transformation-invariant pooling for feature learning in Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Laptev</surname></persName>
							<email>dlaptev@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Savinov</surname></persName>
							<email>nikolay.savinov@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
							<email>jbuhmann@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
							<email>marc.pollefeys@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TI-POOLING: transformation-invariant pooling for feature learning in Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In this paper we present a deep neural network topology that incorporates a simple to implement transformationinvariant pooling operator (TI-POOLING</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent advances in deep learning lead to impressive results in various applications of machine learning and computer vision to different fields. These advances are largely attributed to expressiveness of deep neural networks with many parameters, that are effectively able to approximate any decision function in the data space <ref type="bibr" target="#b17">[18]</ref>.</p><p>While this is true for all the neural network architectures with many layers and with sufficient number of parameters, the most impressive results are being achieved in the fields * The authors assert equal contribution and joint first authorship.</p><p>where deep architectures heavily rely on internal structure of the input data, such as speech recognition, natural language processing and image recognition <ref type="bibr" target="#b15">[16]</ref>. For example, convolutional neural networks <ref type="bibr" target="#b11">[12]</ref> learn kernels to be applied on images or signals reflecting the spatial or temporal dependencies between the neighbouring pixels or moments in time. This structural information serves for internal regularization through weight sharing in convolutional layers <ref type="bibr" target="#b16">[17]</ref>. When combined with the expressiveness of multilayer neural networks, it allows for learning very rich feature representation of input data with little to no preprocessing.</p><p>Incorporating structural information permits to work with the inner dependencies in the representation of the data, but only few works have addressed the possible use of other structural prior information known about the data. For example, many datasets in computer vision contain some nuisance variations, such as rotations, shifts, scale changes, illumination variations, etc. These variations are in many cases known in advance from experts collecting the data and one can significantly improve the performance when being considered during training.</p><p>The effect is even more explicit when dealing with domain-specific problems. E.g. in many medical imaging datasets, the rotation can be irrelevant due to symmetric nature of some biological structures. At the same time, the scale is fixed during imaging process and should not be considered as a nuisance factor. Moreover scale-invariance can even harm the performance if object size is at least somehow informative, for example, in case of classifying healthy cells from cancer cells <ref type="bibr" target="#b23">[24]</ref>. We describe one biomedical example in details in section 4.2.</p><p>The state of the art approach to deal with these variations and the most popular one in deep learning is data augmentation <ref type="bibr" target="#b26">[27]</ref> -a powerful technique that transforms the data point according to some predefined rules and uses it as a separate training sample during the learning procedure. The most common transformations being used in general computer vision are rotations, scale changes and random crops. This approach works especially good when applied with deep learning algorithms, because the models in deep learning are extremely flexible and are able to learn the representation for the original sample and for the transformed ones and therefore are able to generalize also to the variations of the unseen data points <ref type="bibr" target="#b26">[27]</ref>. This approach, however, has some limitations listed below.</p><p>• The algorithm still needs to learn feature representations separately for different variations of the original data. E.g. if a neural network learns edge-detecting features <ref type="bibr" target="#b6">[7]</ref> under rotation-invariance setting, it still needs to learn separately vertical and horizontal edge detectors as separate paths of neuron activations.</p><p>• Some transformations of the data can actually result in the algorithm learning from noise samples or wrong labels. E.g. random crops applied to the input image can capture only a non-representative part of the object in the image, or can fully cut the object out, in which case the algorithm can either overfit to the surrounding or learn from a completely useless representation.</p><p>• The more variations are considered in the data, the more flexible the model needs to be to capture all the variations in the data. This results in more data required, longer training times, less control over the model complexity and larger potential for overfitting.</p><p>On the other hand we use the approach inspired by maxpooling operator <ref type="bibr" target="#b0">[1]</ref> and by multiple-instance learning <ref type="bibr" target="#b27">[28]</ref> to formulate convolutional neural network features to be transformation-invariant. We take the path of neuron activations in the network and feed it, in a similar manner to augmentation, with the original image and its transformed versions (input instances). But instead of treating all the instances as independent samples, we accumulate all of the responses and take the maximum of them (TI-POOLING operator). Because of the maximum, the response is independent from the variations and results in transformationinvariant features that are further propagated through the network. At the same time this allows for more efficient data usage as it learns from only one instance, that already gives maximum response. We call these instances "canonical" and describe in more details in 3.3.</p><p>This topology is implemented as parallel siamese network <ref type="bibr" target="#b1">[2]</ref> layers with shared weights and with inputs corresponding to different transformations, described in details in section 3 and sketched in figure 1. We provide theoretical justification on why features learned in this way are transformation-invariant and elaborate on further properties of TI-POOLING in section 3.3.</p><p>Using TI-POOLING permits to learn smaller number of network parameters than when using data augmentation, and lacks a drawback of some data-points missing relevant information after the applied transformation: it only uses the most representative instance for learning and omits the augmentations that are not useful. We review other approaches dealing with nuisance data variations in section 2.</p><p>We evaluate our approach and demonstrate it's properties on three different datasets. The first two are variations of the original MNIST dataset <ref type="bibr" target="#b18">[19]</ref>, where we significantly outperform the state of the art approaches (for the first variation) or match the current state of the art performance with significantly faster training (on the second variation). The third dataset is a real-world biomedical segmentation dataset with explicit rotation-invariance. On this benchmark we show that incorporating TI-POOLING operator increases the performance over the baselines with similar number of parameters, and also demonstrate the property of TI-POOLING to find canonical transformations of the input for more efficient data usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Transformation invariant features</head><p>Predefined features. One of the easiest ways to ensure transformation-invariance in most computer vision algorithms is to use specially designed features. The most famous examples of general-purpose transformationinvariant features are SIFT (scale-invariant feature transform) <ref type="bibr" target="#b19">[20]</ref> and its rotation-invariant modification RIFT (rotation-invariant feature transform) <ref type="bibr" target="#b14">[15]</ref>. Another example of domain-specific features is rotation-invariant Line filter transforms <ref type="bibr" target="#b22">[23]</ref>, designed specifically to identify elongated structures for blood vessel segmentation.</p><p>Using these features permits the machine learning algorithm to deal with only the inputs that are already invariant to some transformations. But designing the features manually is time-consuming and expensive while not always possible. Furthermore, this approach has two other major limitations: these features are usually not adaptive to the task being solved and they are able to handle only very specific variations in the original data.</p><p>Feature learning. To overcome the limitation of taskadaptability, one could use features learned from the input data. E.g. "bag of visual words" <ref type="bibr" target="#b20">[21]</ref> does not distinguish the positions in which the "visual word" occurs, and therefore it is shift-invariant. With minor modifications, also rotation-invariance can be achieved <ref type="bibr" target="#b28">[29]</ref>.</p><p>Another approach is transformation-invariant decision jungles (TICJ) <ref type="bibr" target="#b12">[13]</ref>. This algorithm learns features to be invariant to any type of predefined transformations as pseudolinear convolutional kernels and then combines them using a decision tree-like algorithm. Two major limitations of this approach are (i) greediness in the feature learning process (only one kernel is learned at a time) and (ii) relatively low expressiveness of the combining machine-learning algorithm. The algorithms that are usually able to overcome both of these limitations are neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep neural networks</head><p>Convolutional deep neural networks <ref type="bibr" target="#b11">[12]</ref> are known to learn very expressive features in an adaptive manner depending on the task. Moreover in many cases they resemble some transformation-invariant properties, such as small shift-invariance due to max-pooling layers <ref type="bibr" target="#b0">[1]</ref>. Because the maximum is taken over the neighbouring pixels, local onepixel shifts usually do not change the output of the subsampling layer. A more general pooling operations <ref type="bibr" target="#b21">[22]</ref> permit to also consider invariance to local changes that however do not correspond to specific prior knowledge.</p><p>To incorporate global transformation-invariance with arbitrary set of transformations, usually the data augmentation is used, as discussed in section 1. But also other approaches exist, such as multi-column deep neural networks <ref type="bibr" target="#b7">[8]</ref> and spatial transformer networks <ref type="bibr" target="#b10">[11]</ref>.</p><p>The idea behind multi-column networks is to train different models with the same topology but using different datasets: the original dataset, and the transformed datasets (one separate model is trained for every transformation considered). Then an average of the outputs of individual models is taken to form the final solution.</p><p>Spatial transformer networks (STN) follow a completely different idea of looking for a canonical appearance of the input data point. They introduce a new layer to the topology of the network, that transforms the input according to the rules of parametrized class of transformations. The key feature of this approach is that it learns the transformation parameters from the data itself without any additional supervision, except of a defined class of transformations.</p><p>The TI-POOLING approach in many ways has very similar properties to STN. As we demonstrate in section 3.3, our method also finds a canonical position of the input image. But instead of defining a class of transformations, we define a more strict set of transformations to be considered. In section 4.1 we show that we achieve similar to STN results on a benchmark introduced by its authors <ref type="bibr" target="#b10">[11]</ref>, but with simpler model and with shorter training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multiple instance learning</head><p>Multi-column networks with model averaging described above fall into a category of more general techniques called "multiple instance learning" (MIL) <ref type="bibr" target="#b27">[28]</ref>. The area of applications of MIL is very broad, and it can also be applied to train the algorithms invariant to some variations defined as a set of transformations Φ.</p><p>Assume that we are given an algorithm A with some input x and scalar (for simplicity) output A(x). Then multiple-instance learning approach suggests that algorithm B(x) will be in many cases transformation-invariant if defined as</p><formula xml:id="formula_0">B(x) = max φ∈Φ A(φ(x))</formula><p>Instead of a maximum, many different operators can be used (such as averaging), but maximum proves to work best in most applications, so we also focus on it in this work.</p><p>The main difference between our approach and MIL is that we propose to learn individual features to be transformation-invariant, and not the algorithm as a whole. Each of the features can then be learned in a way that is most optimal specifically for this feature, allowing different features to rely on different canonical instances and make the most of feature inter-dependencies. We describe this relation in more details in section 3.3. This results in our method significantly outperforming the standard MIL models as we show further in section 4.2.</p><p>Other approaches that are based on the ideas similar to the one presented in this paper are rolling feature maps 1 and multi-view networks <ref type="bibr" target="#b25">[26]</ref>. The former explores a pooling over a set of transformations, but does not guarantee the transformation-invariance of the features learned. And the latter solves a problem of view invariance, not invariance to an expert-defined set of transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method description 3.1. Convolutional neural networks notation</head><p>Convolutional neural networks are usually represented as a sequence of convolutional and subsampling layers with one or more fully-connected layers before the outputs. In this section we for simplicity assume that the input image is two-dimensional (i.e. incorporate no colour channels), but the approach generalizes also for colored images. We also omit the explicit notation for activation functions, assuming activations to be incorporated in the specific form of an operator O defined below.</p><p>Assume that each neuron performs an operation on the input x, that we will refer to as an operator O(x, θ). It can be either a convolution operator, in which case θ is a vectorized representation of a convolutional kernel. Or it can be a subsampling operator, which is usually non-parametric, and has no parameters θ. The size of the output matrix O(x, θ) in each dimension is smaller than the size of x by the size of the kernel in case of a convolution operator, or two times smaller than the input x in case of a subsampling operator.</p><p>We refer to these operators applied in layer l ∈ {1, . . . , L} using superscript l on the parameters θ and we refer to a specific index of the operator within a layer as a subscript. E.g. convolutional operations applied in the first layer of the network can be referred as</p><formula xml:id="formula_1">O(x, θ 1 1 ), . . . , O(x, θ 1 n1 )</formula><p>, where n 1 is the number of neurons in layer 1 (we define all the constants in table 1). Input to the neuron i in layer l is constructed as a sum of outputs of a previous layer:</p><formula xml:id="formula_2">n l−1 j=1 O(x, θ l−1 j ). … … … … … max (a) (b) (c) (d) (e) (f) (g) (i) (h)<label>(j)</label></formula><p>weight sharing weight sharing</p><p>Transformation-invariant features <ref type="figure">Figure 1</ref>. Network topology and pipeline description. First, input image x (a) is transformed according to the considered set of transformations Φ to obtain a set of new image instances φ(x), φ ∈ Φ (b). For every transformed image, a parallel instance of partial siamese network is initialized, consisting only of convolutional and subsampling layers (two copies are shown in the top and in the bottom of the figure). Every instance is then passed through a sequence of convolutional (c, e) and subsampling layers (d), until the vector of scalars is not achieved (e). This vector of scalars is composed of image features f k (φ(x)) learned by the network. Then TI-POOLING (element-wise maximum) (g) is applied on the feature vectors to obtain a vector of transformation-invariant features g k (x) (h). This vector then serves as an input to a fully-connected layer (i), possibly with dropout, and further propagates to the network output (j). Because of the weightsharing between parallel siamese layers, the actual model requires the same amount of memory as just one convolutional neural network. TI-POOLING ensures that the actual training of each features parameters is performed on the most representative instance φ(x).</p><p>We refer to feature f k of the input image x as an output of a neuron k in a layer that contains only scalar values, i.e. layer l such that O(·, θ l i ) ∈ R 1×1 . Formally this feature is defined as the following composition that we for notation clarity split into a sequence of formulas:</p><formula xml:id="formula_3">f i (x) = n l−1 j=1 O(·, θ l−1 j ), where O(·, θ l−1 k ) = n l−2 j=1 O(·, θ l−2 j ), where . . . O(·, θ 1 k ) = O(x, θ 1 j ) (1)</formula><p>On top of these features f k (x), fully-connected layers are usually stacked with some intermediate activation functions, and possibly with dropout masks <ref type="bibr" target="#b9">[10]</ref> during learning. These details are not directly relevant for this paper and therefore not described in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network topology</head><p>Features f k (x), introduced before, are very powerful when all the parameters θ are properly trained. They, however, lack a very important property of incorporating any prior information, such as invariance to some known nuisance variations in the data. We fix this property with a relatively easy trick, inspired by multiple-instance learning (MIL) and max-pooling operator.</p><p>Assume that, given a set of possible transformations Φ, we want to construct new features g k (x) in such a way that their output is independent from the known in advance nuisance variations of the image x. We propose to formulate these features in the following manner:</p><formula xml:id="formula_4">g k (x) = max φ∈Φ f k (φ(x))<label>(2)</label></formula><p>We refer to this max-pooling over transformations as to transformation-invariant pooling or TI-POOLING. Because of the maximum being applied, every learned feature becomes less dependent on the variations being considered. Moreover, for some sets Φ we achieve full transformationinvariance, as we theoretically show in section 3.3.</p><p>As mentioned before and as we show in section 3.3, TI-POOLING ensures that we use the most optimal instance φ(x) for learning, and comparing to MIL models we allow every feature k to find its own optimal transformation φ of the input x: φ = arg max φ∈Φ f k (φ(x)).</p><p>The topology of the proposed model is also briefly sketched and described in <ref type="figure">figure 1</ref>.</p><p>Back-propagation. Let ∇f k (x) be the gradient of the feature f k (x) defined in equations 1 with respect to the outputs O(·, θ l−1 j ) of the previous layer. This gradient is standard for convolutional neural networks and we do not discuss in details how to compute it. From this gradient we can easily formulate the gradient</p><formula xml:id="formula_5">dg k (x)</formula><p>df k (x) of the transformationinvariant feature g k (x) in the following manner:</p><formula xml:id="formula_6">dg k (x) df k (x) = ∇f k (φ(x)), where φ = arg max φ∈Φ f k (φ(x))</formula><p>The gradient of the neurons of the following fullyconnected layer with respect to the output of g k (x) stays exactly the same as for conventional network topology. Therefore, we have all the building blocks for a back-propagation parameter optimization <ref type="bibr" target="#b5">[6]</ref>, which concludes the description of TI-POOLING and of the proposed topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Theory and properties</head><p>Theoretical transformation-invariance. Lemma 1, adapted for our feature from <ref type="bibr" target="#b12">[13]</ref>, formulates the conditions on the set Φ for which the features formulated in equation 2 are indeed transformation-invariant, i.e. give exactly the same output for both the original image x and every considered transformation φ(x), φ ∈ Φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1. Let the function g k (·) to be defined as a maximum over transformations φ ∈ Φ of the function f k (·). This function is transformation-invariant if the set Φ of all possible transformations forms a group, i.e. satisfies the axioms of closure, associativity, invertibility and identity.</head><p>Proof. For the detailed proof please refer to <ref type="bibr" target="#b12">[13]</ref>.</p><p>The statement of the lemma is satisfied for many computer vision tasks: simple transformations, such as rotations or non-linear distortions, as well as their compositions form a group. One common example that does not satisfy this property is local shifts (jittering). E.g. if one wants to consider only one pixel shifts, then the closure axiom of the group does not hold: one pixel shift applied twice gives two-pixel shift, which is not in a transformation set.</p><p>Canonical position identification. From a practical point of view, however, the algorithm achieves approximate transformation-invariance even for local transformations. Even if the set Φ does not form a group, we often observe that the algorithm tries to find a canonical appearance of the image, and then maps a new transformed image to one of the canonical modes. This allows to preserve transformation-invariance in most practical cases with no limitations on Φ. <ref type="figure" target="#fig_0">Figure 2</ref> shows some examples of neuronal structures oriented in the same manner to a canonical orientation for one of the features.</p><p>This property is useful for most problems as it allows for better use of input images. For example, learning discriminative features for every orientation of the image is of course possible with large and deep enough neural network. But assume that now features need to be learned only for canonical orientation (e.g. for membranes oriented in all the same direction). First and third rows show the input patches from neuronal segmentation dataset. For this dataset we consider Φ to be a set of rotations. We then apply a learned model to these patches x and record the angle at which the maximum is achieved for specific feature g k (x). Then we show the same patches rotated by this angle as shown in rows two and four. One could notice that in most cases the membranes (slightly darker elongated structures) are oriented in approximately the same direction. This means that the algorithm considers this orientation to be canonical for this specific feature and rotates new images to appear similarly.  First, for this, much simpler problem, smaller models can be used. Second, the algorithm sees many more examples of canonical vertical edges and therefore can better generalize from them. This brings the next important property of the algorithm.</p><p>Improved performance and convergence. Because of more representative examples being used for network training, we observe better performance and convergence rate, when compared with simple data augmentation. <ref type="figure" target="#fig_2">Figure 3</ref> shows that the larger the transformation set Φ -the better usually the results achieved. This is most probably due to the fact that fewer canonical positions needs to be handled by the learning algorithm.</p><p>What TI-POOLING is doing to achieve that can be formulated as an exhaustive search over the transformed instances for an instance better corresponding to the current response of the feature. Then only this instance is used to even better improve the performance of the feature. On the other hand, we do not limit all the features to use the same canonical appearance, allowing features to better explore interdependencies between the outputs of network layers. We elaborate more on the results in section 4.</p><p>Any type of transformations. Another property of the technique, that is worth mentioning, is that it can work with a set of almost any arbitrary transformations. Many works, such as spatial transformer networks <ref type="bibr" target="#b10">[11]</ref>, focus on only limited class of transformations. Those classes can be very rich, e.g. include all the possible affine transformations or projections. But still, they need to be differentiable with respect to some defined parameters of the transformation, and, depending on the problem at hand, this can be not enough. TI-POOLING, on the contrary, does not rely on differentiability or on any properties of bijective functions or even on the parametrization itself. Examples of common transformations that can be used with our method, and not with <ref type="bibr" target="#b10">[11]</ref> are reflections, most morphological operations and non-linear distortions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation details</head><p>We use Torch7 framework for model formulation and training <ref type="bibr" target="#b8">[9]</ref>. The easiest way to formulate a proposed model is to use parallel network notation with shared weights as described in <ref type="figure">figure 1</ref>. The whole model definition requires just few additional lines of code. An example in pseudo-lua code is provided below. Here nPhi is a size of the set Φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>−− d e f i n e f i r s t s i a m e s e l a y e r s s i a m e s e = S e q u e n t i a l ( ) . . . −− c l o n e and s h a r e w e i g h t s</head><p>p a r a l l e l = P a r a l l e l ( 1 , 3 ) f o r p h i = 1 , n P h i do c l o n e = s i a m e s e : c l o n e ( ) p a r a l l e l : add ( c l o n e ) c l o n e : s h a r e ( s i a m e s e , ' w e i g h t ' , ' b i a s ' , ' g r a d W e i g h t ' , ' g r a d B i a s ' ) end −− f o r m u l a t e a f u l l model model = S e q u e n t i a l ( ) model : add ( p a r a l l e l ) model : add ( S p a t i a l M a x P o o l i n g ( nPhi , 1 , 1 ) ) −− add f u l l y −c o n n e c t e d l a y e r s ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>−− d r o p o u t and o u t p u t l a y e r</head><p>The only other modification is to the data: we increase the dimension of the input data tensor by one and stack input instances φ(x), φ ∈ Φ across the new dimension.</p><p>Computational complexity. It may seem like an exhaustive search in the space of possible transformations Φ significantly increases computational complexity of the pipeline. Indeed, instead of processing one image at a time, we forward-pass |Φ| images through almost the whole network. We can speed it up by sampling from the space of transformations, but in practice, even searching the full space appears to be more efficient than just data augmentation, because of the following reasons.</p><p>• Only partial forward pass is done multiple times for the same image, forward-pass through fully-connected layers and back-propagation are exactly the same computationally as for a standard convolutional neural network with the same number of parameters.</p><p>• Comparing to the data augmentation approach, we make use of every image and it's augmented versions in one pass. Standard convolutional neural network instead makes one pass for every augmented sample, which in the end results in the number of passes equal to the number of augmentated samples to process one image. And because of the previous point, we actually do it more than two times faster.</p><p>• Because we make use of canonical appearance of the image, the proposed pipeline actually trains more efficiently than the standard neural network, and usually requires smaller number of overall parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we present the experimental results on three computer vision datasets. The first two datasets are different variations of MNIST dataset <ref type="bibr" target="#b18">[19]</ref> designed to test artificially-introduced variations in the data. The third one is a neuronal structures segmentation dataset <ref type="bibr" target="#b3">[4]</ref>, that demonstrates a real-world example of rotation invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Rotated MNIST</head><p>Original MNIST dataset <ref type="bibr" target="#b18">[19]</ref> is a very typical toy dataset to check the performance of new computer vision algorithms modifications. Two variations of MNIST exist to test the performance of different algorithms that are designed to be invariant to some specific variations, such as rotations.</p><p>For both the datasets we use the same topology, but slightly different sets Φ. The topology is described in table 1. We perform the training using tuning-free convergent adadelta algorithm <ref type="bibr" target="#b29">[30]</ref> with the batch size equal to 128 and dropout <ref type="bibr" target="#b9">[10]</ref> for fully-connected layers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Error, % ScatNet-2 <ref type="bibr" target="#b2">[3]</ref> 7.48 PCANet-2 <ref type="bibr" target="#b4">[5]</ref> 7.37 TIRBM <ref type="bibr" target="#b24">[25]</ref> 4.2 TI-POOLING (ours) 2.2 <ref type="table">Table 2</ref>. Results on mnist-rot-12k dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">mnist-rot-12k dataset</head><p>The most commonly used variation of MNIST that is used for validating rotation-invariant algorithms is mnistrot <ref type="bibr" target="#b13">[14]</ref>. It consists of images from the original MNIST, rotated by a random angle from 0 to 2π (full circle). This dataset contains 12000 training images, which is significantly smaller, than in the original dataset, and 50000 test samples.</p><p>For this dataset we include a TI POOLING step over Φ containing 24 rotations sampled uniformly from 0 to 2π.</p><p>We train this network on a single GPU for 1200 epochs and compare the achieved test error with the best results published for this dataset. The best approach by <ref type="bibr" target="#b24">[25]</ref> employs restricted boltzmann machines and achieves 4.2% error, while we achieve 2.2% -the results almost two times better in terms of classification error. The final errors for the proposed and the state of the art results are present in the table 2. It can be seen that using TI-POOLING indeed leads to significant improvements with no significant effort of optimising topology and just by better exploiting the variations in the data.  <ref type="table">Table 3</ref>. Results on half-rotated MNIST dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Half-rotated MNIST dataset</head><p>The second dataset we consider is the dataset introduced in <ref type="bibr" target="#b10">[11]</ref>. There are two reasons why the authors decided to advance further from the original mnist-rot-12k. First, mnistrot-12k is very small in size (five times less than training set in MNIST dataset). And second, it has somewhat artificial limitation of images being rotated full circle. So they proposed to take full MNIST dataset, use random angle in</p><formula xml:id="formula_7">the range [− π 2 , π 2 ]</formula><p>(half the circle) and use the input images rotated by this angle as training samples. This makes the problem a little easier, but closer to real-world scenarios.</p><p>As discussed in section 2, the authors of spatial transformer networks <ref type="bibr" target="#b10">[11]</ref> propose an elegant way of optimising the transformation of the image while learning also the canonical orientation. Here we show that for some classes of transformations, we achieve comparable results with simpler model and shorter training time.</p><p>For this problem formulate a set of transformations Φ as a set of angles sampled uniformly from half a circle, to match the dataset formulation, overall 13 angles. With this relatively simple model, we converge to the results of 0.8% error within 360 epochs, while STN was trained for 1280 epochs. Moreover, using TI POOLING does not require grid sampling and therefore each individual iteration is faster. With this we still match the performance of the most general STN model defined for a class of projection transformations. For more narrow class of transformations selected manually (affine transformations), our results are slightly worse (by 0.1%). However, we did not optimise with respect to the transformation classes, and therefore the comparison is not fully fair in this case. <ref type="table">Table 3</ref> shows further comparison with STN and other related baselines on this dataset. Baseline fully-connected (FCN) and standard convolutional (CNN) neural networks are defined in <ref type="bibr" target="#b10">[11]</ref> and tuned to have approximately the same number of parameters as the baseline STN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Neuronal structures segmentation</head><p>The neuronal membrane segmentation dataset was used for ISBI 2012 challenge <ref type="bibr" target="#b3">[4]</ref>. It consists of images of neuronal tissue captured with serial section transmission electron microscopy. The task is to perform pixel segmentation into two classes: cell membranes and inner parts of the neu-Method Error, % MIL over CNN <ref type="bibr" target="#b27">[28]</ref> 8.9 CNN with augmentation <ref type="bibr" target="#b11">[12]</ref> 8.1 TI-POOLING -dropout 7.4 TI-POOLING + dropout 7.0 Because this is a segmentation task, we extract patches around a pixel and classify those patches (here label of the patch is the label of the central pixel of the patch). We perform training on all the available pixel patches (balanced between classes). The patch is decided to be square and has the size of 46 pixel, but after the rotation we crop the patch, so the actual input to the network is a 32 × 32 patch. Some examples of the patches are present in <ref type="figure" target="#fig_0">figure 2</ref>.</p><p>For every algorithm we run for this dataset, we select the same network topology, in order to better evaluate the improvement of the proposed TI-POOLING operator for rotation-invariant feature learning without incorporating any other effects such as model size. As our baselines, we select the following two algorithms, that are closely related to the proposed technique as discussed in sections 1 and 2:</p><p>• standard convolutional neural network with data augmentation, that is able ideally to learn features expressive enough to handle rotations in the data;</p><p>• multiple instance learning of convolutional neural networks, that is able to learn a transformation-invariant algorithm for a given set of transformations, but not the features.</p><p>For all the underlying networks we select the same topology as described in table 1, except of TI-POOLING and the number of outputs (two classes for this dataset). We also report the results with and without dropout, as discussed later. <ref type="table" target="#tab_2">Table 4</ref> shows the pixel error achieved by all the algorithms after 16 epochs. To make the comparison absolutely fair, for standard convolutional neural network with augmentation we record the results after 16 * 24 = 384 epochs, so that the number of images "seen" by the algorithm is the same (because for the proposed approach and for the MIL modification, we take the maximum over all the 24 rotations in one iteration). We also run MIL modification with no dropout, and compare the results with the version of our algorithm trained with no dropout. For both baselines we see the significant improvement for the same topology. From this we can conclude that the proposed TI-POOLING is indeed very helpful for real-world problems with nuisance variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper we propose a novel framework to incorporate expert knowledge on nuisance variations in the data when training deep neural networks. We formulate a set of transformations that should not affect the algorithm decision and generate multiple instances of the image according to these transformations. Those instances, instead of being used for training independently, are passed through initial layers the network and through TI-POOLING operator to form transformation-invariant features. These features are fully-trainable using back-propagation, have the rich expressiveness of standard convolutional neural network features, but at the same time do not depend on the variations in the data.</p><p>Convolutional neural network with TI-POOLING has some theoretical guarantees of being transformationinvariant algorithm for variations common in computer vision problems. But more importantly, it has some nice practical properties that we show in this work, e.g. it permits to learn from the most representative instances, that we call "canonical". Because of that the network does not have to learn features separately for every possible variation of the data from augmented samples, but instead it learns only features that are relevant for one appearance of the image, and then applies it for all the variations. It also allows to better use the input data to learn these features: e.g. all the samples including edges participate in learning transformationinvariant edge detector feature, and no separate vertical or horizontal edge detector features are needed.</p><p>We test the method on three datasets with explicitly defined variability. In all the experiments we either significantly outperform or at least match the performance of baseline state of the art techniques. Often we also show faster convergence rates than baselines with smaller yet smarter data-aware models.</p><p>The proposed TI-POOLING operator can be used as a separate neuronal unit for most networks architectures with very little effort to incorporate prior knowledge on nuisance factors in the data. But the range of its applications goes well beyond that, allowing to incorporate many types of prior information on the data and opening the opportunities for more robust expert-driven algorithms in combination with the powerful expressiveness of deep learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. First and third rows show the input patches from neuronal segmentation dataset. For this dataset we consider Φ to be a set of rotations. We then apply a learned model to these patches x and record the angle at which the maximum is achieved for specific feature g k (x). Then we show the same patches rotated by this angle as shown in rows two and four. One could notice that in most cases the membranes (slightly darker elongated structures) are oriented in approximately the same direction. This means that the algorithm considers this orientation to be canonical for this specific feature and rotates new images to appear similarly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Validation error plot for the neuronal segmentation dataset. Depending on how many angles we sample to form a transformation set Φ (from one, which is equivalent to data augmentation, up to 24) -the results improve significantly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Table 1. The topology of the network in the experiments.</figDesc><table>Layer 

Parameters &amp; channel size 
input 
size: 32x32 
convolution 
kernel: 3x3, channel: 40 
relu 
max pooling kernel: 2x2, stride: 2 
convolution 
kernel: 3x3, channel: 80 
relu 
max pooling kernel: 2x2, stride: 2 
convolution 
kernel: 3x3, channel: 160 
relu 
max pooling kernel: 2x2, stride: 2 
linear 
channel: 5120 
relu 
TI-POOLING transformations: Φ 
dropout 
rate: 0.5 
linear 
channel: 10 
softmax 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Results on neuronal segmentation dataset. ron. We take 2x downsampled images and split them into training and test sets: first 25 sections are used for training, and the last five are left for test. From the neurological experts we know, that membrane appearance does not depend on the orientation of the mem- brane, and therefore we can safely include [0, 2π] rotations in the set of transformations Φ. We sample rotations every 15 degrees, resulting in 24 transformations considered.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://benanne.github.io/2015/03/17/plankton.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: The work is partially funded by the Swiss National Science Foundation projects 157101 and 163910.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theoretical analysis of feature pooling in visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Signature verification using a siamese time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="669" to="688" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An integrated micro-and macroarchitectural analysis of the drosophila brain by computer-assisted serial section electron microscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cardona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saalfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Preibisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pulokas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tomancak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hartenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS biology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pcanet: A simple deep learning baseline for image classification? 24</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Chan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Backpropagation: theory, architectures, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chauvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep neural networks segment neuronal membranes in electron microscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2843" to="2851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop, number EPFL-CONF-192376</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Spatial transformer networks. 2015. 3</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transformation-invariant convolutional jungles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-local affine parts for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC&apos;04)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>1995. 1</idno>
		<imprint>
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
	<note>The handbook of brain theory and neural networks</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Comparison of learning algorithms for handwritten digit recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brunot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sackinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial neural networks</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="53" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks with a nonpolynomial activation function can approximate any function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leshno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schocken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="861" to="867" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition: benchmarking of state-of-the-art techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision, 1999. The proceedings of the seventh IEEE international conference on</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical models of object recognition in cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1019" to="1025" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Segmentation of thin structures in electron micrographs using orientation fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sandberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of structural biology</title>
		<imprint>
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="403" to="415" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Buhmann. Computational tma analysis and cell nucleus classification of renal cell carcinoma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Schüffler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning invariant representations with local transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning (ICML-12)</title>
		<meeting>the 29th International Conference on Machine Learning (ICML-12)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1311" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiview convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The art of data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Van Dyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-L</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep multiple instance learning for image classification and auto-annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3460" to="3469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bag-of-visual-words and spatial extensions for land-use classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems</title>
		<meeting>the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adadelta: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
