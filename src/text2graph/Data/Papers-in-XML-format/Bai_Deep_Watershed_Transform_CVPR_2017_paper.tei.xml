<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Watershed Transform for Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
							<email>mbai@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
							<email>urtasun@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Watershed Transform for Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Instance segmentation seeks to identify the semantic class of each pixel as well as associate each pixel with a physical instance of an object. This is in contrast with semantic segmentation, which is only concerned with the first task. Instance segmentation is particularly challenging in street scenes, where the scale of the objects can vary tremendously. Furthermore, the appearance of objects are affected by partial occlusions, specularities, intensity saturation, and motion blur. Solving this task will, however, tremendously benefit applications such as object manipulation in robotics, or scene understanding and tracking for self-driving cars.</p><p>Current approaches generally use complex pipelines to handle instance extraction involving object proposals <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7]</ref>, conditional random fields (CRF) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>, large recurrent neural networks (RNN) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b1">2]</ref>, or template matching <ref type="bibr" target="#b27">[28]</ref>. In contrast, we present an exceptionally simple and intuitive method that significantly outperforms the state-of-the-art. In particular, we derive a novel approach which brings together classical grouping techniques and modern deep neural networks.</p><p>The watershed transform is a well studied method in mathematical morphology. Its application to image segmentation can be traced back to the 70's <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>. The idea behind this transform is fairly intuitive. Any greyscale image can be considered as a topographic surface. If we flood this surface from its minima and prevent the merging of the waters coming from different sources, we effectively partition the image into different components (i.e., regions). This transformation is typically applied to the image gradient, thus the basins correspond to homogeneous regions in the image. A significant limitation of the watershed transform is its propensity to over-segment the image. One of the possible solutions is to estimate the locations of object instance markers, which guide the selection of a subset of these basins <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21]</ref>. Heuristics on the relative depth of the basins can be exploited in order to merge basins. However, extracting appropriate markers and creating good heuristics is difficult in practice. As a consequence, modern techniques for instance segmentation do not exploit the watershed transform.</p><p>In this paper, we propose a novel approach which combines the strengths of modern deep neural networks with the power of this classical bottom-up grouping technique. We propose to directly learn the energy of the watershed transform such that each basin corresponds to a single instance, while all dividing ridges are at the same height in the energy domain. As a consequence, the components can be extracted by a cut at a single energy level without leading to over-segmentation. Our approach has several key advantages: it can be easily trained end-to-end, and produces very fast and accurate estimates. Our method does not rely on iterative strategies such as RNNs, thus has a constant runtime regardless of the number of object instances.</p><p>We demonstrate the effectiveness of our approach in the challenging Cityscapes Instance Segmentation benchmark <ref type="bibr" target="#b5">[6]</ref>, and show that we more than double the performance of the current state-of-the-art. In the following sections, we first review related work. We then present the details behind our intuition and model design, followed by an analysis of our model's performance. Finally, we explore the impact of various parts of our model in ablation studies.</p><p>1 5221 Sample prediction: the input image is gated by sem. segmentation from <ref type="bibr" target="#b33">[34]</ref> and passed through our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related</head><p>Several instance level segmentation approaches have been proposed in recent years. We now briefly review them.</p><p>Proposal based: Many approaches are based on the refinement of object proposals. For example, <ref type="bibr" target="#b0">[1]</ref> generates object segments proposals, and reasons about combining them into object instances. In a similar spirit, <ref type="bibr" target="#b11">[12]</ref> selects proposals using CNN features and non-maximum suppression. Based on this, <ref type="bibr" target="#b4">[5]</ref> further reasons about multiple object proposals to handle occlusion scenarios where single objects are split into multiple disconnected patches. <ref type="bibr" target="#b6">[7]</ref> uses a deep cascaded neural network to propose object instance bounding boxes, refine instance masks, and semantically label the masks in sequence. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref> generate segmentation proposals using deep CNNs, which are then further refined to achieve better segmentation boundaries. Additionally, <ref type="bibr" target="#b30">[31]</ref> uses a modified R-CNN model to propose instance bounding boxes, which are then further refined to obtain instance level segmentation.</p><p>Deep structured models: <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> first use CNNs to perform local instance disambiguation and labelling, followed by a global conditional random field (CRF) to achieve instance label consistency. Recent work by <ref type="bibr" target="#b1">[2]</ref> uses object detection proposals in conjunction with a deep high order CRF to reason about pixel assignment in overlapping object proposal boxes.</p><p>Template matching: <ref type="bibr" target="#b27">[28]</ref> extracts image features using CNNs to assign a sector label to each pixel in an object instance, which corresponds to one of eight discretized radial bins around the object's visible center. A template matching scheme is then used to associate instance center proposals and pixels with an object instance.</p><p>Recurrent Networks: <ref type="bibr" target="#b23">[24]</ref> uses CNNs for feature extraction, followed by a recurrent neural network (RNN) that generates instance labels for one object at a time. The recurrent structures (based on ConvLSTM <ref type="bibr" target="#b24">[25]</ref>) keep track of instances that have already been generated, and inhibit these regions from further instance generation. Additionally, <ref type="bibr" target="#b22">[23]</ref> extracts image features similar to <ref type="bibr" target="#b27">[28]</ref> and employs a fairly complex pipeline including a ConvLSTM structure to direct a bounding box generation network followed by a segmentation network that extracts individual instances.</p><p>CNN: <ref type="bibr" target="#b14">[15]</ref> leverages only a CNN trained to provide multiple outputs to simultaneously predict instance numbers, bounding box coordinates, and category confidence scores for each pixel. This is followed by generic clustering algorithms to group the resulting output into instance-wise labels. Additionally, <ref type="bibr" target="#b12">[13]</ref> proposed deep convolutional neural network that learns the underlying shapes of objects, and performs multiple unconstrained inference steps to refine regions corresponding to an object instance while ignoring neighboring pixels that do not belong to the primary object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposal + recursion:</head><p>[14] proposed a novel method that recursively refines proposals.</p><p>In contrast, in this paper we propose a novel approach which combines the strengths of modern deep neural networks with the power of the watershed transform. Our model is simple, fast, accurate, and inherently handles an arbitrary number of instances per image with ease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A Review on the Watershed Transform</head><p>We start our discussion with a review of the watershed transform, a well studied method in mathematical morphology. This technique is built on the fact that any greyscale image can be considered as a topographic surface. If we flood this surface from its minima while building barriers to prevent the merging of the waters coming from different sources, we effectively partition the image into different components or regions. These components are called catchment basins. The barriers or watershed lines then represent the boundaries between the different basins (i.e., boundaries between regions).</p><p>This process is illustrated in the first row of <ref type="figure" target="#fig_1">Fig. 2</ref> for a one dimensional energy function. In this case, the watershed transform results in seven components, which are illustrated in different colors. Note that the traditional watershed transform tends to produce an over-segmentation of the image due to spurious small ridges which produce separate components. In this example, although there are 3 main components, the watershed transform over-segments the image because of small perturbations in the energy.</p><p>Several algorithms have been developed to estimate the components. <ref type="bibr" target="#b2">[3]</ref> proposed an algorithm that iteratively fills the watershed landscape from each local minimum, adding dams wherever two neighbouring bodies of water meet. These dams define the segmentation boundaries. Additionally, <ref type="bibr" target="#b17">[18]</ref> details a number of alternative watershed transform algorithms, including topological distance, shortest path algorithms, spanning trees, and marker based methods.</p><p>The watershed transform is typically applied to the image gradient, while the catchment basins correspond to homogeneous grey level regions in the image. However, estimating sharp gradients that represent the boundaries between different instances is a very challenging process. In the next section, we will show an alternative approach which directly learns to predict the energy landscape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Deep Watershed Tranform</head><p>In this section, we present our novel approach to instance level segmentation. In particular, we learn the energy of the watershed transform with a feed-forward neural network. The idea behind our approach is very intuitive. It consists of learning to predict an energy landscape such that each basin corresponds to a single instance, while all ridges are at the same height in the energy domain. As a consequence, the watershed cut corresponds to a single threshold in the energy, which does not lead to over segmentation. We refer the reader to the lower half of <ref type="figure" target="#fig_1">Fig. 2</ref> for an illustration of the desired energy.</p><p>Unfortunately, learning the energy landscape from scratch is a complex task. Therefore, we aid the network by defining an intermediate task, where we learn the direction of descent of the watershed energy. This is then passed through another set of network layers to learn the final energy. In principle, one can interpret this network as learning to perform the distance transform of each point within an object instance to the instance's boundary. <ref type="figure" target="#fig_3">Fig. 4</ref> shows an example of the input, intermediate results, and final output of our method. We refer the reader to <ref type="figure" target="#fig_2">Fig. 3</ref> for an illustration of our network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Direction Network (DN)</head><p>Our approach leverages semantic segmentation as input to focus only on relevant areas of the image. Note that our network is agnostic to the choice of segmentation algorithm. In our paper, we use the semantic segmentation results from PSPNet <ref type="bibr" target="#b33">[34]</ref>.</p><p>The network takes as input the original RGB image gated by a binarized semantic segmentation, where all pixels that are not part of one of the semantic classes of interest are set to zero. The input image is augmented by adding the semantic segmentation as a fourth channel. Because the RGB values range from 0 to 255 for each channel before mean subtraction, we likewise scale the encoding of the semantic segmentation image such that the class labels are equally spaced numerically, and the variance is approximately equal to that of values in the RGB channels.</p><p>To aid the model in producing an accurate energy landscape, we pre-train the overall network's first part (referenced here as the Direction Network, DN) to estimate the direction of descent of the energy at each pixel. We parameterize it with a unit vector pointing away from the nearest point on the object's instance boundary. This supervision gives a very strong training signal: the direction of the nearest boundary is directly encoded in the unit vector. Furthermore, a pair of pixels straddling an occlusion boundary between two objects will have target unit vectors pointing in opposite directions. Here, associating a pixel with the wrong object instance incurs the maximum possible angular error. This is true regardless of the shape of the objects, even when they are highly concave or elongated. This forces the network to learn very accurate boundary localization at the pixel level.</p><p>Let D gt (p) be the ground truth distance transform from a point to the boundary of the instance it belongs to. We define our ground truth targets as the normalized gradient u p of this distance transform. More formally,</p><formula xml:id="formula_0">u p,gt = ∇D gt (p) |∇D gt (p)|</formula><p>Thus, the DN produces a two channel output at the input image's resolution representing the directional unit vector. It is important to note that the normalization layer at the output of the DN restricts the sum of each channel's squared output to be 1. This greatly reduces the difficulty of using the output of non-linear activation functions to model  The feature extraction portion of the DN's architecture is inspired by VGG16 <ref type="bibr" target="#b26">[27]</ref>. However, there are several important modifications. Direction prediction is a very precise task, as the output can vary greatly between neighboring pixels. Thus, it is critical to avoid losing spatial resolution. We utilize a modified version of the first 13 layers of VGG, where the third and fourth pooling layers are changed to average pooling while the fifth pooling layer is removed. To preserve spatial resolution, we exploit a high-capacity, multi-scale information aggregation scheme inspired by popular methods in semantic segmentation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b15">16]</ref>. In particular, the outputs of conv3, conv4, and conv5 individually undergo a 5×5 convolution followed by two 1 × 1 convolutions. After this, the outputs of the second and third paths are upsampled to the resolution of the first path. The resulting feature volumes from the three paths are concatenated. This undergoes an additional set of three 1 × 1 convolutions, before being upsampled to the input resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Watershed Transform Network (WTN)</head><p>The second half of the overall network takes as input the 2-channel unit vector map, and produces a discretized modified watershed transform map with K = 16 possible energy values. In this case, bin 0 corresponds to background or regions within 2 pixels of an instance boundary and is referred to as having an energy level of 0. Meanwhile, higher numbered bins with higher energy levels correspond to regions in the interior of object instances. The bins are chosen to maximize the binning resolution of energy levels near zero (to facilitate accurate cutting), while achieving an approximate balance between the total numbers of pixels within each class.</p><p>The WTN is a fairly generic CNN, with an emphasis on reasoning with high spatial accuracy and resolution. The architecture is shown in the right half of <ref type="figure" target="#fig_2">Fig. 3</ref>. In particular, the network consists of two 5 × 5 convolutional filter blocks each followed by a 2 × 2 average pooling, before undergoing a set of 1 × 1 convolutions and upsampling to the input image resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Network Training</head><p>We first pre-train our DN and WTN networks. This process is followed by end-to-end training of the whole network.</p><p>Direction Network pre-training: We pre-train the network using the mean squared error in the angular domain as loss:</p><formula xml:id="formula_1">l direction = p∈Pobj w p cos −1 &lt; u p,GT , u p,pred &gt; 2</formula><p>where P obj is the set of all pixels belonging to a semantic class with object instances, and w p is a weighting factor proportional to the inverse of the square root of the object instance's area. We use the mean squared angular error as opposed to the commonly used cosine distance, as large angular errors (occuring when the network associates a pixel with the wrong object instance) should incur significant larger penalties than small angular errors (which have little impact). This network is pre-trained using the original input images gated by the ground truth semantic segmentation and the ground truth unit vectors u GT . The lower layers (conv1 to conv5) are initialized with VGG16 <ref type="bibr" target="#b26">[27]</ref>. The weights of the upper layers are initialized randomly according to the Xavier initialization scheme <ref type="bibr" target="#b9">[10]</ref>. However, the intialization variance of the weights of the final layer of each prediction branch is set manually such that the variance of each branch's output is of the same order of magnitude before concatenation. This encourages the network to consider the output of each branch. We use the ADAM optimizer to train the network for 20 epochs with a batch size of 4, with a constant learning rate of 1e-5 and L2 weight penalty of 1e-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Watershed Network pre-training:</head><p>The network is trained using a modified cross-entropy loss</p><formula xml:id="formula_2">l watershed = p∈Pobj K k=1 w p c k (t p,k logȳ p,k + t p,k log y p,k )</formula><p>where t p,k is the k-th element of pixel p's one-hot target vector, y p,k is the k-th channel of the network output at p, w p is a coefficient to adjust the importance of smaller objects (as defined in the DN pre-training section), and c k is a scaling constant specific to each discretization class. Because the single energy level cut occurs at just above zero energy (i.e., near object instance boundaries), an accurate estimation of pixels at low energy levels is of crucial importance. Thus, the set {c k } is selected to be in increasing order. In this case, any errors of low energy levels are assigned a greater level of penalty, which encourages the network to focus on boundary regions. We pre-train the network using the ground truth semantic segmentation and ground truth direction predictions as input, and the discretized ground truth modified watershed transform as target. All weights are initialized using Xavier initialization <ref type="bibr" target="#b9">[10]</ref>. We train the network for 25 epochs using the ADAM optimizer. A batch size of 6, constant learning rate of 5e-4, and a L2 weight penalty of 1e-6 are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End-to-end fine-tuning:</head><p>We cascaded the pre-trained models for the DN and WTN and fine-tuned the complete model for 20 epochs using the RGB image and semantic segmentation output of PSPNet as input, and the ground truth distance transforms as the training target. We use a batch size of 3, constant learning rate of 5e-6, and a L2 weight penalty of 1e-6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Energy Cut and Instance Extraction</head><p>We cut our watershed transform output at energy level 1 (0 to 2 pixels from boundary) for classes with many smaller objects (person, rider, bicycle, motorcycle), and level 2 (3 to 4 pixels from boundary) for classes with larger objects (car, bus, truck, train). Following this, the instances are dilated using a circular structuring element whose radius equals the maximum erosion at the energy level of the cut. This offsets the boundary erosion from the non-zero threshold. The connected components are identified in the resulting image, directly yielding the proposed instances. The proposals are further refined by basic hole-filling. Finally, we remove small, spurious instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Evaluation</head><p>In this section, we evaluate our approach on the challenging Cityscapes Instance-Level Semantic Labelling Task <ref type="bibr" target="#b5">[6]</ref>. The official benchmark test and validation set results are found in <ref type="table">Tables 1 and 2</ref>. We then perform ablation studies with the validation set to examine the performance of various aspects of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset:</head><p>The Cityscapes Instance Labelling Task contains 5000 finely annotated street scene images taken by a vehicle-mounted camera. Each image has a resolution of  <ref type="table">Table 2</ref>: Cityscapes instance segmentation class specific test set AP scores using metrics defined in <ref type="bibr" target="#b5">[6]</ref>.</p><p>2048x1024 pixels. Unlike other commonly used datasets for instance segmentation (e.g., Pascal VOC <ref type="bibr" target="#b7">[8]</ref>, BSDS500 <ref type="bibr" target="#b16">[17]</ref>, and CVPPP <ref type="bibr" target="#b18">[19]</ref>) Cityscapes has a large number of scenes involving dozens of instances with large degrees of occlusions at vastly different scales. Eight of the semantically labelled categories have instance-level labelling. We refer the reader to <ref type="table" target="#tab_2">Table 3</ref> for a summary of the statistics of the object instances in this dataset. Note that while the car and people classes have significant numbers of instances, the other six classes are rather uncommon. As a result, the rare classes have far less training data, and are thus much more challenging. We use the official training, validation, and testing set splits, with 2975, 500, and 1525 images, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric:</head><p>We report the metrics employed by the Cityscapes leaderboard. Several variants of the instancelevel average precision score are used. This is calculated by finding the precision at various levels of intersectionover-union (IoU) between predicted instances and ground truth instances, ranging from 50% to 95%. The main score is the Average Precision (AP). Additionally, there are three minor scores: AP at 50% overlap, AP of objects closer than 50m, and AP of objects closer than 100m. Methods are ranked by the AP score over all classes (mean AP). Note that AP is a detection score, which does not penalize overlapping instances or a large number of predictions, as long as they are ranked in the proper order. Thus, it places approaches (like ours) which predict a single instance label (or background) per pixel at a disadvantage, while favoring detection-based methods. Despite this, we use these metrics to be consistent with the the evaluation of the state-of-the-art. For the {bus,truck,train} classes, we order the instances by simply averaging the semantic segmentation's output confidence within each instance to somewhat counteract the errors by semantic segmentation.   <ref type="bibr" target="#b8">[9]</ref> 71.8% 20.2% PSPNet <ref type="bibr" target="#b33">[34]</ref> 80.2% 21.2% <ref type="table">Table 4</ref>: Comparison of instance segmentation performance on the validation set with various semantic segmentation sources. Semantic segmentation IoU Class scores <ref type="bibr" target="#b5">[6]</ref> are also provided.</p><p>For all other classes, we use a random ordering. Note that sophisticated ranking techniques can be used to further improve the score. However, the segmentation quality remains this same. This again highlights a shortcoming of using AP as the metric.</p><p>In addition to AP, we report the mean weighted coverage score introduced in <ref type="bibr" target="#b25">[26]</ref> on the validation set. This metric enforces a single instance label per pixel, and is therefore more suitable for evaluating our approach. We hope that future work will likewise report this score when applicable.  Comparison to the state-of-the-art: We show the instance segmentation test set scores in <ref type="table">Table 1</ref>. Additionally, we show the class-specific AP scores in <ref type="table">Table 2</ref>. It is evident that we achieve a large improvement over the stateof-the-art in all semantic classes. Moreover, we do not use depth information to train our model, unlike <ref type="bibr" target="#b27">[28]</ref>.</p><p>Analysis of the intermediate training target: Our final network is the result of the end-to-end fine-tuning of two pre-trained sub-networks (DN and WTN). <ref type="figure" target="#fig_3">Fig. 4 (f)</ref> shows the output of the DN after finetuning. It is evident that the fine-tuned model retained the direction prediction as an intermediate task. This suggests that the intermediate training target is effective.</p><p>Influence of semantic segmentation: While we elected to use PSPNet <ref type="bibr" target="#b33">[34]</ref> as our semantic segmentation source, we additionally demonstrate that our method is able to use other sources. <ref type="table">Table 4</ref> shows the use of LRR <ref type="bibr" target="#b8">[9]</ref> for semantic segmentation. Using the same pre-trained DN and DTN models, we perform end-to-end fine-tuning using LRR as semantic segmentation. We note that the performance of our model improves with better semantic segmentation. Thus, future advances in segmentic segmentation methods can further improve the performance of our approach.</p><p>Confidence score estimate: As mentioned, the AP score calculated by the Cityscapes benchmark requires a confidence score for each instance. For the {bus, truck, train} set, we produce a weak ranking based on semantic segmentation softmax confidence. Instances of all other classes are randomly ranked. <ref type="table" target="#tab_4">Table 5</ref> explores the impact of various ordering schemes. We compare our ordering with random for all semantic classes, as well as optimal ordering using oracle IoU. We see that ordering using oracle IoU can increase our model's performance by 6.34%. Note, however, that this has no impact on the actual quality of proposed object instances, which remain the same. This shows the necessity of a different metric such as muCov <ref type="bibr" target="#b25">[26]</ref> that can evaluate segmentation-based approaches fairly.</p><p>Qualitative Results: <ref type="figure" target="#fig_4">Fig. 5</ref> depicts visualizations of sample results on the validation set, which is not used as part of training. It is evident that our model produces very high quality instance segmentation results. In these results, predicted and ground truth object instances only share the same color if they have greater than 50% IoU.</p><p>Failure Modes: Our model has several weaknesses. Some images in <ref type="figure" target="#fig_4">Fig. 5</ref> demonstrate these cases. The first issue is that the current formulation of our method does not handle objects that have been separated into multiple pieces by occlusion. This is most obvious in the 3rd image from the bottom in <ref type="figure" target="#fig_4">Fig. 5</ref> as the far-right vehicle is bisected by the street sign pole, and the bicycle in the right part of the image just above. The resulting pieces are not merged into one component. This is a drawback of most bottom-up grouping approaches. The second issue are cases where two objects sharing an occlusion boundary are mostly but not fully separated by a line of low energy. This is seen in the rightmost vehicle in the 11th image. We anticipate that a combination of our method with top-down reasoning approaches will greatly alleviate these two issues. Because we rely upon correct semantic segmentation, errors from this (such as identifying a train as a bus) cannot be fixed by our method. This is clearly shown by the truck in the last example. A possible solution could be to use semantic segmentation as soft gating, or to reason about semantic and instance segmentation jointly. Finally, some very complex scenes such as some subgroups of people on the left in the second to fourth example are incorrectly separated by our model, and are fused together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have proposed a simple instance segmentation technique inspired by the intuitive and classical watershed transform. Using a novel deep convolutional neural network and innovative loss functions for pretraining and fine-tuning, we proposed a model that generates a modified watershed energy landscape. From this energy landscape, we directly extract high quality object instances. Our experiments show that we can more than double the performance of the state-of-the-art in the challenging Cityscapes Instance Segmentation task. We will release the network weights and code for training and testing our model. In the future, we plan to augment the method to handle object instances bisected by occlusions. Additionally, we wish to explore the possibility of extending our approach to perform joint semantic and instance level segmentation, in hopes of further refining both outputs simultaneously. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample prediction: the input image is gated by sem. segmentation from [34] and passed through our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Conceptual 1-D comparison between the traditional watershed transform and our deep watershed transform. The resulting instances are represented as colors across the top.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Complete network architecture. The network takes the original RGB image gated by semantic segmentation and concatenated with the semantic segmentation as input, and produces the deep watershed transform energy map as the output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Our network takes the RGB image (a) and the semantic segmentation (e) as input, and predicts a unit vector at each foreground pixel pointing directly away from the nearest boundary (f). Based on this, we then predict a modified watershed transform energy (g), upon which we perform cut at a fixed threshold to yield the final predictions (h).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Sample output of our model on the validation set. Note that predicted object instances and ground truth object instances are only given the same color if they have over 50% IoU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Statistics of the training set of the cityscapes in- stance level segmentation. Average size is in pixels. Sem. Seg. Src. Sem. Seg. IoU Inst. Seg. AP LRR</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Comparison of AP scores with various instance ordering techniques using the validation set.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This work was partially supported by ONR-N00014-14-1-0232, Samsung, NVIDIA, Google and NSERC.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Multiscale Combinatorial Grouping</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up instance segmentation using deep higher-order crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The watershed transformation applied to image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Beucher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scanning Microscopy International</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="299" to="314" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
	<note>Suppl:6(1</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Use of watersheds in contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Beucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lantuejoul</surname></persName>
		</author>
		<idno>1976. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Workshop Image Processing</title>
		<meeting>Int. Workshop Image essing</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bottom-up instance segmentation using deep higher-order crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.6" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Laplacian Pyramid Reconstruction and Refinement for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F G</forename><surname>Ghiasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved watershed transform for medical image segmentation using prior information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mewes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alcaiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Warfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="447" to="458" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Iterative instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reversible recursive instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Endto-end instance segmentation and counting with recurrent attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02636</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1202.0216</idno>
		<title level="m">The watershed concept and its use in segmentation: a brief history</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Finely-grained annotated datasets for image-based plant phenotyping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scharr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Segmentation of 3d head mr images using morphological reconstruction under constraints and automatic selection of markers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1075" to="1078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">End-to-end instance segmentation and counting with recurrent attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09410</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Instance segmentation of indoor scenes using a coverage loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pixel-level Encoding and Depth Layering for Instance-level Semantic Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Instance-level Segmentation of Vehicles by Deep Contours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Den Brand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-Scale Context Aggregation by Dilated Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A multipath network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Instance-Level Segmentation with Deep Densely Connected MRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Monocular object instance segmentation and depth ordering with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Pyramid Scene Parsing Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01105</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
