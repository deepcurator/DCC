<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On human motion prediction using recurrent neural networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
							<email>black@tuebingen.mpg.de</email>
							<affiliation key="aff1">
								<orgName type="institution">MPI for Intelligent Systems</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
							<email>javier.romero@bodylabs.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Body Labs Inc</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On human motion prediction using recurrent neural networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>An important component of our capacity to interact with the world resides in the ability to predict its evolution over time. Handing an object to another person, playing sports, or simply walking in a crowded street would be extremely challenging without our understanding of how people move, and our ability to predict what they are likely to do in the following instants. Similarly, machines that are able to perceive and interact with moving people, either in physical or virtual environments, must have a notion of how people move. Since human motion is the result of both physical limitations (e.g. torque exerted by muscles, gravity, moment preservation) and the intentions of subjects (how to perform * Research carried out while Julieta was an intern at MPI. Error <ref type="figure">Figure 1</ref>. Top: Mean average prediction error for different motion prediction methods. Bottom: Ground truth passed to the network is shown in grey, and short-term motion predictions are shown in colour. Previous work, based on deep RNNs, produces strong discontinuities at the start of the prediction (middle column). Our method produce smooth, low-error predictions.</p><p>an intentional motion), motion modeling is a complex task that should be ideally learned from observations. Our focus in this paper is to learn models of human motion from motion capture (mocap) data. More specifically, we are interested in human motion prediction, where we forecast the most likely future 3D poses of a person given their past motion. This problem has received interest in a wide variety of fields, such as action prediction for sociallyaware robotics <ref type="bibr" target="#b20">[21]</ref>, 3D people tracking within computer vision <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43]</ref>, motion generation for computer graphics <ref type="bibr" target="#b21">[22]</ref> or modeling biological motion in psychology <ref type="bibr" target="#b41">[42]</ref>.</p><p>Traditional approaches have typically imposed expert knowledge about motion in their systems in the form of Markovian assumptions <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32]</ref>, smoothness, or low dimensional embeddings <ref type="bibr" target="#b47">[48]</ref>. Recently, a family of methods based on deep recurrent neural networks (RNNs) have shown good performance on this task while trying to be more agnostic in their assumptions. For example, <ref type="bibr" target="#b9">[10]</ref> uses curriculum learning and incorporates representation learning in the architecture, and <ref type="bibr" target="#b17">[18]</ref> manually encodes the semantic similarity between different body parts. These approaches benefit from large, publicly available collections of motion capture data <ref type="bibr" target="#b15">[16]</ref>, as well as recent advances in the optimization of time-series modelling <ref type="bibr" target="#b8">[9]</ref>.</p><p>Recent work has validated its performance via two complementary methods: (1) quantitative prediction error in the short-term, typically measured as a mean-squared loss in angle-space, and (2) qualitative motion synthesis for longer time horizons, where the goal is to generate feasible motion. The first evaluation metric is particularly interesting for computer vision applications such as people tracking, where predictions are continually matched and corrected with new visual evidence. The second criterion, most relevant for open-loop motion generation in graphics, is hard to evaluate quantitatively, because human motion is a highly non-deterministic process over long time horizons. This problem is similar to the one found in recent research on deep generative networks <ref type="bibr" target="#b40">[41]</ref>, where the numerical evaluation based on the negative log-likelihood and Parzen window estimates are known to be far from perfect.</p><p>We have empirically observed that current deep RNNbased methods have difficulty obtaining good performance on both tasks. Current algorithms are often trained to minimize a quantitative loss for short-term prediction, while striving to achieve long-term plausible motion by tweaking the architectures or learning procedures. As a result, their long-term results suffer from occasional unrealistic artifacts such as foot sliding, while their short-term results are not practical for tracking due to clear discontinuities in the first prediction. In fact, the discontinuity problem is so severe, that we have found that state-of-the-art methods are quantitatively outperformed by a range of simple baselines, including a constant pose predictor. While this baseline does not produce interesting motion in the long-run, it highlights both a poor short-term performance, as well as a severe discontinuity problem in current deep RNN approaches. In this work, we argue that (a) the results achieved by recent work are not fully satisfactory for either of these problems, and (b) trying to address both problems at once is very challenging, especially in the absence of a proper quantitative evaluation for long-term plausibility.</p><p>We focus on short-term prediction, which is the most relevant task for a visual tracking scenario. We investigate the reasons for the poor performance of recent methods on this task by analyzing several factors such as the network architectures and the training procedures used in state-of-theart RNN methods. First, we consider the training schedule used in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>. It is a known problem in RNNs <ref type="bibr" target="#b4">[5]</ref> and reinforcement learning <ref type="bibr" target="#b35">[36]</ref> that networks cannot learn to recover from their own mistakes if they are fed only groundtruth during training. The authors of <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref> introduced increasing amounts of random noise during training to compensate for this effect. However, this noise is difficult to tune, makes it harder to choose the best model based on validation error, and has the effect of degrading the quality of the prediction in the first frame. Instead, we propose a simple approach that introduces realistic error in training time without any scheduling; we simply feed the predictions of the net, as it is done in test time. This increases the robustness of the predictions compared to a network trained only on ground truth, while avoiding the need of a difficult-totune schedule.</p><p>Unfortunately, this new architecture is still unable to accurately represent the conditioning poses in its hidden representation, which still results in a discontinuity in the first frame of the prediction. We borrow ideas from research on the statistics of hand motion <ref type="bibr" target="#b14">[15]</ref>, and model velocities instead of absolute joint angles, while keeping the loss in the original angle representation to avoid drift. Therefore, we propose a residual architecture that models first-order motion derivatives, which results in smooth and much more accurate short-term predictions.</p><p>Both of our contributions can be implemented using an architecture that is significantly simpler than those in previous work. In particular, we move from the usual multi-layer LSTM architectures (long short-term memory) to a single GRU (Gated Recurrent Unit), and do not require a spatial encoding layer. This allows us to train a single model on the entire Human 3.6M dataset <ref type="bibr" target="#b15">[16]</ref> in a few hours. This differs from previous approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>, which trained only action-specific models from that dataset. Our approach sets the new state of the art on short-term motion prediction, and overall gives insights into the challenges of motion modelling using RNNs. Our code is publicly available at https://github.com/una-dinosauria/ human-motion-prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Our main task of interest is human motion prediction, with a focus on recent deep RNN architectures <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>. One of our findings is that, similar to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref>, a family of simple baselines outperform recent deep learning approaches. We briefly review the literature on these topics below.</p><p>Modelling of human motion. Learning statistical models of human motion is a difficult task due to the highdimensionality, non-linear dynamics and stochastic nature of human movement. Over the last decade, and exploiting the latent low-dimensionality of action-specific human motion, most work has focused on extensions to latentvariable models that follow state-space equations such as hidden Markov models (HMMs) <ref type="bibr" target="#b25">[26]</ref>, exploring the tradeoffs between model capacity and inference complexity. For example, Wang et al. <ref type="bibr" target="#b47">[48]</ref> use Gaussian-Processes to perform non-linear motion prediction, and learn temporal dynamics using expectation maximization and Markov-chain Monte Carlo. Taylor et al. <ref type="bibr" target="#b39">[40]</ref> assume a binary latent space and model motion using a conditional restricted Boltzman machine (CRBM), which requires sampling for inference. Finally, Lehrmann et al. <ref type="bibr" target="#b25">[26]</ref> use a random forest to nonlinearly choose a linear system that predicts the next frame based on the last few observations.</p><p>Applications of human motion models. Motion is a key part of actions; therefore, the field of action recognition has paid special attention to models and representations of human motion. In their seminal work, Yacoob and Black <ref type="bibr" target="#b50">[51]</ref> model motion with time-scale and time-shifted activity bases from a linear manifold of visual features computed with Principal Component Analysis (PCA). More complex models like mixtures of HMMs <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, latent topic models of visual words <ref type="bibr" target="#b48">[49]</ref> or LSTMs <ref type="bibr" target="#b28">[29]</ref> are used in recent methods. Although their purpose (action classification from a sequence of poses) is different from ours, this field contains interesting insights for motion prediction, such as the importance of a mathematically sound orientation representation <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref> or how learned, compact motion representations improve action recognition accuracy <ref type="bibr" target="#b29">[30]</ref>.</p><p>Another popular use of motion models, specially shortterm ones, is pose tracking. The use of simple linear Markovian models <ref type="bibr" target="#b36">[37]</ref> or PCA models <ref type="bibr" target="#b43">[44]</ref> has evolved to locally linear ones like factor analizers <ref type="bibr" target="#b27">[28]</ref>, non-linear embeddings like Laplacian Eigenmaps <ref type="bibr" target="#b37">[38]</ref>, Isomap <ref type="bibr" target="#b18">[19]</ref>, dynamic variants of Gaussian Process Latent Variable Models (GPLVM) <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b51">52]</ref>, or physics-based models <ref type="bibr" target="#b7">[8]</ref>.</p><p>In animation, similar methods have been used for the generation of human pose sequences. Spaces of HMMs parameterised by style were used by Brand et al. <ref type="bibr" target="#b6">[7]</ref> to generate complex motions. Arikan and Forsyth <ref type="bibr" target="#b1">[2]</ref> collapse full sequences into nodes in a directed graph, connected with possible transitions between them, and in <ref type="bibr" target="#b23">[24]</ref> cluster trees improve the path availability. More recently, motion models based on GPLVM have been used for controlling virtual characters in a physical simulator <ref type="bibr" target="#b26">[27]</ref>. An overview of motion generation for virtual characters can be found in <ref type="bibr" target="#b44">[45]</ref>.</p><p>Deep RNNs for human motion. Our work focuses on recent approaches to motion modelling that are based on deep RNNs. Fragkiadaki et al. <ref type="bibr" target="#b9">[10]</ref> propose two architectures: LSTM-3LR (3 layers of Long Short-Term Memory cells) and ERD (Encoder-Recurrent-Decoder). Both are based on concatenated LSTM units, but the latter adds non-linear space encoders for data pre-processing. The authors also note that, during inference, the network is prone to accumulate errors, and quickly produces unrealistic human motion. Therefore, they propose to gradually add noise to the input during training (as is common in curriculum learning <ref type="bibr" target="#b5">[6]</ref>), which forces the network to be more robust to prediction errors. This noise scheduling makes the network able to generate plausible motion for longer time horizons, specially on cyclic walking sequences. However, tuning the noise schedule is hard in practice.</p><p>More recently, Jain et al. <ref type="bibr" target="#b17">[18]</ref> introduced structural RNNs (SRNNs), an approach that takes a manually designed graph that encodes semantic knowledge about the RNN as input, and creates a bi-layer architecture that assigns individual RNN units to semantically similar parts of the data. The authors also employ the noise scheduling technique introduced by Fragkiadaki et al., and demonstrate that their network outperforms previous work both quantitatively in short-term prediction, as well as qualitatively. Interestingly, SRNNs produce plausible long-term motion for more challenging, locally-periodic actions such as eating and smoking, and does not collapse to unrealistic poses in aperiodic "discussion" sequences.</p><p>Revisiting baselines amid deep learning. The rise and impressive performance of deep learning methods in classical problems such as object recognition <ref type="bibr" target="#b22">[23]</ref> has encouraged researchers to attack both new and historically challenging problems using variations of deep neural networks. For example, there is a now a large body of work on visual question answering (VQA), i.e. the task of answering natural-language questions by looking at images, based almost exclusively on end-to-end trainable systems with deep CNNs for visual processing and deep RNNs for language modelling <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b49">50]</ref>. Recently, however, Zhou et al. <ref type="bibr" target="#b52">[53]</ref> have shown that a simple baseline that concatenates features from questions' words and CNN image features performs comparably to approaches based on deep RNNs. Moreover, Jabri et al. <ref type="bibr" target="#b16">[17]</ref> have shown competitive performance on VQA with a simple baseline that does not take images into account, and state-of-the-art performance with a baseline that is trained to exploit the correlations between questions, images and answers.</p><p>Our work is somewhat similar to that of Jabri et al. <ref type="bibr" target="#b16">[17]</ref>, in that we have found a very simple baseline that outperforms sophisticated state-of-the-art methods based on deep RNNs for short-term motion prediction. In particular, our baseline outperforms the ERD and LSTM-3LR models by Fragkiadi et al. <ref type="bibr" target="#b9">[10]</ref>, as well as the structural RNN (SRNN) method of Jain et al. <ref type="bibr" target="#b17">[18]</ref>. Another example of baselines outperforming recent work in the field of pose models can be found in <ref type="bibr" target="#b24">[25]</ref>, where a Gaussian pose prior outperforms the more complicated GPLVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Recent deep learning methods for human pose prediction <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref> offer an agnostic learning framework that could potentially be integrated with video data <ref type="bibr" target="#b9">[10]</ref> or used for other forecasting applications <ref type="bibr" target="#b17">[18]</ref>. However, for the specific task of motion forecasting, we note that they have a few common pitfalls that we would like to improve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problems</head><p>First frame discontinuity. While both methods generate continuous motion, a noticeable jump between the conditioning ground truth and the first predicted frame is present in their results (see <ref type="figure">Figure 1</ref>). This jump is particularly harmful for tracking applications, where short-term predictions are continuously updated with new visual evidence.</p><p>Hyper-parameter tuning. These methods add to the typical set of network hyper-parameters an additional one, particularly hard to tune: the noise schedule.</p><p>In time series modelling, it is often necessary to model noise as part of the input, in order to improve robustness against noisy observations. For example, in Kalman filtering, a small amount of Gaussian noise is modelled explicitly as part of the standard state-space equations. In applications such as motion synthesis, exposing the method to the errors that the network will make at test time is crucial to prevent the predicted poses from leaving the manifold of plausible human motion. Algorithms like DAGGER <ref type="bibr" target="#b35">[36]</ref>, used in reinforcement learning, use queries to an "expert" during training so that the predictor learns how to correct its own errors. It is, however, not straightforward how one would use this approach for pose prediction.</p><p>The basic architectures that we use, RNNs, typically do not consider this mismatch between train and test input, which makes them prone to accumulate errors at inference time. To alleviate this problem, <ref type="bibr">Fragkiadaki et al.</ref> propose to use noise scheduling; that is, to inject noise of gradually increasing magnitude to the input in training time (see <ref type="figure" target="#fig_1">Fig. 2, left)</ref>, which corresponds to a type of curriculum learning. Jain et al. <ref type="bibr" target="#b17">[18]</ref> similarly adopt this idea, and have found that it helps stabilizing long-term motion synthesis. The downsides are, (1) that both noise distribution and magnitude scheduling are hard to tune, (2) that while this noise improves long-term predictions, it tends to hurt performance in short-term predictions, as they become discontinuous from previous observations, and (3) that the common rule for choosing the best model, based on lowest validation error, is not valid anymore, since lowest validation error typically corresponds to the validation epoch without injected noise.</p><p>Depth and complexity of the models. LSTM-3LR, ERD and SRNN use more than one RNN in their architectures, stacking two or three layers for increased model capacity. While deeper models have empirically shown the best performance on a series of tasks such as machine translation <ref type="bibr" target="#b38">[39]</ref>, deep networks are known to be hard to train when data is scarce (which is the data regime for action-specific motion models). Moreover, recent work has shown that shallow RNNs with minimal representation processing can achieve very competitive results in tasks such as the learning of sentence-level embeddings <ref type="bibr" target="#b19">[20]</ref>, as long as a large corpus of data is available. Finally, deeper models are computationally expensive, which is an important factor to consider in the context of large-scale training datasets.</p><p>Action-specific networks. Although the vision community has recently benefited from large-scale, publicly available datasets of motion capture data <ref type="bibr" target="#b15">[16]</ref>, motion modelling systems have been typically trained on small action-specific subsets. While restricting the training data to coherent subsets makes modelling easier, it is also well-known that deep networks work best when exposed to large and diverse training datasets <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref>. This should specially apply to datasets like Human3.6M, where different actions contain large portions of very similar data (e.g. sitting or walking).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Solutions</head><p>Sequence-to-sequence <ref type="bibr" target="#b38">[39]</ref> architecture. We address short-term motion prediction as the search for a function that maps an input sequence (conditioning ground truth) to an output (predicted) sequence. In this sense, the problem is analogous to machine translation, were sequence-tosequence (seq2seq) architectures are popular. In seq2seq, two networks are trained; (a) an encoder that receives the inputs and generates an internal representation, and (b), a decoder network, that takes the internal state and produces a maximum likelihood estimate for prediction. Unlike the common practice in machine translation, we enforce the encoder and the decoder to share weights, which we found to accelerate convergence. A benefit of this architecture is that the encoding-decoding procedure during training is more similar to the protocol used at test-time. Moreover, there are multiple variations of seq2seq architectures (e.g., with attention mechanisms <ref type="bibr" target="#b3">[4]</ref>, or bi-directional encoders <ref type="bibr" target="#b34">[35]</ref>), that could potentially improve motion prediction.</p><p>Sampling-based loss. While it is often common in RNNs to feed the ground truth at each training time-step to the network, this approach has the downside of the network not being able to recover from its own mistakes. Previous work has addressed this problem by scheduling the rate at which the network sees either the ground truth or its own predictions <ref type="bibr" target="#b4">[5]</ref>, or by co-training and adversarial network to force  <ref type="bibr" target="#b9">[10]</ref>. During training, ground truth is fed to the network at each time-step, and noise is added to the input. Right: Our sequence-to-sequence architecture; During training, the ground truth is fed to an encoder network, and the error is computed on a decoder network that feeds its own predictions. The decoder also has a residual connection, which effectively forces the RNN to internally model angle velocities.</p><p>the internal states of the RNN to be similar during train and test time <ref type="bibr" target="#b10">[11]</ref>. These approaches, however, rely heavily on hyper-parameter tuning, which we want to avoid. Striving for simplicity, during training we let the decoder produce a sequence by always taking as input its own samples. This approach requires absolutely no parameter tuning. Another benefit of this approach is that we can directly control the length of the sequences that we train on. As we will see, training to minimize the error on long-term motions results in networks that produce plausible motion in the long run, while training to minimize error the short-term reduces the error rate in the first few predicted frames.</p><p>Residual architecture. While using a seq2seq architecture trained with a sampling-based loss can produce plausible long-term motion, we have observed that there is still a strong discontinuity between the conditioning sequence and prediction. Our main insight is that motion continuity, a known property of human motion, is easier to express in terms of velocities than in poses. While it takes considerable modelling effort to represent all possible conditioning poses so that the first frame prediction is continuous, it only requires modeling one particular velocity (zero, or close to zero velocity) to achieve the same effect. This idea is simple to implement in current deep learning architectures since it translates into adding a residual connection between the input and the output of each RNN cell (see <ref type="figure" target="#fig_1">Fig. 2, right)</ref>. We note that, although residual connections have been shown to improve performance on very deep convolutional networks <ref type="bibr" target="#b13">[14]</ref>, in our case they help us model prior knowledge about the statistics of human motion.</p><p>Multi-action models. We also explore training a single model to predict motion for multiple actions, in contrast to previous work <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>, which has focused on building action-specific models. While modelling multiple actions is a more difficult task than modelling single-action sets, it is now a common practice to train a single, conditional model, on multiple data modalities, as this allows the network to exploit regularities in large datasets <ref type="bibr" target="#b11">[12]</ref>. Semantic knowledge about each activity can be easily incorporated using one-hot vectors; i.e., concatenating, in the input, a 15-dimensional vector that has zeros everywhere, but a value of one in the index of the indicated action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental setup</head><p>We consider three main sets of experiments to quantify the impact of our contributions:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>Seq2seq architecture and sampling-based loss. First, we train action-specific models using our proposed sequence-to-sequence architecture with samplingbased loss, and compare it to previous work, which uses noise scheduling, and to a baseline that feeds the ground truth at each time-step. The goal of these experiments is to verify that using a sampling-based loss, which does not require parameter tuning, performs on par with previous work on short-term motion prediction, while still producing plausible long-term motion. In these experiments, the network is trained to minimize the loss over 1 second of motion.</p><p>2. Residual architecture. The second set of experiments explore the effects of using a residual architecture that models first-order motion derivatives, while keeping the loss in the original angle space. Here, we are interested in learning whether a residual architecture improves short term prediction; therefore, in these experiments, the network is trained  <ref type="table">Table 1</ref>) with residual connections, sampling-based loss and trained on multiple actions, as well as a zero-velocity baseline.</p><p>to minimize the prediction error over 400 milliseconds.</p><p>3. Multi-action models. Our last round of experiments quantifies the benefits of training our architecture on the entire Human 3.6M dataset, as opposed to building actionspecific models. We consider both a supervised and an unsupervised variant. The supervised variant enhances the input to the model by concatenating one-hot vectors with the 15 action classes. In contrast, the unsupervised variant does not use one-hot input during training nor prediction. In these experiments we also train the network to minimize the prediction error over the next 400 milliseconds.</p><p>Dataset and data pre-processing. Following previous work, we use the Human 3.6M (H3.6M) dataset by Ionescu et al. <ref type="bibr" target="#b15">[16]</ref>, which is currently the largest publicly available dataset of motion capture data. H3.6M includes seven actors performing 15 varied activities such as walking, smoking, engaging in a discussion, taking pictures, and talking on the phone, each in two different trials. For a fair comparison, we adopt the pose representation and evaluation loss from <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>. Pose is represented as an exponential map representation of each joint, with a special preprocessing of global translation and rotation (see <ref type="bibr" target="#b39">[40]</ref> for more details). For evaluation, similar to <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>, we measure the Euclidean distance between our prediction and the ground truth in angle-space for increasing time horizons. We report the average error on eight randomly sampled test sequences, and use the sequences of subject five for testing, while the rest of the sequences are used for training.</p><p>A scalable seq2seq architecture. In all our experiments, we use a single gated recurrent unit <ref type="bibr" target="#b8">[9]</ref> (GRU) with 1024 units, as a computationally less-expensive alternative to LSTMs, and we do not use any time-independent layers for representation learning. Experimentally, we found that stacking recurrent layers makes the architecture harder to train, while it also makes it slower; we also found that the best performance is obtained without a spatial encoder. We do, however, use a spatial decoder to back-project the 1024-dimensional output of the GRU to 54 dimensions, the number of independent joint angles provided in H3.6M.</p><p>We use a learning rate of 0.005 in our multi-action experiments, and a rate of 0.05 in our action-specific experiments; in both cases, the batch size is 16, and we clip the gradients to a maximum L2-norm of 5. During training as well as testing, we feed 2 seconds of motion to the encoder, and predict either 1 second (for long-term experiments) or 400 milliseconds (for short-term prediction) of motion from the decoder. We implemented our architecture using TensorFlow <ref type="bibr" target="#b0">[1]</ref>, which takes 75ms for forward processing and back-propagation per iteration on an NVIDIA Titan GPU.</p><p>Baselines. We compare against two recent approaches to human motion prediction based on deep RNNs: LSTM-3LR and ERD by Fragkiadaki et al. <ref type="bibr" target="#b9">[10]</ref>, and SRNN by Jain et al. <ref type="bibr" target="#b17">[18]</ref>. To reproduce previous work, we rely on the pre-trained models and implementations of ERD, LSTM-3LR and SRNN publicly available <ref type="bibr" target="#b0">1</ref> . These implementations represent the best efforts of the SRNN authors to reproduce the results of the ERD and LSTM-3LR models reported by Fragkiadaki et al. <ref type="bibr" target="#b9">[10]</ref>, as there is no official public implementation for that work. We found that, out of the box, these baselines produce results slightly different (most often better) from those reported by Jain et al. <ref type="bibr" target="#b17">[18]</ref>.</p><p>We also consider an agnostic zero-velocity baseline which constantly predicts the last observed frame. For completeness, we also consider running averages of the last two and four observed frames. While these baselines are very simple to implement, they have not been considered in recent work that uses RNNs to model human motion. <ref type="figure" target="#fig_2">Figure 3</ref> shows a summary of the results obtained by ERD, LSTM-3LR and SRNN, as well as a zero-velocity baseline and our method, on four actions of the Human 3.6 dataset. <ref type="table">Tables 1 and 2</ref> describe these results in more detail, and include results on the rest of the actions. In the remainder of the section we analyze these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>Zero-velocity baseline. The first striking result is the comparatively good performance of the baselines, specially  <ref type="table">Table 2</ref>. Prediction results for our zero-velocity baseline and our main prediction methods on the remainder 11 actions of the H3.6m dataset.</p><p>the zero-velocity one. They clearly outperform state-of-theart results, highlighting the severity of the discontinuities between conditioning and prediction in previous work. The good performance of the baseline also means that deterministic losses are not suitable to evaluate motion forecasting with a long time horizon.</p><p>Sampling-based loss. In <ref type="table">Table 1</ref>, using our samplingbased loss consistently achieves motion prediction error competitive with or better than the state of the art. Moreover, since we have trained our model to minimize the error over a 1-second time horizon, the network retains the ability to generate plausible motion in the long run. <ref type="figure" target="#fig_3">Figure 4</ref> shows a few qualitative examples of long-term motion using this approach. Given that our proposed sampling-based loss does not require any hyper-parameter tuning, we would argue that this is a fast-to-train, interesting alternative to previous work for long-term motion generation using RNNs.</p><p>Residual architecture and multi-action models. Finally, we report the performance obtained by our architecture with sampling-based loss, residual connections and trained on single (SA) or multiple actions (MA) in the bottom subgroup of <ref type="table">Table 1</ref>. We can see that using a residual connection greatly improves performance and pushes our method beyond the state of the art, which highlights the fact that velocity representations are easier to model by our network. Importantly, our method obtains its best performance when trained on multiple actions; this result, together with the simplicity of our approach, uncovers the The first dark blue sequence corresponds to our method, trained on specific actions, and without residual connection, but using sampling-based loss (Sampling-based loss (SA) on <ref type="table">Table 1</ref>). This model produces plausible motion in the long term, but does suffer from discontinuities in short-term predictions. The last blue sequence corresponds to our full model, including residual connections, and trained on multiple actions (Residual sup. (MA) on <ref type="table">Table 1</ref>); this model produces smooth, continuous predictions in the short term, but converges to a mean pose.</p><p>importance of large amounts of training data when learning short-term motion dynamics. We also note that highly aperiodic classes such as discussion, directions and sitting down remain very hard to model.</p><p>Moreover, we observe that adding semantic information to the network in the form of action labels helps in most cases, albeit by a small margin. Likely, this is due to the fact that, for short-term motion prediction, modelling physical constraints (e.g. momentum preservation) is more important than modelling high-level semantic intentions.</p><p>When analysing <ref type="figure" target="#fig_3">Fig. 4</ref>, it becomes obvious that the best numerical results do not correspond to the best qualitative long-term motion -a result that persists even when trained to minimize loss over long horizons (e.g. 1 second). One can hardly blame the method though, since our network is achieving the lowest loss in an independent validation set. In other words, the network is excelling in the task that has been assigned to it. In order to produce better qualitative results, we argue that a different loss that encourages other similarity measures (e.g. adversarial, entropy-based etc.) should be used instead. Our results suggest that it is inherently hard to produce both accurate short-term predictions -which are relatively deterministic and seem to be properly optimized with the current loss -and long-term forecasting using RNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and future work</head><p>We have demonstrated that previous work on human motion modelling using deep RNNs has harshly neglected the important task of short-term motion prediction, as we have shown that a zero-velocity prediction is a simple but hardto-beat baseline that largely outperforms the state of the art. Based on this observation, we have developed a sequenceto-sequence architecture with residual connections which, when trained on a sample-based loss, outperforms previous work. Our proposed architecture, being simple and scalable, can be trained on large-scale datasets of human motion, which we have found to be crucial to learn the shortterm dynamics of human motion. Finally, we have shown that providing high-level supervision to the network in the form of action labels improves performance, but an unsupervised baseline is very competitive nonetheless. We find this last result particularly encouraging, as it departs from previous work in human motion modelling which has typically worked on small, action-specific datasets. Future work may focus on exploring ways to use even larger datasets of motion capture in an unsupervised manner.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Training procedure as done in previous work, and our proposed sequence-to-sequence residual architecture. Green stick figures represent ground truth, and blue stick figures represent predictions. Left: LSTM-3LR architecture, introduced by Fragkiadaki et al. [10]. During training, ground truth is fed to the network at each time-step, and noise is added to the input. Right: Our sequence-to-sequence architecture; During training, the ground truth is fed to an encoder network, and the error is computed on a decoder network that feeds its own predictions. The decoder also has a residual connection, which effectively forces the RNN to internally model angle velocities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Error curves comparing ERD [10], LSTM-3LR [10], SRNN [18] and our method (Residual sup. (MA) in Table 1) with residual connections, sampling-based loss and trained on multiple actions, as well as a zero-velocity baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Qualitative long-term motion generation, showing two seconds of motion prediction on different activities. The gray top sequence corresponds to ground truth and the red one to SRNN. The first dark blue sequence corresponds to our method, trained on specific actions, and without residual connection, but using sampling-based loss (Sampling-based loss (SA) on Table 1). This model produces plausible motion in the long term, but does suffer from discontinuities in short-term predictions. The last blue sequence corresponds to our full model, including residual connections, and trained on multiple actions (Residual sup. (MA) on Table 1); this model produces smooth, continuous predictions in the short term, but converges to a mean pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>.62 1.10 1.07 1.14 0.68 1.04 1.43 1.65 Res. sup. (MA) 0.26 0.47 0.72 0.84 0.75 1.17 1.74 1.83 0.23 0.43 0.69 0.82 0.36 0.71 1.22 1.48 0.51 0.97 1.07 1.16 0.41 1.05 1.49 1.63Res. unsup. (MA) 0.41 0.80 1.43 1.63 0.27 0.56 0.98 1.16 0.32 0.62 1.13 1.30 0.58 0.95 1.37 1.45 0.35 0.62 0.87 0.87 0.39 0.72 1.08 1.22 Res. sup. (MA) 0.39 0.81 1.40 1.62 0.24 0.51 0.90 1.05 0.28 0.53 1.02 1.14 0.56 0.91 1.26 1.40 0.31 0.58 0.87 0.91 0.36 0.67 1.02 1.15</figDesc><table>Walking 

Eating 
Smoking 
Discussion 
milliseconds 
80 160 320 400 
80 160 320 400 
80 160 320 400 
80 160 320 400 

ERD [10] 
0.93 1.18 1.59 1.78 1.27 1.45 1.66 1.80 1.66 1.95 2.35 2.42 2.27 2.47 2.68 2.76 
LSTM-3LR [10] 
0.77 1.00 1.29 1.47 0.89 1.09 1.35 1.46 1.34 1.65 2.04 2.16 1.88 2.12 2.25 2.23 
SRNN [18] 
0.81 0.94 1.16 1.30 0.97 1.14 1.35 1.46 1.45 1.68 1.94 2.08 1.22 1.49 1.83 1.93 

Running avg. 4 
0.64 0.87 1.07 1.20 0.40 0.59 0.77 0.88 0.37 0.58 1.03 1.02 0.60 0.90 1.11 1.15 
Running avg. 2 
0.48 0.74 1.02 1.17 0.32 0.52 0.74 0.87 0.30 0.52 0.99 0.97 0.41 0.74 0.99 1.09 
Zero-velocity 
0.39 0.68 0.99 1.15 0.27 0.48 0.73 0.86 0.26 0.48 0.97 0.95 0.31 0.67 0.94 1.04 

Zero noise (SA) 
0.44 0.71 1.16 1.34 0.39 0.65 1.13 1.36 0.51 0.83 1.48 1.62 0.57 1.47 2.08 2.30 
Sampling-based loss (SA) 0.92 0.98 1.02 1.20 0.98 0.99 1.18 1.31 1.38 1.39 1.56 1.65 1.78 1.80 1.83 1.90 

Residual (SA) 
0.34 0.60 0.95 1.09 0.30 0.53 0.92 1.13 0.36 0.66 1.17 1.27 0.44 0.93 1.45 1.60 
Residual unsup. (MA) 
0.27 0.47 0.70 0.78 0.25 0.43 0.71 0.87 0.33 0.61 1.04 1.19 0.31 0.69 1.03 1.12 
Residual sup. (MA) 
0.28 0.49 0.72 0.81 0.23 0.39 0.62 0.76 0.23 0.39 0.62 0.76 0.31 0.68 1.01 1.09 

Untied (MA) 
0.33 0.54 0.78 0.91 0.28 0.45 0.65 0.83 0.35 0.62 1.03 1.14 0.35 0.71 1.01 1.09 

Table 1. Detailed results for motion prediction, measured in mean angle error for walking, eating, smoking and discussion activities of the 
Human 3.6M dataset. The top section corresponds to previous work based on deep recurrent neural networks. "Zero noise" is a model 
trained by feeding ground truth at each time step. "Sampling-based loss" is trained by letting the decoder feed its own output. SA stands 
for "Single action", and MA stands for "Multi-action". Finally "Untied" is the same model as Residual sup (MA), but with untied weights 
between encoder and decoder. 

Directions 
Greeting 
Phoning 
Posing 
Purchases 
Sitting 
milliseconds 
80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400 

Zero-velocity 
0.25 0.44 0.61 0.68 0.80 1.23 1.81 1.87 0.80 1.23 1.81 1.87 0.32 0.63 1.16 1.45 0.72 1.03 1.46 1.49 0.43 1.12 1.41 1.58 

Res. (SA) 
0.44 0.95 1.27 1.55 0.87 1.40 2.19 2.26 0.31 0.57 0.88 1.04 0.50 0.96 1.64 1.96 0.74 1.60 1.57 1.72 0.44 1.05 1.51 1.69 
Res. unsup. (MA) 0.27 0.47 0.73 0.87 0.77 1.18 1.74 1.84 0.24 0.43 0.68 0.83 0.40 0.77 1.32 1.62 0Untied (MA) 
0.31 0.52 0.77 0.89 0.79 1.19 1.72 1.83 0.27 0.46 0.68 0.85 0.42 0.77 1.29 1.58 0.52 1.01 1.07 1.16 0.51 1.13 1.56 1.74 

Sitting down 
Taking photo 
Waiting 
Walking Dog 
Walking together 
Average 
milliseconds 
80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400 

Zero-velocity 
0.27 0.54 0.93 1.05 0.22 0.47 0.78 0.89 0.27 0.49 0.96 1.12 0.60 0.96 1.27 1.33 0.33 0.60 0.96 1.03 0.42 0.74 1.12 1.20 

Res. (SA) 
0.38 0.77 1.36 1.59 0.37 0.66 1.30 1.70 0.36 0.73 1.31 1.51 0.62 1.02 1.55 1.65 0.44 0.81 1.25 1.36 0.46 0.88 1.35 1.54 
Untied (MA) 
0.47 0.89 1.57 1.72 0.30 0.56 0.95 1.12 0.38 0.64 1.18 1.41 0.61 0.98 1.42 1.54 0.40 0.69 0.98 1.03 0.42 0.74 1.11 1.26 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/asheshjain399/RNNexp</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. The authors thank Laura SevillaLara for proofreading our work. We also thank Nvidia for the donation of some of the GPUs used in this research. This research was supported in part by NSERC.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<title level="m">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org. 6</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interactive motion generation from examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Arikan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="483" to="490" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A simple but tough-to-beat baseline for sentence embeddings. Under review at ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Style machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<editor>SIG-GRAPH</editor>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Physicsbased person tracking using the anthropomorphic walker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoderdecoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent network models for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Professor forcing: A new algorithm for training recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3D Pose from Motion for Cross-view Action Recognition via Non-linear Circulant Temporal Encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Woodham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The statistics of natural hand movements. Experimental Brain Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Ingram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Körding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Wolpert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">188</biblScope>
			<biblScope unit="page" from="223" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human 3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Revisiting visual question answering baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structuralrnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A spatio-temporal extension to isomap nonlinear dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">C</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Matarić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal structure from rgb-d videos for human activity detection and anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Motion graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kovar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pighin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Interactive control of avatars animated with human motion data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S A</forename><surname>Reitsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Pollard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="491" to="500" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A nonparametric bayesian network prior of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient nonlinear markov models for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Continuous character control with low-dimensional embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haraux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Popović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Monocular tracking of 3D human motion with a coordinated mixture of factor analyzers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Regularizing long short term memory with 3D human-skeleton sequences for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning switching linear models of human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maccormick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hankelet-based dynamical systems modeling for 3D action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Presti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Cascia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="29" to="43" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Action classification using a discriminative multilevel HDP-HMM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="149" to="161" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stochastic tracking of 3D human figures using 2D image motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sidenbladh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generative modeling for continuous non-linearly embedded visual inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jepson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Modeling human motion using binary latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A note on the evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Decomposing biological motion: A framework for analysis and synthesis of human gait patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">F</forename><surname>Troje</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">3d people tracking with gaussian process dynamical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Temporal motion models for monocular and multiview 3D human body tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Real time animation of virtual humans: A trade-off between naturalness and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Welbergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J H</forename><surname>Van Basten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Egges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ruttkay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Overmars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3D skeletons as points in a Lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rolling rotations for recognizing human actions from 3D skeletal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellapa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semi-latent dirichlet allocation: A hierarchical model for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sabzmeydani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Human Motion</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="240" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Parameterized modeling and recognition of activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yacoob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning probabilistic non-linear latent variable models for tracking complex activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02167</idno>
		<title level="m">Simple baseline for visual question answering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
