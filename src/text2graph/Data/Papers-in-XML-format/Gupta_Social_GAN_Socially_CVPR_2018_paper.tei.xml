<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cole</forename><forename type="middle">Polytechnique</forename><surname>1é</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fédérate De Lausanne</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Predicting the motion behavior of pedestrians is essential for autonomous moving platforms like self-driving cars or social robots that will share the same ecosystem as humans. Humans can effectively negotiate complex social interactions, and these machines ought to be able to do the same. One concrete and important task to this end is the following: given observed motion trajectories of pedestrians (coordinates for the past e.g. 3.2 seconds), predict all possible future trajectories ( <ref type="figure">Figure 1</ref>).</p><p>Forecasting the behavior of humans is challenging due to the inherent properties of human motion in crowded scenes:</p><p>1. Interpersonal. Each person's motion depends on the people around them. Humans have the innate ability to read the behavior of others when navigating crowds. Jointly modeling these dependencies is a challenge.</p><p>2. Socially Acceptable. Some trajectories are physically possible but socially unacceptable. Pedestrians are gov- <ref type="figure">Figure 1</ref>: Illustration of a scenario where two pedestrians want to avoid each other. There are many possible ways that they can avoid a potential collision. We present a method that given the same observed past, predicts multiple socially acceptable outputs in crowded scenes.</p><p>erned by social norms like yielding right-of-way or respecting personal space. Formalizing them is not trivial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Multimodal. Given a partial history, there is no single correct future prediction. Multiple trajectories are plausible and socially-acceptable.</p><p>Pioneering work in trajectory prediction has tackled some of the above challenges. The interpersonal aspect has been exhaustively addressed by traditional methods based on hand-crafted features <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46]</ref>. Social acceptability has been recently revisited with data-driven techniques based on Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b3">4]</ref>. Finally, the multimodal aspect of the problem has been studied in the context of route choices given a static scene (e.g., which streets to take at an intersection <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b23">24]</ref>). Robicquet et al. <ref type="bibr" target="#b37">[38]</ref> have shown that pedestrians have multiple navigation styles in crowded scenes given a mild or aggressive style of navigation. Therefore, the forecasting task entails outputting different possible outcomes. While existing methods have made great progress in addressing specific challenges, they suffer from two limitations. First, they model a local neighborhood around each person when making the prediction. Hence, they do not have the capacity to model interactions between all people in a scene in a computationally efficient fashion. Second, they tend to learn the "average behavior" because of the commonly used loss function that minimizes the euclidean distance between the ground truth and forecasted outputs. In contrast, we aim in learning multiple "good behaviors", i.e., multiple socially acceptable trajectories.</p><p>To address the limitations of previous works, we propose to leverage the recent progress in generative models. Generative Adversarial Networks (GANs) have been recently developed to overcome the difficulties in approximating intractable probabilistic computation and behavioral inference <ref type="bibr" target="#b13">[14]</ref>. While they have been used to produce photorealistic signals such as images <ref type="bibr" target="#b33">[34]</ref>, we propose to use them to generate multiple socially-acceptable trajectories given an observed past. One network (the generator) generates candidates and the other (the discriminator) evaluates them. The adversarial loss enables our forecasting model to go beyond the limitation of L2 loss and potentially learn the distribution of "good behaviors" that can fool the discriminator. In our work, these behaviors are referred to as socially-accepted motion trajectories in crowded scenes.</p><p>Our proposed GAN is a RNN Encoder-Decoder generator and a RNN based encoder discriminator with the following two novelties: (i) we introduce a variety loss which encourages the generative network of our GAN to spread its distribution and cover the space of possible paths while being consistent with the observed inputs. (ii) We propose a new pooling mechanism that learns a "global" pooling vector which encodes the subtle cues for all people involved in a scene. We refer to our model as "Social GAN". Through experiments on several publicly available real-world crowd datasets, we show state-of-the-art accuracy, speed and demonstrate that our model has the capacity to generate a variety of socially-acceptable trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Research in forecasting human behavior can be grouped as learning to predict human-space interactions or humanhuman interactions. The former learns scene-specific motion patterns <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b48">49]</ref>. The latter models the dynamic content of the scenes, i.e. how pedestrians interact with each other. The focus of our work is the latter: learning to predict human-human interactions. We discuss existing work on this topic as well as relevant work in RNN for sequence prediction and Generative models.</p><p>Human-Human Interaction. Human behavior has been studied from a crowd perspective in macroscopic models or from a individual perspective in microscopic models (the focus of our work). One example of microscopic model is the Social Forces by Helbing and Molnar <ref type="bibr" target="#b16">[17]</ref> which models pedestrian behavior with attractive forces guiding them towards their goal and repulsive forces encouraging collision avoidance. Over the past decades, this method has been often revisited <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b45">46]</ref>. Tools popular in economics have also been used such as the Discrete Choice framework by Antonini et. al. <ref type="bibr" target="#b1">[2]</ref>. Treuille et. al. <ref type="bibr" target="#b41">[42]</ref> use continuum dynamics, and Wang et. al. <ref type="bibr" target="#b43">[44]</ref>, Tay et. al. <ref type="bibr" target="#b40">[41]</ref> use Gaussian processes. Such functions have also been used to study stationary groups <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b46">47]</ref>. However, all these methods use hand crafted energy potentials based on relative distances and specific rules. In contrast, over the past two years, data-driven methods based on RNNs have been used to outperform the above traditional ones.</p><p>RNNs for Sequence Prediction. Recurrent Neural Networks are a rich class of dynamic models which extend feedforward networks for sequence generation in diverse domains like speech recognition <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15]</ref>, machine translation <ref type="bibr" target="#b7">[8]</ref> and image captioning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b38">39]</ref>. However, they lack high-level and spatio-temporal structure <ref type="bibr" target="#b28">[29]</ref>. Several attempts have been made to use multiple networks to capture complex interactions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">40]</ref>. Alahi et al. <ref type="bibr" target="#b0">[1]</ref> use a social pooling layer that models nearby pedestrians. In the rest of this paper, we show that using a Multi-Layer Perceptron (MLP) followed by max pooling is computationally more efficient and works as well or better than the social pooling method from <ref type="bibr" target="#b0">[1]</ref>. Lee et al. <ref type="bibr" target="#b27">[28]</ref> introduce a RNN Encoder-Decoder framework which uses variational autoencoder (VAE) for trajectory prediction. However, they did not model human-human interactions in crowded scenes.</p><p>Generative Modeling. Generative models like variational autoencoders <ref type="bibr" target="#b22">[23]</ref> are trained by maximizing the lower bound of training data likelihood. Goodfellow et al. <ref type="bibr" target="#b13">[14]</ref> propose an alternative approach, Generative Adversarial Networks (GANs), where the training procedure is a minimax game between a generative model and a discriminative model; this overcomes the difficulty of approximating intractable probabilistic computations. Generative models have shown promising results in tasks like superresolution <ref type="bibr" target="#b26">[27]</ref>, image to image translation <ref type="bibr" target="#b18">[19]</ref>, and image synthesis <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48]</ref> which have multiple possible outputs for a given input. However, their application in sequence generation problems like natural language processing has lagged since sampling from these generated outputs to feed to the discriminator is a non-differentiable operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Humans possess an intuitive ability to navigate crowds taking into account the people around them. We plan our paths keeping in mind our goal and also simultaneously taking into account the motion of surrounding people like their direction of motion, velocity, etc. However, often in such situations multiple possible options exist. We need models which not only can understand these complex human interactions but can also capture the variety of options. Current approaches have focused on predicting the average future trajectory which minimizes the L2 distance from the ground truth future trajectory whereas we want to predict multiple "good" trajectories. In this section, we first present our and outputs a pooled vector P i for each person. The decoder generates the future trajectory conditioned on H t obs i and P i . D takes as input T real or T f ake and classifies them as socially acceptable or not (see <ref type="figure" target="#fig_1">Figure 3</ref> for PM).</p><p>GAN based encoder-decoder architecture to address this issue, we then describe our novel pooling layer which models human-human interactions and finally we introduce our variety loss which encourages the network to produce multiple diverse future trajectories for the same observed sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition</head><p>Our goal is to jointly reason and predict the future trajectories of all the agents involved in a scene. We assume that we receive as input all the trajectories for people in a scene as X = X 1 , X 2 , ..., X n and predict the future trajectorieŝ Y =Ŷ 1 ,Ŷ 2 , ...,Ŷ n of all the people simultaneously. The input trajectory of a person i is defined as X i = (x t i , y t i ) from time steps t = 1, ..., t obs and the future trajectory (ground truth) can be defined similarly as Y i = (x t i , y t i ) from time steps t = t obs + 1, ..., t pred . We denote predictions asŶ i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Generative Adversarial Networks</head><p>A Generative Adversarial Network (GAN) consists of two neural networks trained in opposition to each other <ref type="bibr" target="#b13">[14]</ref>. The two adversarially trained models are: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The generator G takes a latent variable z as input, and outputs sample G(z). The discriminator D takes a sample x as input and outputs D(x) which represents the probability that it is real. The training procedure is similar to a two-player min-max game with the following objective function:</p><formula xml:id="formula_0">min G max D V (G, D) = E x∼p data (x) [log D(x)] + E z∼p (z) [log(1 − D(G(z)))].</formula><p>(1) GANs can used for conditional models by providing both the generator and discriminator with additional input c, yielding G(z, c) and D(x, c) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Socially-Aware GAN</head><p>As discussed in Section 1 trajectory prediction is a multimodal problem. Generative models can be used with timeseries data to simulate possible futures. We leverage this insight in designing SGAN which addresses the multimodality of the problem using GANs (see <ref type="figure" target="#fig_0">Figure 2)</ref>. Our model consists of three key components: Generator (G), Pooling Module (PM) and Discriminator (D). G is based on encoder-decoder framework where we link the hidden states of encoder and decoder via PM. G takes as input X i and outputs predicted trajectoryŶ i . D inputs the entire sequence comprising both input trajectory X i and future predictionŶ i (or Y i ) and classifies them as "real/fake".</p><p>Generator. We first embed the location of each person using a single layer MLP to get a fixed length vector e t i . These embeddings are used as input to the LSTM cell of the encoder at time t introducing the following recurrence:</p><formula xml:id="formula_1">e t i = φ(x t i , y t i ; W ee ) h t ei = LST M (h t−1 ei , e t i ; W encoder ) (2)</formula><p>where φ(·) is an embedding function with ReLU nonlinearity, W ee is the embedding weight. The LSTM weights (W encoder ) are shared between all people in a scene.</p><p>Naïve use of one LSTM per person fails to capture interaction between people. Encoder learns the state of a person and stores their history of motion. However, as shown by Alahi et al. <ref type="bibr" target="#b0">[1]</ref> we need a compact representation which combines information from different encoders to effectively reason about social interactions. In our method, we model human-human interaction via a Pooling Module (PM). After t obs we pool hidden states of all the people present in the scene to get a pooled tensor P i for each person. Traditionally, GANs take as input noise and generate samples. Our goal is to produce future scenarios which are consistent with the past. To achieve this we condition the generation of output trajectories by initializing the hidden state of the decoder as:</p><formula xml:id="formula_2">c t i = γ(P i , h t ei ; W c ) h t di = [c t i , z] (3)</formula><p>where γ(·) is a multi-layer perceptron (MLP) with ReLU non-linearity and W c is the embedding weight. We deviate from prior work in two important ways regarding trajectory prediction:</p><p>• Prior work <ref type="bibr" target="#b0">[1]</ref> uses the hidden state to predict parameters of a bivariate Gaussian distribution. However, this introduces difficulty in the training process as backpropagation through sampling process in nondifferentiable. We avoid this by directly predicting the coordinates (x t i ,ŷ t i ).</p><p>• "Social" context is generally provided as input to the LSTM cell <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref> . Instead we provide the pooled context only once as input to the decoder. This also provides us with the ability to choose to pool at specific time steps and results in 16x speed increase as compared to S-LSTM [1] (see <ref type="table">Table 2</ref>). After initializing the decoder states as described above we can obtain predictions as follows:</p><formula xml:id="formula_3">e t i = φ(x t−1 i , y t−1 i ; W ed ) P i = P M (h t−1 d1 , ..., h t dn ) h t di = LST M (γ(P i , h t−1 di ), e t i ; W decoder ) (x t i ,ŷ t i ) = γ(h t di ) (4)</formula><p>where φ(·) is an embedding function with ReLU nonlinearity with W ed as the embedding weights. The LSTM weights are denoted by W decoder and γ is an MLP.</p><p>Discriminator. The discriminator consists of a separate encoder. Specifically, it takes as input</p><formula xml:id="formula_4">T real = [X i , Y i ] or T f ake = [X i ,Ŷ i ]</formula><p>and classifies them as real/fake. We apply a MLP on the encoder's last hidden state to obtain a classification score. The discriminator will ideally learn subtle social interaction rules and classify trajectories which are not socially acceptable as "fake".</p><p>Losses. In addition to adversarial loss, we also apply L2 loss on the predicted trajectory which measures how far the generated samples are from the actual ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Pooling Module</head><p>In order to jointly reason across multiple people we need a mechanism to share information across LSTMs. However, there are several challenges which a method should address:</p><p>• Variable and (potentially) large number of people in a scene. We need a compact representation which combines information from all the people.</p><p>• Scattered Human-Human Interaction. Local information is not always sufficient. Far-away pedestrians might impact each others. Hence, the network needs to model global configuration. Social Pooling <ref type="bibr" target="#b0">[1]</ref> addresses the first issue by proposing a grid based pooling scheme. However, this hand-crafted solution is slow and fails to capture global context. Qi et al. <ref type="bibr" target="#b36">[37]</ref> show that above properties can be achieved by applying a learned symmetric function on transformed elements of the input set of points. As shown in <ref type="figure" target="#fig_0">Figure 2</ref> this can be achieved by passing the input coordinates through a MLP followed by a symmetric function (we use Max-Pooling). The pooled vector P i needs to summarize all the information a person needs to make a decision. Since, we use relative coordinates for translation invariance we augment the input to the pooling module with relative position of each person with respect to person i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Encouraging Diverse Sample Generation</head><p>Trajectory prediction is challenging as given limited past history a model has to reason about multiple possible outcomes. The method described so far produces good predictions, but these predictions try to produce the "average" prediction in cases where there can be multiple outputs. Further, we found that outputs were not very sensitive to changes in noise and produced very similar predictions.</p><p>We propose a variety loss function that encourages the network to produce diverse samples. For each scene we generate k possible output predictions by randomly sampling z from N (0, 1) and choosing the "best" prediction in L2 sense as our prediction.</p><formula xml:id="formula_5">L variety = min k Y i −Ŷ (k) i 2 ,<label>(5)</label></formula><p>where k is a hyperparameter. By considering only the best trajectory, this loss encourages the network to hedge its bets and cover the space of outputs that conform to the past trajectory. The loss is structurally akin to Minimum over N (MoN) loss <ref type="bibr" target="#b10">[11]</ref> but to the  best of our knowledge this has not been used in the context of GANs to encourage diversity of generated samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Implementation Details</head><p>We use LSTM as the RNN in our model for both decoder and encoder. The dimensions of the hidden state for encoder is 16 and decoder is 32. We embed the input coordinates as 16 dimensional vectors. We iteratively train the Generator and Discriminator with a batch size of 64 for 200 epochs using Adam <ref type="bibr" target="#b21">[22]</ref> with an initial learning rate of 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate our method on two publicly available datasets: ETH <ref type="bibr" target="#b35">[36]</ref> and UCY <ref type="bibr" target="#b24">[25]</ref>. These datasets consist of real world human trajectories with rich humanhuman interaction scenarios. We convert all the data to real world coordinates and interpolate to obtain values at every 0.4 seconds. In total there are 5 sets of data (ETH -2, UCY-3) with 4 different scenes which consists of 1536 pedestrians in crowded settings with challenging scenarios like group behavior, people crossing each other, collision avoidance and groups forming and dispersing.</p><p>Evaluation Metrics. Similar to prior work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref> we use two error metrics:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Average Displacement Error (ADE):</head><p>Average L2 distance between ground truth and our prediction over all predicted time steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Final Displacement Error (FDE):</head><p>The distance between the predicted final destination and the true final destination at end of the prediction period T pred .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines:</head><p>We compare against the following baselines:</p><p>1. Linear: A linear regressor that estimates linear parameters by minimizing the least square error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">LSTM:</head><p>A simple LSTM with no pooling mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">S-LSTM:</head><p>The method proposed by Alahi et al. <ref type="bibr" target="#b0">[1]</ref>. Each person is modeled via an LSTM with the hidden states being pooled at each time step using the social pooling layer.</p><p>We also do an ablation study of our model with different control settings. We refer our full method in the section as SGAN-kVP-N where kV signifies if the model was trained using variety loss (k = 1 essentially means no variety loss) and P signifies usage of our proposed pooling module. At test time we sample multiple times from the model and chose the best prediction in L2 sense for quantitative evaluation. N refers to the number of time we sample from our model during test time. Evaluation Methodology. We follow similar evaluation methodology as <ref type="bibr" target="#b0">[1]</ref>. We use leave-one-out approach, train on 4 sets and test on the remaining set. We observe the trajectory for 8 times steps (3.2 seconds) and show prediction results for 8 (3.2 seconds) and 12 (4.8 seconds) time steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative Evaluation</head><p>We compare our method on two metrics ADE and FDE against different baselines in <ref type="table">Table 1</ref>. As expected Linear model is only capable of modeling straight paths and does especially bad in case of longer predictions (t pred = 12). Both LSTM and S-LSTM perform much better than the linear baseline as they can model more complex trajectories. However, in our experiments S-LSTM does not outperform LSTM. We tried our best to reproduce the results of the pa- per. <ref type="bibr" target="#b0">[1]</ref> trained the model on synthetic dataset and then finetuned on real datasets. We don't use synthetic data to train any of our models which could potentially lead to worse performance.</p><p>SGAN-1V-1 performs worse than LSTM as each predicted sample can be any of the multiple possible future trajectories. The conditional output generated by the model represents one of many plausible future predictions which might be different from ground truth prediction. When we consider multiple samples our model outperforms the baseline methods confirming the multi-modal nature of the problem. GANs face mode collapse problem, where the generator resorts to generating a handful of samples which are assigned high probability by the discriminator. We found that samples generated by SGAN-1V-1 didn't capture all possible scenarios. However, SGAN-20V-20 significantly outperforms all other models as the variety loss encourages the network to produce diverse samples. Although our full model with proposed pooling layer performs slightly worse we show in the next section that pooling layer helps the model predict more "socially" plausible paths.</p><p>Speed. Speed is crucial for a method to be used in a realworld setting like autonomous vehicles where you need accurate predictions about pedestrian behavior. We compare our method with two baselines LSTM and S-LSTM. A simple LSTM performs the fastest but can't avoid collisions or make accurate multi-modal predictions. Our method is 16x faster than S-LSTM (see <ref type="table">Table 2</ref>). Speed improvement is because we don't do pooling at each time step. Also, unlike S-LSTM which requires computing a occupancy grid for each pedestrian our pooling mechanism is a simple MLP followed by max pooling. In real-world applications our model can quickly generate 20 samples in the same time it takes S-LSTM to make 1 prediction.</p><p>Evaluating Effect of Diversity. One might wonder what will happen if we simply draw more samples from our model without the variety loss? We compare the performance of SGAN-1V-N with SGAN-NV-N. As a reminder LSTM S-LSTM SGAN SGAN-P  <ref type="table">Table 2</ref>: Speed (in seconds) comparison with S-LSTM. We get 16x speedup as compared to S-LSTM allowing us to draw 16 samples in the same time S-LSTM makes a single prediction. Unlike S-LSTM we don't perform pooling at each time step resulting in significant speed bump without suffering on accuracy. All methods are benchmarked on Tesla P100 GPU SGAN-NV-N refers to a model trained with variety loss with k = N and drawing N samples during testing. As shown in <ref type="figure" target="#fig_3">Figure 4</ref> across all datasets simply drawing more samples from the model trained without variety loss does not lead to better accuracy. Instead, we see a significant performance increase as we increase k with models on average performing 33% better with k = 100 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative Evaluation</head><p>In multi-agent (people) scenarios, it is imperative to model how actions of one person can influence the actions of other people. Traditional approaches for activity forecasting and human trajectory prediction have focused on hand crafted energy potentials modeling attractive and repulsive forces to model these complex interactions. We use a purely data driven approach which models human-human interaction via a novel pooling mechanism. Humans walking in the presence of other people plan their path taking into account their personal space, perceived potential for collision, final destination and their own past motion. In this section, we first evaluate the effect of the pooling layer and then analyze the predictions made by our network in three common social interaction scenarios. Even though our model makes joint predictions for all people in a scene we show predictions for a subset for simplicity. We refer to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Pooling Vs No-Pooling</head><p>On quantitative metrics both methods perform similarly with SGAN slightly outperforming SGAN-P (see <ref type="table">Table  1</ref>. However, qualitatively we find that pooling enforces a global coherency and conformity to social norms. We compare how SGAN and SGAN-P perform in four common social interaction scenarios (see <ref type="figure" target="#fig_4">Figure 5</ref>). We would like to highlight that even though these scenarios were created synthetically, we used models trained on real world data. Moreover, these scenarios were created to evaluate the models and nothing in our design makes these scenarios particularly easy or hard. For each setup we draw 300 samples and plot an approximate distribution of trajectories along with average trajectory prediction. Scenario 1 and 2 depict the collision avoidance capacity of our model by changing direction. In the case of two people heading in the same direction pooling enables the model to predict a socially accepted way of yielding the right of way towards the right. However, SGAN prediction leads to a collision. Similarly, unlike SGAN, SGAN-P is able to model group behavior and predict avoidance while preserving the notion of couple walking together (Scenario 2).</p><p>Humans also tend to vary pace to avoid collisions. Scenario 3 is depicts a person G walking behind person B albeit faster. If they both continue to maintain their pace and direction they would collide. Our model predicts person G overtaking from the right. SGAN fails to predict a socially acceptable path. In Scenario 4, we notice that the model predicts person B slowing down and yielding for person G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Pooling in Action</head><p>We consider three real-scenarios where people have to alter their course to avoid collision (see <ref type="figure">Figure 6</ref>).</p><p>People Merging. (Row 1) In hallways or in roads it is common for people coming from different directions to merge and walk towards a common destination. People use various ways to avoid colliding while continuing towards their destination. For instance a person might slow down, alter their course slightly or use a combination of both depending on the context and behavior of other surrounding people. Our model is able predict variation in both speed and direction of a person to effectively navigate a situation. For instance model predicts that either person B slows down (col 2) or both person B and R change direction to avoid collision. The last prediction (col 4) is particularly interesting as the model predicts a sudden turn for person R but also predicts that person B significantly slows down in response; thus making a globally consistent prediction.</p><p>Group Avoiding. (Row 2) People avoiding each other when moving in opposite direction is another common scenario. This can manifest in various forms like a person avoiding a couple, a couple avoiding a couple etc. To make correct predictions in such cases a person needs to plan ahead and look beyond it's immediate neighborhood. Our model is able to recognize that the people are moving in groups and model group behavior. The model predicts change of direction for either groups as a way of avoiding collision (col 3, 4). In contrast to <ref type="figure" target="#fig_4">Figure 5</ref> even though the convention might be to give way to the right in this particular situation that would lead to a collision. Hence, our models makes prediction where couples give way towards the left.</p><p>Person Following. BEST is the sample closest to the ground-truth; in SLOW and FAST samples, people change speed to avoid collision; in DIR samples people change direction to avoid each other. Our model learns these different avoidance strategies in a data-driven manner, and jointly predicts globally consistent and socially acceptable trajectories for all people in the scene. We also show some failure cases in supplementary material.</p><p>in front. We would like to draw attention to a subtle difference between this situation and its real-life counterpart.</p><p>In reality a person's decision making ability is restricted by their field of view. In contrast, our model has access to ground truth positions of all the people involved in the scene at the time of pooling. This manifests in some interesting cases (see col 3). The model understands that person R is behind person B and is moving faster. Consequently, it predicts that person B gives way by changing their direction and person R maintains their direction and speed. The model is also able to predict overtaking (ground truth).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Structure in Latent Space</head><p>In this experiment we attempt to understand the landscape of the latent space z. Walking on the manifold that is learnt can give us insights about how the model is able to generate diverse samples. Ideally, one can expect that the network imposes some structure in the latent space. We found that certain directions in the latent space were associated with direction and speed <ref type="figure" target="#fig_6">(Figure 7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we tackle the problem of modeling humanhuman interaction and jointly predicting trajectories for all people in a scene. We propose a novel GAN based encoderdecoder framework for trajectory prediction capturing the multi-modality of the future prediction problem. We also propose a novel pooling mechanism enabling the network to learn social norms in a purely data-driven approach. To encourage diversity among predicted samples we propose a simple variety loss which coupled with the pooling layer encourages the network to produce globally coherent, socially compliant diverse samples. We show the efficacy of our method on several complicated real-life scenarios where social norms must be followed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: System overview. Our model consists of three key components: Generator (G), Pooling Module, and Discriminator (D). G takes as input past trajectories X i and encodes the history of the person i as H t i . The pooling module takes as input all H t obs i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison between our pooling mechanism (red dotted arrows) and Social Pooling [1] (red dashed grid) for the red person. Our method computes relative positions between the red and all other people; these positions are concatenated with each person's hidden state, processed independently by an MLP, then pooled elementwise to compute red person's pooling vector P 1 . Social pooling only considers people inside the grid, and cannot model interactions between all pairs of people.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Effect of variety loss. For SGAN-1V-N we train a single model, drawing one sample for each sequence during training and N samples during testing. For SGAN-NV-N we train several models with our variety loss, using N samples during both training and testing. Training with the variety loss significantly improves accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison between our model without pooling (SGAN, top) and with pooling (SGAN-P, bottom) in four collision avoidance scenarios: two people meeting (1), one person meeting a group (2), one person behind another (3), and two people meeting at an angle (4). For each example we draw 300 samples from the model and visualize their density and mean. Due to pooling, SGAN-P predicts socially acceptable trajectories which avoid collisions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>(Row 3 )Figure 6 :</head><label>36</label><figDesc>Figure 6: Examples of diverse predictions from our model. Each row shows a different set of observed trajectories; columns show four different samples from our model for each scenario which demonstrate different types of socially acceptable behavior. BEST is the sample closest to the ground-truth; in SLOW and FAST samples, people change speed to avoid collision; in DIR samples people change direction to avoid each other. Our model learns these different avoidance strategies in a data-driven manner, and jointly predicts globally consistent and socially acceptable trajectories for all people in the scene. We also show some failure cases in supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Latent Space Exploration. Certain directions in the latent manifold are associated with direction (left) and speed (right). Observing the same past but varying the input z along different directions causes the model to predict trajectories going either right/left or fast/slow on average.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgment</head><p>This work was supported by Toyota (1186781-31-UDARO), ONR (1165419-10-TDAUZ), Nvidia and MURI (1186514-1-TBCJE). We thank Jayanth Koushik and De-An Huang for their helpful comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="961" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Discrete choice models of pedestrian walking behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Antonini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bierlaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part B: Methodological</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="667" to="687" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledge transfer for scene-specific motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Castaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Palmieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="697" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Context-aware trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bartoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02503</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified framework for multitarget tracking and collective activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="215" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding collective activitiesof people from videos. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1242" to="1257" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Endto-end continuous speech recognition using attention-based recurrent nn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1602</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">First results. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Point-based path prediction from polar histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Coscia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Castaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Palmieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Fusion (FUSION), 2016 19th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1961" to="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Soft+ hardwired attention: An lstm framework for human trajectory prediction and abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial nets for convolutional face generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gauthier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition</title>
		<imprint>
			<publisher>Winter semester</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04623</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Social force model for pedestrian dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4282</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semanticbased surveillance video retrieval. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1168" to="1181" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Imageto-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07004</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gaussian process regression flow for analysis of motion trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1164" to="1171" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Activity forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="201" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning an image-based motion context for multiple people tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fenzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Everybody needs somebody: Modeling social and grouping behavior on a linear programming multiple people tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Desire: Distant future prediction in dynamic scenes with interacting agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04394</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">People tracking with human motion predictions from social forces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Stork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Tipaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2010 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="464" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Trajectory learning for activity understanding: Unsupervised, multilevel, and long-term adaptive approach. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2287" to="2301" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09585</idno>
		<title level="m">Conditional image synthesis with auxiliary classifier gans</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Social saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving data association by joint modeling of pedestrian trajectories and groupings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00593</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning social etiquette: Human trajectory understanding in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cern: confidenceenergy recurrent network for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modelling smooth paths using gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K C</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laugier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Field and Service Robotics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="381" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Continuum crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Treuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Popović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1160" to="1168" />
			<date type="published" when="2006" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models for human motion. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="298" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Who are you with and where are you going?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1345" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Understanding pedestrian behaviors from stationary crowd groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3488" to="3496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stackgan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03242</idno>
		<title level="m">Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Random field topic model for semantic region analysis in crowded scenes from tracklets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3441" to="3448" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
